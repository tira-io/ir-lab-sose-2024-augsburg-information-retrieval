{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q2d_few_shot_prompt(query, examples):\n",
    "    prompt = \"Write a passage that answers the given query:\\n\\n\"\n",
    "    for example in examples:\n",
    "        prompt += f\"Query: {example['query_text']}\\n\"\n",
    "        prompt += f\"Passage: {example['doc_text']}\\n\\n\"\n",
    "    prompt += f\"Query: {query}\\nPassage: \"\n",
    "    return prompt\n",
    "\n",
    "def q2e_few_shot_prompt(query, examples):\n",
    "    prompt = \"Write a list of keywords for the given query:\\n\\n\"\n",
    "    for example_query, example_keywords in examples.items():\n",
    "        prompt += f\"Query: {example_query}\\n\"\n",
    "        prompt += f\"Keywords: {example_keywords}\\n\\n\"\n",
    "    prompt += f\"Query: {query}\\nKeywords: \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a passage that answers the given query:\n",
      "\n",
      "Query: Algorithm acceleration with Nvidia CUDA\n",
      "Passage: Extraction of topic evolutions from references in scientific articles and its GPU acceleration. This paper provides a topic model for extracting topic evolutions as a corpus-wide transition matrix among latent topics. Recent trends in text mining point to a high demand for exploiting metadata. Especially, exploitation of reference relationships among documents induced by hyperlinking Web pages, citing scientific articles, tumblring blog posts, retweeting tweets, etc., is put in the foreground of the effort for an effective mining. We focus on scholarly activities and propose a topic model for obtaining a corpus-wide view on how research topics evolve along citation relationships. Our model, called TERESA, extends latent Dirichlet allocation (LDA) by introducing a corpus-wide topic transition probability matrix, which models reference relationships as transitions among topics. Our approximated variational inference updates LDA posteriors and topic transition posteriors alternately. The main issue is execution time amounting to O(MK 2 ), where K is the number of topics and M is that of links in citation network. Therefore, we accelerate the inference with Nvidia CUDA compatible GPUs. We compare the effectiveness of TERESA with that of LDA by introducing a new measure called diversity plus focusedness (D+F). We also present topic evolution examples our method gives.\n",
      "\n",
      "Query: principle of a information retrieval indexing\n",
      "Passage: The Subspace Coding Method: A New Indexing Scheme for High-Dimensional Data. This paper presents a new indexing scheme, the Subspace Coding Method (SCM), that offers high performance similarity retrieval. It is based on an analysis of the two superior access methods proposed so far: the SR-tree and the VAFile. Our experimental test by real data provides that the SR-tree offers better performance. However, as dimensionality increases, the large volume of entries in non-leaf nodes degrades the search performance. Based on the analysis, we introduce the SCM, a new indexing scheme applicable to any tree index employing MBR (Minimum Bounding Rectangle) and/or MBS (Minimum Bounding Sphere). The basic idea of the SCM is the introduction of Virtual Bounding Rectangle (VBR) and Virtual Bounding quasiSphere (VBS), which contain and approximate MBR and MBS, respectively. Unlike the approximation of absolute vector positions used in the VA-File, VBRs and VBSs are relative to the parent's VBR. The experimental results demonstrate the effectiveness of the SCM.\n",
      "\n",
      "Query: Query log analysis\n",
      "Passage: Hourly analysis of a very large topically categorized web query log. We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a generalpurpose commercial web search service. Previously, query logs have been studied from a single, cumulative view. In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms.\n",
      "\n",
      "Query: What is your name?\n",
      "Passage: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "query = \"What is your name?\"\n",
    "\n",
    "with open('query_doc_dict.json', 'r') as file:\n",
    "    query_doc_dict = json.load(file)\n",
    "# Get the list of unique query_texts from the keys of the dictionary\n",
    "unique_queries = list(query_doc_dict.keys())\n",
    "\n",
    "# Randomly select num_samples unique query_texts\n",
    "num_samples = 3\n",
    "selected_queries = random.sample(unique_queries, num_samples)\n",
    "# For each selected query_text, randomly choose one document\n",
    "selected_entries = []\n",
    "for query_text in selected_queries:\n",
    "    # Retrieve documents for the current query_text from the preprocessed dictionary\n",
    "    docs_for_query = query_doc_dict.get(query_text, [])\n",
    "    if docs_for_query:\n",
    "        # Randomly select one document from the list\n",
    "        doc_text = random.choice(docs_for_query)\n",
    "        selected_entries.append({\"query_text\": query_text, \"doc_text\": doc_text})\n",
    "    else:\n",
    "        # Raise an exception if no documents are found for a query_text; should actually not happen\n",
    "        raise ValueError(f\"No documents found for query_text: {query_text}\")\n",
    "\n",
    "# Format the prompt with the selected examples\n",
    "prompt = q2d_few_shot_prompt(query, selected_entries)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a passage that answers the given query:\n",
      "\n",
      "Query: what company is primark owned by\n",
      "Passage: High street clothes retailer Primark, which is owned by Associated British Foods, is always in and out of the news. Every year we hear stories of abuse at supplier factories, usually on the Indian sub continent. Despite selling own brand cosmetics Primark has no animal testing policy. The company says:  Primark is against animal testing. Primark and our own label manufacturers do not commission animal testing on any Primark own brand products or ingredients.\n",
      "\n",
      "Query: what age should a child start wearing deodorant\n",
      "Passage: deodorant but what deodorant to use. I have done a lot of research on when kids should start using deodorant. In my opinion if the kids are using a safe natural deodorant it should be whenever the children begin to develop odor that showering once a day will not contain. This can be 5 years old or earlier in some cases. 10 most extreme places on Earth. Children begin to start producing body odor around the same time that they begin puberty. Age of puberty onset varies, and may be from 8 to 14 in girls and about 9 to 15 in boys. These are broad averages, and body odor production does not necessarily begin at the very first signs of puberty\n",
      "\n",
      "Query: what does content mean\n",
      "Passage: 1. Usu., contents. a. something that is contained: the contents of a box. b. the subjects or topics covered in a book or document. c. the chapters or other formal divisions of a book or document. 2. something expressed through some medium, as a work of art: a poetic form adequate to the content. The contents. of something such as a box or room are the things inside it. She emptied out the contents of the bag. Be Careful! Contents is a plural noun. Don't talk about ' a content '. The contents of something such as a document or tape are the things written in it or recorded on it.\n",
      "\n",
      "Query: What is your name?\n",
      "Passage: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('ms-marco_query_doc.json', 'r') as file:\n",
    "    ms_marco_query_doc = json.load(file)\n",
    "\n",
    "# Randomly select num_samples examples for the prompt\n",
    "    selected_entries = random.sample(ms_marco_query_doc, num_samples)\n",
    "    prompt = q2d_few_shot_prompt(query, selected_entries)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a list of keywords for the given query:\n",
      "\n",
      "Query: what makes natural language processing natural\n",
      "Keywords: make, natur, languag, process, decis, support, relat, debug, view, speech\n",
      "\n",
      "Query: what is ahp\n",
      "Keywords: ahp, system, decis, analyt, hierarchi, approach, elsevi, ltd, prefer, reserv\n",
      "\n",
      "Query: audio based animal recognition\n",
      "Keywords: audio, base, anim, recognit, biodivers, identif, plant, speci, snake, lifeclef, herbarium, teaser\n",
      "\n",
      "Query: What is your name?\n",
      "Keywords: \n"
     ]
    }
   ],
   "source": [
    "with open('query_keywords_dict.json', 'r') as file:\n",
    "    query_keywords_dict = json.load(file)\n",
    "\n",
    "# Convert the dictionary to a list of key-value pairs (tuples)\n",
    "items = list(query_keywords_dict.items())\n",
    "\n",
    "sampled_pairs = dict(random.sample(items, num_samples))\n",
    "\n",
    "\n",
    "prompt = q2e_few_shot_prompt(query, sampled_pairs)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(id='ir-lab-sose-2024/ir-acl-anthology-20240504-training', provides=['docs', 'queries', 'qrels'])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from tira.third_party_integrations import persist_and_normalize_run, ir_datasets\n",
    "from tira.rest_api_client import Client\n",
    "\n",
    "input_dataset = 'ir-lab-sose-2024/ir-acl-anthology-20240504-training'\n",
    "dataset = ir_datasets.load(input_dataset)\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Towards an error-free Arabic stemming\\n\\n\\n ABSTRACTStemming is a computational process for reducing words to their roots (or stems). It can be classified as a recall-enhancing or precision-enhancing component.Existing Arabic stemmers suffer from high stemming error-rates. Arabic stemmers blindly stem all the words and perform poorly especially with compound words, nouns and foreign Arabized words.The Educated Text Stemmer (ETS) is presented in this paper. ETS is a dictionary free, simple, and highly effective Arabic stemming algorithm that can reduce stemming errors in addition to decreasing computational time and data storage.The novelty of the work arises from the use of neglected Arabic stop-words. These stop-words can be highly important and can provide a significant improvement to processing Arabic documents.The ETS stemmer is evaluated by comparison with output from human generated stemming and the stemming weight technique.', 'Impact of Stemmer on Arabic Text Retrieval\\n\\n\\n Abstract.Stemming is a process of reducing inflected words to their stem, stem or root from a generally written word form. One of the high inflected words in the languages world is Arabic Language. Stemming improve the retrieval performance by reducing words variants, and in lcrease the similarity between related words. However, an Arabic Information Retrieval (AIR) can use stemming algorithms to retrieve a greater number of documents related to the users\\' query. Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. \\'Information Science Research Institute\" (ISRI), morphological and syntax based lemmatization \"Educated Text Stemmer\" (ETS), and Light10 stemmer) on the Arabic Information Retrieval performance for Arabic language, we used the Linguistic Data Consortium (LDC) Arabic Newswire data set as benchmark dataset. The evaluation of the three different stemmers ranked the best performance was achieved by light10 stemmer in term of mean average precision.', \"Stemming the Qur{'}an\\n\\n\\n In natural language, a stem is the morphological base of a word to which affixes can be attached to form derivatives. Stemming is a process of assigning morphological variants of words to equivalence classes such that each class corresponds to a single stem. Different stemmers have been developed for a wide range of languages and for a variety of purposes. Arabic, a highly inflected language with complex orthography, requires good stemming for effective text analysis. Preliminary investigation indicates that existing approaches to Arabic stemming fail to provide effective and accurate equivalence classes when applied to a text like the Qur'an written in Classical Arabic. Therefore, I propose a new stemming approach based on a light stemming technique that uses a transliterated version of the Qur'an in western script.\"]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import re\n",
    "\n",
    "\n",
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "\n",
    "input_dataset = 'ir-lab-sose-2024/ir-acl-anthology-20240504-training'\n",
    "\n",
    "dataset = ir_datasets.load(input_dataset)\n",
    "\n",
    "docs_store = dataset.docs_store()\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "query = \"stemming arabic\"\n",
    "prf = bm25.search(query)\n",
    "\n",
    "# Extract the top num_samples docno\n",
    "top_k_docnos = prf['docno'][:num_samples].tolist()\n",
    "prf_docs = [docs_store.get(docno).text for docno in top_k_docnos]\n",
    "\n",
    "print(prf_docs)\n",
    "print(len(prf_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Towards an error-free Arabic stemming. Stemming is a computational process for reducing words to their roots (or stems). It can be classified as a recall-enhancing or precision-enhancing component.Existing Arabic stemmers suffer from high stemming error-rates. Arabic stemmers blindly stem all the words and perform poorly especially with compound words, nouns and foreign Arabized words.The Educated Text Stemmer (ETS) is presented in this paper. ETS is a dictionary free, simple, and highly effective Arabic stemming algorithm that can reduce stemming errors in addition to decreasing computational time and data storage.The novelty of the work arises from the use of neglected Arabic stop-words. These stop-words can be highly important and can provide a significant improvement to processing Arabic documents.The ETS stemmer is evaluated by comparison with output from human generated stemming and the stemming weight technique.', 'Impact of Stemmer on Arabic Text Retrieval. Stemming is a process of reducing inflected words to their stem, stem or root from a generally written word form. One of the high inflected words in the languages world is Arabic Language. Stemming improve the retrieval performance by reducing words variants, and in lcrease the similarity between related words. However, an Arabic Information Retrieval (AIR) can use stemming algorithms to retrieve a greater number of documents related to the users\\' query. Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. \\'Information Science Research Institute\" (ISRI), morphological and syntax based lemmatization \"Educated Text Stemmer\" (ETS), and Light10 stemmer) on the Arabic Information Retrieval performance for Arabic language, we used the Linguistic Data Consortium (LDC) Arabic Newswire data set as benchmark dataset. The evaluation of the three different stemmers ranked the best performance was achieved by light10 stemmer in term of mean average precision.', \"Stemming the Qur{'}an In natural language, a stem is the morphological base of a word to which affixes can be attached to form derivatives. Stemming is a process of assigning morphological variants of words to equivalence classes such that each class corresponds to a single stem. Different stemmers have been developed for a wide range of languages and for a variety of purposes. Arabic, a highly inflected language with complex orthography, requires good stemming for effective text analysis. Preliminary investigation indicates that existing approaches to Arabic stemming fail to provide effective and accurate equivalence classes when applied to a text like the Qur'an written in Classical Arabic. Therefore, I propose a new stemming approach based on a light stemming technique that uses a transliterated version of the Qur'an in western script.\"]\n",
      "Towards an\n",
      "Impact of \n",
      "Stemming t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_documents(docs):\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        # Replace multiple newline characters with a single space\n",
    "        doc = re.sub(r'\\n+', ' ', doc)\n",
    "        # Remove any periods immediately following \"ABSTRACT\" or \"Abstract\"\n",
    "        doc = re.sub(r'\\b(ABSTRACT|Abstract)\\s*\\.\\s*', r'\\1 ', doc)\n",
    "        # Insert a space after \"ABSTRACT\" or \"Abstract\" if followed by a non-space character\n",
    "        doc = re.sub(r'\\b(ABSTRACT|Abstract)(\\S)', r'\\1 \\2', doc)\n",
    "        # Replace occurrences of 'Abstract' or 'ABSTRACT' with a period between words\n",
    "        doc = re.sub(r'\\s*(Abstract|ABSTRACT)\\s*', r'. ', doc)\n",
    "        # Normalize multiple spaces to a single space\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "        # Remove \"INTRODUCTION\" or \"Introduction\" followed by a non-space character\n",
    "        doc = re.sub(r'\\b(INTRODUCTION|Introduction)(\\S)', r'\\2', doc)\n",
    "        doc = doc.strip()\n",
    "        processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "processed_prf_docs = process_documents(prf_docs)\n",
    "print(processed_prf_docs)\n",
    "len(processed_prf_docs)\n",
    "\n",
    "for doc in processed_prf_docs:\n",
    "    print(doc[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
