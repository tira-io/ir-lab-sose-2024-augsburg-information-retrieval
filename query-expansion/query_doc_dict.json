{
    "retrieval system improving effectiveness": [
        "A probabilistic model for stemmer generation. In this paper we will present a language-independent probabilistic model which can automatically generate stemmers. Stemmers can improve the retrieval effectiveness of information retrieval systems, however the designing and the implementation of stemmers requires a laborious amount of effort due to the fact that documents and queries are often written or spoken in several different languages. The probabilistic model proposed in this paper aims at the development of stemmers used for several languages. The proposed model describes the mutual reinforcement relationship between stems and derivations and then provides a probabilistic interpretation. A series of experiments shows that the stemmers generated by the probabilistic model are as effective as the ones based on linguistic knowledge.",
        "Learning to Adaptively Rank Document Retrieval System Configurations Modern Information Retrieval (IR) systems have become more and more complex, involving a large number of parameters. For example, a system may choose from a set of possible retrieval models (BM25, language model, etc.), or various query expansion parameters, whose values greatly influence the overall retrieval effectiveness. Traditionally, these parameters are set at a system level based on training queries, and the same parameters are then used for different queries. We observe that it may not be easy to set all these parameters separately, since they can be dependent. In addition, a global setting for all queries may not best fit all individual queries with different characteristics. The parameters should be set according to these characteristics. In this article, we propose a novel approach to tackle this problem by dealing with the entire system configurations (i.e., a set of parameters representing an IR system behaviour) instead of selecting a single parameter at a time. The selection of the best configuration is cast as a problem of ranking different possible configurations given a query. We apply learning-to-rank approaches for this task. We exploit both the query features and the system configuration features in the learning-to-rank method so that the selection of configuration is query dependent. The experiments we conducted on four TREC ad hoc collections show that this approach can significantly outperform the traditional method to tune system configuration globally (i.e., grid search) and leads to higher effectiveness than the top performing systems of the TREC tracks. We also perform an ablation analysis on the impact of different features on the model learning capability and show that query expansion features are among the most important for adaptive systems.CCS Concepts: \u2022 Information systems \u2192 Retrieval effectiveness; Learning to rank; Information retrieval query processing;Additional Key Words and Phrases: Information systems, information retrieval, learning to rank, retrieval system parameters, adaptive information retrieval, query features, data analytics Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ACM Modern Information Retrieval (IR) systems involve more and more complex operations, which require setting a large number of parameters. For example, at the very basic preprocessing level, we have to choose among different options of word stemming. Then a retrieval model should be chosen. This latter often involves a set of parameters as well-language models require smoothing parameters and BM25 has another set of parameters. Finally, the pseudo-relevance feedback step requires yet another set of parameters: the number of expansion terms to be added to the query, their weighting scheme, the number of feedback documents to consider, and so on . Over the years, and through evaluation forums such as TREC, 1 CLEF, 2 and NTCIR, 3 the IR community has produced an abundant field of knowledge, however scattered in the literature, on setting the appropriate values of these parameters to optimise the performance of the retrieval systems. For example, we know that the number of pseudo-relevance feedback documents used in IR experiments typically varies between 10 and 50, and the number of expansion terms is in the range of 10 to 20 . BM25 or a language model is often chosen, and they are believed to be effective on most test collections. When a specific retrieval method involves some parameters (e.g., the parameters related to query expansion), one typically tunes them on a set of training queries to maximise the global effectiveness. The typical method for parameter tuning is through grid search [73]: A set of possible values is defined for each parameter, and grid search determines the best value for each parameter to maximise the effectiveness of the retrieval system on a set of training queries.To be more robust, one also test different settings on several test collections. For example, Reference [80] analysed the influence of the smoothing function in Language Modelling (LM) on several test collections, and some specific range of the smoothing parameter is recommended. Alternatively, it is possible to optimize the studied parameter value using a collection and observe its effects on other collections , which is a form of transfer learning.These methodologies for parameter tuning assume that the same selected parameters would fit all the queries. In practice, even if the selected system configuration is the best for a set of queries, it has been often observed that it behaves differently on different queries: It may excel on some queries while failing miserably on some others . This fact indicates a critical problem in the usual way to set system parameters: It is done once and for all queries. It is desirable that we choose the appropriate parameters for each query at hand, thus avoiding the problem of the one-size-fits-all solution.There is an abundant literature on the effects of individual system parameters on retrieval results. Indeed, for any new method proposed, it is required that an analysis is made in depth to evaluate the effect of parameter setting , e.g., how the method behaves along with the changes of its inherent parameters. However, there have been few studies trying to determine the parameters automatically for a given query.There are also only a few descriptive analyses of cross parameter effects , which examine the results and the effects of various parameter settings. In Reference [23], the authors analysed the influence of indexing and retrieval parameters on retrieval effectiveness; while the authors of Reference analysed an even larger set of parameters. The authors of Reference analysed the correlation between effectiveness measures and system parameters. However, none of these studies attempted to determine automatically the best parameters at the query level for new queries.",
        "Automatic document prior feature selection for web retrieval. Document prior features, such as Pagerank and URL depth, can improve the retrieval effectiveness of Web Information Retrieval (IR) systems. However, not all queries equally benefit from the application of a document prior feature. This paper aims to investigate whether the retrieval performance can be further enhanced by selecting the best document prior feature on a per-query basis. We present a novel method for selecting the best document prior feature on a per-query basis. We evaluate our technique on the TREC .GOV Web test collection and its associated TREC 2003 Web search tasks. Our experiments demonstrate the effectiveness and robustness of our proposed selection method.",
        "Structured Index System at NTCIR1: Information Retrieval using Dependency Relationship between Words It is difficult to improve retrieval effectiveness using only keyword-based retrieval, the major method in document retrieval, due to its high dependence on statistical word distribution. We therefore propose a method to enhance retrieval effectiveness using dependency relationships between words in a sentence. In our method, we create a Structured Index, represented by a binary tree through dependency analysis and compound nouns analysis based on a word bigram. This paper describes our methodology and shows the result of the retrieval experiments on the IR test collection of NTCIR1. \u00c3 \u00c4 '(IP communication) occur is given higher score of dependency than a relevant document, in which 'TCP/IP' and 'ATM \u00c3 \u00c4",
        "Bayesian Optimization for Optimizing Retrieval Systems. The effectiveness of information retrieval systems heavily depends on a large number of hyperparameters that need to be tuned. Hyperparameters range from the choice of different system components, e.g., stopword lists, stemming methods, or retrieval models, to model parameters, such as k1 and b in BM25, or the number of query expansion terms. Grid and random search, the dominant methods to search for the optimal system configuration, lack a search strategy that can guide them in the hyperparameter space. This makes them inefficient and ineffective. In this paper, we propose to use Bayesian Optimization to jointly search and optimize over the hyperparameter space. Bayesian Optimization, a sequential decision making method, suggests the next most promising configuration to be tested on the basis of the retrieval effectiveness of configurations that have been examined so far. To demonstrate the efficiency and effectiveness of Bayesian Optimization we conduct experiments on TREC collections, and show that Bayesian Optimization outperforms manual tuning, grid search and random search, both in terms of retrieval effectiveness of the configuration found, and in terms of efficiency in finding this configuration.",
        "Evaluating and Optimizing Autonomous Text Classification Systems. Text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go. In contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other IR systems must make decisions without human input or supervision. It is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed. We show how to do this for binary text classification systems, emphasizing that different goals for the system lead to different optimal behaviors. Optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used. Ranked retrieval is the information retrieval (IR) researcher's favorite tool for dealing with information overload. Ranked retrieval systems display documents in order of probability of relevance or some similar measure. Users see the best documents first, and decide how far down the ranking to go in examining the available information.The central role played by ranking in this approach has led researchers to evaluate IR systems primarily, often exclusively, on the quality of their rankings. (See, for instance, the TREC evaluations [1].)In some IR applications, however, ranking is not enough:q A company provides an SDI (selective dissemination of information) service which filters newswire feeds. Relevant articles are faxed each morning 'to clients.Interaction between customer and system takes place infrequently.The cost of resources (tying up phone lines, fax machine paper, etc.) is a factor to consider in operating the system.. A text categorization system assigns controlled vocabulary categories to incoming documents as they are stored in a text database. Cost cutting has eliminated manual checking of catego~assignments.Permission to nmke digLM/hw d copIcs of a II or port of 11]1smalcri:d withuut fee is granted provided llmt {I]c copies arc n~)l m:lde or distributed for prolit or comnwrci:li :idv:m[agc, LIW ACM copyright/ server notice, (he title of t!le puhlic:~ti[)n :Ind its cl:ite :lppcm, and notice is. given tlmt copyright is l~y pmnission of' LIN Asso~i:lti~~n for Cotnputmg M~chinery, Inc, (A Ch4). To copy otherwise, to repul)li+, to post on servers or to reciistrihu(c to lisls, requires spccilic permission and/or fee. SIGIR'95 Sattlc WA USA [\" 1995 ACN4 0-89791 -714-6/95 /07. S3.50 o An \"agent\" program monitors low content text streams (e.g.Usenet newsgroups) and alerts a user when a relevant message appears. On most days, the user is not alerted and has no interaction with the agent.A ranked retrieval system is a tool for people who are actively pursuing information.Implicit in its design is the assumption that a user wants to examine at least some of the available information. Many users, however, are actively avoiding information.They want to spend no time with, and have no awareness of, a particular information source unless highly relevant material becomes available. The decision of which items, if any, are grounds for disturbing the user becomes critical.Increasingly, IR systems are autonomous in this fashion, making decisions without immediate human oversight or awareness. We propose the following method for constructing and operating autonomous IR systems:1. 2. 3.Define an effectiveness measure that, when applied to a set of decisions made by the system, computes a score indicating how good those decisions were.Tune the system so that the expected effectiveness of its decisions will be the best possible.Have the system estimate the effectiveness of its decisions in an ongoing fashion, and notify an appropriate party when these estimates indicate a problem with handling new data.This method would seem too obvious to present, except that few IR systems are currently constructed in this fashion.In this paper we present the details of this approach for a class of simple autonomous IR systems: systems that decide whether or not a document belongs to a single class. While simplistic, this binary classification approach covers a wide range of useful decisions: to fax an article or not, to assign a category or not, to alert a user or not, and so on.Section 2 discusses how the effectiveness of a binary classifier can be measured when the correct classification for each document is known.Section 3 then defines the expected effectiveness of a binary classifier in an operational setting, where the correct classifications are not known. Section 4 discusses rules for making the expected effectiveness of a classifier the best possible, and points out that the proper rule will vary with the particular effectiveness measure chosen. This variation in classification rules is strikingly exhibited in Sections 5 to 7, which present three different effectiveness measures and the classification rules appropriate for each. The concluding sections discuss the implications of our analysis for classifier design, and point out some directions for future research. 246",
        "Automatic ranking of information retrieval systems using data fusion. Measuring effectiveness of information retrieval (IR) systems is essential for research and development and for monitoring search quality in dynamic environments. In this study, we employ new methods for automatic ranking of retrieval systems. In these methods, we merge the retrieval results of multiple systems using various data fusion algorithms, use the top-ranked documents in the merged result as the ''(pseudo) relevant documents,'' and employ these documents to evaluate and rank the systems. Experiments using Text REtrieval Conference (TREC) data provide statistically significant strong correlations with human-based assessments of the same systems. We hypothesize that the selection of systems that would return documents different from the majority could eliminate the ordinary systems from data fusion and provide better discrimination among the documents and systems. This could improve the effectiveness of automatic ranking. Based on this intuition, we introduce a new method for the selection of systems to be used for data fusion. For this purpose, we use the bias concept that measures the deviation of a system from the norm or majority and employ the systems with higher bias in the data fusion process. This approach provides even higher correlations with the humanbased results. We demonstrate that our approach outperforms the previously proposed automatic ranking methods.",
        "Dynamic two-stage image retrieval from large multimedia databases a b s t r a c tContent-based image retrieval (CBIR) with global features is notoriously noisy, especially for image queries with low percentages of relevant images in a collection. Moreover, CBIR typically ranks the whole collection, which is inefficient for large databases. We experiment with a method for image retrieval from multimedia databases, which improves both the effectiveness and efficiency of traditional CBIR by exploring secondary media. We perform retrieval in a two-stage fashion: first rank by a secondary medium, and then perform CBIR only on the top-K items. Thus, effectiveness is improved by performing CBIR on a 'better' subset. Using a relatively 'cheap' first stage, efficiency is also improved via the fewer CBIR operations performed. Our main novelty is that K is dynamic, i.e. estimated per query to optimize a predefined effectiveness measure. We show that our dynamic two-stage method can be significantly more effective and robust than similar setups with static thresholds previously proposed. In additional experiments using local feature derivatives in the visual stage instead of global, such as the emerging visual codebook approach, we find that two-stage does not work very well. We attribute the weaker performance of the visual codebook to the enhanced visual diversity produced by the textual stage which diminishes codebook's advantage over global features. Furthermore, we compare dynamic two-stage retrieval to traditional score-based fusion of results retrieved visually and textually. We find that fusion is also significantly more effective than single-medium baselines. Although, there is no clear winner between two-stage and fusion, the methods exhibit different robustness features; nevertheless, two-stage retrieval provides efficiency benefits over fusion.",
        "Performance analysis of distributed information retrieval architectures using an improved network simulation model. The increasing number of documents that have to be indexed in different environments, particularly on the Web, and the lack of scalability of a single centralised index lead to the use of distributed information retrieval systems to effectively search for and locate the required information. In this study, we present several improvements over the two main bottlenecks in a distributed information retrieval system (the network and the brokers). We extend a simulation network model in order to represent a switched network. The new simulation model is validated by comparing the estimated response times with those obtained using a real system. We show that the use of a switched network reduces the saturation of the interconnection network, especially in a replicated system, and some improvements may be achieved using multicast messages and faster connections with the brokers. We also demonstrate that reducing the partial results sets will improve the response time of a distributed system by 53%, with a negligible probability of changing the system's precision and recall values. Finally, we present a simple hierarchical distributed broker model that will reduce the response times for a distributed system by 55%.",
        "Translation enhancement: a new relevance feedback method for cross-language information retrieval. As an effective technique for improving retrieval effectiveness, relevance feedback (RF) has been widely studied in both monolingual and cross-language information retrieval (CLIR) settings. The studies of RF in CLIR have been focused on query expansion (QE), in which queries are reformulated before and/or after they are translated. However, RF in CLIR actually not only can help select better query terms, but also can enhance query translation by adjusting translation probabilities and even resolve some out-of-vocabulary terms. In this paper, we propose a novel RF method called translation enhancement (TE), which uses the extracted translation relationships from relevant documents to revise the translation probabilities of query terms and to identify extra translation alternatives if available so that the translated queries are more tuned to the current search. We studied TE using pseudo relevance feedback (PRF) and interactive relevance feedback (IRF). Our results show that TE can significantly improve CLIR with both types of RF methods, and that the improvement is comparable to that of QE. More importantly, the effects of TE and QE are complementary. Their integration can produce further improvement, and makes CLIR more robust for a variety of queries.",
        "A multi-system analysis of document and term selection for blind feedback. Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems. Each system demonstrated improved effectiveness with the use of blind feedback, in which the results of a preliminary retrieval step were used to augment the efficacy of a secondary retrieval step. The hybrid combination of primary and secondary retrieval steps from different systems in a number of cases yielded better effectiveness than either of the constituent systems alone. This positive combining effect was observed when entire documents were passed between the two retrieval steps, but not when only the expansion terms were passed. Several combinations of primary and secondary retrieval steps were fused using the CombMNZ algorithm; all yielded significant effectiveness improvement over the individual systems, with the best yielding a an improvement of 13% (p = 10 \u22126 ) over the best individual system and an improvement of 4% (p = 10 \u22125 ) over a simple fusion of the eight systems.",
        "The Potential and Actual Effectiveness of Interactive Query Expansion In query expansion, terms from a source such as relevance feedback are added to the query. This often improves retrieval effectiveness but results are variable across queries. In interactive query expansion (IQE) the automatically-derived terms are instead offered as suggestions to the searcher, who decides which to add. There is little evidence of whether IQE is likely to be effective over multiple iterations in a large scale retrieval context, or whether inexperienced users can achieve this effectiveness in practice. These experiments address these two questions. A small but significant improvement in potential retrieval effectiveness is found. This is consistent across a range of topics. Inexperienced users' term selections consistently fail to improve on automatic query expansion, however. It is concluded that interactive query expansion has good potential, particular y for term sources that are porer than relevance feedback. But it may be difficult for searchers to realise this potential without experience or training in term selection and free-text search strategies. Background and motivationIn free-text retrieval systems, queries can often be improved by adding extra terms that appear in relevant documents but which were not included in the original query. This is called query expansion. If the terms are provided by the system, as a result of relevance feedback for example, then it is called automatic query expansion. The potential of automatic query expansion for improving retrieval effectiveness has been investigated by many researchers, trying various sources of new terms with variable results.Using relevance feedback terms has produced the greatest improvements (Harman 1992; Srdton and Buckley 1990), although on occasion it has also been shown to degrade performance . The effectiveness of relevance feedback query expansion depends on many factors, including the document ranking functions used (Smeaton and van Rijsbergen 1983), the document collection and queries (Sakon and Buckley 1990), and the number of terms that are used Harrnan 1988).Exploiting co-occurrence of terms in the document collection has generally achieved little or no effect on overall retrieval performance (Minker et al. 1972; Peat and WNett 1991;. However, as with other query expansion term sources, some irtdvidual queries may be improved by adding co-occurring terms whilst others are degraded (Harrnan 1988).Permissionto make digitalhrd copies of all or part of thismaterial for personal or classroom use is granted without f= providedthatthe copies sre not made or distributed for profit or mrnrnemial sdvantage, the copy- These studies show that automatic query expansion is capable of producing large improvements in retrieval effectiveness, particularly when relevance feedback terms are used, but that the effectiveness varies greatly, particularly across queries. Akhough nearly all of these experiments have used only a single application of query expansion improvements can continue to be made over many iterations (Harrnan 1992).Relevance feedback may produce poor query expansion terms for a number of reasons. The sample of relevant documents may be very small, terms maybe extracted from non-relevant sections of otherwise relevant documents, and some relevant terms may also attract non-relevant topics that are already over-emphasised. It seems reasonable to assume that a searcher, given a list of the query expansion terms, will be able to dktinguish the good terms from the bad terms. This is the assumption underlying the use of interactive query expansion. In interactive query expansion the potential query expansion terms are shown to the searcher as suggestions. The searcher then decides which to add and which not to add. This technique can be used with any source of 324",
        "Seeking and implementing automated assistance during the search process. Searchers seldom make use of the advanced searching features that could improve the quality of the search process because they do not know these features exist, do not understand how to use them, or do not believe they are effective or efficient. Information retrieval systems offering automated assistance could greatly improve search effectiveness by suggesting or implementing assistance automatically. A critical issue in designing such systems is determining when the system should intervene in the search process. In this paper, we report the results of an empirical study analyzing when during the search process users seek automated searching assistance from the system and when they implement the assistance. We designed a fully functional, automated assistance application and conducted a study with 30 subjects interacting with the system. The study used a 2G TREC document collection and TREC topics. Approximately 50% of the subjects sought assistance, and over 80% of those implemented that assistance. Results from the evaluation indicate that users are willing to accept automated assistance during the search process, especially after viewing results and locating relevant documents. We discuss implications for interactive information retrieval system design and directions for future research.",
        "The weighted Condorcet fusion in information retrieval a b s t r a c tThe Condorcet fusion is a distinctive fusion method and was found useful in information retrieval. Two basic requirements for the Condorcet fusion to improve retrieval effectiveness are: (1) all component systems involved should be more or less equally effective; and (2) each information retrieval system should be developed independently and thus each component result is more or less equally different from the others. These two requirements may not be satisfied in many cases, then weighted Condorcet becomes a good option. However, how to assign weights for the weighted Condorcet has not been investigated.In this paper, we present a linear discriminant analysis (LDA) based approach to training weights. Some properties of Condorcet fusion and weighted Condorcet fusion are discussed. Experiments are conducted with three groups of runs submitted to TREC to evaluate the performance of a group of data fusion methods. The empirical investigation finds that Condorcet fusion is a good ranking-based method in good conditions, while weighted Condorcet fusion can make significant improvement over Condorcet fusion when the conditions are not favourable for Condorcet fusion. The experiments also show that the proposed LDA weighting schema is effective and Condorcet fusion with LDA based weighting schema is more effective than all other data fusion methods involved.",
        "Diagnostic Evaluation of Information Retrieval Models Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.",
        "Employing web mining and data fusion to improve weak ad hoc retrieval. When a user issues a reasonable query to a retrieval system and obtains no relevant documents, he or she is bound to feel frustrated. We call these weak queries and retrievals. Improving their effectiveness is an important issue for ad hoc retrieval and would be most rewarding for these users. We explain why data fusion of sufficiently dissimilar retrieval lists can improve weak query results and confirm this with experiments using short and medium size queries. To realize sufficiently dissimilar retrieval lists, we propose composing alternate queries through web search and mining, employ them for target retrieval, and combine with the original query retrieval list. Methods of forming web probes from longer queries, including salient term selection and query text window rotation, are investigated. When compared with normal ad hoc retrieval, web assistance and data fusion can more than double the original weak query effectiveness. Other queries can also improve along with weak ones.",
        "Techniques for improving web retrieval effectiveness. This paper talks about several schemes for improving retrieval effectiveness that can be used in the named page finding tasks of web information retrieval (Overview of the TREC-2002 web track. In: Proceedings of the Eleventh Text Retrieval Conference TREC-2002, NIST Special Publication #500-251, 2003. These methods were applied on top of the basic information retrieval model as additional mechanisms to upgrade the system. Use of the title of web pages was found to be effective. It was confirmed that anchor texts of incoming links was beneficial as suggested in other works. Sentence-query similarity is a new type of information proposed by us and was identified to be the best information to take advantage of. Stratifying and re-ranking the retrieval list based on the maximum count of index terms in common between a sentence and a query resulted in significant improvement of performance. To demonstrate these facts a large-scale web information retrieval system was developed and used for experimentation.",
        "Examining and improving the effectiveness of relevance feedback for retrieval of scanned text documents. Important legacy paper documents are digitized and collected in online accessible archives. This enables the preservation, sharing, and significantly the searching of these documents. The text contents of these document images can be transcribed automatically using OCR systems and then stored in an information retrieval system. However, OCR systems make errors in character recognition which have previously been shown to impact on document retrieval behaviour. In particular relevance feedback query-expansion methods, which are often effective for improving electronic text retrieval, are observed to be less reliable for retrieval of scanned document images. Our experimental examination of the effects of character recognition errors on an ad hoc OCR retrieval task demonstrates that, while baseline information retrieval can remain relatively unaffected by transcription errors, relevance feedback via query expansion becomes highly unstable. This paper examines the reason for this behaviour, and introduces novel modifications to standard relevance feedback methods. These methods are shown experimentally to improve the effectiveness of relevance feedback for errorful OCR transcriptions. The new methods combine similar recognised character strings based on term collection frequency and a string edit-distance measure. The techniques are domain independent and make no use of external resources such as dictionaries or training data.",
        "Improving Retrieval Effectiveness by Reranking Documents Based on Controlled Vocabulary",
        "Vector-Space Ranking with Effective Early Termination. Considerable research effort has been invested in improving the effectiveness of information retrieval systems. Techniques such as relevance feedback, thesaural expansion, and pivoting all provide better quality responses to queries when tested in standard evaluation frameworks. But such enhancements can add to the cost of evaluating queries. In this paper we consider the pragmatic issue of how to improve the cost-effectiveness of searching. We describe a new inverted file structure using quantized weights that provides superior retrieval effectiveness compared to conventional inverted file structures when early termination heuristics are employed. That is, we are able to reach similar effectiveness levels with less computational cost, and so provide a better cost/performance compromise than previous inverted file organisations.",
        "Evaluating the impact of MeSH (Medical Subject Headings) terms on different types of searchers a b s t r a c tTo what extent do MeSH terms improve search effectiveness for different kinds of users? We observed four different kinds of information seekers using an experimental information retrieval system: (1) search novices; (2) domain experts; (3) search experts and (4) medical librarians. Participants searched using either a version of the system in which MeSH terms were displayed or another version in which they had to formulate their own terms. The information needs were a subset of the relatively difficult topics originally created for the Text REtrieval Conference (TREC). Effectiveness of retrieval was based on the relevance judgments provided by TREC. The results of the study provide experimental evidence of the usefulness of MeSH terms and further identify the significant relationship between the user characteristics of domain knowledge and search training and the search performance in an interactive search environment.",
        "UB at the NTCIR-12 SpokenQuery&Doc-2: Spoken Content Retrieval Using Multiple ASR Hypotheses and Syllables The University at Buffalo (UB) team participated in the SpokenQuery&Doc task at the NTCIR-12, working on the Spoken Content Retrieval (SCR) subtask. We investigated the use of multiple ASR hypotheses (words) and subword units (syllables) for improving retrieval effectiveness. We also compared the retrieval effectiveness based on texts generated by two automatic speech recognition (ASR) engines, namely Julius and KALDI. Our experiment results showed that using multiple ASR hypotheses did not improve retrieval effectiveness, while using ASR syllables alone led to lower mean average precision than using ASR words. Furthermore, ASR texts generated by the KALDI system resulted in significantly better retrieval effectiveness than those by the Julius system. Future areas of work are discussed.",
        "RELIEF: Combining Expressiveness and Rapidity into a Single System. This paper constitutes a proposal for an efficient and effective logical information retrieval system. Following a relational indexing approach, which is in our opinion a necessity to cope with the emerging applications such as those based on multimedia, we use the conceptual graphs formalism as our indexing language. This choice allows for relational indexing support and captures all the useful properties of the logical information retrieval model, in a workable system. First order logic and standard information retrieval techniques are combined together, to the same effect: obtaining an expressive system, able to accurately handle complex documents, improve retrieval effectiveness, and achieve good time performance. Experimentations on an image test collection, within a system available on the Web, provide an illustration of the role that logic may have in the future development of information retrieval systems.",
        "Effective Retrieval with Distributed Collections. This paper evaluates the retrieval effectiveness of distributed information retrieval systems in realistic environments.We find that when a large number of collections are available, the retrieval effectiveness is significantly worse than that of centralized systems, mainly because typical queries are not adequate for the purpose of choosing the right collections. We propose two techniques to address the problem. One is to use phrase information in the collection selection index and the other is query expansion. Both techniques enhance the discriminatory power of typical queries for choosing the right collections and hence significantly improve retrieval results. Query expansion, in particular, brings the effectiveness of searching a large set of distributed collections close to that of searching a centralized collection.",
        "Using information gain to improve multi-modal information retrieval systems. Nowadays, access to information requires managing multimedia databases effectively, and so, multi-modal retrieval techniques (particularly images retrieval) have become an active research direction. In the past few years, a lot of content-based image retrieval (CBIR) systems have been developed. However, despite the progress achieved in the CBIR, the retrieval accuracy of current systems is still limited and often worse than only textual information retrieval systems. In this paper, we propose to combine content-based and text-based approaches to multi-modal retrieval in order to achieve better results and overcome the lacks of these techniques when they are taken separately. For this purpose, we use a medical collection that includes both images and non-structured text. We retrieve images from a CBIR system and textual information through a traditional information retrieval system. Then, we combine the results obtained from both systems in order to improve the final performance. Furthermore, we use the information gain (IG) measure to reduce and improve the textual information included in multi-modal information retrieval systems. We have carried out several experiments that combine this reduction technique with a visual and textual information merger. The results obtained are highly promising and show the profit obtained when textual information is managed to improve conventional multi-modal systems.",
        "Exploiting User Signals and Stochastic Models to Improve Information Retrieval Systems and Evaluation. Progress and innovation are driven by experiments, but experimentation is useless without an objective evaluation measure that allow researchers to detect the improvements and identify the successful strategies. Due to the experimental nature of Information Retrieval (IR), accurately interpreting the result of a system in terms of user satisfaction is fundamental to push the research in the correct direction. Therefore, measuring systems effectiveness continues to be an active area of research and discussion in the scientific community. It is also the case of this thesis, whose leitmotiv is an investigation of effectiveness measures exploited in different aspects of IR.Our first aim was to provide a formal and theoretical definition of effectiveness measure. Several evaluation measures have been proposed since the beginning of IR, starting from simple ratios between relevant and retrieved documents to more complex functions discounting each rank positions and accounting for plausible user models. However, even if much research was conducted, a prior question is still just partially fulfilled: what is a general definition of IR evaluation measure? We tackle this challenge and give a formal definition of utility-oriented measurement of retrieval effectiveness [2], based on the representational theory of measurement.A further complexity of evaluation in IR is represented by relevance. Unfortunately relevance is subjective, the information need is unique, and the user is the only person able to provide a fair and reliable judgement of a document in terms of relevance. Since it is not possible to directly ask to the user to provide relevance judgements when she performs a search, IR evaluation relies on external assessors to judge documents for relevance.The same query-document pair is assigned to more than one crowd assessor to prevent potential errors caused by wrong labels. This makes necessary to merge possibly discording labels generated by different workers. We propose our upstream approach called Assessordriven Weighted Averages for Retrieval Evaluation (AWARE) . AWARE is defined as an upstream approach because it directly combines the scores of the evaluation measures computed from the relevance labels of each assessor, instead of merging the labels and then computing the measures. This allows to account for the error introduced by incorrect labels and to develop a framework which estimates performance measures in a way more robust to the potential noise introduced by crowd assessors.",
        "Error correction vs. query garbling for Arabic OCR document retrieval Due to the existence of large numbers of legacy documents (such as old books and newspapers), improving retrieval effectiveness for OCR'ed documents continues to be an important problem. This article compares the effect of OCR error correction with and without language modeling and the effect of query garbling with weighted structured queries on the retrieval of OCR degraded Arabic documents. The results suggest that moderate error correction does not yield statistically significant improvement in retrieval effectiveness when indexing and searching using n-grams. Also, reversing error correction models to perform query garbling in conjunction with weighted structured queries yields improved retrieval effectiveness. Lastly, using very good error correction that utilizes language modeling yields the best improvement in retrieval effectiveness.",
        "Pseudo relevance feedback using semantic clustering in relevance language model. Pseudo relevance feedback has demonstrated to be in general an effective technique for improving retrieval effectiveness, but the noise in the top retrieved documents still can cause topic drift problem that affects the performance of certain topics. By viewing a document as an interaction of a set of independent hidden topics, we propose a novel semantic clustering technique using independent component analysis. Then within the language modeling framework, we apply the obtained semantic topic clusters into the query sampling process so that the sampling depends on the activated topics rather than on the individual document language model. Therefore, we obtain a semantic cluster based relevance language model, which uses pseudo relevance feedback technique without requiring any relevance training information. We applied the model on five TREC data sets. The experiments show that our model can significantly improve retrieval performance over traditional language models including relevance-based and clustering-based retrieval language models. The main contribution of the improvements comes from the estimation of the relevance model on the semantic clusters that are closely related to the query.",
        "Improving the effectiveness of retrieval systems by information structures",
        "Improve the effectiveness of the opinion retrieval and opinion polarity classification. Opinion retrieval is a document retrieving and ranking process. A relevant document must be relevant to the query and contain opinions toward the query. Opinion polarity classification is an extension of opinion retrieval. It classifies the retrieved document as positive, negative or mixed, according to the overall polarity of the query relevant opinions in the document. This paper proposes several new techniques that help improve the effectiveness of an existing opinion retrieval system; (2) presents a novel two-stage model to solve the opinion polarity classification problem. In this model, every query relevant opinionated sentence in a document retrieved by our opinion retrieval system is classified as positive or negative respectively by a SVM classifier. Then a second classifier determines the overall opinion polarity of the document. Experimental results show that both the opinion retrieval system with the proposed opinion retrieval techniques and the polarity classification model outperformed the best reported systems respectively."
    ],
    "machine learning language identification": [
        "INLI@FIRE-2018: A Native Language Identification System using Convolutional Neural Networks Native Language Identification is the problem of identifying the first language of speakers based on his/her writings in another language. The proposed approach is a deep learning based methodology using convolutional neural networks. Convolutional neural networks are a class of neural networks that have proven very effective in areas such as pattern recognition and classification. They are able to capture the local texture within the text and can be used to find the representative patterns in a text document. The proposed system consists of a language identification model, which is trained by a corpus of 1233 documents. The experiments were conducted using the dataset provided for INLI@FIRE2018. The results indicate that the system is capable of giving performance comparable to the methods employing more sophisticated approaches.",
        "Labeling of Query Words using Conditional Random Field This paper describes our approach on Query Word Labeling as an attempt in the shared task on Mixed Script Information Retrieval at Forum for Information Retrieval Evaluation (FIRE) 2015. The query is written in Roman script and the words were in English or transliterated from Indian regional languages. A total of eight Indian languages were present in addition to English. We also identified the Named Entities and special symbols as part of our task. A CRF based machine learning framework was used for labeling the individual words with their corresponding language labels. We used a dictionary based approach for language identification. We also took into account the context of the word while identifying the language. Our system demonstrated an overall accuracy of 75.5% for token level language identification. The strict F-measure scores for the identification of token level language labels for Bengali, English and Hindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure of our system was 0.7498.",
        "DalTeam@INLI-FIRE-2017: Native Language Identification using SVM with SGD Training Native Language Identification (NLI), as a variant of Language Identification task, focuses on determining an author's native language, based on a writing sample in their non-native language. In recent years, the challenging nature of NLI has drawn much attention from the research community. Its application and importance are relevant in many fields, such as personalization of a new language learning environment, personalized grammar correction, and authorship attribution in forensic linguistics. We participated in the INLI Shared Task 2017 held in conjunction with FIRE 2017 conference. To implement a machine learning method for Native Language Identification, we used Character and Word N-grams with SVM (Support Vector Machines) classifier trained with SGD (Stochastic Gradient Descent) method. We achieved F1 measure of 89.60% (using 10-fold cross validation), using provided social media dataset and 48.80% was reported in the final testing done by INLI workshop organisers. CCS CONCEPTS \u2022 Computing methodologies \u2192 Supervised learning by classification; Classification and regression trees; \u2022 Social and professional topics \u2192 Cultural characteristics;",
        "Mangalore-University@INLI-FIRE-2017: Indian Native Language Identification using Support Vector Machines and Ensemble approach This paper describes the systems submitted by our team for Indian Native Language Identification (INLI) task held in conjunction with FIRE 2017. Native Language Identification (NLI) is an important task that has different applications in different areas such as social-media analysis, authorship identification, second language acquisition and forensic investigation. We submitted two systems using Support Vector Machine (SVM) and Ensemble Classifier based on three different classifiers representing the comments (data) as vector space model for both systems and achieved accuracy of 47.60% and 47.30% respectively and secured second rank over all submissions for the task. CCS CONCEPTS \u2022 Information systems \u2192 Web and social media search; Multilingual and cross-lingual retrieval; \u2022 Computing methodologies \u2192 Language resources;",
        "SeerNet@INLI-FIRE-2017: Hierarchical Ensemble for Indian Native Language Identification Native Language Identification has played an important role in forensics primarily for author profiling and identification. In this work, we discuss our approach to the shared task of Indian Language Identification. The task is primarily to identify the native language of the writer from the given XML file which contains a set of Facebook comments in the English language. We propose a hierarchical ensemble approach which combines various machine learning techniques along with language agnostic feature extraction to perform the final classification. Our hierarchical ensemble improves the TF-IDF based baseline accuracy by 3.9%. The proposed system stood 3 rd across unique team submissions..",
        "Machine Learning Approach for Language Identification & Transliteration. In this paper, we describe the system that we developed as part of our participation to the FIRE-2014 Shared Task on Transliterated Search. We participated only for Subtask 1 that focused on labeling the query words. The entire process consists of the following components: language identification of each word in text, named entity recognition and classification (NERC) and transliteration of Indian language words written in non-native scripts to the corresponding native Indic scripts. The proposed methods of language identification and NERC are based on supervised approaches, where we use several machine learning algorithms. Our transliteration framework is based on modified joint source channel model. Experiments on benchmark setup show that we achieve quite encouraging performance for both the pairs of languages, viz. Bangla-English and Hindi-English. It is also to be noted that we did not make use of heavy domainspecific resources and/or tools, and therefore this can be easily adapted to the other domains and/or languages.",
        "Statistical testing based feature selection for Native Language Identification INLI@FIRE2018 Native Language Identification is the process of determining the language native to the author from the written text. We have proposed a system that uses machine learning algorithms to identify the native language from the written text. We extracted Term frequencyinverse document frequency (Tf-idf) as the feature from the given document and used statistical based measures such as Analysis of Variance-F value measure, Chi-square measure for selecting the best features. The selected features are fed to Multi Layer Perceptron and Stochastic Gradient Descent classifier to classify the native language into one of 6 listed Indian languages. This work was submitted to Indian Native Language Identification task INLI@FIRE2018. We have investigated the performance of the proposed system using three classifiers namely Multi Layer Perceptron (MLP) classifier with Analysis of Variance-F value measure, MLP classifier with Chi-square measure and Stochastic Gradient Descent classifier with Chi-square measure. From the results we have observed that SGD classifier with Chi-square measure has performed better than the other two classifiers.",
        "Team WebArch at FIRE-2018 Track on Indian Native Language Identification Native Language Identification (NLI) is the task which involves identification of native language (L1) of an individual based on his/her language production in a learned language (L2). It is basically a classification task where we are classifying L1 into a number of different languages. In this task I have to identify an individual's native language (L1) among the following six Indian languages: Bengali, Hindi, Kannada, Malayalam, Tamil, and Telugu using their Facebook comments written in English language (L2). In this paper I propose to use machine learning models such as classification models together with N-grams as features and Tfidf as vectorizer.",
        "A Hybrid Approach for Transliterated Word-Level Language Identification: CRF with Post-Processing Heuristics. In this paper, we describe a hybrid approach for word-level language (WLL) identification of Bangla words written in Roman script and mixed with English words as part of our participation in the shared task on transliterated search at Forum for Information Retrieval Evaluation (FIRE) in 2014. A CRF based machine learning model and post-processing heuristics are employed for the WLL identification task. In addition to language identification, two transliteration systems were built to transliterate detected Bangla words written in Roman script into native Bangla script. The system demonstrated an overall token level language identification accuracy of 0.905. The token level Bangla and English language identification F-scores are 0.899, 0.920 respectively. The two transliteration systems achieved accuracies of 0.062 and 0.037. The word-level language identification system presented in this paper resulted in the best scores across almost all metrics among all the participating systems for the Bangla-English language pair.",
        "A Machine Learning Approach to Indian Native Language Identification NLI (Native Language Identification) determines the native language of the non-native users using their writings in a foreign language. It has several applications namely forensic and security, author profiling and identification, and educational applications. English is a most common language used in social media by many non-English people in the world to share their thoughts and ideas. They blend English with their native language for their posts and comments. Identifying the native language from the short text in English is still a challenging task. In this paper, we present a language agnostic approach without any language specific processing and employed machine learning approach with and without feature selection to identify the native language of a Indian speaker using their comments and posts in social network. The bag of word features are extracted from the text posted by the user and the feature vectors are constructed using TF-IDF score for the training data. We have used a statistical feature selection methodology to select the features that are significantly contributing to NLI task. The classifier with highest cross validation accuracy was used for predicting the native language of the user. Our approaches are evaluated using INLI@FIRE2018 shared task data set.",
        "AmritaCEN_NLP @ FIRE 2015 Language Identification for Indian Languages in Social Media Text The progression of social media contents, similar like Twitter and Facebook messages and blog post, has created, many new opportunities for language technology. The user generated contents such as tweets and blogs in most of the languages are written using Roman script due to distinct social culture and technology. Some of them using own language script and mixed script. The primary challenges in process the short message is identifying languages. Therefore, the language identification is not restricted to a language but also to multiple languages. The task is to label the words with the following categories L1, L2, Named Entities, Mixed, Punctuation and Others This paper presents the AmritaCen_NLP team participation in FIRE2015-Shared Task on Mixed Script Information Retrieval Subtask 1: Query Word Labeling on language identification of each word in text, Named Entities, Mixed, Punctuation and Others which uses sequence level query labelling with Support Vector Machine. CCS Concepts \u2022 Theory of computation~Support vector machines \u2022 Computing methodologies~Natural language Processing \u2022 Information systems~Information extraction \u2022 Humancentered computing~Social tagging systems",
        "Language Identification in Mixed Script Social Media Text With the spurt in usage of smart devices, large amounts of unstructured text is generated by numerous social media tools. This text is often filled with stylistic or linguistic variations making the text analytics using traditional machine learning tools to be less effective. One of the specific problem in Indian context is to deal with large number of languages used by social media users in their roman form. As part of FIRE-2015 shared task on mixed script information retrieval, we address the problem of word level language identification. Our approach consists of a two stage algorithm for language identification. First level classification is done using sentence level character n-grams and second level consists of word level character n-grams based classifier. This approach effectively captures the linguistic mode of author in social texting enviroment. The overall weighted F-Score for the run submitted to FIRE Shared task is 0.7692. The sentence level classification algorithm which is used in achiving this result has an accuracy of 0.6887. We could further improve the accuracy of sentence level classifier further by 1.6% using additional social media text crawled from other sources. Naive Bayes classifier showed largest improvement (5.5%) in accuracy level by the addition of supplementary tuples. We also observed that using semi-supervised learning algorithm such as Expectation Maximization with Naive Bayes, the accuracy could be improved to 0.7977."
    ],
    "social media detect self-harm": [
        "INAOE-CIMAT at eRisk 2020: Detecting Signs of Self-Harm using Sub-Emotions and Words In this paper, we present our approach to the detection of self-harm at eRisk 2020. The main objective of this shared task was to identify as soon as possible if a user presents signs of committing selfharm by using their posts on Reddit. To tackle this problem, we used a representation called Bag of Sub-Emotions (BoSE), an approach that represents the posts of the users in a set of sub-emotions, in combination with a Bag of Words. With this strategy, we were able to capture the sub-emotions and topics that users with signs of self-harm tend to use. For the early classification, we choose five different strategies based on the temporal stability shown by the users through their posts. Our approach showed competitive performance in comparison with other participants. Additionally, the interpretability and simplicity of our representation present an opportunity for the analysis detection of different mental disorders in social media.",
        "NLP-UNED at eRisk 2020: Self-harm Early Risk Detection with Sentiment Analysis and Linguistic Features Mental health problems such as depression are conditions that, going undetected, can have serious consequences. A less-known mental health problem that has been linked to depression is self-harm. There is evidence suggesting that people's writings can reflect these problems, and research has been done to detect these individuals through their content on social media. Early detection is crucial for mental health problems, and for this purpose a shared task named eRisk was proposed. This paper describes NLP-UNED's participation on the 2020 T1 subtask. Participants were asked to create systems that detected early self-harm signs on Reddit users. Our team shows a data analysis of the 2019 T2 subtask and proposes a simple feature-driven classifier with features based on first-person pronoun use, sentiment analysis and self-harm terminology.",
        "Transfer Learning for Depression: Early Detection and Severity Prediction from Social Media Postings Online social media platforms allow open sharing of thoughts and dialogue. These platforms generate large amounts of data about their users' online behaviour, which can be repurposed for the development of technologies which can detect mental health disorders. Towards this goal, we applied transfer and supervised learning techniques for predicting the severity and risk of depression for the eRisk 2019 Lab at the CLEF workshop. Both tasks were very difficult due to lack of training data, motivating our efforts to learn signals from other pre-trained models and datasets. For the early detection of signs of self-harm (Task 2), our classifiers that operated at the level of posts were too sensitive, resulting in low precision. For the task that evaluated ability to measure the severity of the signs of depression, we found that our submissions did not outperform chance or simple predictions for three of the four metrics. As pre-trained language models improve, we are optimistic that transfer learning will accelerate progress in early risk prediction on the internet.",
        "Early Risk Detection of Self-Harm and Depression Severity using BERT-based Transformers This paper briefly describes our research groups' efforts in tackling Task 1 (Early Detection of Signs of Self-Harm), and Task 2 (Measuring the Severity of the Signs of Depression) from the CLEF eRisk Track. Core to how we approached these problems was the use of BERT-based classifiers which were trained specifically for each task. Our results on both tasks indicate that this approach delivers high performance across a series of measures, particularly for Task 1, where our submissions obtained the best performance for precision, F1, latencyweighted F1 and ERDE at 5 and 50. This work suggests that BERTbased classifiers, when trained appropriately, can accurately infer which social media users are at risk of self-harming, with precision up to 91.3% for Task 1. Given these promising results, it will be interesting to further refine the training regime, classifier and early detection scoring mechanism, as well as apply the same approach to other related tasks (e.g., anorexia, depression, suicide).",
        "Using Surface and Semantic Features for Detecting Early Signs of Self-Harm in Social Media Postings This paper describes the University of Hildesheim submission to the CLEF eRisk 2020 shared task on detecting early signs of self-harm in social media posts. We introduce four systems that apply different methods trying to address this task and a fifth ensemble system that combines the four other systems. The first four systems make use of features of different types, such as time intervals between posts, the sentiment and semantics of the writings by using bag-of-words vectors and contextualized word embeddings in a neural network approach. The results show that while all our systems achieve a high recall, the focus of future work should be further improvement of the precision. All systems and the ensemble model achieve a comparable performance of F latency values in the range of 0.367 to 0.424.",
        "BioInfo@UAVR at eRisk 2019: delving into Social Media Texts for the Early Detection of Mental and Food Disorders This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the shared tasks of CLEF eRisk 2019 1. The objective of the eRisk initiative is to encourage research in the area of information retrieval for the automatic detection of risk situations on the internet. The challenge was organized in three tasks, focused on the early detection of anorexia (T1), self-harm (T2) and severity of depression (T3). We addressed these tasks using a mix approach that combines machine learning with psycholinguistics and behavioural patterns. The results obtained validate the use of such patterns in the context of social media mining and motivate future research into this field.",
        "UDE at eRisk 2019: Early Risk Prediction on the Internet In this paper, we describe our participation in CLEF eRisk workshop. eRisk 2019 is the third edition of this track 3 , which was first introduced in 2017. In the current edition, the organizers are targeting Social Media users, namely Reddit, who may be under the risk of Anorexia, self-harm, and depression. We participated in both tasks of early risk detection of Anorexia and self-harm. Our predictions are based on Natural Language Processing using supervised machine learning with Support Vector Machines (SVM) and neural networks. SVM gave the best results among our five submitted models with latency-weighted F1 of 0.58 and ERDE5 of 0.08 and ERDE50 of 0.04 for Anorexia detection task, while our more complicated neural network models did not show the desired performances.",
        "Attentive Multi-stage Learning for Early Risk Detection of Signs of Anorexia and Self-harm on Social Media Three tasks are proposed at CLEF eRisk-2019 for predicting mental disorder using users posts on Reddit. Two tasks (T1 and T2) focus on early risk detection of signs of anorexia and self-harm respectively. The other one (T3) focus on estimation of the severity level of depression from a thread of user submissions. In this paper, we present the participation of LIRMM (Laboratoire d'Informatique, de Robotique et de Micro\u00e9lectronique de Montpellier) in both tasks on early detection (T1 and T2). The proposed model addresses this problem by modeling the temporal mood variation detected from user posts through multistage learning phases. The proposed architectures use only textual information without any hand-crafted features or dictionaries. The basic architecture uses two learning phases through exploration of state-of-theart deep language models. The proposed models perform comparably to other contributions.",
        "Detection of Early Sign of Self-harm on Reddit using Multi-level Machine This paper describes the participation of the EFE research team in task1 of CLEF eRisk 2020 competitions. This challenge basically focuses on the early detection of symptoms of self-harm from users' posts on social media. Identifying mental illnesses especially in the early stages can help people and avoid risky behaviors. Personal notes on social media are often indicative of one's psychological state, therefore using natural language processing techniques on users' posts one can develop an early risk detection system. The proposed method is basically consisting of Word2Vec representation, an ensemble of SVM and deep neural network and also attention layers. The obtained results are very competitive and show the strength of the system provided in the early diagnosis of selfharm.",
        "Deep Learning Architectures and Strategies for Early Detection of Self-harm and Depression Level Prediction This paper summarizes the contributions of the PRHLT-UPV team as a participant in the eRisk 2020 tasks on self-harm detection and prediction of depression levels from social media. Computational methods based on machine learning and natural language processing have a great potential to assist with early detection of mental disorders of social media users, based on their online activity. We use multi-dimensional representations of language, and compare various deep learning models' performance, exploring rarely approached avenues in previous research, including hierarchical deep learning architectures and pre-trained transformers and language models.",
        "A Baseline Approach for Early Detection of Signs of Anorexia and Self-harm in Reddit Posts This paper describes the systems developed by the BiTeM team for the CLEF eRisk Task 1 and 2, 2019. The goal was to predict the risk of anorexia and self-harm from user-generated content on Reddit. Several approaches based on supervised learning were used to estimate the risk of anorexia and self-harm. The systems were able to achieve low to moderate results.",
        "UNSL at eRisk 2019: a Unified Approach for Anorexia, Self-harm and Depression Detection in Social Media In this paper we describe the participation of our research group at the CLEF eRisk 2019. The eRisk goal is the early detection of atrisk people by means of machine learning techniques based on language usage. This year eRisk edition was divided into three tasks, T1, T2, and T3. The first two were focused on early detection of anorexia and self-harm on Reddit users. T3 focused on measuring users' severity of depression. To carry out this task, models had to automatically fill the standard BDI depression questionnaire based on the evidence found in the user's history of postings. We used the same classifier, SS3, to carry out these three tasks with the same hyper-parameters configuration. SS3 is a recently introduced text classifier[1] that was created with the goal to deal with early risk detection scenarios in an integrated manner: it naturally supports incremental and early classification over text streams and additionally, it has the ability to visually explain its rationale. The final results for all these three tasks show that SS3 is a very robust and efficient classifier. SS3 was the fastest method and obtained the best ERDE and overall best ranking-based measures in all the tasks. Additionally, it obtained the best P recision, F 1 and F 1 latency for task T2. Finally, in task T3, it obtained the best AHR and ACR values, and the second-best ADODL and DCHR. This was quite remarkable taking into account that the same classifier was used here to fill users' BDI questionnaires, which is a task completely different from the other two \"yes or no\" tasks."
    ],
    "stemming for arabic languages": [
        "Empirical studies in strategies for Arabic retrieval. This work evaluates a few search strategies for Arabic monolingual and cross-lingual retrieval, using the TREC Arabic corpus as the test-bed. The release by NIST in 2001 of an Arabic corpus of nearly 400k documents with both monolingual and cross-lingual queries and relevance judgments has been a new enabler for empirical studies. Experimental results show that spelling normalization and stemming can significantly improve Arabic monolingual retrieval. Character tri-grams from stems improved retrieval modestly on the test corpus, but the improvement is not statistically significant. To further improve retrieval, we propose a novel thesaurus-based technique. Different from existing approaches to thesaurus-based retrieval, ours formulates word synonyms as probabilistic term translations that can be automatically derived from a parallel corpus. Retrieval results show that the thesaurus can significantly improve Arabic monolingual retrieval. For cross-lingual retrieval (CLIR), we found that spelling normalization and stemming have little impact.",
        "Stemming to improve translation lexicon creation form bitexts. Arabic is a morphologically rich language that presents significant challenges to many natural language processing applications because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix). By segmenting words into morphemes, we could improve the performance of English/Arabic translation pair\u00d5s extraction from parallel texts. This paper describes two algorithms and their combination to automatically extract an English/Arabic bilingual dictionary from parallel texts that exist in the Internet archive after using an Arabic light stemmer as a preprocessing step. Before using the Arabic light stemmer, the total system precision and recall were 88.6% and 81.5% respectively, then the system precision an recall increased to 91.6% and 82.6% respectively after applying the Arabic light stemmer on the Arabic documents.The algorithms have certain variables which values can be changed to control the system precision and recall. Like most of the systems do, the accuracy of our system is directly proportional to the number of sentence pairs used. However our system is able to extract translation pairs from a very small parallel corpus. This new system can extract translations from only two sentences in one language and two sentences in the other language if the requirements of the system accomplished. Moreover, this system is able to extract word pairs that are translation of each others, synonyms and the explanation of the word in the other language as well. By controlling the system variables, we could achieve 100% precision for the output bilingual dictionary with a small recall.",
        "Towards an error-free Arabic stemming. Stemming is a computational process for reducing words to their roots (or stems). It can be classified as a recall-enhancing or precision-enhancing component.Existing Arabic stemmers suffer from high stemming error-rates. Arabic stemmers blindly stem all the words and perform poorly especially with compound words, nouns and foreign Arabized words.The Educated Text Stemmer (ETS) is presented in this paper. ETS is a dictionary free, simple, and highly effective Arabic stemming algorithm that can reduce stemming errors in addition to decreasing computational time and data storage.The novelty of the work arises from the use of neglected Arabic stop-words. These stop-words can be highly important and can provide a significant improvement to processing Arabic documents.The ETS stemmer is evaluated by comparison with output from human generated stemming and the stemming weight technique.",
        "Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis. Arabic, a highly inflected language, requires good stemming for effective information retrieval, yet no standard approach to stemming has emerged. We developed several light stemmers based on heuristics and a statistical stemmer based on co-occurrence for Arabic retrieval. We compared the retrieval effectiveness of our stemmers and of a morphological analyzer on the TREC-2001 data. The best light stemmer was more effective for cross-language retrieval than a morphological stemmer which tried to find the root for each word. A repartitioning process consisting of vowel removal followed by clustering using co-occurrence analysis produced stem classes which were better than no stemming or very light stemming, but still inferior to good light stemming or morphological analysis.",
        "Broken plural detection for arabic information retrieval. Due to the high number of inflectional variations of Arabic words, empirical results suggest that stemming is essential for Arabic information retrieval. However, current light stemming algorithms do not extract the correct stem of irregular (so-called broken) plurals, which constitute ~10% of Arabic texts and ~41% of plurals. Although light stemming in particular has led to improvements in information retrieval , the effects of broken plurals on the performance of information retrieval systems has not been examined.We propose a light stemmer that incorporates a broken plural recognition component, and evaluate it within the context of information retrieval. Our results show that identifying broken plurals and reducing them to their correct stems does result in a significant improvement in the performance of information retrieval systems.",
        "Distance Measures and Stemming Impact on 8206Arabic Document Clustering. Clustering of Arabic documents is considered as a vital aspect of obtaining optimal results from unsupervised learning. Its aim is to automatically group similar documents into a single cluster using different similarities or distance measures. However, diverse similarities and distance measures are available and their effectiveness in document clustering with a syntactic structure of the stemming is still not obvious. Therefore, this study aims to evaluate the impact of five similarity/distance measures (i.e., cosine similarity, the Jaccard coefficient, Pearson's correlation coefficient, Euclidean distance, and averaged Kullback-Leibler divergence) with two stemming algorithms (i.e., morphologyand syntax-based lemmatization; and morphology-based Information Science Research Institute (ISRI) stemming on clustering Arabic text dataset. We aim to identify the best performing similarity and distance measures and determine which measure is most suitable for Arabic document clustering. Our experimental method, which is based on syntactic structure and morphology, outperformed other stemming methods that use any of the five similarity/distance measures for Arabic document clustering. The best performing similarity/distance measures are cosine similarity and Euclidean distance , respectively.Keywords: Similarity/distance measures, partitional clustering, lemmatization stemming, Arabic document clustering. The advent of information and communication technology and the rapid growth of the internet have significantly increased the need for highly effective information search and retrieval systems. These technological advancements have greatly benefited English language information search and retrieval. However, it is regrettable that Arabic language has not received significant attention in this field. Compared with other languages, insufficient effort has been made to advance Arabic language information search and retrieval.",
        "On arabic search: improving the retrieval effectiveness via a light stemming approach. The inflectional structure of a word impacts the retrieval accuracy of information retrieval systems of Latin-based languages. We present two stemming algorithms for Arabic information retrieval systems. We empirically investigate the effectiveness of surfacebased retrieval. This approach degrades retrieval precision since Arabic is a highly inflected language. Accordingly, we propose root-based retrieval. We notice a statistically significant improvement over the surface-based approach. Many variant word senses are based on an identical root; thus, the root-based algorithm creates invalid conflation classes that result in an ambiguous query which degrades the performance by adding extraneous terms. To resolve ambiguity, we propose a novel lightstemming algorithm for Arabic texts. This automatic rule-based stemming algorithm is not as aggressive as the root extraction algorithm. We show that the light stemming algorithm significantly outperforms the root-based algorithm. We also show that a significant improvement in retrieval precision can be achieved with light inflectional analysis of Arabic words.",
        "Corpus-Based Arabic Stemming Using N-Grams. In languages with high word inflation such as Arabic, stemming improves text retrieval performance by reducing words variants. We propose a change in the corpus-based stemming approach proposed by Xu and Croft for English and Spanish languages in order to stem Arabic words. We generate the conflation classes by clustering 3-gram representations of the words found in only 10% of the data in the first stage. In the second stage, these clusters are refined using different similarity measures and thresholds. We conducted retrieval experiments using row data, Light-10 stemmer and 8 different variations of the similarity measures and thresholds and compared the results. The experiments show that 3-gram stemming using the dice distance for clustering and the EM similarity measure for refinement performs better than using no stemming; but slightly worse than Light-10 stemmer. Our method potentially could outperform Light-10 stemmer if more text is sampled in the first stage.",
        "A novel Arabic lemmatization algorithm. Tokenization is a fundamental step in processing textual data preceding the tasks of information retrieval, text mining, and natural language processing. Tokenization is a languagedependent approach, including normalization, stop words removal, lemmatization and stemming. Both stemming and lemmatization share a common goal of reducing a word to its base. However, lemmatization is more robust than stemming as it often involves usage of vocabulary and morphological analysis, as opposed to simply removing the suffix of the word. In this work, we introduce a novel lemmatization algorithm for the Arabic Language. The new lemmatizer proposed here is a part of a comprehensive Arabic tokenization system, with a stop words list exceeding 2200 Arabic words. Currently, there are two Arabic leading stemmers: the root-based stemmer and the light stemmer. We hypothesize that lemmatization would be more effective than stemming in mining Arabic text. We investigate the impact of our new lemmatizer on unsupervised data mining techniques in comparison to the leading Arabic stemmers. We conclude that lemmatization is a better word normalization method than stemming for Arabic text.",
        "Simple Arabic Stemmer. We propose a root stemmer for the Modern Standard Arabic (MSA) language in an attempt to enhance the performance of Arabic Information Retrieval (AIR). The new Simple Arabic Stemmer (SAS) is based on the Quran morphology, since the Quran was a key source for the derivation of Arabic morphological rules. The stemmer is developed by decomposing all of the Quran words and studying their internal morphological structure including the roots, the patterns, and the affixes employed in the generation process. We were able to construct a relatively small lexicon capable of finding the root for most of the MSA vocabulary. Using the TREC corpus and queries, we test our approach against two wellknown root stemmers, Khoja and Sebawai. The results show that SAS gives an improvement in terms of precision.",
        "Language processing for arabic microblog retrieval. The use of social media has profoundly affected social and political dynamics in the Arab world. In this paper, we explore the Arabic microblogs retrieval. We illustrate some of the challenges associated with Arabic microblog retrieval, which mainly stem from the use of different Arabic dialects that vary in lexical selection, morphology, and phonetics and lack orthographic and spelling conventions. We present some of the required processing for effective retrieval such as improved letter normalization, elongated word handling, stopword removal, and stemming.",
        "Impact of Stemmer on Arabic Text Retrieval. Stemming is a process of reducing inflected words to their stem, stem or root from a generally written word form. One of the high inflected words in the languages world is Arabic Language. Stemming improve the retrieval performance by reducing words variants, and in lcrease the similarity between related words. However, an Arabic Information Retrieval (AIR) can use stemming algorithms to retrieve a greater number of documents related to the users' query. Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. 'Information Science Research Institute\" (ISRI), morphological and syntax based lemmatization \"Educated Text Stemmer\" (ETS), and Light10 stemmer) on the Arabic Information Retrieval performance for Arabic language, we used the Linguistic Data Consortium (LDC) Arabic Newswire data set as benchmark dataset. The evaluation of the three different stemmers ranked the best performance was achieved by light10 stemmer in term of mean average precision.",
        "The Effect of Stemming on Arabic Text Classification: An Empirical Study"
    ],
    "audio based animal recognition": [
        "Bird Species Recognition via Neural Architecture Search This paper presents the winning approach of the BirdCLEF 2020 challenge. The challenge is to automatically recognize bird sounds in continuous soundscapes. In our approach, a deep convolutional neural network model is used that directly operates on the audio data. This neural network architecture is based on a neural architecture search and contains multiple auxiliary heads and recurrent layers. During the training process, scheduled drop path is used as a regularization method and extensive data augmentation is applied to the audio input. Furthermore, species location lists are used in the post-processing step to reject unlikely classes. Our best run on the test set obtains a classification mean average precision score (cmap) of 13.1% and a retrieval mean average precision score (rmap) of 19.2%.",
        "Recognizing Bird Species in Audio Files Using Transfer Learning In this paper, a method to identify bird species in audio recordings is presented. For this purpose, a pre-trained Inception-v3 convolutional neural network was used. The network was fine-tuned on 36,492 audio recordings representing 1,500 bird species in the context of the BirdCLEF 2017 task. Audio records were transformed into spectrograms and further processed by applying bandpass filtering, noise filtering, and silent region removal. For data augmentation purposes, time shifting, time stretching, pitch shifting, and pitch stretching were applied. This paper shows that fine-tuning a pre-trained convolutional neural network performs better than training a neural network from scratch. Domain adaptation from image to audio domain could be successfully applied. The networks' results were evaluated in the BirdCLEF 2017 task and achieved an official mean average precision (MAP) score of 0.567 for traditional records and a MAP score of 0.496 for records with background species on the test dataset.",
        "Instance-based Bird Species Identification with Undiscriminant Features Pruning This paper reports the participation of Inria to the audiobased bird species identification challenge of LifeCLEF 2014 campaign. Inspired by recent works on fine-grained image classification, we introduce an instance-based classification scheme based on the dense indexing of MFCC features and the pruning of the non-discriminant ones. To make such strategy scalable to the 30M of MFCC features extracted from the tens of thousands audio recordings of the training set, we used highdimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Further improvements are obtained by (i) using a sliding classifier with max pooling (ii) weighting the query features according to their semantic coherence (iii) making use of the metadata to filter incoherent species. Results show the effectiveness of the proposed technique which ranked 3rd among the 10 participating groups.",
        "Audio-only Bird Classification Using Unsupervised Feature Learning We describe our method for automatic bird species classification, which uses raw audio without segmentation and without using any auxiliary metadata. It successfully classifies among 501 bird categories, and was by far the highest scoring audio-only bird recognition algorithm submitted to BirdCLEF 2014. Our method uses unsupervised feature learning, a technique which learns regularities in spectro-temporal content without reference to the training labels, which helps a classifier to generalise to further content of the same type. Our strongest submission uses two layers of feature learning to capture regularities at two different time scales.",
        "Audio-based Bird Species Identification with Deep Convolutional Neural Networks This paper presents deep learning techniques for audio-based bird identification at very large scale. Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 1500 species. Various data augmentation techniques are applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the BirdCLEF 2018 campaign and provides the best system in all subtasks. It surpasses previous state-of-the-art by 15.8 % identifying foreground species and 20.2 % considering also background species achieving a mean reciprocal rank (MRR) of 82.7 % and 74.0 % on the official BirdCLEF Subtask1 test set.",
        "LifeCLEF 2020 Teaser: Biodiversity Identification and Prediction Challenges Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals in the field is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2020 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: location-based prediction of species based on environmental and occurrence data, and (iv) Snake-CLEF: image-based snake identification.",
        "Bird Species Identification in Soundscapes This paper presents deep learning techniques for audio-based bird identification in soundscapes. Deep Convolutional Neural Networks are trained to classify 659 species. Different data augmentation techniques are applied to prevent overfitting and improve model accuracy and generalization. The proposed approach is evaluated in the BirdCLEF 2019 campaign and provides the best system to identify bird species in wildlife monitoring recordings. With an ensemble of different single-and multi-label classification models it obtains a classification mean average precision (c-mAP) of 35.6 % and a retrieval mean average precision (r-mAP) of 74.6 % on the official BirdCLEF test set. In terms of classification precision, single model performance surpasses previous stateof-the-art by more than 20 %.",
        "LifeCLEF 2021 Teaser: Biodiversity Identification and Prediction Challenges Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals in the field is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2021 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: location-based prediction of species based on environmental and occurrence data and (iv) Snake-CLEF: image-based snake identification.",
        "Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes The BirdCLEF challenge-as part of the 2019 LifeCLEF Lab [7]-offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings. The challenge uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that BirdCLEF is close to the conditions of real-world application, in particular with regard to the number of species in the training set (659). In 2019, the challenge was focused on the difficult task of recognizing all birds vocalizing in omni-directional soundscape recordings. Therefore, the dataset of the previous year was extended with more than 350 hours of manually annotated soundscapes that were recorded using 30 field recorders in Ithaca (NY, USA). This paper describes the methodology of the conducted evaluation as well as the synthesis of the main results and lessons learned.",
        "Audio Based Bird Species Identification using Deep Learning Techniques In this paper we present a new audio classification method for bird species identification. Whereas most approaches apply nearest neighbour matching [6] or decision trees [8] using extracted templates for each bird species, ours draws upon techniques from speech recognition and recent advances in the domain of deep learning. With novel preprocessing and data augmentation methods, we train a convolutional neural network on the biggest publicly available dataset [5]. Our network architecture achieves a mean average precision score of 0.686 when predicting the main species of each sound file and scores 0.555 when background species are used as additional prediction targets. As this performance surpasses current state of the art results, our approach won this years international BirdCLEF 2016 Recognition Challenge [3,4,1].",
        "Recognizing Bird Species in Audio Recordings using Deep Convolutional Neural Networks This paper summarizes a method for purely audio-based bird species recognition through the application of convolutional neural networks. The approach is evaluated in the context of the LifeCLEF 2016 bird identification task-an open challenge conducted on a dataset containing 34 128 audio recordings representing 999 bird species from South America. Three different network architectures and a simple ensemble model are considered for this task, with the ensemble submission achieving a mean average precision of 41.2% (official score) and 52.9% for foreground species."
    ],
    "Comparison different retrieval models": [
        "Improving the effectiveness of language modeling approaches to information retrieval: bridging the theory-effectiveness gap. Improving the effectiveness of general retrieval models has been a long-standing difficult challenge in information retrieval research, yet is also a fundamentally important task, because an improved general retrieval model would benefit every search engine. The language modeling approach to information retrieval has recently attracted much attention. In the language modeling approach, we assume that a query is a sample drawn from a language model: given a query Q and a document D, we compute the likelihood of \"generating\" query Q with a document language model estimated based on document D. We can then rank documents based on the likelihood of generating the query, i.e., query likelihood. On the one hand, with sound statistical foundation, the language modeling approach makes it easier to set and optimize retrieval parameters, and often outperforms traditional retrieval models. On the other hand, however, after more than one decade of research, the basic language modeling approach to retrieval still remains the same, mainly because the difficulty in accurately modeling the highly empirical notion of relevance within a standard statistical model has led to slow progress in optimizing language modeling approaches; this suggests that the theoretical framework of language models has a clear gap from what is needed to make a retrieval model empirically effective, a general problem we refer to as the \"theory-effectiveness gap\". We have identified the following theory-effectiveness gaps in current language modeling approaches:First, one critical common component in any language modeling approach is the document language model. Traditional document language models follow the bag-of-words assumption that assumes term independence and ignores the positions of the query terms in a document. For example, in a query \"computer virus\", the occurrences of two query terms may be close to each other in one document (likely to mean computer virus) while far apart in another document (not necessarily about computer virus), which makes a huge difference for indicating relevance but is largely underexplored, suggesting the existence of a theoryeffectiveness in standard document language models.Second, accurate estimation of query language models plays a critical role in the language modeling approach to information retrieval. Pseudo-relevance feedback (PRF) has proven very effective for improving query language models. The basic idea of PRF is to assume that",
        "An Information-Based Cross-Language Information Retrieval Model. We present in this paper well-founded cross-language extensions of the recently introduced models in the information-based family for information retrieval, namely the LL (log-logistic) and SPL (smoothed power law) models of . These extensions are based on (a) a generalization of the notion of information used in the information-based family, (b) a generalization of the random variables also used in this family, and (c) the direct expansion of query terms with their translations. We then review these extensions from a theoretical pointof-view, prior to assessing them experimentally. The results of the experimental comparisons between these extensions and existing CLIR systems, on three collections and three language pairs, reveal that the cross-language extension of the LL model provides a state-of-the-art CLIR system, yielding the best performance overall.",
        "Generative Modeling of Entity Comparisons in Text. Users frequently rely on online reviews for decision making. In addition to allowing users to evaluate the quality of individual products, reviews also support comparison shopping. One key user activity is to compare two (or more) products based on a specific aspect. However, making a comparison across two different reviews, written by different authors, is not always equitable due to the different standards and preferences of individual authors. Therefore, we focus instead on comparative sentences, whereby two products are compared directly by a review author within a single sentence.We study the problem of comparative relation mining. Given a set of comparative sentences, each relating a pair of entities, our objective is two-fold: to interpret the comparative direction in each sentence, and to determine the relative merits of each entity. This requires mining comparative relations at two levels of resolution: at the sentence level, as well as at the entity level. Our key observation is that there is significant synergy between the two levels. We therefore propose a generative model for comparative text, which jointly models comparative directions at the sentence level, and ranking at the entity level. This model is tested comprehensively on Amazon reviews dataset with good empirical outperformance over the state-of-the-art baselines.",
        "Retrieval models for question and answer archives. Retrieval in a question and answer archive involves finding good answers for a user's question. In contrast to typical document retrieval, a retrieval model for this task can exploit question similarity as well as ranking the associated answers. In this paper, we propose a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. The proposed model incorporates word-to-word translation probabilities learned through exploiting different sources of information. Experiments show that the proposed translation based language model for the question part outperforms baseline methods significantly. By combining with the query likelihood language model for the answer part, substantial additional effectiveness improvements are obtained.",
        "Discriminative probabilistic models for passage based retrieval. The approach of using passage-level evidence for document retrieval has shown mixed results when it is applied to a variety of test beds with different characteristics. One main reason of the inconsistent performance is that there exists no unified framework to model the evidence of individual passages within a document. This paper proposes two probabilistic models to formally model the evidence of a set of top ranked passages in a document. The first probabilistic model follows the retrieval criterion that a document is relevant if any passage in the document is relevant, and models each passage independently. The second probabilistic model goes a step further and incorporates the similarity correlations among the passages. Both models are trained in a discriminative manner. Furthermore, we present a combination approach to combine the ranked lists of document retrieval and passage-based retrieval.An extensive set of experiments have been conducted on four different TREC test beds to show the effectiveness of the proposed discriminative probabilistic models for passagebased retrieval. The proposed algorithms are compared with a state-of-the-art document retrieval algorithm and a language model approach for passage-based retrieval. Furthermore, our combined approach has been shown to provide better results than both document retrieval and passagebased retrieval approaches.",
        "Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval. Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text (local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACRR, a novel context-aware neural IR model. Extensive comparisons with established models on Trec Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons 1 .",
        "Semantic discovery from web comparison queries. Users frequently pose comparison queries (e.g., ibm vs apple) on web search engines. However, little research has been done on understanding these queries. To fill in this gap, this paper describes a first solution to discovering and mining comparison queries. We present a novel snowballing algorithm that \"crawls\" comparison queries from search engines via their query autocompletion services. We propose a novel modeling approach that represents comparison queries in a comparison graph and develop a novel algorithm that mines closely related concepts from comparison graphs via spectral clustering. Initial experiments indicate that our approach can reveal the inherent semantic relationship among the concepts and discover different senses of a concept, e.g., \"toyota\" as a car brand or a company name.",
        "Geographical topic discovery and comparison. This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents. GPSassociated documents become popular with the pervasiveness of location-acquisition technologies. For example, in Flickr, the geo-tagged photos are associated with tags and GPS locations. In Twitter, the locations of the tweets can be identified by the GPS locations from smart phones. Many interesting concepts, including cultures, scenes, and product sales, correspond to specialized geographical distributions. In this paper, we are interested in two questions: (1) how to discover different topics of interests that are coherent in geographical regions? (2) how to compare several topics across different geographical locations? To answer these questions, this paper proposes and compares three ways of modeling geographical topics: location-driven model, text-driven model, and a novel joint model called LGTA (Latent Geographical Topic Analysis) that combines location and text. To make a fair comparison, we collect several representative datasets from Flickr website including Landscape, Activity, Manhattan, National park, Festival, Car, and Food. The results show that the first two methods work in some datasets but fail in others.LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations. The results confirm our hypothesis that the geographical distributions can help modeling topics, while topics provide important cues to group different geographical regions.",
        "Formal models for expert finding in enterprise corpora. Searching an organization's document repositories for experts provides a cost effective solution for the task of expert finding. We present two general strategies to expert searching given a document collection which are formalized using generative probabilistic models. The first of these directly models an expert's knowledge based on the documents that they are associated with, whilst the second locates documents on topic, and then finds the associated expert. Forming reliable associations is crucial to the performance of expert finding systems. Consequently, in our evaluation we compare the different approaches, exploring a variety of associations along with other operational parameters (such as topicality). Using the TREC Enterprise corpora, we show that the second strategy consistently outperforms the first. A comparison against other unsupervised techniques, reveals that our second model delivers excellent performance.",
        "Kullback-Leibler Divergence Revisited. e KL divergence is the most commonly used measure for comparing query and document language models in the language modeling framework to ad hoc retrieval. Since KL is rank equivalent to a speci c weighted geometric mean, we examine alternative weighted means for language-model comparison, as well as alternative divergence measures.e study includes analysis of the inverse document frequency (IDF) e ect of the language-model comparison methods. Empirical evaluation, performed with di erent types of queries (short and verbose) and query-model induction approaches, shows that there are methods that o en outperform the KL divergence in some se ings.",
        "Modeling Intransitivity in Matchup and Comparison Data. We present a method for learning potentially intransitive preference relations from pairwise comparison and matchup data. Unlike standard preference-learning models that represent the properties of each item/player as a single number, our method infers a multi-dimensional representation for the different aspects of each item/player's strength. We show that our model can represent any pairwise stochastic preference relation and provide a comprehensive evaluation of its predictive performance on a wide range of pairwise comparison tasks and matchup problems from online video games and sports, to peer grading and election. We find that several of these task -especially matchups in online video games -show substantial intransitivity that our method can model effectively.",
        "Can We Get A Better Retrieval Function From Machine? The quality of an information retrieval system heavily depends on its retrieval function, which returns a similarity measurement between the query and each document in the collection. Documents are sorted according to their similarity values with the query and those with high rank are assumed to be relevant. Okapi BM25 and their variations are very popular retrieval functions and they seem to be the default retrieval function for the IR research community; and there are many other widely used and well studied functions, for example, Pivoted TFIDF and INQUERY. Most of these retrieval functions being used today are made based on probabilistic theories and they are adjusted in real world according to different contexts and information needs. In this paper, we propose the idea that a good retrieval function can be discovered by a pure machine learning approach, without using probabilistic theories and knowledge-based techniques. Two machine learning algorithms, Support Vector Machine (SVM) and Genetic Programming (GP) are used for retrieval function discovery, and GP is found to be a more effective approach. The retrieval functions discovered by GP might be hard for human interpretation, but their performance is superior to Okapi BM25, one of the most popular functions. The new retrieval function is combined with query expansion techniques and the retrieval performance is improved significantly. Based on our observations in the empirical study, the GP function is more reliable and effective than Okapi BM25 when query expansion techniques are used.",
        "A Comparison of Retrieval Models using Term Dependencies. A number of retrieval models incorporating term dependencies have recently been introduced. Most of these modify existing \"bag-ofwords\" retrieval models by including features based on the proximity of pairs of terms (or bi-terms). Although these term dependency models have been shown to be significantly more effective than the bag-of-words models, there have been no previous systematic comparisons between the different approaches that have been proposed. In this paper, we compare the effectiveness of recent bi-term dependency models over a range of TREC collections, for both short (title) and long (description) queries. To ensure the reproducibility of our study, all experiments are performed on widely available TREC collections, and all tuned retrieval model parameters are made public. These comparisons show that the weighted sequential dependence model is at least as effective as, and often significantly better than, any other model across this range of collections and queries. We observe that dependency features are much more valuable in improving the performance of longer queries than for shorter queries. We then examine the effectiveness of dependence models that incorporate proximity features involving more than two terms. The results show that these features can improve effectiveness, but not consistently, over the available data sets.",
        "An exploration of axiomatic approaches to information retrieval. Existing retrieval models generally do not offer any guarantee for optimal retrieval performance. Indeed, it is even difficult, if not impossible, to predict a model's empirical performance analytically. This limitation is at least partly caused by the way existing retrieval models are developed where relevance is only coarsely modeled at the level of documents and queries as opposed to a finer granularity level of terms. In this paper, we present a new axiomatic approach to developing retrieval models based on direct modeling of relevance with formalized retrieval constraints defined at the level of terms. The basic idea of this axiomatic approach is to search in a space of candidate retrieval functions for one that can satisfy a set of reasonable retrieval constraints. To constrain the search space, we propose to define a retrieval function inductively and decompose a retrieval function into three component functions. Inspired by the analysis of the existing retrieval functions with the inductive definition, we derive several new retrieval functions using the axiomatic retrieval framework. Experiment results show that the derived new retrieval functions are more robust and less sensitive to parameter settings than the existing retrieval functions with comparable optimal performance.",
        "Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking Counterfactual evaluation can estimate Click-Through-Rate (CTR) differences between ranking systems based on historical interaction data, while mitigating the effect of position bias and item-selection bias. We introduce the novel Logging-Policy Optimization Algorithm (LogOpt), which optimizes the policy for logging data so that the counterfactual estimate has minimal variance. As minimizing variance leads to faster convergence, LogOpt increases the dataefficiency of counterfactual estimation. LogOpt turns the counterfactual approach-which is indifferent to the logging policy-into an online approach, where the algorithm decides what rankings to display. We prove that, as an online evaluation method, LogOpt is unbiased w.r.t. position and item-selection bias, unlike existing interleaving methods. Furthermore, we perform large-scale experiments by simulating comparisons between thousands of rankers. Our results show that while interleaving methods make systematic errors, LogOpt is as efficient as interleaving without being biased.",
        "Structured queries, language modeling, and relevance modeling in cross-language information retrieval. Two probabilistic approaches to cross-lingual retrieval are in wide use today, those based on probabilistic models of relevance, as exemplified by INQUERY, and those based on language modeling. INQUERY, as a query net model, allows the easy incorporation of query operators, including a synonym operator, which has proven to be extremely useful in cross-language information retrieval (CLIR), in an approach often called structured query translation. In contrast, language models incorporate translation probabilities into a unified framework. We compare the two approaches on Arabic and Spanish data sets, using two kinds of bilingual dictionaries--one derived from a conventional dictionary, and one derived from a parallel corpus. We find that structured query processing gives slightly better results when queries are not expanded. On the other hand, when queries are expanded, language modeling gives better results, but only when using a probabilistic dictionary derived from a parallel corpus.We pursue two additional issues inherent in the comparison of structured query processing with language modeling. The first concerns query expansion, and the second is the role of translation probabilities. We compare conventional expansion techniques (pseudo-relevance feedback) with relevance modeling, a new IR approach which fits into the formal framework of language modeling. We find that relevance modeling and pseudo-relevance feedback achieve comparable levels of retrieval and that good translation probabilities confer a small but significant advantage.",
        "A comparison of time-aware ranking methods. When searching a temporal document collection, e.g., news archives or blogs, the time dimension must be explicitly incorporated into a retrieval model in order to improve relevance ranking. Previous work has followed one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, or 2) a probabilistic model generating a query from the textual and temporal part of a document independently. In this paper, we compare the effectiveness of different time-aware ranking methods by using a mixture model applied to all methods. Extensive evaluation is conducted using the New York Times Annotated Corpus, queries and relevance judgments obtained using the Amazon Mechanical Turk.",
        "Axiomatic analysis and optimization of information retrieval models. Axiomatic approach provides a systematic way to think about heuristics, identify the weakness of existing methods, and optimize the existing methods accordingly. This tutorial aims to promote axiomatic thinking that can benefit not only the study of IR models but also the methods for many IR applications. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval modelsGeneral Terms: Algorithms Keywords: axiomatic analysis; information retrieval models; retrieval constraints; optimization OVERVIEWOne of the most important research problems in Information Retrieval (IR) is to develop optimal general IR models. Since IR models determine how to compute the relevance score of a document for a given query, they directly affect the accuracy of a search engine. An improved retrieval model is expected to enable every search engine to achieve higher search accuracy. Thus, it has been a long-standing challenge to develop a general effective retrieval model.Over the past decades, significant amount of research has focused on developing and studying various retrieval models. These efforts have led to a few state of the art retrieval functions including Pivoted normalization function from the vector space model, Okapi BM25 from the classical probabilistic model, Dirichlet prior smoothing from the language modeling approach and PL2 from the divergence from randomness model. However, it has been shown to be difficult to further improve these state of the art retrieval functions.Recently, there has been a promising breakthrough in IR model research where a novel axiomatic framework is used to model the relevance, diagnose deficiencies of existing IR models and improve them. In particular, the basic idea is to model the relevance more directly with formally defined Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6-11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2611178. mathematical constraints on retrieval functions which capture desirable properties of a reasonable retrieval function. These constraints enable analytical comparison of retrieval functions to assess their effectiveness and provide guidance on developing more effective retrieval functions. Moreover, the axiomatic analysis and optimization of IR models enabled deeper understanding of deficiencies of existing IR models through constraint analysis and diagnostic evaluation, and has led to the improvement of multiple state of the art IR models. While the existing work have mostly explored this approach for optimizing retrieval models, the basic idea of axiomatic analysis is quite general and can be potentially useful for optimizing models and methods in many other problem domains.This tutorial provides a systematical review and explanation of this promising axiomatic approach to developing optimal IR models. We introduce the basic idea and methodology of applying axiomatic thinking to develop effective retrieval models, summarize the research work done in this area, and discuss promising future research challenges and opportunities. More information about this topic is available at http://www.eecis.udel.edu/~hfang/AX.html. TUTORIAL OUTLINE",
        "Evaluating and Analyzing Click Simulation in Web Search. We evaluate and analyze the quality of click models with respect to their ability to simulate users' click behavior. To this end, we propose distribution-based metrics for measuring the quality of click simulation in addition to metrics that directly compare simulated and real clicks. We perform a comparison of widely-used click models in terms of the quality of click simulation and analyze this quality for queries with di erent frequencies. We nd that click models fail to accurately simulate user clicks, especially when simulating sessions with no clicks and sessions with a click on the rst position. We also nd that click models with higher click prediction performance simulate clicks be er than other models.",
        "A Study of Document Expansion using Translation Models and Dimensionality Reduction Methods. Over a decade of research on document expansion methods resulted in several independent avenues, including smoothing methods, translation models, and dimensionality reduction techniques, such as matrix decompositions and topic models. Although these research avenues have been individually explored in many previous studies, there is still a lack of understanding of how state-of-the-art methods for each of these directions compare with each other in terms of retrieval accuracy. This paper attempts to fill in this void by reporting the results of an empirical comparison of document expansion methods using translation models estimated based on word co-occurrence and cosine similarity between low-dimensional word embeddings, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), on standard TREC collections. Experimental results indicate that LDA-based document expansion consistently outperforms both types of translation models and NMF according to all evaluation metrics for all and difficult queries, which is closely followed by translation model using word embeddings.",
        "Simulation of User Judgments in Bibliographic Retrieval Systems Abs t ractThe general model and simulation algorithms for bibliographic retrieval systems presented in an earlier paper I are expanded.The new model integrates the physical as well as the logical and semantic elements of these systems. A modified algorithm is developed for the simulation of user relevance judgments, and is validated, by means of recall-precision curves and a Kolmogorov-Smirnov test of recall, for two test collections.Other approaches to goodness-of-fit testing are suggested.Zeigler 2 defines a real system as a part of the world which is a source of behavioral data, a model as a set of instructions for generating such behavioral data, and a computer simulati'on as the computational process which, by means of a suitable encoding of the model instructions, can actually generate the data.The real systems described by the model and simulation algorithms in this paper are bibliographic retrieval systems, i.e., systems which provide data in the form of references or document descriptions relating to an informational query.These systems presently exist in a variety of commercial, experimental, automated, and non-automated forms.The purpose in modeling and simulating such systems is to attempt to optimize certain aspects of their operation, notably the effectiveness of the document and query representations and the efficiency of the accessing algorithms and associated data structures.Simulation permits a controlled variation of such parameters as indexing exhaustivity, vocabulary size, document set size, query exhaustivity, and search expression structure,",
        "Axiomatic Analysis and Optimization of Information Retrieval Models. The accuracy of a search engine is mostly determined by the optimality of the retrieval model used in the search engine. Develoing optimal retrieval models has always been a very important fundamental research problem in information retrieval because an improved general retrieval model would enable all search engines to be more useful, thus have immediate broad impact. Extensive research has been done on developing an optimal retrieval model since 1960s, leading to multiple effective retrieval models, including, e.g., Pivoted Normalization Vector Space model, BM25, Dirichlet Prior Query Likelihood, and PL2. However, these state of the art retrieval models were all developed at least a decade ago, suggesting that it has been difficult to further improve them. One reason why we could not easily improve these models is because we do not have a good understanding of their deficiencies and have mostly relied on empirical evaluation to assess the superiority of a retrieval model.Recently, an axiomatic way of analyzing and optimizing retrieval models has been developed and shown great promise in both understanding the deficiencies of retrieval models and developing more effective ones. The basic idea of this axiomatic framework is to specify a number of formal constraints that an optimal retrieval model is expected to satisfy, and use them to assess the optimality of a retrieval model. Such an axiomatic way of modeling relevance provides a theoretical way to study how to develop an ultimately optimal retrieval model, enables analytical comparison of different retrieval models without necessarily requiring empirical evaluation, and has led to the development of multiple more effective retrieval models.The purpose of this tutorial is to systematically explain this emerging axiomatic approach to developing optimal retrieval models, review and summarize the research progress achieved so far on this topic, and discuss promising future research directions in optimizing general retrieval models. Tutorial attendees can expect to learn, among others, (1) the basic methodology of axiomatic analysis and optimization of retrieval models, (2) how to formalize retrieval heuristics with mathematical constraints, (3) the major retrieval Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). ICTIR '13, Sep 29 -Oct 02 2013, Copenhagen, Denmark ACM 978-1-4503-2107-5/13/09 http://dx.doi.org/10.1145 constraints proposed so far, (4) the new retrieval functions derived by using the axiomatic approaches, (5) specific research directions to further develop more effective retrieval models, and general open challenges in developing an ultimately optimal retrieval model. The tutorial should appeal to those who work on information retrieval models and those who are interested in applying axiomatic analysis to optimize specific retrieval functions in real applications. The tutorial should also be interesting to researchers who work on ranking problems in general. Attendees will be assumed to know the basic concepts in information retrieval models.",
        "A Reproducibility Study of Information Retrieval Models. Developing effective information retrieval models has been a long standing challenge in Information Retrieval (IR), and significant progresses have been made over the years. With the increasiqng number of developed retrieval functions and the release of new data collections, it becomes more difficult, if not impossible, to compare a new retrieval function with all existing retrieval functions over all available data collections. To tackle this problem, this paper describes our efforts on constructing a platform that aims to improve the reproducibility of IR research and facilitate the evaluation and comparison of retrieval functions. With the developed platform, more than 20 state of the art retrieval functions have been implemented and systematically evaluated over 16 standard TREC collections (including the newly released ClueWeb datasets). Our reproducibility study leads to several interesting observations. First, the performance difference between the reproduced results and those reported in the original papers is small for most retrieval functions. Second, the optimal performance of a few representative retrieval functions is still comparable over the new TREC ClueWeb collections. Finally, the developed platform (i.e., RISE) is made publicly available so that any IR researchers would be able to utilize it to evaluate other retrieval functions."
    ],
    "Cache architecture": [
        "Active caching for similarity queries based on shared-neighbor information. Novel applications such as recommender systems, uncertain databases, and multimedia databases are designed to process similarity queries that produce ranked lists of objects as their results. Similarity queries typically result in disk access latency and incur a substantial computational cost. In this paper, we propose an 'active caching' technique for similarity queries that is capable of synthesizing query results from cached information even when the required result list is not explicitly stored in the cache. Our solution, the Cache Estimated Significance (CES) model, is based on sharedneighbor similarity measures, which assess the strength of the relationship between two objects as a function of the number of other objects in the common intersection of their neighborhoods. The proposed method is general in that it does not require that the features be drawn from a metric space, nor does it require that the partial orders induced by the similarity measure be monotonic. Experimental results on real data sets show a substantial cache hit rate when compared with traditional caching approaches.",
        "Caching to Reduce Mobile App Energy Consumption Mobile applications consume device energy for their operations, and the fast rate of battery depletion on mobile devices poses a major usability hurdle. After the display, data communication is the second-biggest consumer of mobile device energy. At the same time, software applications that run on mobile devices represent a fast-growing product segment. Typically, these applications serve as front-end display mechanisms, which fetch data from remote servers and display the information to the user in an appropriate formatincurring significant data communication overheads in the process. In this work, we propose methods to reduce energy overheads in mobile devices due to data communication by leveraging data caching technology. A review of existing caching mechanisms revealed that they are primarily designed for optimizing response time performance and cannot be easily ported to mobile devices for energy savings. Further, architectural differences between traditional client-server and mobile communications infrastructures make the use of existing caching technologies unsuitable in mobile devices. In this article, we propose a set of two new caching approaches specifically designed with the constraints of mobile devices in mind: (a) a response caching approach and (b) an object caching approach. Our experiments show that, even for a small cache size of 250MB, object caching can reduce energy consumption on average by 45% compared to the no-cache case, and response caching can reduce energy consumption by 20% compared to the no-cache case. The benefits increase with larger cache sizes. These results demonstrate the efficacy of our proposed method and raise the possibility of significantly extending mobile device battery life.",
        "Caching for Realtime Search. Modern search engines feature real-time indices, which incorporate changes to content within seconds. As search engines also cache search results for reducing user latency and back-end load, without careful real-time management of search results caches, the engine might return stale search results to users despite the efforts invested in keeping the underlying index up to date. A recent paper proposed an architectural component called CIP -the cache invalidation predictor. CIPs invalidate supposedly stale cache entries upon index modifications. Initial evaluation showed the ability to keep the performance benefits of caching without sacrificing much the freshness of search results returned to users. However, it was conducted on a synthetic workload in a simplified setting, using many assumptions. We propose new CIP heuristics, and evaluate them in an authentic environment -on the real evolving corpus and query stream of a large commercial news search engine. Our CIPs operate in conjunction with realistic cache settings, and we use standard metrics for evaluating cache performance. We show that a classical cache replacement policy, LRU, completely fails to guarantee freshness over time, whereas our CIPs serve 97% of the queries with fresh results. Our policies incur a negligible impact on the baseline's cache hit rate, in contrast with traditional age-based invalidation, which must severely reduce the cache performance in order to achieve the same freshness. We demonstrate that the computational overhead of our algorithms is minor, and that they even allow reducing the cache's memory footprint.",
        "Architecture of a quality based intelligent proxy (QBIX) for MPEG-4 videos. Due to the increasing availability and use of digital video data on the Web, video caching will be an important performance factor in the future WWW. We propose an architecture of a video proxy cache that integrates modern multimedia and communication standards. Especially we describe features of the MPEG-4 and MPEG-7 multimedia standards that can be helpful for a video proxy cache.QBIX supports real-time adaptation in the compressed and in the decompressed domain. It uses adaptation to improve the cache replacement strategies in the proxy, but also to realize media gateway functionality driven by the clients' terminal capabilities.",
        "RCached-tree: an index structure for efficiently answering popular queries. In many applications of similarity searching in databases, a set of similar queries appear more frequently. Since it is rare that a query point with its associated parameters (range or number of nearest neighbors) will repeat exactly, intelligent caching mechanisms are required to efficiently answer such queries. In addition, the performance of non-repeating and non-cached queries should not suffer too much either. In this paper, we propose RCached-tree, belonging to the family of R-trees, that aims to solve this problem. In every internal node of the tree up to a certain level, a portion of the space is reserved for storing popular queries and their solutions. For a new query that is encompassed by a cached query, this enables bypassing the traversal of lower levels of the subtree corresponding to the node as the answers can be obtained directly from the result set of the cached query. The structure adapts itself to varying query patterns; new popular queries replace the old cached ones that are not popular any more. Queries that are not popular as well as insertions, deletions and updates are handled in the same manner as in a general R-tree. Experiments show that the RCached-tree can outperform R-tree and other such structures by a significant margin when the proportion of popular queries is 20% or more by reserving 30-40% of the internal nodes as cache.",
        "Latency-aware strategy for static list caching in flash-based web search engines. Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing algorithms even on HDD-based architecture.",
        "Efficient URL caching for world wide web crawling. Crawling the web is deceptively simple: the basic algorithm is (a) Fetch a page (b) Parse it to extract all linked URLs (c) For all the URLs not seen before, repeat (a)-(c). However, the size of the web (estimated at over 4 billion pages) and its rate of change (estimated at 7% per week) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge. Indeed, these two factors alone imply that for a reasonably fresh and complete crawl of the web, step (a) must be executed about a thousand times per second, and thus the membership test (c) must be done well over ten thousand times per second against a set too large to store in main memory. This requires a distributed architecture, which further complicates the membership test.A crucial way to speed up the test is to cache, that is, to store in main memory a (dynamic) subset of the \"seen\" URLs. The main goal of this paper is to carefully investigate several URL caching techniques for web crawling. We consider both practical algorithms: random replacement, static cache, LRU, and CLOCK, and theoretical limits: clairvoyant caching and infinite cache. We performed about 1,800 simulations using these algorithms with various cache sizes, using actual log data extracted from a massive 33 day web crawl that issued over one billion HTTP requests.Our main conclusion is that caching is very effective -in our setup, a cache of roughly 50,000 entries can achieve a hit rate of almost 80%. Interestingly, this cache size falls at a critical point: a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit. We conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon.",
        "Cache Design of SSD-Based Search Engine Architectures: An Experimental Study Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines.",
        "A five-level static cache architecture for web search engines a b s t r a c tCaching is a crucial performance component of large-scale web search engines, as it greatly helps reducing average query response times and query processing workloads on backend search clusters. In this paper, we describe a multi-level static cache architecture that stores five different item types: query results, precomputed scores, posting lists, precomputed intersections of posting lists, and documents. Moreover, we propose a greedy heuristic to prioritize items for caching, based on gains computed by using items' past access frequencies, estimated computational costs, and storage overheads. This heuristic takes into account the inter-dependency between individual items when making its caching decisions, i.e., after a particular item is cached, gains of all items that are affected by this decision are updated. Our simulations under realistic assumptions reveal that the proposed heuristic performs better than dividing the entire cache space among particular item types at fixed proportions.",
        "Alternative Architectures and Protocols for Providing Strong Consistency in Dynamic Web Applications. Dynamic Web applications have gained a great deal of popularity. Improving the performance of these applications has recently attracted the attention of many researchers. One of the most important techniques proposed for this purpose is caching, which can be done at different locations and within different stages of the process of generating a dynamic Web page. Most of the caching schemes proposed in literature are lenient about the issue of consistency; they assume that users can tolerate receiving stale data. However, an important class of dynamic Web applications are those in which users always expect to get the freshest data available. Any caching scheme has to incur a significant overhead to be able to provide this level of consistency (i.e., strong consistency); the overhead may be so much that it neutralizes the benefits of caching. In this paper, three alternative architectures are investigated for dynamic Web applications that require strong consistency. A proxy caching scheme is designed and implemented, which performs caching at the level of database queries. This caching system is used in one of the alternative architectures. The performance experiments show that, despite the high overhead of providing strong consistency in database caching, this technique can improve the performance of dynamic Web applications, especially when there is a long network latency between clients and the (origin) server.",
        "How Caching Improves Efficiency and Result Completeness for Querying Linked Data Link traversal based query execution is a novel query approach which enables applications that exploit the Web of Data to its full potential. This approach makes use of the characteristics of Linked Data: During query execution it traverses data links to discover data that may contribute to query results. Once retrieved from the Web, the data can be cached and reused for subsequent queries. We expect such a reuse to be beneficial for two reasons: First, it may improve query performance because it reduces the need to retrieve data multiple times; second, it may provide for additional query results, calculated based on cached data that would not be discoverable by a link traversal based execution alone. However, no systematic analysis exist that justifies the application of caching strategies based on these assumptions. In this paper we evaluate the potential of caching to improve efficiency and result completeness in link traversal based query execution systems. We conceptually analyze the potential benefit of keeping and reusing retrieved data. Furthermore, we verify the theoretical impact of caching by conducting a comprehensive experiment that is based on a real-world application scenario.",
        "The impact of solid state drive on search engine cache management. Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSDbased search engines.",
        "Cache architecture for on-demand streaming on the Web On-demand streaming from a remote server through best-effort Internet poses several challenges because of network losses and variable delays. The primary technique used to improve the quality of distributed content service is replication. In the context of the Internet, Web caching is the traditional mechanism that is used. In this article we develop a new staged delivery model for a distributed architecture in which video is streamed from remote servers to edge caches where the video is buffered and then streamed to the client through a last-mile connection. The model uses a novel revolving indexed cache buffer management mechanism at the edge cache and employs selective retransmissions of lost packets between the remote and edge cache for a best-effort recovery of the losses. The new Web cache buffer management scheme includes a dynamic adjustment of cache buffer parameters based on network conditions. In addition, performance of buffer management and retransmission policies at the edge cache is modeled and assessed using a probabilistic analysis of the streaming process as well as system simulations. The influence of different endogenous control parameters on the quality of stream received by the client is studied. Calibration curves on the QoS metrics for different network conditions have been obtained using simulations. Edge cache management can be done using these calibration curves. ISPs can make use of calibration curves to set the values of the endogenous control parameters for specific QoS in real-time streaming operations based on network conditions. A methodology to benchmark transmission characteristics using real-time traffic data is developed to enable effective decision making on edge cache buffer allocation and management strategies.",
        "A metric cache for similarity search. Similarity search in metric spaces is a general paradigm that can be used in several application fields. It can also be effectively exploited in content-based image retrieval systems, which are shifting their target towards the Web-scale dimension. In this context, an important issue becomes the design of scalable solutions, which combine parallel and distributed architectures with caching at several levels.To this end, we investigate the design of a similarity cache that works in metric spaces. It is able to answer with exact and approximate results: even when an exact match is not present in cache, our cache may return an approximate result set with quality guarantees. By conducting tests on a collection of one million high-quality digital photos, we show that the proposed caching techniques can have a significant impact on performance, like caching on text queries has been proved effective for traditional Web search engines.",
        "Cache-oblivious nested-loop joins. We propose to adapt the newly emerged cache-oblivious model to relational query processing. Our goal is to automatically achieve an overall performance comparable to that of fine-tuned algorithms on a multi-level memory hierarchy. This automaticity is because cache-oblivious algorithms assume no knowledge about any specific parameter values, such as the capacity and block size of each level of the hierarchy. As a first step, we propose recursive partitioning to implement cache-oblivious nested-loop joins (NLJs) without indexes, and recursive clustering and buffering to implement cache-oblivious NLJs with indexes. Our theoretical results and empirical evaluation on three different architectures show that our cache-oblivious NLJs match the performance of their manually optimized, cache-conscious counterparts.",
        "Three-Level Caching for Efficient Query Processing in Large Web Search Engines. Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395"
    ],
    "Document scoping formula": [
        "A Theoretical Analysis of Google's PageRank. Our work starts from the definition of an intuitive formula that can be used to order the Web pages according to their importance, showing the need of a modification of this formula on a mathematical basis. Following the thread of this argument we get to a well-founded general formula, that covers many interesting different cases, and among them that of PageRank, the algorithm used by the Google search engine, as it is currently proposed in recent works . Then we prove the substantial equivalence between this PageRank formula and the classic formula proposed in . As an example of the versatility of our general formula we derive from it a version of PageRank based on a user personalization. Finally, we discuss the problem of the \"objectivity\" of classic PageRank, demonstrating that a certain degree of subjectivity persists, since the order of Web pages given by this algorithm depends on the value of a parameter.",
        "Two-Stage Document Length Normalization for Information Retrieval The standard approach for term frequency normalization is based only on the document length. However, it does not distinguish the verbosity from the scope, these being the two main factors determining the document length. Because the verbosity and scope have largely different effects on the increase in term frequency, the standard approach can easily suffer from insufficient or excessive penalization depending on the specific type of long document. To overcome these problems, this article proposes two-stage normalization by performing verbosity and scope normalization separately, and by employing different penalization functions. In verbosity normalization, each document is prenormalized by dividing the term frequency by the verbosity of the document. In scope normalization, an existing retrieval model is applied in a straightforward manner to the prenormalized document, finally leading us to formulate our proposed verbosity normalized",
        "Concept-based document readability in domain specific information retrieval. Domain specific information retrieval has become in demand. Not only domain experts, but also average non-expert users are interested in searching domain specific (e.g., medical and health) information from online resources. However, a typical problem to average users is that the search results are always a mixture of documents with different levels of readability. Non-expert users may want to see documents with higher readability on the top of the list. Consequently the search results need to be re-ranked in a descending order of readability. It is often not practical for domain experts to manually label the readability of documents for large databases. Computational models of readability needs to be investigated. However, traditional readability formulas are designed for general purpose text and insufficient to deal with technical materials for domain specific information retrieval. More advanced algorithms such as textual coherence model are computationally expensive for re-ranking a large number of retrieved documents. In this paper, we propose an effective and computationally tractable concept-based model of text readability. In addition to textual genres of a document, our model also takes into account domain specific knowledge, i.e., how the domain-specific concepts contained in the document affect the document's readability. Three major readability formulas are proposed and applied to health and medical information retrieval. Experimental results show that our proposed readability formulas lead to remarkable improvements in terms of correlation with users' readability ratings over four traditional readability measures."
    ],
    "Pseudo-relevance feedback": [
        "Mining Specific and General Features in Both Positive and Negative Relevance Feedback: QUT E-Discovery Lab at the TREC 2010 Relevance Feedback Track User relevance feedback is usually utilized by Web systems to interpret user information needs and retrieve effective results for users. However, how to discover useful knowledge in user relevance feedback and how to wisely use the discovered knowledge are two critical problems. However, understanding what makes an individual document good or bad for feedback can lead to the solution of the previous problem. In TREC 2010, we participated in the Relevance Feedback Track and experimented two models for extracting pseudo-relevance feedback to improve the ranking of retrieved documents. The first one, the main run, was a pattern-based model, whereas the second one, the optional run, was a term-based model. The two models consisted of two stages: one using relevance feedback provided by TREC'10 to expand queries to extract pseudo-relevance feedback; one using pseudo-relevance feedback to find useful patterns and terms according to their relevance and irrelevance judgements to rank documents. In this paper, the detailed description of those models is presented.",
        "Simple questions to improve pseudo-relevance feedback results. We explore interactive methods to further improve the performance of pseudo-relevance feedback. Studies suggest that new methods for tackling difficult queries are required. Our approach is to gather more information about the query from the user by asking her simple questions. The equally simple responses are used to modify the original query. Our experiments using the TREC Robust Track queries show that we can obtain a significant improvement in mean average precision averaging around 5% over pseudo-relevance feedback. This improvement is also spread across more queries compared to ordinary pseudo-relevance feedback, as suggested by geometric mean average precision.",
        "A new probabilistic retrieval model based on the dirichlet compound multinomial distribution. The classical probabilistic models attempt to capture the Ad hoc information retrieval problem within a rigorous probabilistic framework. It has long been recognized that the primary obstacle to effective performance of the probabilistic models is the need to estimate a relevance model. The Dirichlet compound multinomial (DCM) distribution , which relies on hierarchical Bayesian modeling techniques, or the Polya Urn scheme, is a more appropriate generative model than the traditional multinomial distribution for text documents. We explore a new probabilistic model based on the DCM distribution, which enables efficient retrieval and accurate ranking. Because the DCM distribution captures the dependency of repetitive word occurrences, the new probabilistic model is able to model the concavity of the score function more effectively. To avoid the empirical tuning of retrieval parameters, we design several parameter estimation algorithms to automatically set model parameters. Additionally, we propose a pseudo-relevance feedback algorithm based on the latent mixture modeling of the Dirichlet compound multinomial distribution to further improve retrieval accuracy. Finally, our experiments show that both the baseline probabilistic retrieval algorithm based on the DCM distribution and the corresponding pseudo-relevance feedback algorithm outperform the existing language modeling systems on several TREC retrieval tasks.",
        "Regularized estimation of mixture models for robust pseudo-relevance feedback. Pseudo-relevance feedback has proven to be an effective strategy for improving retrieval accuracy in all retrieval models. However the performance of existing pseudo feedback methods is often affected significantly by some parameters, such as the number of feedback documents to use and the relative weight of original query terms; these parameters generally have to be set by trial-and-error without any guidance. In this paper, we present a more robust method for pseudo feedback based on statistical language models. Our main idea is to integrate the original query with feedback documents in a single probabilistic mixture model and regularize the estimation of the language model parameters in the model so that the information in the feedback documents can be gradually added to the original query. Unlike most existing feedback methods, our new method has no parameter to tune. Experiment results on two representative data sets show that the new method is significantly more robust than a state-of-the-art baseline language modeling approach for feedback with comparable or better retrieval accuracy.",
        "Positional relevance model for pseudo-relevance feedback. Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.",
        "An incremental approach to efficient pseudo-relevance feedback. Pseudo-relevance feedback is an important strategy to improve search accuracy. It is often implemented as a tworound retrieval process: the first round is to retrieve an initial set of documents relevant to an original query, and the second round is to retrieve final retrieval results using the original query expanded with terms selected from the previously retrieved documents. This two-round retrieval process is clearly time consuming, which could arguably be one of main reasons that hinder the wide adaptation of the pseudorelevance feedback methods in real-world IR systems.In this paper, we study how to improve the efficiency of pseudo-relevance feedback methods. The basic idea is to reduce the time needed for the second round of retrieval by leveraging the query processing results of the first round. Specifically, instead of processing the expand query as a newly submitted query, we propose an incremental approach, which resumes the query processing results (i.e. document accumulators) for the first round of retrieval and process the second round of retrieval mainly as a step of adjusting the scores in the accumulators. Experimental results on TREC Terabyte collections show that the proposed incremental approach can improve the efficiency of pseudo-relevance feedback methods by a factor of two without sacrificing their effectiveness.",
        "Extending Relevance Model for Relevance Feedback Relevance feedback is the retrieval task where the system is given not only an information need, but also some relevance judgement information, usually from users' feedback for an initial result list by the system. With different amount of feedback information available, the optimal feedback strategy might be very different. In TREC Relevance Feedback task, the system is given different sets of feedback information from 1 relevant document to over 40 judgements with at least 3 relevant. Thus, in this work, we try to develop a feedback algorithm that works well on all levels of feedback by extending the relevance model for pseudo relevance feedback to include judged relevant documents when scoring feedback terms. Within these different levels of feedback, it is more difficult for the feedback algorithm to perform well when given minimal amount of feedback. Experiments show that our algorithm performs robustly in those difficult cases.",
        "A deterministic resampling method using overlapping document clusters for pseudo-relevance feedback a b s t r a c tTypical pseudo-relevance feedback methods assume the top-retrieved documents are relevant and use these pseudo-relevant documents to expand terms. The initial retrieval set can, however, contain a great deal of noise. In this paper, we present a cluster-based resampling method to select novel pseudo-relevant documents based on Lavrenko's relevance model approach. The main idea is to use overlapping clusters to find dominant documents for the initial retrieval set, and to repeatedly use these documents to emphasize the core topics of a query.The proposed resampling method can skip some documents in the initial high-ranked documents and deterministically construct overlapping clusters as sampling units. The hypothesis behind using overlapping clusters is that a good representative document for a query may have several nearest neighbors with high similarities, participating in several different clusters. Experimental results on large-scale web TREC collections show significant improvements over the baseline relevance model.To justify the proposed approach, we examine the relevance density and redundancy ratio of feedback documents. A higher relevance density will result in greater retrieval accuracy, ultimately approaching true relevance feedback. The resampling approach shows higher relevance density than the baseline relevance model on all collections, resulting in better retrieval accuracy in pseudo-relevance feedback.",
        "A query term re-weighting approach using document similarity a b s t r a c tPseudo-relevance feedback is the basis of a category of automatic query modification techniques. Pseudo-relevance feedback methods assume the initial retrieved set of documents to be relevant. Then they use these documents to extract more relevant terms for the query or just re-weigh the user's original query. In this paper, we propose a straightforward, yet effective use of pseudo-relevance feedback method in detecting more informative query terms and re-weighting them. The query-by-query analysis of our results indicates that our method is capable of identifying the most important keywords even in short queries. Our main idea is that some of the top documents may contain a closer context to the user's information need than the others. Therefore, re-examining the similarity of those top documents and weighting this set based on their context could help in identifying and re-weighting informative query terms. Our experimental results in standard English and Persian test collections show that our method improves retrieval performance, in terms of MAP criterion, up to 7% over traditional query term re-weighting methods."
    ],
    "How to represent natural conversations in word nets": [
        "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge. Data-driven supervised approaches, especially the ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single-and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. In the meanwhile, we discuss the pros and cons of these approaches and provide their experimental results on benchmark databases. We expect that this overview can facilitate the development of the robustness of speech recognition systems in acoustic noisy environments.",
        "Out-of-context noun phrase semantic interpretation with cross-linguistic evidence. The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text. Motivated by the problem of building a noun phrase-level semantic parser and adapting it to various applications, such as machine translation and multilingual question answering, in this paper we present a domainindependent model for noun phrase semantic interpretation. We investigate the problem based on cross-linguistic evidence from a set of four Romance languages: Spanish, Italian, French, and Romanian. The focus on Romance languages is well motivated. It is generally the case that English noun phrases translate into constructions of the form \"N P N \" in Romance languages where, as we will show, the P (preposition) varies in ways that correlate with the semantics. Thus, based on a set of 22 semantic interpretation categories (such as part-whole, agent, possession) we present empirical observations regarding the distribution of these semantic categories in a cross-lingual corpus and their mapping to various syntactic constructions in English and Romance. Furthermore, given a training set of English noun phrases along with their translations in the four Romance languages, our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation. Experimental results are compared against a state-of-the-art model reported in the literature.",
        "Representing Mathematical Formulae in Content MathML using Wikidata In this paper, we describe how to represent mathematical formulae in Content MathML referring to the open knowledge-base Wikidata for the grounding of the semantics. By doing so, we link identifiers and symbols in MathML to Wikidata items to annotate mathematical identifiers or operators. In contrast to other mathematical knowledgebases, which define symbols in a deductive fashion, the terms in Wikidata emerged inductively from Wikipedia articles in different languages. In this context, we discuss the term of a mathematical formula content and its relation to the data representation in Content MathML and Wikidata in detail.",
        "Hierarchical Variational Memory Network for Dialogue Generation. Dialogue systems help various real applications interact with humans in an intelligent natural way. In dialogue systems, the task of dialogue generation aims to generate utterances given previous utterances as contexts. Among various spectrums of dialogue generation approaches, end-to-end neural generation models have received an increase of attention. These end-to-end neural generation models are capable of generating natural-sounding sentences with a unified neural encoder-decoder network structure. The end-to-end structure sequentially encodes each word in an input context and generates the response word-by-word deterministically during decoding. However, lack of variation and limited ability in capturing long-term dependencies between utterances still challenge existing approaches. In this paper, we propose a novel hierarchical variational memory network (HVMN), by adding the hierarchical structure and the variational memory network into a neural encoder-decoder network. By emulating human-to-human dialogues, our proposed method can capture both the high-level abstract variations and long-term memories during dialogue tracking, which enables the random access of relevant dialogue histories. Extensive experiments conducted on three large real-world datasets verify a significant improvement of our proposed model against state-of-the-art baselines for dialogue generation.",
        "KNET: A General Framework for Learning Word Embedding Using Morphological Knowledge Neural network techniques are widely applied to obtain high-quality distributed representations of words (i.e., word embeddings) to address text mining, information retrieval, and natural language processing tasks. Most recent efforts have proposed several efficient methods to learn word embeddings from context such that they can encode both semantic and syntactic relationships between words. However, it is quite challenging to handle unseen or rare words with insufficient context. Inspired by the study on the word recognition process in cognitive psychology, in this article, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both words' contextual information and morphological knowledge to learn word embeddings. Meanwhile, this new learning architecture is also able to benefit from noisy knowledge and balance between contextual information and morphological knowledge. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.",
        "Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification Phrase/Sentence representation is one of the most important problems in natural language processing. Many neural network models such as Convolutional Neural Network (CNN), Recursive Neural Network (RNN), and Long Short-Term Memory (LSTM) have been proposed to learn representations of phrase/sentence, however, rich syntactic knowledge has not been fully explored when composing a longer text from its shorter constituent words. In most traditional models, only word embeddings are utilized to compose phrase/sentence representations, while the syntactic information of words is yet to be explored. In this article, we discover that encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation. Specifically, we propose to learn tag-specific composition functions and tag embeddings in recursive neural networks, and propose to utilize POS tags to control the gates of tree-structured LSTM networks. We evaluate these models on two benchmark datasets for sentiment classification, and demonstrate that improvements can be obtained with such syntactic knowledge encoded.",
        "Conversation specification: a new approach to design and analysis of e-service composition. This paper introduces a framework for modeling and specifying the global behavior of e-service compositions. Under this framework, peers (individual e-services) communicate through asynchronous messages and each peer maintains a queue for incoming messages. A global \"watcher\" keeps track of messages as they occur. We propose and study a central notion of a \"conversation\", which is a sequence of (classes of) messages observed by the watcher. We consider the case where the peers are represented by Mealy machines (finite state machines with input and output). The sets of conversations exhibit unexpected behaviors. For example, there exists a composite e-service based on Mealy peers whose set of conversations is not context free (and not regular). (The set of conversations is always context sensitive.) One cause for this is the queuing of messages; we introduce an operator \"prepone\" that simulates queue delays from a global perspective and show that the set of conversations of each Mealy e-service is closed under prepone. We illustrate that the global prepone fails to completely capture the queue delay effects and refine prepone to a \"local\" version on conversations seen by individual peers. On the other hand, Mealy implementations of a composite e-service will always generate conversations whose \"projections\" are consistent with individual e-services. We use projection-join to reflect such situations. However, there are still Mealy peers whose set of conversations is not the local prepone and projection-join closure of any regular language. Therefore, we propose conversation specifications as a formalism to define the conversations allowed by an e-service composition. We give two technical results concerning the interplay between the local behaviors of Mealy peers and the global behaviors of their compositions. One result shows that for each regular language L, its local prepone and projection-join closure corresponds to the set of conversations by some Mealy peers effectively constructed from L. The second result gives a condition on the shape of a composition which guarantees that the set of conversations that can be realized is the local prepone and projection-join closure of a regular language.",
        "Addressing Overgeneration Error: An Effective and Effcient Approach to Keyphrase Extraction from Scientific Papers Keyphrases provide a concise summary of a document and play an important role for many other tasks like searching and clustering. With the large and increasing amount of online documents, automatic keyphrase extraction has attracted much attention. Existing unsupervised methods suffer from overgeneration error, since they typically identify key \"words\" and then return phrases that contain keywords as keyphrases. To alleviate this problem, we propose an unsupervised ranking scheme directly on \"phrases\" by exploring essential properties of keyphrases such as informativeness and positional preference. Experiments on two datasets show our approach significantly alleviates the overgeneration error and obtains improvement in performance over stateof-the-art keyphrase extraction approaches. Candidate word selection Keyphrase formation Candidate word scoring Suffering overgeneration error (a) Classic Scheme Candidate phrase selection Keyphrase ranking",
        "Mark my words!: linguistic style accommodation in social media. The psycholinguistic theory of communication accommodation accounts for the general observation that participants in conversations tend to converge to one another's communicative behavior: they coordinate in a variety of dimensions including choice of words, syntax, utterance length, pitch and gestures. In its almost forty years of existence, this theory has been empirically supported exclusively through smallscale or controlled laboratory studies. Here we address this phenomenon in the context of Twitter conversations. Undoubtedly, this setting is unlike any other in which accommodation was observed and, thus, challenging to the theory. Its novelty comes not only from its size, but also from the non real-time nature of conversations, from the 140 character length restriction, from the wide variety of social relation types, and from a design that was initially not geared towards conversation at all. Given such constraints, it is not clear a priori whether accommodation is robust enough to occur given the constraints of this new environment. To investigate this, we develop a probabilistic framework that can model accommodation and measure its effects. We apply it to a large Twitter conversational dataset specifically developed for this task. This is the first time the hypothesis of linguistic style accommodation has been examined (and verified) in a large scale, real world setting.Furthermore, when investigating concepts such as stylistic influence and symmetry of accommodation, we discover a complexity of the phenomenon which was never observed before. We also explore the potential relation between stylistic influence and network features commonly associated with social status."
    ],
    "Algorithm acceleration with Nvidia CUDA": [
        "CUSNTF: A Scalable Sparse Non-negative Tensor Factorization Model for Large-scale Industrial Applications on Multi-GPU. Given a high-order, large-scale and sparse data from big data and industrial applications, how can we acquire useful patterns in a real-time and low memory overhead manner? Sparse Non-negative tensor factorization (SNTF) possesses high-order representation, non-negativity and dimension reduction inherence. Thus, SNTF has become a useful tool to represent and analyze the sparse data, which has been incorporated with extra contextual information, i.e., time and location, etc, more than the matrix, which can only model the 2 ways data. However, current SNTF techniques suffer from a) non-linear time and space overhead, b) intermediate data explosion, and c) inability on GPU and multi-GPU. To address these issues, a single-thread-based SNTF is proposed, which involves the feature elements rather than on the whole factor matrices, and can avoid the forming of large-scale intermediate matrices. Then, a CUDA parallelizing singlethread-based SNTF (CUSNTF) model is proposed for industrial applications on GPU and multi-GPU (MCUSNTF). Thus, CUSNTF has linear computing and space complexity, and linear communication cost on multi-GPU. We implement CUSNTF and MCUSNTF on 8 P100 GPUs, and compare it with state-of-the-art parallel and distributed methods. Experimental results from several industrial datasets demonstrate that the linear scalability and efficiency of CUSNTF.",
        "Sorting using BItonic netwoRk wIth CUDA Novel \"manycore\" architectures, such as graphics processors, are high-parallel and high-performance shared-memory architectures [7] born to solve specific problems such as the graphical ones. Those architectures can be exploited to solve a wider range of problems by designing the related algorithm for such architectures. We present a fast sorting algorithm implementing an efficient bitonic sorting network. This algorithm is highly suitable for information retrieval applications. Sorting is a fundamental and universal problem in computer science. Even if sort has been extensively addressed by many research works, it still remains an interesting challenge to make it faster by exploiting novel technologies. In this light, this paper shows how to use graphics processors as coprocessors to speed up sorting while allowing CPU to perform other tasks. Our new algorithm exploits a memory-efficient data access pattern maintaining the minimum number of accesses to the memory out of the chip. We introduce an efficient instruction dispatch mechanism to improve the overall sorting performance. We also present a cache-based computational model for graphics processors. Experimental results highlight remarkable improvements over prior CPU-based sorting methods, and a significant improvement over previous GPU-based sorting algorithms.",
        "Extraction of topic evolutions from references in scientific articles and its GPU acceleration. This paper provides a topic model for extracting topic evolutions as a corpus-wide transition matrix among latent topics. Recent trends in text mining point to a high demand for exploiting metadata. Especially, exploitation of reference relationships among documents induced by hyperlinking Web pages, citing scientific articles, tumblring blog posts, retweeting tweets, etc., is put in the foreground of the effort for an effective mining. We focus on scholarly activities and propose a topic model for obtaining a corpus-wide view on how research topics evolve along citation relationships. Our model, called TERESA, extends latent Dirichlet allocation (LDA) by introducing a corpus-wide topic transition probability matrix, which models reference relationships as transitions among topics. Our approximated variational inference updates LDA posteriors and topic transition posteriors alternately. The main issue is execution time amounting to O(MK 2 ), where K is the number of topics and M is that of links in citation network. Therefore, we accelerate the inference with Nvidia CUDA compatible GPUs. We compare the effectiveness of TERESA with that of LDA by introducing a new measure called diversity plus focusedness (D+F). We also present topic evolution examples our method gives.",
        "Regim Lab Team at ImageCLEF Lifelog Moment Retrieval Task 2018 In this paper we describe our approach for the ImageCLE-Flifelog Moment Retrieval task. A total of five runs were submitted, which used visual features, textual features or combination. The first run was based only on the concepts gived by the organizers. In the second and third runs, we used respectively fine-tuned Googlenet and Alexnet for images description. The fourth run was based on the fusion of the two previous runs. For the fifth run, we crossed the results of our best run (based on Alexnet model) with the result of XQuery FLWOR expression applied to the XML file containing the semantic location and activities data. Our architecture is implemented using Neural Network Toolbox, Parallel Computing Toolbox and GPU coder which generates CUDA from MATLAB. The results obtained are promising for a first participation to such a task, with F1-measure@10=0.424 which placed us at third behind AILabGTi Team with 0.545 and HCMUS Team with 0.479.",
        "Density-based clustering using graphics processors. During the last few years, GPUs have evolved from simple devices for the display signal preparation into powerful coprocessors that do not only support typical computer graphics tasks but can also be used for general numeric and symbolic computation tasks. As major advantage GPUs provide extremely high parallelism combined with a high bandwidth in memory transfer at low cost. We want to exploit these advantages in density-based clustering, an important paradigm in clustering since typical algorithms of this category are noise and outlier robust and search for clusters of an arbitrary shape in metric and vector spaces. Moreover, with a time complexity ranging from O(n log n) to O(n 2 ) these algorithms are scalable to large data sets in a database system. In this paper, we propose CUDA-DClust, a massively parallel algorithm for density-based clustering for the use of a Graphics Processing Unit (GPU). While the result of this algorithm is guaranteed to be equivalent to that of DBSCAN, we demonstrate a high speed-up, particularly in combination with a novel index structure for use in GPUs."
    ],
    "mention of algorithm": [
        "The Knowledge in Multiple Human Relevance Judgments We show first that the pooling of multiple human judgments of relevance provides a predictor of relevance that is superior to that obtained from a single human's relevance judgments. A learning algorithm applied to a set of relevance judgments obtained from a single human would be expected to perform on new material at a level somewhat below that human. However, we examine two learning methods which when trained on the superior source of pooled human relevance judgments are able to perform at the level of a single human on new material. All performance comparisons are based on an independent human judge. Both algorithms function by producing term weights-one by a log odds calculation and the other by producing a least-squares fit to human relevance ratings. Some characteristics of the algorithms are examined.",
        "Induced Sorting Suffixes in External Memory We present in this article an external memory algorithm, called disk SA-IS (DSA-IS), to exactly emulate the induced sorting algorithm SA-IS previously proposed for sorting suffixes in RAM. DSA-IS is a new diskfriendly method for sequentially retrieving the preceding character of a sorted suffix to induce the order of the preceding suffix. For a size-n string of a constant or integer alphabet, given the RAM capacity ((nW ) 0.5 ), where W is the size of each I/O buffer that is large enough to amortize the overhead of each access to disk, both the CPU time and peak disk use of DSA-IS are O(n). Our experimental study shows that on average, DSA-IS achieves the best time and space results of all of the existing external memory algorithms based on the induced sorting principle.",
        "Joint Recognition and Linking of Fine-Grained Locations from Tweets. Many users casually reveal their locations such as restaurants, landmarks, and shops in their tweets. Recognizing such fine-grained locations from tweets and then linking the location mentions to well-defined location profiles (e.g., with formal name, detailed address, and geo-coordinates etc.) offer a tremendous opportunity for many applications. Different from existing solutions which perform location recognition and linking as two sub-tasks sequentially in a pipeline setting, in this paper, we propose a novel joint framework to perform location recognition and location linking simultaneously in a joint search space. We formulate this end-to-end location linking problem as a structured prediction problem and propose a beamsearch based algorithm. Based on the concept of multi-view learning, we further enable the algorithm to learn from unlabeled data to alleviate the dearth of labeled data. Extensive experiments are conducted to recognize locations mentioned in tweets and link them to location profiles in Foursquare. Experimental results show that the proposed joint learning algorithm outperforms the state-of-theart solutions, and learning from unlabeled data improves both the recognition and linking accuracy.",
        "Contextual Query Intent Extraction for Paid Search Selection. Paid Search algorithms play an important role in online advertising where a set of related ads is returned based on a searched query. The Paid Search algorithms mostly consist of two main steps. First, a given searched query is converted to different sub-queries or similar phrases which preserve the core intent of the query. Second, the generated sub-queries are matched to the ads bidded keywords in the data set, and a set of ads with highest utility measuring relevance to the original query are returned. The focus of this paper is optimizing the first step by proposing a contextual query intent extraction algorithm to generate sub-queries online which preserve the intent of the original query the best. Experimental results over a very large real-world data set demonstrate the superb performance of proposed approach in optimizing both relevance and monetization metrics compared with one of the existing successful algorithms in our system.",
        "Supervised identification and linking of concept mentions to a domain-specific ontology. We propose a pipelined supervised learning approach named SDOI to the task of interlinking the concepts mentioned within a document to the concepts within an ontology. Concept mention identification is performed by training a sequential tagging model. Each identified concept mention is then associated with a set of candidate ontology concepts along with a feature vector based on features proposed in the literature and novel ones based on new data sources, such as from the training corpus itself. An iterative algorithm is defined for handling collective features. We show a lift in performance over applicable baselines against the ability to identify the concept mentions within the 139 KDD-2009 conference paper abstracts, and to link these concept mentions to a domain-specific ontology for the field of data mining. Additional experiments of 22 ICDM-2009 abstracts suggest that the trained models are portable both in terms of accuracy and in their ability to reduce annotation time.",
        "A matrix density based algorithm to hierarchically co-cluster documents and words. This paper proposes an algorithm to hierarchically cluster documents. Each cluster is actually a cluster of documents and an associated cluster of words, thus a document-word co-cluster. Note that, the vector model for documents creates the document-word matrix, of which every co-cluster is a submatrix. One would intuitively expect a submatrix made up of high values to be a good document cluster, with the corresponding word cluster containing its most distinctive features. Our algorithm looks to exploit this. We have defined matrix density, and our algorithm basically uses matrix density considerations in its working.The algorithm is a partitional-agglomerative algorithm. The partitioning step involves the identification of dense submatrices so that the respective row sets partition the row set of the complete matrix. The hierarchical agglomerative step involves merging the most \"similar\" submatrices until we are down to the required number of clusters (if we want a flat clustering) or until we have just the single complete matrix left (if we are interested in a hierarchical arrangement of documents). It also generates apt labels for each cluster or hierarchy node. The similarity measure between clusters used for merging is based on the fact that the clusters here are co-clusters, and is a key point of difference from existing agglomerative algorithms. We will refer to the proposed algorithm as RPSA (Rowset Partitioning and Submatrix Agglomeration). We have compared it as a clustering algorithm with Spherical K-Means and Spectral Graph Partitioning. We have also evaluated some hierarchies generated by the algorithm.",
        "Joint user knowledge and matrix factorization for recommender systems. Currently, most of the existing recommendation methods treat social network users equally, which assume that the effect of recommendation on a user is decided by the user's own preferences and social influence. However, a user's own knowledge in a field has not been considered. In other words, to what extent does a user accept recommendations in social networks need to consider the user's own knowledge or expertise in the field. In this paper, we propose a novel matrix factorization recommendation algorithm based on integrating social network information such as trust relationships, rating information of users and users' own knowledge. Specifically, since we cannot directly measure a user's knowledge in the field, we first use a user's status in a social network to indicate a user's knowledge in a field, and users' status is inferred from the distributions of users' ratings and followers across fields or the structure of domain-specific social network. Then, we model the final rating of decision-making as a linear combination of the user's own preferences, social influence and user's own knowledge. Experimental results on real world data sets show that our proposed approach generally outperforms the state-of-the-art recommendation algorithms that do not consider the knowledge level difference between the users.",
        "Intelligent algorithms for improving communication patterns in thematic P2P search a b s t r a c tThe Internet is a cooperative and decentralized network built out of millions of participants that store and share large amounts of information with other users. Peer-to-peer systems go hand-in-hand with this huge decentralized network, where each individual node can serve content as well as request it. In this scenario, the analysis, development and testing of distributed search algorithms is a key research avenue. In particular, thematic search algorithms should lead to and benefit from the emergence of semantic communities that are the result of the interaction among participants. As a result, intelligent algorithms for neighbor selection should give rise to a logical network topology reflecting efficient communication patterns. This paper presents a series of algorithms which are specifically aimed at reducing the propagation of queries in the network, by applying a novel approach for learning peers' interests. These algorithms were constructed in an incremental way so that each new algorithm presents some improvements over the previous ones. Several simulations were completed to analyze the connectivity and query propagation patterns of the emergent logical networks. The results indicate that the algorithms with better behavior are those that induce greater collaboration among peers.",
        "Acquiring temporal constraints between relations. We consider the problem of automatically acquiring knowledge about the typical temporal orderings among relations (e.g., actedIn(person, film) typically occurs before wonPrize (film, award)), given only a database of known facts (relation instances) without time information, and a large document collection. Our approach is based on the conjecture that the narrative order of verb mentions within documents correlates with the temporal order of the relations they represent. We propose a family of algorithms based on this conjecture, utilizing a corpus of 890m dependency parsed sentences to obtain verbs that represent relations of interest, and utilizing Wikipedia documents to gather statistics on narrative order of verb mentions. Our proposed algorithm, GraphOrder, is a novel and scalable graph-based label propagation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions. This algorithm achieves as high as 38.4% absolute improvement in F1 over a random baseline. Finally, we demonstrate the utility of this learned general knowledge about typical temporal orderings among relations, by showing that these temporal constraints can be successfully used by a joint inference framework to assign specific temporal scopes to individual facts.",
        "A nearly-optimal Fano-based coding algorithm",
        "ShARe/CLEF eHealth 2013 Named Entity Recognition and Normalization of Disorders Challenge Objective: There are abundant mentions of clinical conditions, anatomical sites, medications and procedures in clinical documents. This paper describes use of a cascade of machine learners to automatically extract mentions of named entities about disorders from clinical notes. Tasks: A Conditional Random Field (CRF) machine learner has been used for named entity recognition and to capture more complex (multiple word) named entities we have used Support Vector Machines (SVM). Firstly, the training data was converted to the CRF format. Different feature sets were applied using 10-fold cross validation to find the best feature set for the machine learning model. Secondly, the identified named entities were passed to the SVM to find any relation among the identified disorder mentions to decide whether they are a part of a complex disorder. Approach: Our approach was based on a novel supervised learning model which incorporates two machine learning algorithms (CRF and SVM). Evaluation of each step included precision, recall and F-score metrics. Resources: We have used several tools which are created in our lab including TTSCT (Text to SNOMED CT) service, Lexical Management System (LMS) and Ring-fencing approach. A set of gazetteers was created from the training data and employed in analysis as well. Results: Evaluation results produced a precision of 0.766, recall of 0.726 and F-score of 0.746 for named entity recognition based on 10-fold cross validation; and precision, recall and F-measure of 0.927 for relation extraction based on 5-fold cross validation on the training data. On the official test data on strict mode a precision of 0.686, recall of 0.539 and F-score of 0.604 was achieved. Based on the results our team was the 11 th out of 25 participating teams. In the relaxed mode a precision of 0.912, recall of 0.701 and F-score of 0.793 was recorded and our team was the 12 th. A multi stage supervised machine learning method with mixed computational strategies seems to provide a reasonable strategy for automated extraction of disorders.",
        "Comparing Pointwise and Listwise Objective Functions for Random-Forest-Based Learning-to-Rank Current random-forest (RF)-based learning-to-rank (LtR) algorithms use a classification or regression framework to solve the ranking problem in a pointwise manner. The success of this simple yet effective approach coupled with the inherent parallelizability of the learning algorithm makes it a strong candidate for widespread adoption. In this article, we aim to better understand the effectiveness of RF-based rank-learning algorithms with a focus on the comparison between pointwise and listwise approaches.We introduce what we believe to be the first listwise version of an RF-based LtR algorithm. The algorithm directly optimizes an information retrieval metric of choice (in our case, NDCG) in a greedy manner. Direct optimization of the listwise objective functions is computationally prohibitive for most learning algorithms, but possible in RF since each tree maximizes the objective in a coordinate-wise fashion. Computational complexity of the listwise approach is higher than the pointwise counterpart; hence for larger datasets, we design a hybrid algorithm that combines a listwise objective in the early stages of tree construction and a pointwise objective in the latter stages. We also study the effect of the discount function of NDCG on the listwise algorithm.Experimental results on several publicly available LtR datasets reveal that the listwise/hybrid algorithm outperforms the pointwise approach on the majority (but not all) of the datasets. We then investigate several aspects of the two algorithms to better understand the inevitable performance tradeoffs. The aspects include examining an RF-based unsupervised LtR algorithm and comparing individual tree strength. Finally, we compare the the investigated RF-based algorithms with several other LtR algorithms.",
        "A General SIMD-Based Approach to Accelerating Compression Algorithms Compression algorithms are important for data-oriented tasks, especially in the era of \"Big Data.\" Modern processors equipped with powerful SIMD instruction sets provide us with an opportunity for achieving better compression performance. Previous research has shown that SIMD-based optimizations can multiply decoding speeds. Following these pioneering studies, we propose a general approach to accelerate compression algorithms. By instantiating the approach, we have developed several novel integer compression algorithms, called Group-Simple, Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding vectorized versions. We evaluate the proposed algorithms on two public TREC datasets, a Wikipedia dataset, and a Twitter dataset. With competitive compression ratios and encoding speeds, our SIMD-based algorithms outperform state-of-the-art nonvectorized algorithms with respect to decoding speeds.",
        "Evaluation of Folksonomy Induction Algorithms Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date.",
        "Transfer Learning for Behavior Ranking Intelligent recommendation has been well recognized as one of the major approaches to address the information overload problem in the big data era. A typical intelligent recommendation engine usually consists of three major components, that is, data as the main input, algorithms for preference learning, and system for user interaction and high-performance computation. We observe that the data (e.g., users' behavior) are usually in different forms, such as examinations (e.g., browse and collection) and ratings, where the former are often much more abundant than the latter. Although the data are in different representations, they are both related to users' true preferences and are also deemed complementary to each other for preference learning. However, very few ranking or recommendation algorithms have been developed to exploit such two types of user behavior.In this article, we focus on jointly modeling the examination behavior and rating behavior and develop a novel and efficient ranking-oriented recommendation algorithm accordingly. First, we formally define a new recommendation problem termed behavior ranking, which aims to build a ranking-oriented model by exploiting both the examination behavior and rating behavior. Second, we develop a simple and generic transfer to rank (ToR) algorithm for behavior ranking, which transfers knowledge of candidate items from a global preference learning task to a local preference learning task. Compared with the previous work on integrating heterogeneous user behavior, our ToR algorithm is the first ranking-oriented solution, which can effectively generate recommendations in a more direct manner than those regression-oriented methods. Extensive empirical studies show that our ToR algorithm performs significantly more accurately than the state-of-the-art methods in most cases. Furthermore, our ToR algorithm is very efficient in terms of the time complexity, which is similar to those for homogeneous user behavior alone.",
        "An Extension of Ukkonen's Enhanced Dynamic Programming ASM Algorithm We describe an improvement on Ukkonen's Enhanced Dynamic Programming (EHD) approximate string-matching algorithm for unit-penalty four-edit comparisons. The new algorithm has an asymptotic complexity similar to that of Ukkonen's but is significantly faster due to a decrease in the number of array cell calculations. A 42% speedup was achieved in an application involving name comparisons. Even greater improvements are possible when comparing longer and more dissimilar strings. Although the speed of the algorithm under consideration is comparable to other fast ASM algorithms, it has greater effectiveness in text-processing applications because it supports all four basic Damerau-type editing operations.",
        "An Improved Algorithm to Find the Length of the Longest Common Subsequence of Two Strings (n21ogn).We present an improvement to this algorithm which changes the time complexity to O(r + n(LLCS ). Some experimental results show dramatic improvements for large n.",
        "Detecting, categorizing and clustering entity mentions in Chinese text. The work presented in this paper is motivated by the practical need for content extraction, and the available data source and evaluation benchmark from the ACE program. The Chinese Entity Detection and Recognition (EDR) task is of particular interest to us. This task presents us several language-independent and language-dependent challenges, e.g. rising from the complication of extraction targets and the problem of word segmentation, etc. In this paper, we propose a novel solution to alleviate the problems special in the task. Mention detection takes advantages of machine learning approaches and character-based models. It manipulates different types of entities being mentioned and different constitution units (i.e. extents and heads) separately. Mentions referring to the same entity are linked together by integrating most-specific-first and closest-first rule based pairwise clustering algorithms. Types of mentions and entities are determined by head-driven classification approaches. The implemented system achieves ACE value of 66.1 when evaluated on the EDR 2005 Chinese corpus, which has been one of the top-tier results. Alternative approaches to mention detection and clustering are also discussed and analyzed.",
        "Unsupervised Concept Categorization and Extraction from Scientific Document Titles. is paper studies the automated categorization and extraction of scienti c concepts from titles of scienti c articles, in order to gain a deeper understanding of their key contributions and facilitate the construction of a generic academic knowledgebase. Towards this goal, we propose an unsupervised, domain-independent, and scalable two-phase algorithm to type and extract key concept mentions into aspects of interest (e.g., Techniques, Applications, etc.). In the rst phase of our algorithm we propose PhraseType, a probabilistic generative model which exploits textual features and limited POS tags to broadly segment text snippets into aspect-typed phrases. We extend this model to simultaneously learn aspect-speci c features and identify academic domains in multi-domain corpora, since the two tasks mutually enhance each other. In the second phase, we propose an approach based on adaptor grammars to extract ne grained concept mentions from the aspect-typed phrases without the need for any external resources or human e ort, in a purely data-driven manner. We apply our technique to study literature from diverse scienti c domains and show signi cant gains over state-of-the-art concept extraction techniques. We also present a qualitative analysis of the results obtained.",
        "Concept-based analysis of scientific literature. This paper studies the importance of identifying and categorizing scientific concepts as a way to achieve a deeper understanding of the research literature of a scientific community. To reach this goal, we propose an unsupervised bootstrapping algorithm for identifying and categorizing mentions of concepts. We then propose a new clustering algorithm that uses citations' context as a way to cluster the extracted mentions into coherent concepts. Our evaluation of the algorithms against gold standards shows significant improvement over state-of-the-art results. More importantly, we analyze the computational linguistic literature using the proposed algorithms and show four different ways to summarize and understand the research community which are difficult to obtain using existing techniques.",
        "De-anonymizing social graphs via node similarity. Recently, a number of anonymization algorithms have been developed to protect the privacy of social graph data. However, in order to satisfy higher level of privacy requirements, it is sometimes impossible to maintain sufficient utility. Is it really easy to de-anonymize \"lightly\" anonymized social graphs? Here \"light\" anonymization algorithms stand for those algorithms that maintain higher data utility. To answer this question, we proposed a de-anonymization algorithm based on a node similarity measurement. Using the proposed algorithm, we evaluated the privacy risk of several \"light\" anonymization algorithms on real datasets.",
        "AIDA-light: High-Throughput Named-Entity Disambiguation To advance the Web of Linked Data, mapping ambiguous names in structured and unstructured contents onto knowledge bases would be a vital asset. State-of-the-art methods for Named Entity Disambiguation (NED) face major tradeoffs regarding efficiency/scalability vs. accuracy. Fast methods use relatively simple context features and avoid computationally expensive algorithms for joint inference. While doing very well on prominent entities in clear input texts, these methods achieve only moderate accuracy when fed with difficult inputs. On the other hand, methods that rely on rich context features and joint inference for mapping names onto entities pay the price of being much slower. This paper presents AIDA-light which achieves high accuracy on difficult inputs while also being fast and scalable. AIDA-light uses a novel kind of two-stage mapping algorithm. It first identifies a set of \"easy\" mentions with low ambiguity and links them to entities in a very efficient manner. This stage also determines the thematic domain of the input text as an important and novel kind of feature. The second stage harnesses the high-confidence linkage for the \"easy\" mentions to establish more reliable contexts for the disambiguation of the remaining mentions. Our experiments with four different datasets demonstrates that the accuracy of AIDA-light is competitive to the very best NED systems, while its run-time is comparable to or better than the performance of the fastest systems.",
        "Candidate working set strategy based SMO algorithm in support vector machine a b s t r a c tSequential minimal optimization (SMO) is quite an efficient algorithm for training the support vector machine. The most important step of this algorithm is the selection of the working set, which greatly affects the training speed. The feasible direction strategy for the working set selection can decrease the objective function, however, may augment to the total calculation for selecting the working set in each of the iteration. In this paper, a new candidate working set (CWS) Strategy is presented considering the cost on the working set selection and cache performance. This new strategy can select several greatest violating samples from Cache as the iterative working sets for the next several optimizing steps, which can improve the efficiency of the kernel cache usage and reduce the computational cost related to the working set selection. The results of the theory analysis and experiments demonstrate that the proposed method can reduce the training time, especially on the large-scale datasets.",
        "Choosing a Candidate Using Efficient Allocation of Biased Information This article deals with a decision-making problem concerning an agent who wants to choose a partner from multiple candidates for long-term collaboration. To choose the best partner, the agent can rely on prior information he knows about the candidates. However, to improve his decision, he can request additional information from information sources. Nonetheless, acquiring information from external information sources about candidates may be biased due to different personalities of the agent searching for a partner and the information source. In addition, information may be costly. Considering the bias and the cost of the information sources, the optimization problem addressed in this article is threefold: (1) determining the necessary amount of additional information, (2) selecting information sources from which to request the information, and (3) choosing the candidates on whom to request the additional information. We propose a heuristic to solve this optimization problem. The results of experiments on simulated and real-world domains demonstrate the efficiency of our algorithm.",
        "Collective entity linking in web text: a graph-based method. Entity Linking (EL) is the task of linking name mentions in Web text with their referent entities in a knowledge base. Traditional EL methods usually link name mentions in a document by assuming them to be independent. However, there is often additional interdependence between different EL decisions, i.e., the entities in the same document should be semantically related to each other. In these cases, Collective Entity Linking, in which the name mentions in the same document are linked jointly by exploiting the interdependence between them, can improve the entity linking accuracy. This paper proposes a graph-based collective EL method, which can model and exploit the global interdependence between different EL decisions. Specifically, we first propose a graphbased representation, called Referent Graph, which can model the global interdependence between different EL decisions. Then we propose a collective inference algorithm, which can jointly infer the referent entities of all name mentions by exploiting the interdependence captured in Referent Graph. The key benefit of our method comes from: 1) The global interdependence model of EL decisions; 2) The purely collective nature of the inference algorithm, in which evidence for related EL decisions can be reinforced into high-probability decisions. Experimental results show that our method can achieve significant performance improvement over the traditional EL methods.",
        "Integrated cTAKES for Concept Mention Detection and Normalization We participated Task 1 using an existing system MedTagger implemented in integrated cTAKES (icTAKES). The concept mention detection is based on Conditional Random Fields (CRF) and the concept mention normalization is based on a greedy dictionary lookup algorithm. A distinctive feature in MedTagger compared to other concept mention detection systems is the incorporation of dictionary lookup results into a machine learning framework for sequential labeling. Dictionary lookup results of MedLex and semantic vectors representing distributed semantics were used as features. Overall, the precision, recall, and F-measure of our best run for concept mention are 0.8, 0.573, and 0.668 respectively for strict evaluation and 0.939, 0.766, and 0.844 for relaxed evaluation. The accuracy of our best run for concept mention normalization is 54.6% and 87.0% for strict and relaxed mapping, respectively.",
        "Whom to mention: expand the diffusion of tweets by @ recommendation on micro-blogging systems. Nowadays, micro-blogging systems like Twitter have become one of the most important ways for information sharing. In Twitter, a user posts a message (tweet) and the others can forward the message (retweet). Mention is a new feature in micro-blogging systems. By mentioning users in a tweet, they will receive notifications and their possible retweets may help to initiate large cascade diffusion of the tweet. To enhance a tweet's diffusion by finding the right persons to mention, we propose in this paper a novel recommendation scheme named as whom-to-mention. Specifically, we present an in-depth study of mention mechanism and propose a recommendation scheme to solve the essential question of whom to mention in a tweet. In this paper, whom-tomention is formulated as a ranking problem and we try to address several new challenges which are not well studied in the traditional information retrieval tasks. By adopting features including user interest match, content-dependent user relationship and user influence, a machine learned ranking function is trained based on newly defined information diffusion based relevance. The extensive evaluation using data gathered from real users demonstrates the advantage of our proposed algorithm compared with the traditional recommendation methods.",
        "Personalized app recommendation based on app permissions. With the development of science and technology, the popularity of smart phones has made exponential growth in mobile phone application market. How to help users to select applications they prefer has become a hot topic in recommendation algorithm. As traditional recommendation algorithms are based on popularity and download, they inadvertently fail to recommend the desirable applications. At the same time, many users tend to pay more attention to permissions of those applications, because of some privacy and security reasons. There are few recommendation algorithms which take account of apps' permissions, functionalities and users' interests altogether. Some of them only consider permissions while neglecting the users' interests, others just perform linear combination of apps' permissions, functionalities and users' interests to implement top-N recommendation. In this paper, we devise a recommendation method based on both permissions and functionalities. After demonstrating the correlation of apps' permissions and users' interests, we design an app risk score calculating method ARSM based on app-permission bipartite graph model. Furthermore, we propose a novel matrix factorization algorithm MFPF based on users' interests, apps' permissions and functionalities to handle personalized app recommendation. We compare our work with some of the state-of-the-art recommendation algorithms, and the results indicate that our work can improve the recommendation accuracy remarkably."
    ],
    "at least three authors": [
        "Venue-author-coupling: A measure for identifying disciplines through author communities",
        "Overview of the Author Identification Task at PAN 2014 The author identification task at PAN-2014 focuses on author verification. Similar to PAN-2013 we are given a set of documents by the same author along with exactly one document of questioned authorship, and the task is to determine whether the known and the questioned documents are by the same author or not. In comparison to PAN-2013, a significantly larger corpus was built comprising hundreds of documents in four natural languages (Dutch, English, Greek, and Spanish) and four genres (essays, reviews, novels, opinion articles). In addition, more suitable performance measures are used focusing on the accuracy and the confidence of the predictions as well as the ability of the submitted methods to leave some problems unanswered in case there is great uncertainty. To this end, we adopt the c@1 measure, originally proposed for the question answering task. We received 13 software submissions that were evaluated in the TIRA framework. Analytical evaluation results are presented where one language-independent approach serves as a challenging baseline. Moreover, we continue the successful practice of the PAN labs to examine meta-models based on the combination of all submitted systems. Last but not least, we provide statistical significance tests to demonstrate the important differences between the submitted approaches.",
        "FiDo: Ubiquitous Fine-Grained WiFi-based Localization for Unlabelled Users via Domain Adaptation To fully support the emerging location-aware applications, location information with meter-level resolution (or even higher) is required anytime and anywhere. Unfortunately, most of the current location sources (e.g., GPS and check-in data) either are unavailable indoor or provide only house-level resolutions. To fill the gap, this paper * The first two authors contributed equally to this paper. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.",
        "Overview of PAN 2021: Authorship Verification, Profiling Hate Speech Spreaders on Twitter, and Style Change Detection - Extended. The paper gives a brief overview of the three shared tasks to be organized at the PAN 2021 lab on digital text forensics and stylometry hosted at the CLEF conference. The tasks include authorship verification across domains, author profiling for hate speech spreaders, and style change detection for multi-author documents. In part the tasks are new and in part they continue and advance past shared tasks, with the overall goal of advancing the state of the art, providing for an objective evaluation on newly developed benchmark datasets.",
        "ECIR 2013: 35th european conference on information retrieval in Moscow (Russia). The conference was jointly organized by Yandex and Higher School of Economics (HSE). The conference became the easternmost ECIR ever.ECIR 2013 in Moscow marks an important step in the development of the conference series: geographic expansion, a high number of participants, and the largest number of accepted papers (which resulted in an impressive 900-page proceedings volume). In addition, the Moscow edition of the conference introduced some new features: a two-tier program committee, a redesigned mentoring program, a boaster session for poster presentations, and attracted many new participants.ECIR 2013 received a total of 287 submissions in three categories: 191 full papers, 78 posters, and 18 demonstrations. The geographical distribution of the submissions is as follows: 70% were from Europe (including 9% from Russia), 17% from Asia, 12% from North and South America, and 3% from the rest of the world. All submissions were reviewed by at least three members of an international two-tier Program Committee. Of the papers submitted to the main research track, 30 were selected for oral presentation and 25 for poster/short presentation (16% and 13% respectively, hence a 29% acceptance rate). In addition, 38 posters (49%) and 10 demonstrations (56%) were accepted. Out of accepted contributions 66% have a student as the primary author. Additionally, ECIR 2013 hosted four tutorials and two workshops covering various IR-related topics.",
        "Extracting and Summarizing Situational Information from the Twitter Social Media during Disasters Microblogging sites like Twitter have become important sources of real-time information during disaster events. A large amount of valuable situational information is posted in these sites during disasters; however, the information is dispersed among hundreds of thousands of tweets containing sentiments and opinions of the masses. To effectively utilize microblogging sites during disaster events, it is necessary to not only extract the situational information from the large amounts of sentiments and opinions, but also to summarize the large amounts of situational information posted in real-time. During disasters in countries like India, a sizable number of tweets are posted in local resource-poor languages besides the normal English-language tweets. For instance, in the Indian subcontinent, a large number of tweets are posted in Hindi/Devanagari (the national language of India), and some of the information contained in such non-English tweets is not available (or available at a later point of time) through English tweets. In this work, we develop a novel classification-summarization framework which handles tweets in both English and Hindi-we first extract tweets containing situational information, and then summarize this information. Our proposed methodology is developed based on the understanding of how several concepts evolve in Twitter during disaster. This understanding helps us achieve superior performance compared to the state-of-the-art tweet classifiers and summarization approaches on English tweets. Additionally, to our knowledge, this is the first attempt to extract situational information from non-English tweets. Authors' addresses: K. Rudra, N. Ganguly, P. Goyal, and S. Ghosh, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur 721302, India; emails: koustav.rudra@cse.iitkgp.ernet.in, krudra5@gmail.com, {niloy, pawang, saptarshi}@cse.iitkgp.ac.in. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Microblogging sites such as Twitter and Weibo have become important sources of information in today's Web. These sites are used by millions of users to exchange information on various events in real-time, i.e., as the event is happening. In particular, several recent studies have shown that microblogging sites play a key role in obtaining situational information during disaster events .During a disaster event, various types of information, including situational updates, personal opinions (e.g., on the adequacy of relief operations), and sentiments (e.g., sympathy for those affected by the disaster) are posted by users in huge volume and at rapid rates. While different types of information have different utilities, situational information-information which helps the concerned authorities (e.g., governmental and non-governmental agencies) to gain a high-level understanding of the situation-is critical for the authorities to plan relief efforts . Hence, it is important to develop automated methods to extract microblogs /tweets which contribute to situational information . 1 A related, yet different, challenge is to deal with the rapid rate at which microblogs are posted during such events, which calls for summarization of the situational information. Since time is critical in a disaster situation, these tasks have to be performed in near real-time, so that the processed information is readily available to the authorities.Several recent studies have attempted to address the challenges of extracting situational information from microblogs and summarizing such information . However, these prior works have certain limitations, as detailed in Section 2. For instance, most of the classifiers developed to distinguish between situational and non-situational tweets rely on the vocabulary of particular events, and hence do not generalize to various types of disaster events. Again, most of the summarization methodologies do not consider the salient features of tweets posted during disaster events. Most importantly, all the prior studies focus only on English tweets, in order to extend it to a resource-poor Indian language (say, Hindi 2 ), several modifications need to be made. This is particularly important from the Indian context where information posted on Twitter in Hindi and English, respectively, is often different, i.e., some information is present only in Hindi tweets and is not available via English ones (details in Section 3).The present work proposes a novel framework for extracting and summarizing situational information from microblog streams posted during disaster scenarios. In brief, the tweets are first preprocessed and fragmented based on end-markers such as \"!\" and \"?\". The fragmented tweets are then classified to extract situational tweets, and the situational tweet stream is then summarized (after removing duplicate tweets). The proposed methodology takes advantage of some specific traits of tweet streams posted during disasters. Our major contributions are listed below.",
        "\"Mind the five\": Guidelines for data privacy and security in humanitarian work with undocumented migrants and other vulnerable populations",
        "Authoring SMIL Documents by Direct Manipulations During Presentation",
        "Authors' status and the perceived quality of their work: Measuring citation sentiment change in nobel articles",
        "Who reads research articles? An altmetrics analysis of Mendeley user categories",
        "Learning author-topic models from text corpora We propose an unsupervised learning technique for extracting information about authors and topics from large text collections. We model documents as if they were generated by a two-stage stochastic process. An author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words. The probability distribution over topics in a multi-author paper is a mixture of the distributions associated with the authors. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to three large text corpora: 150,000 abstracts from the CiteSeer digital library, 1740 papers from the Neural Information Processing Systems (NIPS) Conferences, and 121,000 emails from the Enron corporation. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, parsing of abstracts by topics and authors, and detection of unusual papers by specific authors. Experiments based on perplexity scores for test documents and precision-recall for document retrieval are used to illustrate systematic differences between the proposed author-topic model and a number of alternatives. Extensions to the model, allowing for example, generalizations of the notion of an author, are also briefly discussed.",
        "Exploring characteristics of highly cited authors according to citation location and content",
        "Overview of the Author Identification Task at PAN-2017: Style Breach Detection and Author Clustering Several authorship analysis tasks require the decomposition of a multiauthored text into its authorial components. In this regard two basic prerequisites need to be addressed: (1) style breach detection, i.e., the segmenting of a text into stylistically homogeneous parts, and (2) author clustering, i.e., the grouping of paragraph-length texts by authorship. In the current edition of PAN we focus on these two unsupervised authorship analysis tasks and provide both benchmark data and an evaluation framework to compare different approaches. We received three submissions for the style breach detection task and six submissions for the author clustering task; we analyze the submissions with different baselines while highlighting their strengths and weaknesses.",
        "Ranking indirect connections in literature-based discovery: The role of medical subject headings",
        "Text Network Analysis and Visualization of Hungarian, Communist-era Political Reports This paper is presenting a partof our research project which aims to filter and visualize authority networks embedded in Hungarian communist-era political reports. The structure and development of authority networks are reconstructable owning to the well documented archive materials, reports and recorded interviews. The research focuses on the informal relations latent in authority networks. The corpus of the analysis is built by great amount of textual data mainly originated from reports recorded on party committee meetings. The quality of digitalization of these documents takes place on the wide scale of perfect readability and perfect unusability; therefore, it is a huge challenge to process these documents. Among others, the very basis of the process of text network analysis, its tools and methodology are presented; moreover, a step-by-step argument of visualization techniques is provided. Last but not least, on the basis of pilot analysis, the excellent opportunities of text analysis are demonstrated. Furthermore, the research aims the future application of sentiment and topic analysis in order to support or deny the previous findings.",
        "Style Change Detection with Feed-forward Neural Networks The majority of previous authorship attribution studies mainly focus on a dataset of documents (or parts of documents) with labeled authorship. This scenario, however, is not applicable to documents written by more than one author. Detecting the authorship switches within multi-author documents has been shown to be a challenging task in previous PAN tasks. A simplified version of the style change task is thus organized by PAN 2019, which aims at identifying the number of authors in a given document. To this end, we present a system consisting of two modules, one for distinguishing the single-author documents from the multiauthor documents and the other for determining the exact number of authors in the multi-author documents.",
        "An initial investigation on evaluating semantic web instance data. Many emerging semantic web applications include ontologies from one set of authors and instance data from another (often much larger) set of authors. Often ontologies are reused and instance data is integrated in manners unanticipated by their authors. Not surprisingly, many instance data rich applications encounter instance data that is not compatible with the expectations of the original ontology author(s). This line of work focuses on issues related to semantic expectation mismatches in instance data. Our initial results include a customizable and extensible service-oriented evaluation architecture, and a domain implementation called PmlValidator, which checks instance data using the corresponding ontologies and additional style requirements.",
        "Multi-task Learning for Author Profiling with Hierarchical Features. Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (MTAP), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the characterlevel, word-level and topic-level features, respectively. MTAP thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that MTAP has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks 2 .",
        "Trustworthiness criteria for supporting users to assess the credibility of web information. Assessing the quality of information on the Web is a challenging issue for at least two reasons. First, as a decentralized data publishing platform in which anyone can share nearly anything, the Web has no inherent quality control mechanisms to ensure that content published is valid, legitimate, or even just interesting. Second, when assessing the trustworthiness of web pages, users tend to base their judgments upon descriptive criteria such as the visual presentation of the website rather than more robust normative criteria such as the author's reputation and the source's review process. As a result, Web users are liable to make incorrect assessments, particularly when making quick judgments on a large scale. Therefore, Web users need credibility criteria and tools to help them assess the trustworthiness of Web information in order to place trust in it. In this paper, we investigate the criteria that can be used to collect supportive data about a piece of information in order to improve a person's ability to quickly judge the trustworthiness of the information. We propose the normative trustworthiness criteria namely, authority, currency, accuracy and relevance which can be used to support users' assessments of the trustworthiness of Web information. In addition, we validate these criteria using an expert panel. The results show that the proposed criteria are helpful. Moreover, we obtain weighting scores for criteria which can be used to calculate the trustworthiness of information and suggest a piece of information that is more likely to be trustworthy to Web users.",
        "On the Discovery of Success Trajectories of Authors. Understanding the qualitative patterns of research endeavor of scientific authors in terms of publication count and their impact (citation) is important in order to quantify success trajectories. Here, we examine the career profile of authors in computer science and physics domains and discover at least six different success trajectories in terms of normalized citation count in longitudinal scale. Initial observations of individual trajectories lead us to characterize the authors in each category. We further leverage this trajectory information to build a two-stage stratification model to predict future success of an author at the early stage of her career. Our model outperforms the baseline with an average improvement of 15.68% for both the datasets.",
        "A cast of thousands: Coauthorship and subauthorship collaboration in the 20th century as manifested in the scholarly journal literature of psychology and philosophy",
        "Detecting Wikipedia Vandalism using WikiTrust - Lab Report for PAN at CLEF 2010 WikiTrust is a reputation system for Wikipedia authors and content. WikiTrust computes three main quantities: edit quality, author reputation, and content reputation. The edit quality measures how well each edit, that is, each change introduced in a revision, is preserved in subsequent revisions. Authors who perform good quality edits gain reputation, and text which is revised by several high-reputation authors gains reputation. Since vandalism on the Wikipedia is usually performed by anonymous or new users (not least because long-time vandals end up banned), and is usually reverted in a reasonably short span of time, edit quality, author reputation, and content reputation are obvious candidates as features to identify vandalism on the Wikipedia. Indeed, using the full set of features computed by WikiTrust, we have been able to construct classifiers that identify vandalism with a recall of 83.5%, a precision of 48.5%, and a false positive rate of 8%, for an area under the ROC curve of 93.4%. If we limit ourselves to the set of features available at the time an edit is made (when the edit quality is still unknown), the classifier achieves a recall of 77.1%, a precision of 36.9%, and a false positive rate of 12.2%, for an area under the ROC curve of 90.4%. Using these classifiers, we have implemented a simple Web API that provides the vandalism estimate for every revision of the English Wikipedia. The API can be used both to identify vandalism that needs to be reverted, and to select highquality, non-vandalized recent revisions of any given Wikipedia article. These recent high-quality revisions can be included in static snapshots of the Wikipedia, or they can be used whenever tolerance to vandalism is low (as in a school setting, or whenever the material is widely disseminated).",
        "The 2nd workshop on Vertical Search Relevance at WSDM 2015 It is our great pleasure to welcome you to the VSR 2015, the second International Workshop on Vertical Search Relevance, held as part of the WSDM 2015 conference in Shanghai, China.Background: As the popularity of search engines has grown, the information needs of end users continue being refined. One of the emerging trends is using vertical search intent. For example, a user may want to find a restaurant near her current location; another user may want to follow the recent development of a breaking event such as the earthquake in Japan. Some recent studies show that at least 20% of Web queries have some local intent . As a result, vertical search engines start attracting more and more attention. For example, many search engines provide specialized vertical search results for local search 1 and for real-time search 2 . Furthermore, vertical search results are often slotted into general Web search results . Thus, designing effective ranking functions for vertical search has become practically important to improve users' search experience.A natural way to build a vertical search engine is to apply the existing ranking techniques on a vertical. In the TREC conference, several specialized tracks such as blog and chemical tracks have been introduced to provide a test bed to study retrieval tasks on these special text collections. The main focus of these tracks is on content-based relevance and most participants extend traditional IR techniques to consider a few task-specific ranking signals. Recently, learning to rank approaches have been studied extensively and shown to be effective to combine many useful signals into a ranking function. To adapt such a technique on a vertical, an intuitive approach is to construct a training data set by collecting a set of queries and documents which belong to the vertical and asking human editors to give a single relevance label between a query and a document. A ranking function thus can be learned for the vertical.1 Google Local. http://local.google.com/. 2 Cinicalkey. https://www.clinicalkey.com/ Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). WSDM'15, February 2-6, 2015, Shanghai, China. ACM 978-1-4503-3317-7/15/02. http://dx.doi.org/10.1145/2684822.2697031.However, we observed that in many verticals, the meaning of relevance is domain-specific and usually consists of multiple well-defined aspects. Blindly applying the conventional learning to rank approaches by ignoring vertical-specific domain knowledge may not be effective. Thus, we have identified a list of challenging research issues in the field of relevance for vertical search, which mainly fall into two categories. First category includes how to learn an effective ranking model considering multi-facets relevance: (1) Since there are several pertinent aspects in a vertical, human editors naturally need to consider and tradeoff the relevance from different aspects before making the overall relevance judgment. Thus assessing aspect relevance is a necessary step. (2) Trading off multiple aspects is not trivial since such a tradeoff can vary for different queries or in different contexts. (3) For different verticals, different aspects are involved and the tradeoff among aspects is vertical-dependent. Collecting training data with overall relevance for a new vertical requires human editors learn how to appropriately tradeoff different aspects. Second category focuses on building effective business model in the context of specific vertical search systems.Objectives: This workshop is dedicated to presentations and discussions on relevance for vertical search. For the workshop, we aim to bring together researchers from IR, ML, NLP, and other areas of computer and information science who are working on or interested in this area, and provide a forum for them to identify the issues and the challenges, to share their latest research results, to express a diverse range of opinions about this topic, and to discuss future directions.Review Process: The call for papers solicited submissions in the area of vertical search. Themes of interests were diverse and spanned technologies and applications, models, algorithms, and evaluation methods. Each submission was reviewed by at least two Program Committee members and final decisions were made by the workshop chairs. PC members are listed below and include many experts in the field. We are grateful to all of them for their thorough and insightful reviews.",
        "Hierarchical Clustering Analysis: The Best-Performing Approach at PAN 2017 Author Clustering Task. The author clustering problem consists in grouping documents written by the same author so that each group corresponds to a different author. We described our approach to the author clustering task at PAN 2017, which resulted in the best-performing system at the aforementioned task. Our method performs a hierarchical clustering analysis using document features such as typed and untyped character n-grams, word n-grams, and stylometric features. We experimented with two feature representation methods, log-entropy model, and TF-IDF, while tuning minimum frequency threshold values to reduce the feature dimensionality. We identified the optimal number of different clusters (authors) dynamically for each collection using the Cali\u0144ski Harabasz score. The implementation of our system is available open source (https://github. com/helenpy/clusterPAN2017)."
    ],
    " mention of open source ": [
        "A Framework for Bridging the Gap Between Open Source Search Tools",
        "A web-based degree program in open source education: a case study. In this paper, we describe the details of an interactive online web-based degree program in the area of Computer Science with specialization in Free/Open Source Software (FOSS) that has been successfully running for two years in a leading technological university in India. The subjects taught as well as the tools and platforms used in delivering the course are exclusively FOSS and put together by the university team, as described here. We also describe the details of the program, its goals and purpose, the manner of its implementation, the learnings we have had and the challenges being faced in going forward.",
        "Open source information petrieval: a report on the SIGIR 2012 workshop. On August 16, 2012 the SIGIR 2012 Workshop on Open Source Information Retrieval was held as part of the SIGIR 2012 conference in Portland, Oregon, USA. There were 2 invited talks, one from industry and one from academia. There were 6 full papers and 6 short papers presented as well as demonstrations of 4 open source tools. Finally there was a lively discussion on future directions for the open source Information Retrieval community. This contribution discusses the events of the workshop and outlines future directions for the community.",
        "Accelerating Standards Adoption: A Vendor's Perspective on Contributing to Open Standards and Open Source",
        "Impact of license choice on Open Source Software development activity",
        "Dexter: an open source framework for entity linking. We introduce Dexter, an open source framework for entity linking. The entity linking task aims at identifying all the small text fragments in a document referring to an entity contained in a given knowledge base, e.g., Wikipedia. The annotation is usually organized in three tasks. Given an input document the first task consists in discovering the fragments that could refer to an entity. Since a mention could refer to multiple entities, it is necessary to perform a disambiguation step, where the correct entity is selected among the candidates. Finally, discovered entities are ranked by some measure of relevance. Many entity linking algorithms have been proposed, but unfortunately only a few authors have released the source code or some APIs. As a result, evaluating today the performance of a method on a single subtask, or comparing different techniques is difficult. In this work we present a new open framework, called Dexter, which implements some popular algorithms and provides all the tools needed to develop any entity linking technique. We believe that a shared framework is fundamental to perform fair comparisons and improve the state of the art.",
        "Open knowledge management: Lessons from the open source revolution",
        "Overview of the 2019 Open-Source IR Replicability Challenge (OSIRRC 2019)",
        "Manual Annotation of Semi-Structured Documents for Entity-Linking. The Entity Linking (EL) problem consists in automatically linking short fragments of text within a document to entities in a given Knowledge Base like Wikipedia. Due to its impact in several text-understanding related tasks, EL is an hot research topic. The correlated problem of devising the most relevant entities mentioned in the document, a.k.a. salient entities (SE), is also attracting increasing interest. Unfortunately, publicly available evaluation datasets that contain accurate and supervised knowledge about mentioned entities and their relevance ranking are currently very poor both in number and quality. This lack makes very difficult to compare different EL and SE solutions on a fair basis, as well as to devise innovative techniques that relies on these datasets to train machine learning models, in turn used to automatically link and rank entities.In this demo paper we propose a Web-deployed tool that allows to crowdsource the creation of these datasets, by supporting the collaborative human annotation of semi-structured documents. The tool, called Elianto, is actually an open source framework, which provides a user friendly and reactive Web interface to support both EL and SE labelling tasks, through a guided two-step process.",
        "Entity Retrieval Docker Image for OSIRRC at SIGIR 2019 With emerging of structured data, retrieving entities instead of documents becomes more prevalent in order to satisfy the information need related to a query. Therefore, several high-performance entity retrieval methods have been introduced to the Information Retrieval (IR) community in recent years. Replicating and reproducing the standard entity retrieval methods are considered as challenging tasks in the IR community. Open-Source IR Replicability Challenge (OSIRRC) has addressed this problem by introducing a unified framework for dockerizing a variety of retrieval tasks. In this paper, a Docker image is built for six different entity retrieval models including, LM, MLM-tc, MLM-all, PRMS, SDM, FSDM. Also, Entity Linking incorporated Retrieval(ELR) extension, has been implemented that can be applied on top of all the mentioned models. The entity retrieval docker can retrieve relevant entities for any given topic.",
        "CoreDB: a Data Lake Service. The continuous improvement in connectivity, storage and data processing capabilities allow access to a data deluge from sensors, social-media, news, user-generated, government and private data sources. Accordingly, in a modern data-oriented landscape, with the advent of various data capture and management technologies, organizations are rapidly shifting to datafication of their processes. In such an environment, analysts may need to deal with a collection of datasets, from relational to NoSQL, that holds a vast amount of data gathered from various private/open data islands, i.e. Data Lake. Organizing, indexing and querying the growing volume of internal data and metadata, in a data lake, is challenging and requires various skills and experiences to deal with dozens of new databases and indexing technologies: How to store information items? What technology to use for persisting the data? How to deal with the large volume of streaming data? How to trace and persist information about data? What technology to use for indexing the data? How to query the data lake? To address the above mentioned challenges, we present CoreDB -an open source data lake service -which offers researchers and developers a single REST API to organize, index and query their data and metadata. CoreDB manages multiple database technologies and offers a built-in design for security and tracing.",
        "SMART: An Open Source Framework for Searching the Physical World",
        "Towards an Efficient and Effective Search Engine",
        "STELLA: Towards a Framework for the Reproducibility of Online Search Experiments",
        "Applications of open search tools. It costs about 300M dollars to just build a search system that scales to the web. Web services which open up web search results to the public allow academics, developers, and entrepreneurs to achieve instant web search parity for free and enable them to focus on building their additional secret sauce on top to create even grander relevant services. For example, a social network could leverage open search and their data about users to personalize web search. Additionally, one of the best ways to gather data about web search behavior is to build your own search system. Prototype IR and web search systems based on open search can be used to gather user interaction data and test the applicability of research ideas. Open Source tools like Lucene and Nutch and open search services like from major search engines can greatly help developers implement these types of systems quickly, allowing for fast production and evaluation. We will give detailed overviews of the popular open search tools, showcasing examples of search and IR algorithms and systems implemented using them, as well as discussing how evaluation can be performed.",
        "Building voiceXML browsers with openVXI. The OpenVXI is a portable open source based toolkit that interprets the VoiceXML dialog markup language. It is designed to serve as a framework for system integrators and platform vendors who want to incorporate VoiceXML into their platform. A first version of the toolkit was released in the winter of 2001, with a second version released in September of 2001. A number of companies and individuals have adopted the toolkit for their platforms. In this paper we discuss the architecture of the toolkit, the architectural issues involved with implementing a framework for VoiceXML, performance results with the OpenVXI, and future directions for the toolkit.",
        "Population dynamics in open source communities: an ecological approach applied to github. Open Source Software (OSS) has gained high amount of popularity during the last few years. It is becoming used by public and private institutions, even companies release portions of their code to obtain feedback from the community of voluntary developers. As OSS is based on the voluntary contributions of developers, the number of participants represents one of the key elements that impact the quality of the software. In order to understand how the the population of contributors evolve over time, we propose a methodology that adapts Lotka-Volterra-based biological models used for describing host-parasite interactions. Experiments based on data from the Github collaborative platform showed that the proposed approach performs effectively in terms of providing an estimation of the population of developers for each project over time."
    ],
    "Inclusion of text-mining": [
        "WSDM 2017 Workshop on Mining Online Health Reports: MOHRS 2017. The workshop on Mining Online Health Reports (MOHRS) draws upon the rapidly developing field of Computational Health, focusing on textual content that has been generated through various activities on the Web. Online usergenerated information mining, especially from social media platforms and search engines, has been in the forefront of many research efforts, especially in the fields of Information Retrieval and Natural Language Processing. The incorporation of such data and techniques in a number of health-oriented applications has provided strong evidence of the potential benefits, which include better population coverage, timeliness and applicability to places with less established health infrastructure. The workshop provides an opportunity to present relevant state-of-the-art research, and a venue for discussion between researchers with crossdisciplinary backgrounds. It will focus on the characterisation of data sources, the essential methods for mining this textual information, as well as potential real-world applications and the arising ethical issues. MOHRS '17 will feature 3 keynote talks and 4 accepted paper presentations, as well as a panel discussion.",
        "DOM based content extraction via text density. In addition to the main content, most web pages also contain navigation panels, advertisements and copyright and disclaimer notices. This additional content, which is also known as noise, is typically not related to the main subject and may hamper the performance of web data mining, and hence needs to be removed properly. In this paper, we present Content Extraction via Text Density (CETD)-a fast, accurate and general method for extracting content from diverse web pages, and using DOM (Document Object Model) node text density to preserve the original structure. For this purpose, we introduce two concepts to measure the importance of nodes: Text Density and Composite Text Density. In order to extract content intact, we propose a technique called DensitySum to replace Data Smoothing. The approach was evaluated with the CleanEval benchmark and with randomly selected pages from well-known websites, where various web domains and styles are tested. The average F1-scores with our method were 8.79% higher than the best scores among several alternative methods.",
        "Helmholtz principle based supervised and unsupervised feature selection methods for text mining a b s t r a c tOne of the important problems in text classification is the high dimensionality of the feature space. Feature selection methods are used to reduce the dimensionality of the feature space by selecting the most valuable features for classification. Apart from reducing the dimensionality, feature selection methods have potential to improve text classifiers' performance both in terms of accuracy and time. Furthermore, it helps to build simpler and as a result more comprehensible models. In this study we propose new methods for feature selection from textual data, called Meaning Based Feature Selection (MBFS) which is based on the Helmholtz principle from the Gestalt theory of human perception which is used in image processing. The proposed approaches are extensively evaluated by their effect on the classification performance of two well-known classifiers on several datasets and compared with several feature selection algorithms commonly used in text mining. Our results demonstrate the value of the MBFS methods in terms of classification accuracy and execution time.",
        "Graded-Inclusion-Based Information Retrieval Systems. This paper investigates the use of fuzzy logic mechanisms coming from the database community, namely graded inclusions, to model the information retrieval process. In this framework, documents and queries are represented by fuzzy sets, which are paired with operations like fuzzy implications and T-norms. Through different experiments, it is shown that only some among the wide range of fuzzy operations are relevant for information retrieval. When appropriate settings are chosen, it is possible to mimic classical systems, thus yielding results rivaling those of state-of-theart systems. These positive results validate the proposed approach, while negative ones give some insights on the properties needed by such a model. Moreover, this paper shows the added-value of this graded inclusion-based model, which gives new and theoretically grounded ways for a user to easily weight his query terms, to include negative information in his queries, or to expand them with related terms.",
        "Citation mining: Integrating text mining and bibliometrics for research user profiling",
        "Introduction to the Special Section on Intelligent Visual Interfaces for Text Analysis Text analysis technologies aim at automatically extracting knowledge and uncovering information buried under seas of text documents, including academic publications, news articles, emails, and patient records. Complex analysis results of these technologies, however, are often difficult for average users to digest and leverage. Furthermore, such results may be inaccurate or contain ambiguous or even misleading information. This special section presents several visual interfaces for text analytics technologies designed to help users better consume text analysis results.In the past, most work focused on either using basic visualization (e.g., bar charts and pie charts) to present the final results of complex text analysis or inventing visual metaphors to illustrate the results of simple text analysis (e.g., tf-idf measure of keywords). Based on the type of information visualized, these systems have been classified into two categories: metadata-based and content-based text visualization.Metadata While text analysis and text visualization have separately received much attention, little work has focused on tightly integrating interactive visualization with text mining techniques for information exploration and scalable decision support. This is an important research field that directly addresses real-world problems in many domains, including but not limited to business intelligence, fraud detection, emergency management, customer relationship management, market intelligence, and healthcare analytics. For this reason, it has started to attract the attention of both academia and industry and will continue to do so in the future.To highlight the work in this emerging area, this special section includes seven excellent articles from the leading researchers in the field. These articles cover most key aspects of visual text analytics, including interactive text search and clustering, text browsing driven by user interests, as well as interactive exploration of large text collections with topic modeling and sentiment analysis.In \"Watch the Story Unfold with TextWheel: Visualization of Large-Scale News Streams\", Cui et al. study how interactive visualization can help users understand keyword-based searching and clustering of news articles. The proposed TextWheel system aims at conveying multiple attributes of news articles and the macro/micro relationships between news streams and keywords in context. In \"Visual. ion and Ordering in Faceted Browsing of Text Collections\", Thai et al. consider facet navigation c",
        "Probabilistic Topic Models for Text Data Retrieval and Analysis. Text data include all kinds of natural language text such as web pages, news articles, scienti c literature, emails, enterprise documents, and social media posts. As text data continues to grow quickly, it is increasingly important to develop intelligent systems to help people manage and make use of vast amounts of text data (\"big text data ). As a new family of e ective general approaches to text data retrieval and analysis, probabilistic topic models, notably Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocations (LDA), and many extensions of them, have been studied actively in the past decade with widespread applications. ese topic models are powerful tools for extracting and analyzing latent topics contained in text data; they also provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics. is tutorial systematically reviews the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. e tutorial provides (1) an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, (2) a broad overview of all the major representative topic models (that are usually extensions of PLSA or LDA), and (3) a discussion of major challenges and future research directions. MOTIVATIONText data include all kinds of natural language text such as web pages, news articles, scienti c literature, emails, enterprise documents, and social media posts. In contrast to non-textual data which are usually generated by physical devices, text data are generated by humans and meant to be consumed by humans. Due to the rapid growth of text data, we can no longer digest all the relevant information in a timely manner. us there is a pressing need for developing intelligent so ware tools to help people manage and make use of vast amounts of text data (\"big text data ) for various tasks, especially those involving complex decision-making. Logically, to harness big text data, we would need to rst identify the relevant text data to a particular application problem (i.e., perform Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored.. ing with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan \u00a9 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080823 text data retrieval) and then analyze the identi ed relevant text data in more depth to extract any needed knowledge for a task (i.e. text data analysis). Text data retrieval is essential for ltering out non-relevant text data to a particular problem, thus dramatically improving the e ciency since any further processing can be focused on the much smaller subset of relevant text data and avoid touching all the raw text data. As a subsequent step, text data analysis can further help users digest all the relevant text content by revealing the most useful knowledge or interesting pa erns in the text data, thus providing users with more direct support for their tasks.Due to the di culty in natural language understanding by computers, the approaches that work well for text retrieval and text analysis tend to be statistical approaches. In the past decade, a special kind of statistical approaches, called probabilistic topic models, represented by Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocations (LDA) [1] and many of their extensions, have been studied actively with widespread applications.ese topic models provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics.Speci cally, probabilistic topic models have many applications in information retrieval, including particularly improvement of text representation by representing text documents with low-dimensional latent topic vectors, summarization of search results, modeling user information needs, and topic-based feedback. Probabilistic topic models have also been applied to analyze text data to discover topics and reveal their variations over context variables such as time, location, and sources of text data, generating useful topic pa erns to facilitate digestion of text content and knowledge discovery from text data. As a result, recent years have seen increasing uses of topic models in the papers published in ACM SIGIR conferences. However, there has not yet been a tutorial at SIGIR on this topic.is tutorial is meant to ll in this gap. OBJECTIVESe goal of the tutorial is to systematically review the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. e tutorial will provide an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, a broad overview of all the major representative topic models that are usually extensions of PLSA or LDA), and a discussion of major challenges and future research directions.e tutorial would be appealing to anyone who would like to learn about topic models, how and why they work, their widespread",
        "Text mining in genomics and systems biology. There is an increasing need of complementing the information available for the analysis of biological systems in Systems Biology and Genomics projects. A need that makes interesting the integration of information directly extracted from textual sources using Information Extraction and Text Mining approaches. My group has been working in developing Text Mining approaches and in their integration in large-scale projects together with other experimental and bioinformatics methods. In this occasion I will present the developments related with the characterization of the human mitotic spindle apparatus, developed in the context of the ENFIN NoE. For these, and other, applications it is crucial to have an accurate estimation of the capacity of the current Text Mining systems. The BioCreative II challenge organized by CNIO, MITRE and NCBI in collaboration with the MINT and INTACT databases (http://biocreative.sourceforge.net, Genome Biology, August 2008 Special Issue) provides such an overview. BioCreative II was in two task: 1) gene name identification and normalization, where many systems were able to achieve a consistent 80% balance precision / recall. And 2) protein interaction detection that was divided in four sub-tasks: a) ranking of publications by their relevance on experimental determination of protein interactions, b) detection of protein interaction partners in text, c) detection of key sentences describing protein interactions, and d) detection of the experimental technique used to determine the interactions. The results were quite good in the categories of publication raking, detection of experimental methods, and highlighting of relevant sentences, while they pointed to persistent problems in the correct normalization of gene/protein names. Furthermore BioCreative has channel the collaboration of several teams for the creation of the first Text Mining meta-server (The BioCreative Meta-server, Leitner et al., Genome Biology 2008 BioCreative special issue). We are working now in the preparation of BioCreative III, with particular focus in fostering the creation of Text Mining systems that can be integrated in Genome analysis pipelines, and contribute effectively to the understanding of complex Biological Systems.",
        "Incorporating Phrase-level Sentiment Analysis on Textual Reviews for Personalized Recommendation. Previous research on Recommender Systems (RS), especially the continuously popular approach of Collaborative Filtering (CF), has been mostly focusing on the information resource of explicit user numerical ratings or implicit (still numerical) feedbacks. However, the ever-growing availability of textual user reviews has become an important information resource, where a wealth of explicit product attributes/features and user attitudes/sentiments are expressed therein. This information rich resource of textual reviews have clearly exhibited brand-new approaches to solving many of the important problems that have been perplexing the research community for years, such as the paradox of cold-start, the explanation of recommendation, and the automatic generation of user or item profiles. However, it is only recently that the fundamental importance of textual reviews has gained wide recognition, perhaps mainly because of the difficulty in formatting, structuring and analyzing the free-texts. In this research, we stress the importance of incorporating textual reviews for recommendation through phrase-level sentiment analysis, and further investigate the role that the texts play in various important recommendation tasks.",
        "Proceedings of the ACM sixth international workshop on Data and text mining in biomedical informatics, DTMBIO@CIKM 2012, Maui, Hawaii, USA, October 29, 2012",
        "InCaToMi: integrative causal topic miner between textual and non-textual time series data. Topic modeling is popular for text mining tasks. Recently, topic modeling has been combined with time lines when textual data is related to external non-textual time series data such as stock prices. However, no previous work has used the external non-textual time series data in the process of topic modeling. In this paper, we describe a novel text mining system, Integrative Causal Topic Miner (InCaToMi) that integrates textual and non-textual time series data. InCaToMi automatically finds causal relationships and topics using text data and external non-textual time series data using Granger Testing. Moreover, InCaToMi considers the non-textual time series data in the topic modeling process, using the time series data to iteratively improve modeling results through interactions between it and the textual data at both topic and word levels.",
        "Using LSI for Text Classification in the Presence of Background Text. This paper presents work that uses Latent Semantic Indexing (LSI) for text classification. However, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available \"background\" text in the classification process. Rather than performing LSI's singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text. We report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.",
        "A hybrid approach to protein name identification in biomedical texts. This paper presents a hybrid approach to identifying protein names in biomedical texts, which is regarded as a crucial step for text mining. Our approach employs a set of simple heuristics for initial detection of protein names and uses a probabilistic model for locating complete protein names. In addition, a protein name dictionary is complementarily consulted. In contrast to previously proposed methods, our proposed method avoids the use of natural language processing tools such as part-of-speech taggers and syntactic parsers and solely relies on surface clues, so as to reduce the processing overhead. Moreover, we propose a framework to automatically create a large-scale corpus annotated with protein names, which can be then used for training our probabilistic model. We implemented a protein name identification system, named PROTEX ROTEX , based on our proposed method and evaluated it by comparing with a system developed by other researchers on a common test set. The experiments showed that the automatically constructed corpus is equally useful in training as compared with manually annotated corpora and that effective performance can be achieved in identifying compound protein names with PROTEX ROTEX.",
        "Indices of novelty for emerging topic detection a b s t r a c tEmerging topic detection is a vital research area for researchers and scholars interested in searching for and tracking new research trends and topics. The current methods of text mining and data mining used for this purpose focus only on the frequency of which subjects are mentioned, and ignore the novelty of the subject which is also critical, but beyond the scope of a frequency study. This work tackles this inadequacy to propose a new set of indices for emerging topic detection. They are the novelty index (NI) and the published volume index (PVI). This new set of indices is created based on time, volume, frequency and represents a resolution to provide a more precise set of prediction indices. They are then utilized to determine the detection point (DP) of new emerging topics. Following the detection point, the intersection decides the worth of a new topic. The algorithms presented in this paper can be used to decide the novelty and life span of an emerging topic in a specific field. The entire comprehensive collection of the ACM Digital Library is examined in the experiments. The application of the NI and PVI gives a promising indication of emerging topics in conferences and journals.Crown",
        "Event Mining over Distributed Text Streams. This research presents a new set of techniques to deal with event mining from different text sources, a complex set of NLP tasks which aim to extract events of interest and their components including authors, targets, locations, and event categories. Our focus is on distributed text streams, such as tweets from different news agencies, in order to accurately retrieve events and its components by combining such sources in different ways using text stream mining. Therefore this research project aims to fill the gap between batch event mining, text stream mining and distributed data mining which have been used separately to address related learning tasks. We propose a multi-task and multi-stream mining approach to combine information from multiple text streams to accurately extract and categorise events under the assumptions of stream mining. Our approach also combines ontology matching to boost accuracy under imbalanced distributions. In addition, we plan to address two relatively unexplored event mining tasks: event coreference and event synthesis. Preliminary results show the appropriateness of our proposal, which is giving an increase of around 20% on macro prequential metrics for the event classification task.",
        "Summarizing Text Documents: Sentence Selection and Evaluation Metrics. Human-quality text summarization systems are di cult to design, and even more di cult to evaluate, in part because documents can di er along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents our analysis of news-article summaries generated by sentence selection. Sentences are ranked for potential inclusion in the summary using a weighted combination of statistical and linguistic features. The statistical features were adapted from standard IR methods. The potential linguistic ones were derived from an analysis of news-wire summaries. To e v aluate these features we use a normalized version of precision-recall curves, with a baseline of random sentence selection, as well as analyze the properties of such a baseline. We illustrate our discussions with empirical results showing the importance of corpus-dependent baseline summarization standards, compression ratios and carefully crafted long queries.",
        "Predicting protein-protein relationships from literature using collapsed variational latent dirichlet allocation. This paper investigates applying statistical topic models to extract and predict relationships between biological entities, especially protein mentions. A statistical topic model, Latent Dirichlet Allocation (LDA) is promising; however, it has not been investigated for such a task. In this paper, we apply the state-of-the-art Collapsed Variational Bayesian Inference and Gibbs Sampling inference to estimating the LDA model, and compared them from the viewpoints of log-likelihoods, classification accuracy and retrieval effectiveness. We demonstrate through experiments that the Collapsed Variational LDA gives better results than the other, especially in terms of classification accuracy and retrieval effectiveness in the task of the protein-protein relationship prediction.",
        "A Clustering Algorithm for Asymmetrically Related Data with Applications to Text Mining. Clustering techniques find a collection of subsets of a data set such that the collection satisfies a criterion that is dependent on a relation defined on the data set. The underlying relation is traditionally assumed to be symmetric. However, there exist many practical scenarios where the underlying relation is asymmetric. One example of an asymmetric relation in text analysis is the inclusion relation, i.e., the inclusion of the meaning of a block of text in the meaning of another block. In this paper, we consider the general problem of clustering of asymmetrically related data and propose an algorithm to cluster such data. To demonstrate its usefulness, we consider two applications in text mining: (1) summarization of short documents, and (2) generation of a concept hierarchy from a set of documents. Our experiments show that the performance of the proposed algorithm is superior to that of more traditional algorithms.",
        "Efficient filtering and ranking schemes for finding inclusion dependencies on the web. Data integrity constraints are fundamental in various applications, such as data management, integration, cleaning, and schema extraction. In this paper, we address the problem of finding inclusion dependencies on the Web. The problem is important because (1) applications of inclusion dependencies, such as data quality management, are beneficial in the Web context, and (2) such dependencies are not explicitly given in general. In our approach, we enumerate pairs of HTML/XML elements that possibly represent inclusion dependencies and then rank the results for verification. First, we propose a bit-based signature scheme to efficiently select candidates (element pairs) in the enumeration process. The signature scheme is unique in that it supports Jaccard containment to deal with the incomplete nature of data on the Web, and preserves the semiorder inclusion relationship among sets of words. Second, we propose a ranking scheme to support a user in checking whether each enumerated pair actually suggests inclusion dependencies. The ranking scheme sorts the enumerated pairs so that we can examine a small number of pairs for simultaneously verifying many pairs.",
        "A text mining approach to assist the general public in the retrieval of legal documents",
        "Finding Patient Visits in EMR Using LUXID\u00ae INTRODUCTION : Free text sections of the Electronic Medical Records (EMR) contain information that cannot be appropriately constrained in the structured forms. Several studies have shown the potential utility in mining EMR free texts for identifying adverse events (e.g. EU-PSIP, EU-ALERT), and large public-private research projects (e.g. IMI-EHR4CR, CLOUD4HEALTH) aim at mining them further, e.g. for clinical trial optimisation and pharmacovigilance purposes. AIM : The purpose of this work has been to assess the performance of LUXID R , an o-the-shelve commercial natural language processing system, using the dictionary-and rule-based Medical Entity Relationships Skill Cartridge R and KNIME as automation workow engine for result combination and formatting, on the University of Pittsburgh BLULab NLP Repository benchmark, in the context of the TREC 2011 Medical Records Retrieval Track (TREC-MED2011). RESULTS : The system here described achieved the best score for one of the 34 queries (dened as query 111) and overall classied as top 7th-8th (according to the scoring used) in the manual track of TREC-MED2011. More than 80% of the queries of TREC-MED2011 could be appropriately processed automatically. Performance of manually interpreted queries did not dier substantially from those automatically processed. More than 60% of the queries submitted by our system delivered a performance above or on the median of all participants. Very high precision of the system, delivering in certain cases a very low number of hits, correlated statistically with the overall performance. CONCLUSIONS : Initial results, error analysis are reported and strategies for improvements of the system are outlined; fully supporting the appropriateness in using this technology for identifying patients matching inclusion/exclusion criteria using plain text from unstructured EMR.",
        "Inferring appropriate eligibility criteria in clinical trial protocols without labeled data. We consider the user task of designing clinical trial protocols and propose a method that outputs the most appropriate eligibility criteria from a potentially huge set of candidates. Each document d in our collection D is a clinical trial protocol which itself contains a set of eligibility criteria. Given a small set of sample documents D , |D | |D|, a user has initially identified as relevant e.g., via a user query interface, our scoring method automatically suggests eligibility criteria from D by ranking them according to how appropriate they are to the clinical trial protocol currently being designed. We view a document as a mixture of latent topics and our method exploits this by applying a three-step procedure. First, we infer the latent topics in the sample documents using Latent Dirichlet Allocation (LDA) . Next, we use logistic regression models to compute the probability that a given candidate criterion belongs to a particular topic. Lastly, we score each criterion by computing its expected value, the probability-weighted sum of the topic proportions inferred from the set of sample documents. Intuitively, the greater the probability that a candidate criterion belongs to the topics that are dominant in the samples, the higher its expected value or score. Results from our experiments indicate that our proposed method is 8 and 9 times better (resp., for inclusion and exclusion criteria) than randomly choosing from a set of candidates obtained from relevant documents. In user simulation experiments, we were able to automatically construct eligibility criteria that are on the average 75% and 70% (resp., for inclusion and exclusion criteria) similar to the correct eligibility criteria.",
        "Mining common topics from multiple asynchronous text streams. Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.",
        "A Flexible Text Mining System for Entity and Relation Extraction in PubMed. Due to an enormous number of scientific publications that cannot be handled manually, there is a rising interest in text-mining techniques for automated information extraction, especially in the biomedical field. Such techniques provide effective means of information search, knowledge discovery, and hypothesis generation. Therefore, we present PKDE4J, a comprehensive textmining system that integrates dictionary-based entity extraction and rule-based relation extraction in a highly flexible and extensible framework. Extending Stanford CoreNLP, we developed the system with multiple types of entities and relations.We demonstrate the performance by evaluating on various corpora such as CRAFT, GENETAG, AnEM Corpus, NCBI Disease Corpus, DDI Corpus, Metabolite and Enzyme Corpus for NER and BioInfer, AIMed, GAD, CoMAGC, and PolySearch for RE and achieve with average F-measures of 85% for entity extraction and 82% for relation extraction. As advantages of this system, one is a configurability in various combinations of text-processing components that can be plugged in for different tasks. The other is an extensible framework for extraction; extensible rule engine for relation extraction (Plug-and-play approach).As shown in , the system contains two major pipelines for public knowledge discovery. The first pipeline extracts target entities based on dictionaries by extending the Stanford CoreNLP. The second pipeline applies dependency tree-based rules to sentences with two or more entities to extract relationships among those entities. ACKNOWLEDGMENTS",
        "Building Text-mining Framework for Gene-Phenotype Relation Extraction using Deep Leaning. The scientific literature is a rich resource for information retrieval on the biological knowledge. Nevertheless, the unstructured textual data in the research articles makes it difficult to access the information with computer-aided systems. Textmining is one of the solution that can transform unstructured information in the text into database content, and most of the approaches are based on the machine learning models. Since these approaches require high-dimensional features, the performance of the model is heavily dependent on the selection of features. However, it is usually difficult and labor-intensive to choose good features, because feature extraction requires prior knowledge and ingenuity of human experts. Here, we suggest a novel framework to extract biological relations from the texts by using hierarchical text features that enhance the effectiveness of relation extraction model.The proposed framework is composed of two parts, node and edge detection, using deep belief networks. Each part is based on the hierarchical text features learned by Gaussian-Bernoulli restricted Boltzmann machine (GBRBM). In this work, we performed gene-cancer relation extraction task as a pilot study. The classification model was trained based on both GE09 corpus from BioNLP'09 Shared Task and CoMAGC corpus. The results show that our model achieved better performance than other handcrafted feature-based approaches. The evaluation results suggest that deep belief networks offers the optimized and generalized hierarchical text features for the large-scale text mining.",
        "Multilingual sentence categorization and novelty mining a b s t r a c tA challenge for sentence categorization and novelty mining is to detect not only when text is relevant to the user's information need, but also when it contains something new which the user has not seen before. It involves two tasks that need to be solved. The first is identifying relevant sentences (categorization) and the second is identifying new information from those relevant sentences (novelty mining). Many previous studies of relevant sentence retrieval and novelty mining have been conducted on the English language, but few papers have addressed the problem of multilingual sentence categorization and novelty mining. This is an important issue in global business environments, where mining knowledge from text in a single language is not sufficient. In this paper, we perform the first task by categorizing Malay and Chinese sentences, then comparing their performances with that of English. Thereafter, we conduct novelty mining to identify the sentences with new information. Experimental results on TREC 2004 Novelty Track data show similar categorization performance on Malay and English sentences, which greatly outperform Chinese. In the second task, it is observed that we can achieve similar novelty mining results for all three languages, which indicates that our algorithm is suitable for novelty mining of multilingual sentences. In addition, after benchmarking our results with novelty mining without categorization, it is learnt that categorization is necessary for the successful performance of novelty mining.",
        "Proceedings of the ACM Ninth International Workshop on Data and Text Mining in Biomedical Informatics, DTMBIO 2015, Melbourne, Australia, October 23, 2015",
        "Clustering the tagged web. Automatically clustering web pages into semantic groups promises improved search and browsing on the web. In this paper, we demonstrate how user-generated tags from largescale social bookmarking websites such as del.icio.us can be used as a complementary data source to page text and anchor text for improving automatic clustering of web pages. This paper explores the use of tags in 1) K-means clustering in an extended vector space model that includes tags as well as page text and 2) a novel generative clustering algorithm based on latent Dirichlet allocation that jointly models text and tags. We evaluate the models by comparing their output to an established web directory. We find that the naive inclusion of tagging data improves cluster quality versus page text alone, but a more principled inclusion can substantially improve the quality of all models with a statistically significant absolute F-score increase of 4%. The generative model outperforms K-means with another 8% F-score increase.",
        "Constructing classification features using minimal predictive patterns. Choosing good features to represent objects can be crucial to the success of supervised machine learning methods. Recently, there has been a great interest in applying data mining techniques to construct new classification features. The rationale behind this approach is that patterns (feature-value combinations) could capture more underlying semantics than single features. Hence the inclusion of some patterns can improve the classification performance. Currently, most methods adopt a two-phases approach by generating all frequent patterns in the first phase and selecting the discriminative patterns in the second phase. However, this approach has limited success because it is usually very difficult to correctly identify important predictive patterns in a large set of highly correlated frequent patternsIn this paper, we introduce the minimal predictive patterns framework to directly mine a compact set of highly predictive patterns. The idea is to integrate pattern mining and feature selection in order to filter out non-informative and redundant patterns while being generated. We propose some pruning techniques to speed up the mining process. Our extensive experimental evaluation on many datasets demonstrates the advantage of our method by outperforming many well known classifiers.",
        "Proceeding of the 3rd International Workshop on Data and Text Mining in Bioinformatics, DTMBIO 2009, Hong Kong, China, November 6, 2009",
        "Inferring Undiscovered Public Knowledge by Using Text Mining-driven Graph Model. Due to the recent development of Information Technology, the number of publications is increasing exponentially. In response to the increasing number of publications, there has been a sharp surge in the demand for replacing the existing manual text data processing by an automatic text data processing. Swanson proposed ABC model [1] on the top of text mining as a part of literature-based knowledge discovery for finding new possible biomedical hypotheses about three decades ago. The following clinical scholars proved the effectiveness of the possible hypotheses found by ABC model . Such effectiveness let scholars try various literature-based knowledge discovery approaches . However, their trials are not fully automated but hybrids of automatic and manual processes. The manual process requires the intervention of experts. In addition, their trials consider a single perspective. Even trials involving network theory have difficulties in mal-understanding the entire network structure of the relationships among concepts and the systematic interpretation on the structure . Thus, this study proposes a novel approach to discover various relationships by extending the intermediate concept B to a multileveled concept. By applying a graph-based path finding method based on co-occurrence and the relational entities among concepts, we attempt to systematically analyze and investigate the relationships between two concepts of a source node and a target node in the total paths. For the analysis of our study, we set our baseline as the result of This study provides distinct perspectives for literature-based discovery by not only discovering the meaningful relationship among concepts in biomedical literature through graph-based path interference but also being able to generate feasible new hypotheses.",
        "A semantic framework for intelligent matchmaking for clinical trial eligibility criteria An integral step in the discovery of new treatments for medical conditions is the matching of potential subjects with appropriate clinical trials. Eligibility criteria for clinical trials are typically specified as inclusion and exclusion criteria for each study in freetext form. While this is sufficient for a human to guide a recruitment interview, it cannot be reliably and computationally construed to identify potential subjects. Standardization of the representation of eligibility criteria can enhance the efficiency and accuracy of this process. This article presents a semantic framework that facilitates intelligent matchmaking by identifying a minimal set of eligibility criteria with maximal coverage of clinical trials. In contrast to existing top-down manual standardization efforts, a bottom-up data driven approach is presented to find a canonical nonredundant representation of an arbitrary collection of clinical trial criteria. The methodology has been validated with a corpus of 709 clinical trials related to Generalized Anxiety Disorder containing 2,760 inclusion and 4,871 exclusion eligibility criteria. This corpus is well represented by a relatively small number of 126 inclusion clusters and 175 exclusion clusters, each of which corresponds to a semantically distinct criterion. Internal and external validation measures provide an objective evaluation of the method. An eligibility criteria ontology has been constructed based on the clustering. The resulting model has been incorporated into the development of the MindTrial clinical trial recruiting system. The prototype for clinical trial recruitment illustrates the effectiveness of the methodology in characterizing clinical trials and subjects and accurate matching between them.",
        "Context-based methods for text categorisation. We propose several context-based methods for text categorization. One method, a small modification to the PPM compression-based model which is known to significantly degrade compression performance, counter-intuitively has the opposite effect on categorization performance. Another method, called C-measure, simply counts the presence of higher order character contexts, and outperforms all other approaches investigated. Categories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous General TermsAlgorithms,Verification Keywords language modeling,text categorization MOTIVATIONThe purpose of this paper is to report on the performance of various context based methods for text categorisation. This work is part of a larger project incorporating biometrics for authentication, user modelling/profiling and user interface optimisations. Hence we seek a general classifier that is able to cope well with a variety of different data streams, including user interactions, mouse movements, and textual input. An important component of authentication is authorship attribution, a multi-cass text categorization problem, so our work described here draws upon and extends on the work described in .The traditional machine learning approach to text categorization is either word or feature-based . However, we expect to encounter very different data streams to the textual data that is encountered in standard text categorization tasks. In these situations, the notion of a word may simply be meaningless, and the definition of what constitutes a useful feature both non-obvious and problematical. A character-based approach on the other hand neatly sidesteps these issues by treating all information as important, and the standard Markov-based assumption of restricting to fixed order contexts can be applied to reduce the computational overheads and statistical inaccuracies from insufficient training data. Results published in [2] also strongly indicate that a character-based approach may in fact be superior to the more traditional approach for authorship attribution tasks.In the next sections, we describe two context based methods, one a relative entropy based method, and another a simple context counting method. Experimental results are subsequently discussed, with discussion in the final section. PPM-BASED LANGUAGE MODELSThe main idea behind using relative entropy for classification is that it well ground in information theory. To guess the correct class of the document, one uses the formul\u00e2where where contexti = x1, x2, . . . xi\u22121. In practice, PPM uses a Markov approximation and assumes a fixed order context. PPM uses an approximate blending technique called exclusions based on the escape mechanism to exclude lower order predictions from the final probability estimate. A fixed order of around 5 or 6 is found to work well in practice, but increasing beyond that can degrade performance.",
        "Should I Visit This Place? Inclusion and Exclusion Phrase Mining from Reviews Although several automatic itinerary generation services have made travel planning easy, often times travellers find themselves in unique situations where they cannot make the best out of their trip. Visitors differ in terms of many factors such as suffering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowledge this is the first work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classifier provides a binary overlap F1 of \u223c80 and \u223c82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classifier provides an F1 of \u223c98 and \u223c97 for 11-class inclusion and exclusion classification respectively. We believe that our work can significantly improve the quality of an automatic itinerary generation service.",
        "Bootstrapping a Game with a Purpose for Commonsense Collection Text mining has been very successful in extracting huge amounts of commonsense knowledge from data, but the extracted knowledge tends to be extremely noisy. Manual construction of knowledge repositories, on the other hand, tends to produce high-quality data in very small amounts. We propose an architecture to combine the best of both worlds: A game with a purpose that induces humans to clean up data automatically extracted by text mining. First, a text miner trained on a set of known commonsense facts harvests many more candidate facts from corpora. Then, a simple slot-machine-with-a-purpose game presents these candidate facts to the players for verification by playing. As a result, a new dataset of high precision commonsense knowledge is created. This combined architecture is able to produce significantly better commonsense facts than the state-of-the-art text miner alone. Furthermore, we report that bootstrapping (i.e., training the text miner on the output of the game) improves the subsequent performance of the text miner.",
        "Mining translations of OOV terms from the web through cross-lingual query expansion. Translating out-of-vocabulary (OOV) terms is a great challenge for the Cross-lingual Information Retrieval and Datadriven Machine Translation systems. Several approaches have been proposed to mine translations for OOV terms from the web, especially from pages containing mixed languages. In this paper, we propose a novel approach to automatically translate OOV terms on the fly through crosslingual query expansion. The proposed approach does not require any web crawling and has achieved an inclusion rate of 95% and overall translation accuracy of 90%, outperforming state-of-the-art OOV translation techniques.",
        "Michael W. Berry and Jacob Kogan (eds.): Text mining: applications and theory - John Wiley and Sons, Ltd, 2010, xiv + 207 pp, \u00a355.00/66.00, hardcover, ISBN: 978-0-470-74982-1",
        "Evaluation of Disease-Associated Text-Mining Databases. There are about 20 million scientific articles in PubMed and this is a great source of knowledge. Extraction of information from the articles is one of challenges in biology and thus many text-mining approaches have been developed. However, the accuracy of textmined results is still in question. Here we evaluated three textmining databases with genes associated with Alzheimer's disease. Their per-gene accuracy is high (57-100%), but their per-abstract accuracy is relatively low (33-64%). This represents that the association of gene and disease is well-identified when abundant articles are available. However, genes with fewer articles could be wrongfully identified associated. Consequently, human-curation is still complementary to current text-mining approaches and future text-mining methods should improve their accuracy for genes with few articles or information."
    ],
    "The Ethics of Artificial Intelligence": [
        "An emotion-based responding model for natural language conversation. As an important task of artificial intelligence, natural language conversation has attracted wide attention of researchers in natural language processing. Existing works in this field mainly focus on consistency of neural response generation whilst ignoring the effect of emotion state on the response generation. In this paper, we propose an Emotion-based natural language Responding Model (ERM) to address the challenging issue in conversation. ERM encodes the emotion state of the conversation as distributed embedding into the process of response generation, redefines an objective function that jointly trains our model and introduces a novel re-rank function to select the appropriate response. Experimental results on Chinese conversation dataset show that our method yields qualitative performance improvements in the Perplexity (PPL), Word Error-rate (WER) and Bilingual Evaluation Understudy (BLEU) compared with the baseline sequence-to-sequence (Seq2Seq) model, and achieves better performance than the state-of-the-art in terms of emotion and content consistency of the response.",
        "Computer and Information Ethics, by John Weckert and Douglas Adeney",
        "Measuring the web crawler ethics. Web crawlers are highly automated and seldom regulated manually. The diversity of crawler activities often leads to ethical problems such as spam and service attacks. In this research, quantitative models are proposed to measure the web crawler ethics based on their behaviors on web servers. We investigate and define rules to measure crawler ethics, referring to the extent to which web crawlers respect the regulations set forth in robots.txt configuration files. We propose a vector space model to represent crawler behavior and measure the ethics of web crawlers based on the behavior vectors. The results show that ethicality scores vary significantly among crawlers. Most commercial web crawlers' behaviors are ethical. However, many commercial crawlers still consistently violate or misinterpret certain robots.txt rules. We also measure the ethics of big search engine crawlers in terms of return on investment. The results show that Google has a higher score than other search engines for a US website but has a lower score than Baidu for Chinese websites.",
        "Introduction to the Special Issue on Crowd in Intelligent Systems Crowdsourcing involves outsourcing tasks to a large number of people with unspecified skills and abilities. Because crowdsourcing leverages human intelligence, it is a wellsuited strategy to address problems that are difficult for computers. In recent years, there have been compelling demonstrations that information or judgments collected from the crowd can, when appropriately aggregated, be productively applied in a range of areas from knowledge management to scientific inquiry. Effectively, crowdsourcing extends the reach of intelligent systems by filling the gap between artificial intelligence and human intelligence with the wisdom of a crowd.This special issue contributes to addressing key research questions centered on the role of the crowd in intelligent systems, with the aim of promoting balanced and effective combinations of artificial and human intelligence. The articles in this special issue are positioned in the intersection of crowdsourcing, intelligent systems, and realworld applications. Topically, the articles interlock, but for the purposes of providing a structured overview, we divide them into three clusters.The first cluster addresses theoretical aspects of crowdsourcing in intelligent systems, focusing on incentives, as well as on methodologies and tools. \"Incentives for Effort in Crowdsourcing Using the Peer Truth Serum,\" by G. Radanovic, B. Faltings, and R. Jurca, proposes a game mechanism that creates incentives for crowdworkers to engage seriously with the tasks that they carry out. \"A Game-Theory Approach for Effective Crowdsource-Based Relevance Assessment,\" by Y. Moshfeghi, A. F. Huertas Rosero, and J. M. Jose, studies gamification in crowdsourcing in the context of relevance assessments. The article describes a game framework where competition among workers is used as incentive. \"PPLib: Towards the Automated Generation of Crowd Computing Programs Using Process Recombination and Auto-Experimentation,\" by P. de Boer and A. Bernstein, presents a methodology and program library that allows for the automated recombination of known processes stored in a repository. \"Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control,\" by S. Han, P. Dai, P. Paritosh, and D. Huynh, describes the design and implementation of Wernicke, a crowdsourcing tool that collects behavioral features and use these features to predict annotation quality. The focus is then on workers' behavioral signals instead of just past performance.The second cluster is dedicated to approaches that use crowdsourcing to improve information systems or create resources. \"Using the Crowd to Improve Search Result Ranking and the Search Experience,\" by Y. Kim, K. Collins-Thompson, and J. Teevan, explores crowdsourcing approaches aimed at improving the effectiveness of search systems. The article considers the tradeoff between the extra time required if the crowd is in the loop and provides suggestions on how to most appropriately incorporate the crowd. \"Crowdsourcing Empathetic Intelligence: The Case of the Annotation of EMMA Database for Emotion and Mood Recognition,\" by C. Katsimerou, J. Albeda, A. Huldtgren, I. Heynderickx, and J. A. Redi, discusses how crowdsourcing can be used 2016",
        "The organization and use of information: Contributions of information science, computational linguistics and artificial intelligence",
        "Artificial intelligence: Underlying assumptions and basic objectives",
        "Artificial intelligence in healthcare robots: A social informatics study of knowledge embodiment",
        "Is It Possible to Preserve Privacy in the Age of AI? Artificial Intelligence (AI) hopes to provide a positive paradigm shift in technology by providing new features and personalized experience to our digital and physical world. In the future, almost all our digital services and physical devices will be enhanced by AI to provide us with better features. However, as training artificially intelligent models require a large amount of data, it poses a threat to user privacy. The increasing prevalence of AI promotes data collection and consequently poses a threat to privacy. To address these concerns, some research efforts have been directed towards developing techniques to train AI systems while preserving privacy and help users preserve their privacy. In this paper, we survey the literature and identify these privacy-preserving approaches that can be employed to preserve privacy. We also suggest some future directions based on our analysis. We find that privacy-preserving research, specifically for AI, is in its early stage and requires more effort to address the current challenges and research gaps.",
        "Ethics and Computing: Living Responsibly in a Computerized World, by Kevin W. Bowyer",
        "Between Rigour and Ethics in Studies Involving Participants with Disabilities: A Tutorial",
        "Exploring Potential Pathways to Address Bias and Ethics in IR. The interplay between human biases and the underlying data collection and algorithmic methods to present users with relevant information in information retrieval (IR) systems have undesirable side effects, such as filter bubbles, censorship (a human rights concern with respect to freedom of expression) and development of beliefs in false information. IR systems and humans that interact with them both have biases , raising ethical concerns . On a societal level, the current issues in IR systems, as well as proposed solutions, raise many ethical concerns.Previous work in the areas of biases in interactive information retrieval, document classification, behavioral economics and user profiling (e.g. respectively) provide the foundation for our research. Using existing knowledge about human bias and profile data, we propose leveraging this information to inform users about their behavior in the frame of IR systems and inferences made to allow for the possibility of better decisions. It is our goal to understand to what extent user behavior is changed in light of such information.Our position is that education and awareness are much better approaches to address the ethical and human rights concerns when compared to regulatory measures and non-transparent changes to IR algorithms. Recent work by Fuhr et al. [4] is one of many possible educational approaches, and provides useful guidance into potential information to collect about users and how it might be presented.Nudge and boost approaches have achieved desirable behavior in non-IR domains (e.g. health and retirement decision making), with limited but promising results in IR itself (e.g. [3]). Our research investigates the possibility to use profiling data about users to 'nudge' or 'boost' users towards more desirable behavior in search and to gain insight into how users would interact and respond to such an approach.The research assumes that it is in the interest of society and the individual to minimize consumption of information that is toxic, misleading / deceitful, and of low quality. It also assumes that maximization of exposure to multiple perspectives is a worthy goal. Thus, an important component of the main study will be to determine the agreement of participants with this view.Research Questions: To what extent is cognitive load increased due to a user being presented with a profile about their behavior? Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8-12, 2018, Ann Arbor, MI, USA \u00a9 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10. 1145/3209978.3210218 For users that (agree vs. disagree) with our assumptions about most desirable consumption behavior, to what extent has their behavior changed in relation to their information biases? For users that (agree vs. disagree) with our assumptions about most desirable consumption behavior, to what extent would they choose to have information presented in an alternative manner (e.g. re-ranking) to address their biases?Probabilistic methods to classify documents, as well as crowdsourced data from previous research, will be used to determine the factors such as toxicity, deceitfulness, quality and political slant of documents deemed to be relevant to participants and used to build their profile. This meta-information will be useful for presentation in a mock-SERP for which participants will interact. We will investigate behavioral changes through a combination of click, eye-tracking and survey data.It is believed our approach has the potential to dampen the effects of filter bubbles, reduce consumption of misleading and potentially hateful content, to broaden perspectives and protect the fundamental human right to freedom of expression. CCS CONCEPTS\u2022 Information systems \u2192 Users and interactive retrieval; \u2022 Social and professional topics \u2192 User characteristics; KEYWORDS information retrieval, human decision making, human rights ACKNOWLEDGMENTS",
        "Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned. Researchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learned models and data-driven systems, and the potential for such systems to discriminate against certain population groups, due to biases in algorithmic decision-making systems. This tutorial aims to present an overview of algorithmic bias / discrimination issues observed over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving fairness in machine learning systems. We will motivate the need for adopting a \"fairness-first\" approach (as opposed to viewing algorithmic bias / fairness considerations as an afterthought), when developing machine learning based models and systems for different consumer and enterprise applications. Then, we will focus on the application of fairness-aware machine learning techniques in practice, by presenting case studies from different technology companies. Based on our experiences in industry, we will identify open problems and research challenges for the data mining / machine learning community. OUTLINE OF THE TUTORIAL 1.Legal Frameworks on Bias and DiscriminationOver the last several decades, legal frameworks around the world have prohibited the use of protected attributes such as race, gender, sexual orientation, and age in certain decision making tasks. Disparate treatment and disparate impact are two common approaches for enforcing such laws in practice. Avoiding disparate treatment requires that protected attributes should not be actively used as a criterion for decision making and no group of people should Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s be discriminated because of their membership in some protected group. Avoiding disparate impact requires that the end result of the decision making task should result in equal opportunities for members of all protected groups irrespective of how the decision is made.",
        "Parsimonious citer-based measures: The artificial intelligence domain as a case study",
        "Artificial intelligence and the world of work, a co-constitutive relationship",
        "AI Principles in Identifying Toxicity in Online Conversation: Keynote at the Third Workshop on Fairness, Accountability, Transparency, Ethics and Society on the Web",
        "Artificial Intelligence Implications for Information Retrieval",
        "Detecting online commercial intention (OCI). Understanding goals and preferences behind a user's online activities can greatly help information providers, such as search engine and E-Commerce web sites, to personalize contents and thus improve user satisfaction. Understanding a user's intention could also provide other business advantages to information providers. For example, information providers can decide whether to display commercial content based on user's intent to purchase. Previous work on Web search defines three major types of user search goals for search queries: navigational, informational and transactional or resource [1] . In this paper, we focus our attention on capturing commercial intention from search queries and Web pages, i.e., when a user submits the query or browse a Web page, whether he / she is about to commit or in the middle of a commercial activity, such as purchase, auction, selling, paid service, etc. We call the commercial intentions behind a user's online activities as OCI (Online Commercial Intention). We also propose the notion of \"Commercial Activity Phase\" (CAP), which identifies in which phase a user is in his/her commercial activities: Research or Commit. We present the framework of building machine learning models to learn OCI based on any Web page content. Based on that framework, we build models to detect OCI from search queries and Web pages. We train machine learning models from two types of data sources for a given search query: content of algorithmic search result page(s) and contents of top sites returned by a search engine. Our experiments show that the model based on the first data source achieved better performance. We also discover that frequent queries are more likely to have commercial intention. Finally we propose our future work in learning richer commercial intention behind users' online activities.",
        "Knowledge-Driven Analytics and Sensor Signal Processing in Human-Centric Applications Technology disruption through knowledge driven intelligent systems is increasingly controlling human life. Management of the present and future knowledge-driven artificial intelligence-based technologies is of highest importance to maximize its progressive influence to human life and human society. Life style diseases, social network affinity, impulsive financial decision, technology-abuse negatively affect our physical, emotional, social and mental health. Conversely, intelligent systems can bring positive impact on human life. This paper brings forward those positive applications and technologies as well as the path towards transformation of intelligent systems through some exemplary analysis that minimizes the negative impact. The push is to promote the development of human-centric intelligent technologies like precise and personalized medication and treatment plan, drug discovery of untreatable diseases, improved elderly care, minimizing private data theft, big data analytics for prediction of macro or micro economic condition, effective and fair trading practices, retail decision management, knowledge-driven energy and resource management, deep learning and artificial intelligence based applications for risk prediction and augmented human capability generation. The main focus of this paper is to demonstrate the knowledgedriven technologies, developments, applications for ensuring improvement of human quality of life. The impact would be micro-level, where human life is impacted in daily basis and at macro-level where human life would be impacted in long term that eventually influences the betterment to human society.",
        "Ethics, Information and Technology: Readings edited by Richard N. Stichler and Robert Hauptman",
        "Introduction to the Special Topic Issue: Artificial Intelligence Techniques for Emerging Information Systems Applications",
        "Automated opinion detection: Implications of the level of agreement between human raters a b s t r a c tThe ability to agree with the TREC Blog06 opinion assessments was measured for seven human assessors and compared with the submitted results of the Blog06 participants. The assessors achieved a fair level of agreement between their assessments, although the range between the assessors was large. It is recommended that multiple assessors are used to assess opinion data, or a pre-test of assessors is completed to remove the most dissenting assessors from a pool of assessors prior to the assessment process. The possibility of inconsistent assessments in a corpus also raises concerns about training data for an automated opinion detection system (AODS), so a further recommendation is that AODS training data be assembled from a variety of sources. This paper establishes an aspirational value for an AODS by determining the level of agreement achievable by human assessors when assessing the existence of an opinion on a given topic. Knowing the level of agreement amongst humans is important because it sets an upper bound on the expected performance of AODS. While the AODSs surveyed achieved satisfactory results, none achieved a result close to the upper bound.",
        "Applying the Publication Power Approach to Artificial Intelligence Journals",
        "CUSAT NLP@AILA-FIRE2019: Similarity in Legal Texts using Document Level Embeddings Text retrieval has taken its role in almost all domains of knowledge understanding. It has applications in the legal field where there is an extensive collection of structured and unstructured texts. Artificial Intelligence is now applied in this area to understand and retrieve legal documents. This paper explains a working model developed for the track Artificial Intelligence for Legal Assistance in Forum for Information Retrieval Evaluation, 2019 (AILA-FIRE2019). We have used an embedding model approach to represent these legal texts in a semantic vector space. The similarity between these document embeddings is found using an existing method of cosine similarity. The corpus used for building embedding models is the dataset provided in AILA-FIRE2019.",
        "Artificial intelligence: Concepts, techniques, applications, promise",
        "SoBigData: Social Mining & Big Data Ecosystem. One of the most pressing and fascinating challenges scientists face today, is understanding the complexity of our globally interconnected society. The big data arising from the digital breadcrumbs of human activities has the potential of providing a powerful social microscope, which can help us understand many complex and hidden socio-economic phenomena. Such challenge requires high-level analytics, modeling and reasoning across all the social dimensions above. There is a need to harness these opportunities for scientific advancement and for the social good, compared to the currently prevalent exploitation of big data for commercial purposes or, worse, social control and surveillance. The main obstacle to this accomplishment, besides the scarcity of data scientists, is the lack of a large-scale open ecosystem where big data and social mining research can be carried out. The SoBigData Research Infrastructure (RI) provides an integrated ecosystem for ethic-sensitive scientific discoveries and advanced applications of social data mining on the various dimensions of social life as recorded by \"big data\". The research community uses the SoBigData facilities as a \"secure digital wind-tunnel\" for large-scale social data analysis and simulation experiments. SoBigData promotes repeatable and open science and supports data science research projects by providing: i) an ever-growing, distributed data ecosystem for procurement, access and curation and management of big social data, to underpin social data mining research within an ethic-sensitive context; ii) an ever-growing, distributed platform of interoperable, social data mining methods and associated skills: tools, methodologies and services for mining, analysing, and visualising complex and massive datasets, harnessing the techno-legal barriers to the ethically safe deployment of big data for social mining; iii) an ecosystem where protection of personal information and the respect for fundamental human rights can coexist with a safe use of the same information for scientific purposes of broad and central societal interest. SoBigData has a dedicated ethical and legal board, which is implementing a legal and ethical framework.",
        "Tropes, history, and ethics in professional discourse and information science",
        "Artificial intelligence in information retrieval systems",
        "Supporting Ethical Web Research: A New Research Ethics Review. Research ethics is an important and timely topic. In academia, federally regulated Institutional Review Boards (IRBs) protect participants of human subjects research, and offer researchers a mechanism to assess the ethical implications of their work. Industry research labs are not subject to the same requirements, and may lack processes for research ethics review. We describe the creation of a new ethics framework and a research ethics submission system (RESS) within Microsoft Research (MSR). This RESS is customized to the needs of web researchers. We describe our iterative development process, including our assessment of the current state of web research, developing a framework of methods based on a survey of 358 research papers; build and evaluate our system with 14 users to identify the benefits and pitfalls of full deployment; evaluate how our system matches with existing federal regulations; and, suggest next steps for supporting ethical web research."
    ],
    "machine learning for more relevant results": [
        "Web search result summarization: title selection algorithms and user satisfaction. Eye tracking experiments have shown that titles of Web search results play a crucial role in guiding a user's search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pairwise modeling approach gives better results in terms of three offline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process -search success -that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success.",
        "Exploration-exploitation tradeoff in interactive relevance feedback. We study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively optimize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration.",
        "Mining officially unrecognized side effects of drugs by combining web search and machine learning. We consider the problem of finding officially unrecognized side effects of drugs. By submitting queries to the Web involving a given drug name, it is possible to retrieve pages concerning the drug. However, many retrieved pages are irrelevant and some relevant pages are not retrieved. More relevant pages can be obtained by adding the active ingredient of the drug to the query. In order to eliminate irrelevant pages, we propose a machine learning process to filter out the undesirable pages. The process is shown experimentally to be very effective. Since obtaining training data for the machine learning process can be time consuming and expensive, we provide an automatic method to generate the training data. The method is also shown to be very accurate. The side effects of three drugs which are not recognized by FDA are validated by an expert. We believe that the same approach can be applied to many real life problems and will yield high precision. Thus, this could lead a new way to perform retrieval with high accuracy.",
        "Deep Learning for Matching in Search and Recommendation. Matching is the key problem in both search and recommendation, that is to measure the relevance of a document to a query or the interest of a user on an item. Previously, machine learning methods have been exploited to address the problem, which learns a matching function from labeled data, also referred to as \"learning to match\" . In recent years, deep learning has been successfully applied to matching and significant progresses have been made. Deep semantic matching models for search and neural collaborative filtering models for recommendation are becoming the state-of-the-art technologies. The key to the success of the deep learning approach is its strong ability in learning of representations and generalization of matching patterns from raw data (e.g., queries, documents, users, and items, particularly in their raw forms). In this tutorial, we aim to give a comprehensive survey on recent progress in deep learning for matching in search and recommendation. Our tutorial is unique in that we try to give a unified view on search and recommendation. In this way, we expect researchers from the two fields can get deep understanding and accurate insight on the spaces, stimulate more ideas and discussions, and promote developments of technologies.The tutorial mainly consists of three parts. Firstly, we introduce the general problem of matching, which is fundamental in both search and recommendation. Secondly, we explain how traditional machine learning techniques are utilized to address the matching problem in search and recommendation. Lastly, we elaborate how deep learning can be effectively used to solve the matching problems in both tasks.",
        "Learning to suggest: a machine learning framework for ranking query suggestions. We consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines.",
        "A machine learning approach for result caching in web search engines a b s t r a c tA commonly used technique for improving search engine performance is result caching. In result caching, precomputed results (e.g., URLs and snippets of best matching pages) of certain queries are stored in a fast-access storage. The future occurrences of a query whose results are already stored in the cache can be directly served by the result cache, eliminating the need to process the query using costly computing resources. Although other performance metrics are possible, the main performance metric for evaluating the success of a result cache is hit rate. In this work, we present a machine learning approach to improve the hit rate of a result cache by facilitating a large number of features extracted from search engine query logs. We then apply the proposed machine learning approach to static, dynamic, and static-dynamic caching. Compared to the previous methods in the literature, the proposed approach improves the hit rate of the result cache up to 0.66%, which corresponds to 9.60% of the potential room for improvement.",
        "On composition of a federated web search result page: using online users to provide pairwise preference for heterogeneous verticals. Modern web search engines are federated -a user query is sent to the numerous specialized search engines called verticals like web (text documents), News, Image, Video, etc. and the results returned by these engines are then aggregated and composed into a search result page (SERP) and presented to the user. For a specific query, multiple verticals could be relevant, which makes the placement of these vertical results within blocks of textual web results challenging: how do we represent, assess, and compare the relevance of these heterogeneous entities?In this paper we present a machine-learning framework for SERP composition in the presence of multiple relevant verticals. First, instead of using the traditional label generation method of human judgment guidelines and trained judges, we use a randomized online auditioning system that allows us to evaluate triples of the form <query, web block, vertical>. We use a pairwise click preference to evaluate whether the web block or the vertical block had a better users' engagement. Next, we use a hinged feature vector that contains features from the web block to create a common reference frame and augment it with features representing the specific vertical judged by the user. A gradient boosted decision tree is then learned from the training data. For the final composition of the SERP, we place a vertical result at a slot if the score is higher than a computed threshold. The thresholds are algorithmically determined to guarantee specific coverage for verticals at each slot.We use correlation of clicks as our offline metric and show that click-preference target has a better correlation than human judgments based models. Furthermore, on online tests for News and Image verticals we show higher user engagement for both head and tail queries.",
        "Learning to rank for freshness and relevance. Freshness of results is important in modern web search. Failing to recognize the temporal aspect of a query can negatively affect the user experience, and make the search engine appear stale. While freshness and relevance can be closely related for some topics (e.g., news queries), they are more independent in others (e.g., time insensitive queries). Therefore, optimizing one criterion does not necessarily improve the other, and can even do harm in some cases.We propose a machine-learning framework for simultaneously optimizing freshness and relevance, in which the trade-off is automatically adaptive to query temporal characteristics. We start by illustrating different temporal characteristics of queries, and the features that can be used for capturing these properties. We then introduce our supervised framework that leverages the temporal profile of queries (inferred from pseudo-feedback documents) along with the other ranking features to improve both freshness and relevance of search results. Our experiments on a large archival web corpus demonstrate the efficacy of our techniques.",
        "Learning to Rank from Noisy Data Learning to rank, which learns the ranking function from training data, has become an emerging research area in information retrieval and machine learning. Most existing work on learning to rank assumes that the training data is clean, which is not always true, however. The ambiguity of query intent, the lack of domain knowledge, and the vague definition of relevance levels all make it difficult for common annotators to give reliable relevance labels to some documents. As a result, the relevance labels in the training data of learning to rank usually contain noise. If we ignore this fact, the performance of learning-to-rank algorithms will be damaged.In this article, we propose considering the labeling noise in the process of learning to rank and using a two-step approach to extend existing algorithms to handle noisy training data. In the first step, we estimate the degree of labeling noise for a training document. To this end, we assume that the majority of the relevance labels in the training data are reliable and we use a graphical model to describe the generative process of a training query, the feature vectors of its associated documents, and the relevance labels of these documents. The parameters in the graphical model are learned by means of maximum likelihood estimation. Then the conditional probability of the relevance label given the feature vector of a document is computed. If the probability is large, we regard the degree of labeling noise for this document as small; otherwise, we regard the degree as large. In the second step, we extend existing learning-to-rank algorithms by incorporating the estimated degree of labeling noise into their loss functions. Specifically, we give larger weights to those training documents with smaller degrees of labeling noise and smaller weights to those with larger degrees of labeling noise. As examples, we demonstrate the extensions for McRank, RankSVM, RankBoost, and RankNet. Empirical results on benchmark datasets show that the proposed approach can effectively distinguish noisy documents from clean ones, and the extended learning-to-rank algorithms can achieve better performances than baselines.",
        "Machine Learning Approach for Homepage Finding Task This paper describes new machine learning approaches to predict the correct homepage in response to a user's homepage finding query. This involves two phases. In the first phase, a decision tree is generated to predict whether a URL is a homepage URL or not. The decision tree then is used to filter out non-homepages from the webpages returned by a standard vector space IR system. In the second phase, a logistic regression analysis is used to combine multiple sources of evidence on the remaining webpages to predict which homepage is most relevant to a user's query. 100 queries are used to train the logistic regression model and another 145 testing queries are used to evaluate the model derived. Our results show that about 84% of the testing queries had the correct homepage returned within the top 10 pages. This shows that our machine learning approaches are effective since without any machine learning approaches, only 59% of the testing queries had their correct answers returned within the top 10 hits.",
        "Reinforcement Learning to Rank. :Interactive systems such as search engines or recommender systems are increasingly moving away from single-turn exchanges with users. Instead, series of exchanges between the user and the system are becoming mainstream, especially when users have complex needs or when the system struggles to understand the user's intent. Standard machine learning has helped us a lot in the singleturn paradigm, where we use it to predict: intent, relevance, user satisfaction, etc. When we think of search or recommendation as a series of exchanges, we need to turn to bandit algorithms to determine which action the system should take next, or to reinforcement learning to determine not just the next action but also to plan future actions and estimate their potential pay-off. The use of reinforcement learning for search and recommendations comes with a number of challenges, because of the very large action spaces, the large number of potential contexts, and noisy feedback signals characteristic for this domain. This presentation will survey some recent success stories of reinforcement learning for search, recommendation, and conversations; and will identify promising future research directions for reinforcement learning for search and recommendation.",
        "Hybrid.AI: A Learning Search Engine for Large-scale Structured Data. Variety of Big data ] is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information.Here we describe Hybrid.AI, a learning search engine for largescale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs) to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain.",
        "Improving ad relevance in sponsored search. We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs.Our relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.",
        "Probabilistic models for personalizing web search. We present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries.",
        "Estimating Semantic Similarity between Expanded Query and Tweet Content for Microblog Retrieval This paper reports the systems we submitted to the Microblog Track shared in TREC 2014 which focuses on ad hoc retrieval (i.e., retrieving top 1, 000 relevant tweet for every given topic). To address this task, we adopted a two-stage framework, i.e., firstly, we performed query expansion (i.e., expanding relevant inforamtion using pseudorelevance feedback and Google search engine results) to retrieve more relevant tweets, then extracted several effective semantic features (e.g., Jansen-Shannon Distance, Overlap Similarity, Lucene Score, etc) from retrieved results and built ranking model using supervised machine learning algorithms with the aid of these features to perform re-ranking. Our systems ranked 3th out of 21 teams.",
        "Virtual relevant documents in text categorization with support vector machines. This paper explores the incorporation of prior knowledge into support vector machines as a means of compensating for a shortage of training data in text categorization. The prior knowledge about transformation invariance is generated by a virtual document method. The method applies a simple transformation to documents, i.e., making virtual documents by combining relevant document pairs for a topic in the training set. The virtual document thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting relevant terms that are not given high importance in individual real documents. Artificially generated documents result in the change in the distribution of training data without the randomization. Experiments with support vector machines based on linear, polynomial and radial-basis function kernels showed the effectiveness on Reuters-21578 set for the topics with a small number of relevant documents. The proposed method achieved 131%, 34%, 12% improvements in micro-averaged F 1 for 25, 46, and 58 topics with less than 10, 30, and 50 relevant documents in learning, respectively. The result analysis indicates that incorporating virtual documents contributes to a steady improvement on the performance.",
        "A dynamic bayesian network click model for web search ranking. As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias -urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.",
        "An Online Learning Framework for Refining Recency Search Results with User Click Feedback Traditional machine-learned ranking systems for Web search are often trained to capture stationary relevance of documents to queries, which have limited ability to track nonstationary user intention in a timely manner. In recency search, for instance, the relevance of documents to a query on breaking news often changes significantly over time, requiring effective adaptation to user intention. In this article, we focus on recency search and study a number of algorithms to improve ranking results by leveraging user click feedback. Our contributions are threefold. First, we use commercial search engine sessions collected in a random exploration bucket for reliable offline evaluation of these algorithms, which provides an unbiased comparison across algorithms without online bucket tests. Second, we propose an online learning approach that reranks and improves the search results for recency queries near real-time based on user clicks. This approach is very general and can be combined with sophisticated click models. Third, our empirical comparison of a dozen algorithms on real-world search data suggests importance of a few algorithmic choices in these applications, including generalization across different query-document pairs, specialization to popular queries, and near real-time adaptation of user clicks for reranking.",
        "A regression framework for learning ranking functions using relative relevance judgments. Effective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods.",
        "The Title Says It All: A Title Term Weighting Strategy For eCommerce Ranking. Search relevance is a very critical component in e-commerce applications. One of the strongest signals that determine the relevance of an item listing to an e-commerce query is the title of the item. Traditional methods for capturing this signal compare words in listing titles and the user query using tf-idf scores, or use a machine learned model with words as features and target clicks or relevance labels. Contrary to these approaches, we build a parameterized model to determine the weights of popular title terms for a query and then use these title term weights to compute the relevance of a listing title to the query. For this, we use human judged binary relevance labels of query and item title pairs as labeled data and train a model leveraging a variety of features to learn these query specific title term weights. We propose two novel approaches to model these title term weights using the relevance target and explore several novel features specific to e-commerce for this term weighting model. We use the resulting title relevance score as a feature in eBay's machine learned ranker for e-commerce search serving millions of queries each day. We observe a significant improvement over a baseline click-based binary independence model for capturing item title relevance in several metrics including model accuracy and overall relevance and engagement observed through A/B testing. We also experimentally illustrate that this feature optimized for relevance works well in conjunction with textual features optimized for demand."
    ],
    "Crawling websites using machine learning": [
        "DistanceRank: An intelligent ranking algorithm for web pages. A fast and efficient page ranking mechanism for web crawling and retrieval remains as a challenging issue. Recently, several link based ranking algorithms like PageRank, HITS and OPIC have been proposed. In this paper, we propose a novel recursive method based on reinforcement learning which considers distance between pages as punishment, called ''DistanceRank'' to compute ranks of web pages. The distance is defined as the number of ''average clicks'' between two pages. The objective is to minimize punishment or distance so that a page with less distance to have a higher rank. Experimental results indicate that DistanceRank outperforms other ranking algorithms in page ranking and crawling scheduling. Furthermore, the complexity of DistanceRank is low. We have used University of California at Berkeley's web for our experiments.",
        "The Representativeness of Automated Web Crawls as a Surrogate for Human Browsing Large-scale Web crawls have emerged as the state of the art for studying characteristics of the Web. In particular, they are a core tool for online tracking research. Web crawling is an attractive approach to data collection, as crawls can be run at relatively low infrastructure cost and don't require handling sensitive user data such as browsing histories. However, the biases introduced by using crawls as a proxy for human browsing data have not been well studied. Crawls may fail to capture the diversity of user environments, and the snapshot view of the Web presented by one-time crawls does not reflect its constantly evolving nature, which hinders reproducibility of crawl-based studies. In this paper, we quantify the repeatability and representativeness of Web crawls in terms of common tracking and fingerprinting metrics, considering both variation across crawls and divergence from human browser usage. We quantify baseline variation of simultaneous crawls, then isolate the effects of time, cloud IP address vs. residential, and operating system. This provides a foundation to assess the agreement between crawls visiting a standard list of high-traffic websites and actual browsing behaviour measured from an opt-in sample of over 50,000 users of the Firefox Web browser. Our analysis reveals differences between the treatment of stateless crawling infrastructure and generally stateful human browsing, showing, for example, that crawlers tend to experience higher rates of third-party activity than human browser users on loading pages from the same domains.",
        "Researcher homepage classification using unlabeled data. A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying \"irrelevant\" pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end, we design novel URL-based features and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.In addition, we propose a novel technique for \"learning a conforming pair of classifiers\" using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make \"similar\" predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Miscellaneous General TermsAlgorithms Keywords co-training, consensus maximization, gradient descent MOTIVATIONProfessional homepages of researchers, which typically summarize research interests, publications and other metadata related to researchers, are shown to be rich sources of information for digital libraries . Researchers' homepages Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.WWW 2013, May 13-17, 2013, Rio de Janeiro, Brazil. ACM 978-1-4503-2035 (also referred to as academic homepages or simply homepages in this paper) have been successfully employed in tasks such as expertise search , extraction of academic networks, author profile extraction and disambiguation as they provide crucial evidence for improving these tasks in digital libraries.Furthermore, digital library systems such as CiteSeer 1 , ArnetMiner 2 , and Google Scholar 3 are primarily interested in obtaining and tracking researchers' homepages in order to retrieve appropriate scientific research publications. Given the infeasibility of collecting the entire content on the Web, a focused crawler aims to minimize the use of network bandwidth and hardware by selectively crawling only pages relevant to a (specified) set of topics . A key component for such a crawler is a classification module that identifies whether a webpage being accessed during the crawl process is potentially useful to the collection. For digital libraries, the \"yield\" of such crawlers highly depends on the accuracy of researcher homepage classification.Supervised methods for learning homepage classifiers rely on the availability of large amounts of labeled data. A widely used labeled dataset for webpage classification is the WebKB dataset 4 that is built in 1997. However, due to recent changes in the information content on academic websites, this dataset is becoming outdated. For example, there are now pages on academic websites that are related to various activities such as invited talks, news, events that do not occur in the WebKB dataset. We refer to university, department and research center websites as \"academic websites\" in this paper. Compared to few decades back, it is easier now to find faculty information, links to their homepages, information on research groups, course related notes and documents, and research papers from academic websites. Similarly, job postings, seminar announcements and notices are also being uploaded onto departmental websites in recent times .How can a homepage classifier keep up in the face of rapidly changing types of pages on the Web? Specifically, given a classifier that identifies homepages with reasonable accuracy (as measured on the training datasets), how does it perform in the potentially different deployment environment? Semi-supervised methods that can exploit large amounts of unlabeled data together with limited amounts of labeled data for learning accurate classifiers have received significant attention in recent research in machine learning due to Against this background, one question that can be raised is: Can we design techniques to effectively adjust the previouslytrained classifier to the changed content on the Web, while minimizing the human effort required for labeling new data, and under what conditions, can such an adjustment be possible? The research that we describe in this paper addresses specifically this question.Contributions and Organization. We present two approaches to researcher homepage classification using unlabeled data readily available from academic websites. More precisely, we first adapt the well-known co-training approach to reflect the change in the data distribution over time. Second, we design an iterative algorithm based on the Minibatch Gradient Descent technique for learning a conforming pair of predictors using two different views of the data. We restrict ourselves to homepages of researchers in Computer Science since training datasets are available for this discipline. To the best of our knowledge, the problem of researcher homepage classification using unlabeled data available during focused crawling was not addressed in previous research. The contributions of this work are as follows:\u2022 We show that with the classifiers trained on existing datasets for researcher homepage classification we incorrectly identify pages of types not seen in the training datasets as homepages. This results in unacceptable yield from the perspective of a focused crawler.\u2022 We design novel features based on URL surface patterns and terms to complement term and HTML features extracted from the content of homepages and show that these two sets of features can be treated as two \"views\" for a researcher homepage instance.\u2022 We show that the URL and content-based views can be used successfully in a co-training setup to adapt classifiers to the changing academic environments using unlabeled data. This finding enables us to accurately crawl the new academic website content without having to label a new dataset for re-training the classifier.\u2022 Inspired by the success of co-training on this problem, we investigate loss functions that capture the disparity between the classifiers' predictions on the two views afforded by co-training. We design an iterative algorithm based on the Mini-batch Gradient Descent technique for minimizing this loss and learning a conforming pair of predictors with the two views.\u2022 Finally, we show that minimizing our proposed loss function on unlabeled data closely corresponds to the effect demonstrated by co-training techniques. We posit that this loss can, therefore, be used as a measure, for tracking the progress of co-training schemes even in the absence of a validation dataset.Although in this paper we focus on the design of accurate approaches for researcher homepage classification, our objective is to integrate this classification component in the context of focused crawling, to improve retrieval and indexing of scientific publications in digital libraries such as CiteSeer and ArnetMiner. In these usage environments, since maintaining up-to-date collections of research literature is of primary importance, having an accurate list of homepage URLs for frequent, periodic tracking is both feasible and scalable, compared to examining the entire content at academic websites each time. The rest of this paper is organized as follows: We briefly summarize closely related work in Section 2. Researcher homepage classification is discussed in Section 3. We elaborate on details of our co-training experiments and learning conforming predictor pairs in Section 4. Experimental setup, datasets and results are discussed in Section 5, followed by a summary and future extensions to our work. RELATED WORKResearcher homepage classification is a well-studied webpage classification problem in context of digital libraries such as CiteSeer and ArnetMiner . Typically, contentbased term features and HTML structure-based features are used for classifying webpages . We propose the use of URL features as additional evidence for homepage identification. A smaller set (compared to ours) of URL-based features (presence of part of the name, presence of the character '\u223c', etc.), was used in isolating homepages among the search engine results for researcher name queries by Tang, et al [40]. URL-based features are widely used in tasks related to the Web. For example, URL strings were used for extracting rules to solve the webpage de-duplication problem . Shih and Karger [38] used URL features, the visual placements of links in the referring pages to a URL for improving applications such as ad-blocking and recommendation, whereas a preliminary study by Kan and Thi [20] illustrates the use of URLs in performing fast webpage classification.The problem of collecting a high-quality researcher homepage collection was studied for Japanese websites by Wang, et al. using on-page and anchor text features . Tang, et al. studied homepage acquisition from search engine results using researcher names as queries . In contrast, we seek to apply focused crawling using a seed list of academic websites (where researcher homepages are typically hosted) to acquire such a collection. Focused crawling first proposed by Bra, et al. is a rich area of research on the Web . Chakrabarti, et al. [7] present a discussion on the main components involved in building a focused crawler. Although focused crawling is our motivating application, this paper deals with the classifier component of the crawler and not with the crawler itself.We show that the focused crawling scenario presents novel challenges in using a pre-trained homepage classifier in identifying relevant pages. Specifically, the classifier needs to be attuned to the changing types of pages on the Web. Cotraining is proposed as a solution for addressing this challenge for homepage classification. Blum and Mitchell [4] first proposed co-training, an approach for semi-supervised learning when the number of labeled examples available for training is limited, and applied it to webpage classification. This approach requires having two views of features for the instances and has been shown to work well when the two views satisfy certain assumptions on \"sufficiency\" and \"independence\" . Recent research addresses techniques for decomposing the feature set into two views when such a split is not naturally available .Multiview learning (of which co-training is a special case, with two views) is typically addressed by maximizing \"consensus\" or agreement among the different views . Most solutions to multiview learning tend to frame the problem in terms of a global optimization problem and simulta-472 neously learn classifiers for all the underlying views. In some cases, the solutions depend on underlying classification algorithm used . Although our proposed algorithm based on mini-batch gradient descent seeks to maximize consensus as well, our approach is a generic technique assuming only that the underlying classifiers output initial \"parameter vectors\" that are altered using a simple, iterative algorithm. FEATURES FOR HOMEPAGE CLASSIFICATIONWebpage or text classification is typically handled using \"bag of words\" approaches. Specifically, the frequently occurring and discerning terms are collected from training data to form a feature dictionary that is used to represent instances as normalized term frequency or TFIDF vectors . Homepage classification was previously studied as a text classification problem using term features . Previous work on the same problem also used other content-based features related to the HTML structure of the page such as the number of images/tables on the page, and the terms commonly found in anchor text of homepages . In this study, we extracted content-based and URL-based features from our training sets. These features and the size of feature sets are summarized in . The term dictionaries contain terms that occur in at least three documents (i.e., webpages) and at least five times in the training set.In addition to term dictionaries, we hypothesize that the URL strings of homepages can provide additional evidence for identifying homepages. Hence, we design novel URLbased features based on surface patterns and presence in WordNet 5 . The URL-based features are explained in the next subsection. URL strings as additional evidenceThe idea of using URL strings in academic homepage identification comes from an error analysis of a crawl obtained with the content-based classifier. Consider some example URLs we encountered in our crawl listed in .With some knowledge in academic browsing, one can confidently guess that the webpages at the URLs (1), (2), and (3) are unlikely to be researcher homepages. Similarly, among the URLs (4), and (5), while the former seems to be a homepage, the latter seems to lead to a course page. The above conjectures are based on the presumption that the URL strings are not \"arbitrary\", but, instead conventions are observed that are indicative of the target content at the URL. For instance, in the previous examples, words such as \"projects\", \"events\", alphanumeric patterns of the terms in the URL indicate that the URLs, (1), (2), (3) and (5) are most possibly not researcher homepages.Treating \"/\" as delimiters, we extract features from the URL string following the domain name of a webpage. The list of all unigrams and bigrams from URL strings that occur more than thrice in the training dataset, comprise the URL-term dictionary. For terms in the URL not present in this dictionary, we look for their presence in WordNet to check if they are common words or proper nouns. WordNet is a large, lexical database of nouns, verbs, adjectives and adverbs for English, organized as a concept graph .In addition, we capture the surface patterns of the URLs including the presence of hyphenated or underscored words, alphanumeric patterns, long words (i.e., words having greater than 30 characters), question marks and the presence of characters such as tilde. These features are designed to filter out the URLs that commonly represent course pages, announcements, calendars and other auto-generated content. For instance, a typical homepage URL string in Computer Science departments has the name of the researcher following the \u223c character after the domain name (e.g., http://people.cs.umass.edu/\u223cmccallum/). This pattern is usually captured by our \"TILDENONDICT\" feature, where mccallum is a non-dictionary term. Partial sets of extracted features are shown along with the URLs listed in .The above sets of features perform very well on the training datasets as shown Section 5. We, therefore, do not study other complicated, problem-specific feature design or feature selection. Instead our focus in this work is to study how these classifiers perform \"in the wild\". We also note here that, a classifier that can make accurate predictions using URL features can be quite beneficial from the perspective of efficiency for a focused crawler. A crawler can potentially bypass examining the content of a page if a confident decision can be made based on the URL string. However, we may not be able to always extract features from the URL strings. For instance, consider the following URLs from our crawls:http://john.blitzer.com/ http://clgiles.ist.psu.edu/ http://ben.adida.net/ In these cases, it is not clear from the URL string that the target content refers to academic homepages. Even if complicated name-extraction based features were designed for the above cases, it is rare to find academic homepages with '.com' and '.net' domain suffixes. Based on the URL alone, we cannot be confident if the target content is an academic homepage or a company/personal homepage. For the second case, 'clgiles' could refer to a machine name. In addition to the above cases, given that feature dictionaries typically comprise features that meet a frequency requirement, we may not be able to extract features for all URLs. In our training datasets (Section 5), we were unable to extract URL features for about 27% of the instances. Therefore, contentbased and URL features complement each other while identifying homepage instances and a focused crawler might be required to use either or both of these sets of features. HOMEPAGE CLASSIFICATION USING UNLABELED DATAWe show in our experiments (Section 5) that, although content-based features perform extremely well on the training datasets, they are not very successful on the validation and test sets that were collected from the current-day academic websites. On the other hand, URL features show good performance on both training and validation datasets. However, as pointed out in the previous section, we may not be able to extract URL features for all instances and it is, therefore, imperative to have an accurate content-based classifier as well. We now address the questions: Can we adapt the contentbased classifier to perform well in the deployment environment with the help of the URL-based classifier? Can the two classifiers \"teach\" each other so as to perform better in the new environment, using the co-training approach? Since the URL and content features provide evidence for classifying a webpage instance independently, intuitively, it appears possible that there are instances that the URL classifier makes mistakes on, which the content-based classifier identifies correctly and vice versa.Blum and Mitchell proposed co-training in context of webpage classification . In their datasets, webpages are representable in terms of two distinct views: using terms on webpages and terms in the anchor text of hyperlinks pointing to these pages. When few labeled examples are available for training, they showed that co-training could be used to obtain predictions on the unlabeled data to enlarge the training set. Blum and Mitchell's experiments and the subsequent experiments by Nigam and Ghani showed that when a natural split of features is available, co-training that explicitly leverages this split has the potential to outperform classifiers that do not.We study the applicability and extension of co-training for our problem. Although the essential motivation is to make use of the naturally available feature split and enable classifiers to learn from each other, we highlight the following aspects of our setup: Previous studies and benefits from co-training were illustrated on datasets where the unlabeled data is arguably from a similar distribution. That is, the positive and negative instances in the labeled datasets are representative of those in the unlabeled data. This is in contrast to our case, where our positive class is fairly welldefined (homepages), whereas the negative class is described in terms of \"not positive\". More precisely, although our training dataset has examples for the negative class, webpages encountered during the crawls can belong to types not encountered in the labeled data. We present an error analysis in Section 5, that illustrates the \"new\" types of webpages encountered in our crawl, potentially causing the pre-trained content-based classifiers to underperform during crawling.The number of negative instances encountered during our crawls is higher in comparison with the number of positive instances. While this aspect was noticed during our experiments, a previous estimation experiment using markrecapture methods had indicated that academic homepages comprise a minute fraction of the Web . We can expect this imbalance to become more prominent as more examples are sampled over the co-training rounds. In the algorithm studied by Blum and Mitchell, the ratio between the number of positive and negative instances added from the unlabeled data is maintained to be the same as that in the training dataset during each iteration of co-training . We argue that avoiding this constraint is better in our scenario since we want the datasets to be more representative of the changing distribution.Most classification algorithms are sensitive to the number of positive and negative instances available in the training data and are known to learn biased classifiers in case of severe imbalance . We employ the idea of altering the mis-classification costs for the underlying classifiers during each round of co-training to handle this problem. For example, if the training dataset has 10 positive and 100 negative instances, we can set the penalty incurred on making mistakes on a negative instance to be 1 10 th of the penalty incurred on making mistakes on a positive instance. For most implementations of classification algorithms, the misclassification costs can be specified as a parameter during the training process .Our co-training setup is detailed in Algorithm 1. L and U represent the labeled and unlabeled datasets, respectively, available at each iteration. They comprise instances with both the views (content-based and URL-based feature sets). For a round of co-training, we train classifiers, C1 and C2, on the two available views, using misclassification costs, \u03c11 and \u03c12, respectively. Next, \"s\" number of examples are sampled without replacement into S from the unlabeled data and C1 and C2 are used to obtain predictions for these instances. The GetConf identEgs method is a generic placeholder that stands for a function that determines what instances from S are chosen for addition in subsequent rounds of co-training. We use the notation L + 1 to represent the positive instances in the set L1 whereas L 1 1 indicates that the view 1 (or feature set 1) of the examples in L1 is being used.Based on previous studies in co-training , we studied the following strategies for this function:\u2022 AddBoth: In this scheme, we add all examples from S that are labeled by C1 or C2 confidently to the training set for the next round. This approach is similar to selftraining used in semi-supervised learning where confidently predicted unlabeled instances are added to the training set for retraining the classifier in subsequent rounds . However, in contrast with self-training that uses a single view, in AddBoth, confident predictions are obtained from two sources (view 1 and 2) for addition into subsequent rounds.\u2022 AddCross: In this scheme, examples from S, confidently labeled by C1 are added to view 2 for the next 474 round and vice versa. That is, we use the examples confidently labeled by one classifier while training the other classifier in the next round. Cross-addition also seems resilient to handling the possibility of cascaded errors over the iterations. If a classifier makes a confident but incorrect prediction, we would like to avoid feeding this example in the next round to the same classifier, a common problem in self-training [45].\u2022 AddCrossRC: This scheme is similar to AddCross with the additional constraint on the number of positive and negative instances added in each round. This constraint was originally studied by Blum and Mitchell and ensures that the ratio of the number of positive and negative instances added in each round is the same as that in the initial labeled dataset .Algorithm 1 Procedure for Co-trainingCompute \u03c1 1 usingThe co-training algorithm is general and can be applied with any choice of classifiers on the two views. Blum and Mitchell provided a PAC-style analysis of co-training with probabilistic classifiers and showed that co-training works when the assumptions on sufficiency and independence are met. That is, each view should be sufficient to predict the class label, and the two views are independent given the class label. Recent studies have proposed relaxed criteria under which co-training techniques still work . However, in practice, it is tricky to judge if co-training works for a problem and to verify if the assumptions are satisfied . These questions are more relevant in context of recent research in obtaining two views from a single view when two views are not naturally available for applying co-training . With this context, we now discuss our formulation of the effect obtained with co-training, in terms of a loss function. This formulation allows us to track whether the co-training process is beneficial for a given problem, even without the use of a validation dataset. Learning Conforming Predictors on Unlabeled DataWe assume that classifiers, C1 and C2 trained on the two views are parameterized in terms of their weight vectors, w1 and w2. Most classification algorithms e.g., Support Vector Machines (SVM) and Maximum Entropy (MaxEnt), output weight vectors capturing the importance of each feature as part of the training process .One can expect co-training to benefit a classification problem if one classifier (say, C1) can \"guide\" the other (C2) on examples that the latter makes mistakes on. This guidance is provided by adding examples confidently labeled by C1 to the subsequent round of training C2. This observation hints at the possibility of directly manipulating C2, based on C1's prediction for an example that C2 is not confident about. This effect can be achieved by optimizing a function that directly captures the mismatch in the predictions of the two classifiers.Elaborating further, given that the concept classes, \"positive\" and \"negative\" are still the same on unlabeled data, if C1 and C2 are accurate, they would make similar predictions on the unlabeled data. This intuition is the basis for \"consensus maximization\" widely adopted in multiview learning, of which co-training is a special case with two views . The mismatch in predictions by C1 and C2 on unlabeled data can be quantified using a loss function. The squared error loss function commonly used in machine learning captures this loss as:The above formulation captures the average squared-difference in predictions from the two views on unlabeled data. w1 and w2 correspond to the parameter vectors corresponding to C1 and C2, respectively, and u refers to an example from U , having two views, u1 and u2. For a given example, u = (u1, u2), the functions, f1 and f2 act on u1 and u2 respectively, and make the predictions from C1 and C2 comparable. These functions could be generic (e.g. a function that outputs the probability that the instance is positive) or classifier-dependent (for e.g. a function that outputs scaled distances from the separating hyperplane in case of Support Vector Machines). Minimizing L corresponds to adjusting the weight vectors, w1 and w2, so that they make similar predictions on U . In contrast with multiview learning methods, where learning the classifiers is folded into a global objective function in sophisticated ways , we adopt a simpler approach that works off the initial parameter vectors and iteratively modifies them in a \"co-training like\" manner. Note that this initialization plays a crucial role in avoiding trivial solutions (such as w1, w2 = 0) that are potentially possible since the loss is optimized only on unlabeled instances. Our proposed technique for obtaining the \"pair of conforming classifiers\" is described in Algorithm 2.In Algorithm 2, we start with the original parameter vectors w1 and w2 from classifiers C1 and C2, respectively, and iteratively adjust these vectors so that the values of f1(w1, u1) and f2(w2, u2) look similar for all u \u2208 U . The input parameter, #oIters, refers to the number of times the inner loop comprising of the two gradient descent steps is executed, where as, the #iIters, and \u03b1 are parameters for the gradient descent algorithm. Overall, the values of #oIters, #iIters, and \u03b1 control the rate of convergence of the algorithm and can be set experimentally. These parameters can be set based on the base classifiers used, noting when the decrease in the objective function value is below a threshold. Adaptive tuning of these parameters by tracking the change in the value of the objective function in every iteration is a subject for future study .In each iteration, we employ mini-batch gradient descent to minimize the loss function, once w.r.t. w1 and next w.r.t. w2. The mini-batch gradient descent algorithm is a hybrid approach often used for large-scale machine learning problems. This approach combines the best of stochastic (on- line) gradient descent and batch gradient descent to obtain fast convergence during optimization by running gradient descent on small batches of randomly selected examples .In our algorithm, in each iteration, a small batch of instances are randomly sampled from the unlabeled data, U and the loss function defined using instances for which w1 makes confident predictions from this sampled set. This loss is minimized using gradient descent to adjust w2. A similar process is then applied for adjusting w1 using confident predictions from w2. In effect, as the algorithm proceeds, we are adjusting the parameters of each classifier so that it makes predictions that are aligned with those of the other classifier's confident predictions. Upon convergence, both w1 and w2 are adjusted so that they make conforming predictions on the unlabeled data.In our experiments, we used the differentiable, logistic sigmoid function for f1 and f2. Typically, classifiers use the parameter vector, w, for computing decision values for each instance. That is, given an instance x, the dot product value, w, x , is used for determining the label assignment for the instance. This value can be 'squashed' to a number between 0 and 1 indicating that the probability that instance has a particular label with the logistic function [3]: P (t) = 1 1 + e \u2212t with dP (t) dt = P (t) \u00b7 (1 \u2212 P (t)) Given, the simple form for the derivative, we can directly use the values of f1 and f2 (that we compute anyway), for computing the gradients in Algorithm 2. Although the effect obtained by Algorithm 2 is similar to that of co-training, the conformity loss directly measures the effect of co-training as it is being applied. In contrast, Algorithm 1 is typically terminated either when no more examples are available or by tracking the performance on a validation dataset.We provide a preliminary, experimental demonstration of the connection between co-training and our proposed algorithm in Section 5. A more detailed analysis, study of other choices for the loss function L and the functions, f1 and f2, are a subject of future work. Nevertheless, quantifying the discrepancy in predictions from the two views and an algorithm to directly address this aspect is an exciting step in understanding when co-training works. We show in Section 5 that our method can be used in lieu of a validation dataset for tracking the performance of co-training. EXPERIMENTSWe discuss 3 types of experiments: First, we study the performance of content-based and URL-based features on the training and validation datasets. Second, we show that co-training can successfully address the problem of mismatch in the training and deployment environments for homepage classification. Finally, we show that our proposed algorithm (Algorithm 2), achieves the same effect as co-training.",
        "Adversarial Bandits Policy for Crawling Commercial Web Content The rapid growth of commercial web content has driven the development of shopping search services to help users find product offers. Due to the dynamic nature of commercial content, an effective recrawl policy is a key component in a shopping search service; it ensures that users have access to the up-to-date product details. Most of the existing strategies either relied on simple heuristics, or overlooked the resource budgets. To address this, Azar et al. [5] recently proposed an optimization strategy LambdaCrawl aiming to maximize content freshness within a given resource budget. In this paper, we demonstrate that the effectiveness of LambdaCrawl is governed in large part by how well future content change rate can be estimated. By adopting the state-of-the-art deep learning models for change rate prediction, we obtain a substantial increase of content freshness over the common LambdaCrawl implementation with change rate estimated from the past history. Moreover, we demonstrate that while LambdaCrawl is a significant advancement upon existing recrawl strategies, it can be further improved upon by a unified multi-strategy recrawl policy. To this end, we adopt the K-armed adversarial bandits algorithm that can provably optimize the overall freshness by combining multiple strategies. Empirical results over a large-scale production dataset confirm its superiority to LambdaCrawl, especially under tight resource budgets. CCS CONCEPTS \u2022 Information systems \u2192 Web crawling; E-commerce infrastructure; Search engine architectures and scalability.",
        "Learning to Discover Domain-Specific Web Content. The ability to discover all content relevant to an information domain has many applications, from helping in the understanding of humanitarian crises to countering human and arms trafficking. In such applications, time is of essence: it is crucial to both maximize coverage and identify new content as soon as it becomes available, so that appropriate actions can be taken. In this paper, we propose new methods for efficient domain-specific re-crawling that maximize the yield for new content. By learning patterns of pages that have a high yield, our methods select a small set of pages that can be re-crawled frequently, increasing the coverage and freshness while conserving resources. Unlike previous approaches to this problem, our methods combine different factors to optimize the re-crawling strategy, do not require full snapshots for the learning step, and dynamically adapt the strategy as the crawl progresses. In an empirical evaluation, we have simulated the framework over 600 partial crawl snapshots in three different domains. The results show that our approach can achieve 150% higher coverage compared to existing, state-of-the-art techniques. In addition, it is also able to capture 80% of new relevant content within less than 4 hours of publication.",
        "Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines. Tracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on \"diffing\" XML-encoded literary documents or XMLencoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software.",
        "URL normalization for de-duplication of web pages. Presence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these learnt rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract specific rules from URLs belonging to each cluster. Preserving each mined rules for de-duplication is not efficient due to the large number of specific rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We demonstrate the effectiveness of our techniques through experimental evaluation.",
        "Context-Sensitive Learning Methods for Text Categorization Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the \"context\" of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.",
        "On the design of a learning crawler for topical resource discovery In recent years, the World Wide Web has shown enormous growth in size. Vast repositories of information are available on practically every possible topic. In such cases, it is valuable to perform topical resource discovery effectively. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the World Wide Web quickly, without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the World Wide Web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries, or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. We discuss how to intelligently select features which are most useful for a given crawl. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates. This is the extended version of the preliminary paper ] which has appeared in the ACM published"
    ],
    "Recommenders influence on users": [
        "Mining user trails in critiquing based recommenders. Critiquing based recommenders are very commonly used to help users navigate through the product space to find the required product by tweaking/critiquing one or more features. By critiquing a product, the user gives an informative feedback(i.e, which feature needs to be modified) about why they rejected a product and preferred the other one. As a user interacts with such a system, trails are left behind. We propose ways of leveraging these trails to induce preference models of items which can be used to estimate the relative utilities of products which can be used in ranking the recommendations presented to the user. The idea is to effectively complement knowledge of explicit user interactions in traditional social recommenders with knowledge implicitly obtained from trails.",
        "Exploring the filter bubble: the effect of using recommender systems on content diversity. Eli Pariser coined the term 'filter bubble' to describe the potential for online personalization to effectively isolate people from a diversity of viewpoints or content. Online recommender systems -built on algorithms that attempt to predict which items users will most enjoy consuming -are one family of technologies that potentially suffers from this effect. Because recommender systems have become so prevalent, it is important to investigate their impact on users in these terms. This paper examines the longitudinal impacts of a collaborative filtering-based recommender system on users. To the best of our knowledge, it is the first paper to measure the filter bubble effect in terms of content diversity at the individual level. We contribute a novel metric to measure content diversity based on information encoded in user-generated tags, and we present a new set of methods to examine the temporal effect of recommender systems on the user experience. We do find that recommender systems expose users to a slightly narrowing set of items over time. However, we also see evidence that users who actually consume the items recommended to them experience lessened narrowing effects and rate items more positively.",
        "Investigating the Healthiness of Internet-Sourced Recipes: Implications for Meal Planning and Recommender Systems. Food recommenders have the potential to positively influence the eating habits of users. To achieve this, however, we need to understand how healthy recommendations are and the factors which influence this. Focusing on two approaches from the literature (single item and daily meal plan recommendation) and utilizing a large Internet sourced dataset from Allrecipes.com, we show how algorithmic solutions relate to the healthiness of the underlying recipe collection. First, we analyze the healthiness of Allrecipes.com recipes using nutritional standards from the World Health Organisation and the United Kingdom Food Standards Agency. Second, we investigate user interaction patterns and how these relate to the healthiness of recipes. Third, we experiment with both recommendation approaches. Our results indicate that overall the recipes in the collection are quite unhealthy, but this varies across categories on the website. Users in general tend to interact most often with the least healthy recipes. Recommender algorithms tend to score popular items highly and thus on average promote unhealthy items. This can be tempered, however, with simple post-filtering approaches, which we show by experiment are better suited to some algorithms than others. Similarly, we show that the generation of meal plans can dramatically increase the number of healthy options open to users. One of the main findings is, nevertheless, that the utility of both approaches is strongly restricted by the recipe collection. Based on our findings we draw conclusions how researchers should attempt to make food recommendation systems promote healthy nutrition.",
        "When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance. We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase.",
        "User Fatigue in Online News Recommendation. Many aspects and properties of Recommender Systems have been well studied in the past decade, however, the impact of User Fatigue has been mostly ignored in the literature. User fatigue represents the phenomenon that a user quickly loses the interest on the recommended item if the same item has been presented to this user multiple times before. The direct impact caused by the user fatigue is the dramatic decrease of the Click Through Rate (CTR, i.e., the ratio of clicks to impressions).In this paper, we present a comprehensive study on the research of the user fatigue in online recommender systems. By analyzing user behavioral logs from Bing Now news recommendation, we find that user fatigue is a severe problem that greatly affects the user experience. We also notice that different users engage differently with repeated recommendations. Depending on the previous users' interaction with repeated recommendations, we illustrate that under certain condition the previously seen items should be demoted, while some other times they should be promoted. We demonstrate how statistics about the analysis of the user fatigue can be incorporated into ranking algorithms for personalized recommendations. Our experimental results indicate that significant gains can be achieved by introducing features that reflect users' interaction with previously seen recommendations (up to 15% enhancement on all users and 34% improvement on heavy users).",
        "Information passing in online recommendation. In this paper, we analyze the information passing in an online recommendation system. Our dataset consists of a \"read\" network between users and books, and a user-follower network. We first investigate in general, if one's recommendations have impacts on her followers' decisions. We then analyze the correlation between one's influence and her network centrality. Finally, we investigate how recommendation effectiveness changes with the recommendation number. This investigation is taken from both senders' and receivers' perspectives. Results show that a user does have influence over her followers' decisions. Such influence is not correlated with her centrality. The more a book is recommended, the more likely that one will accept it. However, there is a saturate point beyond which more recommendations will have no more impact. On the other hand, the more recommendations one makes, the more likely that her recommendations will be accepted. This trend has no saturate point.",
        "Improving personalized recommendations using community membership information a b s t r a c tWhile early recommender systems have mostly focused on numeric ratings to model their interests, recent research in this area has explored a range of other sources that can provide information about user interests, such as their bookmarks, tags, social links, or reviews. One source of information that has received little attention so far is users' membership in online communities. Online communities frequently evolve around specific topics. Therefore, user membership in a community could be interpreted as a sign of user interests in the topics of a particular community, and furthermore, could apply to personalized recommendations as a source of information. This paper explores the feasibility and the value of using users' community membership as a source of personalized recommendations for individual users. The first part of the paper focuses on feasibility. It attempts to assess to what extent the interests of users within the same community are truly similar. The second part focuses on the value of this information to personalized recommendations. It suggests several recommendation approaches that use community membership information. It also assesses the comparative quality of recommendations that are generated by these approaches. In particular, we substantiate our approach with one typical social bookmarking system, CiteULike . The results of our study demonstrate that the interests of members of the same communities are significantly closer than the interests of non-connected users. Moreover, we found that recommendation approaches based on community membership produce recommendations that are as accurate as those produced through a collaborative filtering approach, but with better efficiency. The recommendations are also more complete than those produced by a collaborative filtering approach. In addition, for cold-start users who have insufficient bookmarking information to reliably represent their interests, recommendations based on community membership are the most valuable.",
        "Dual influence embedded social recommendation. Recommender systems are designed to solve the information overload problem and have been widely studied for many years. Conventional recommender systems tend to take ratings of users on products into account. With the development of Web 2.0, Rating Networks in many online communities (e.g. Netflix and Douban) allow users not only to co-comment or co-rate their interests (e.g. movies and books), but also to build explicit social networks. Recent recommendation models use various social data, such as observable links, but these explicit pieces of social information incorporating recommendations normally adopt similarity measures (e.g. cosine similarity) to evaluate the explicit relationships in the network -they do not consider the latent and implicit relationships in the network, such as social influence. A target user's purchase behavior or interest, for instance, is not always determined by their directly connected relationships and may be significantly influenced by the high reputation of people they do not know in the network, or others who have expertise in specific domains (e.g. famous social communities). In this paper, based on the above observations, we first simulate the social influence diffusion in the network to find the global and local influence nodes and then embed this dual influence data into a",
        "On the negative impact of social influence in recommender systems: A study of bribery in collaborative hybrid algorithms",
        "Understanding Echo Chambers in E-commerce Recommender Systems Personalized recommendation benefits users in accessing contents of interests effectively. Current research on recommender systems mostly focuses on matching users with proper items based on user interests. However, significant efforts are missing to understand how the recommendations influence user preferences and behaviors, e.g., if and how recommendations result in echo chambers. Extensive efforts have been made in examining the phenomenon in online media and social network systems. Meanwhile, there are growing concerns that recommender systems might lead to the self-reinforcing of user's interests due to narrowed exposure of items, which may be the potential cause of echo chamber. In this paper, we aim to analyze the echo chamber phenomenon in Alibaba Taobao-one of the largest e-commerce platforms in the world. Echo chamber means the effect of user interests being reinforced through repeated exposure to similar contents. Based on the definition, we examine the presence of echo chamber in two steps. First, we explore whether user interests have been reinforced. Second, we check whether the reinforcement results from the exposure of similar contents. Our evaluations are enhanced with robust metrics, including cluster validity and statistical significance. Experiments are performed on extensive collections of real-world data consisting of user clicks, purchases, and browse logs from Alibaba Taobao. Evidence suggests the tendency of echo chamber in user click behaviors, while it is relatively mitigated in user purchase behaviors. Insights from the results guide the refinement of recommendation algorithms in real-world e-commerce systems. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems; Web log analysis; Test collections. * Co-first authors with equal contributions. \u2020 This work was done when Yingqiang Ge worked as an intern in Alibaba."
    ],
    "Search engine caching effects": [
        "Design trade-offs for search engine caching In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
        "Boosting the performance of Web search engines: Caching and prefetching query results by exploiting historical usage data This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users.",
        "Efficient query processing in distributed search engines. Web search engines have to deal with a rapidly increasing amount of information, high query loads and tight performance constraints. The success of a search engine depends on the speed with which it answers queries (efficiency) and the quality of its answers (effectiveness). These two metrics have a large impact on the operational costs of the search engine and the overall user satisfaction, which determine the revenue of the search engine. In this context, any improvement in query processing efficiency can reduce the operational costs and improve user satisfaction, hence improve the overall benefit.In this thesis, we elaborate on query processing efficiency, address several problems within partitioned query processing, pruning and caching and propose several novel techniques:First, we look at term-wise partitioned indexes and address the main limitations of the state-of-the-art query processing methods. Our first approach combines the advantage of pipelined and traditional (non-pipelined) query processing. This approach assumes one disk access per posting list and traditional term-at-a-time processing. For the second approach, we follow an alternative direction and look at document-at-a-time processing of sub-queries and skipping. Subsequently, we present several skipping extensions to pipelined query processing, which as we show can improve the query processing performance and/or the quality of results. Then, we extend one of these methods with intra-query parallelism, which as we show can improve the performance at low query loads.Second, we look at skipping and pruning optimizations designed for a monolithic index. We present an efficient self-skipping inverted index designed for modern index compression methods and several query processing optimizations. We show that these optimizations can provide a significant speed-up compared to a full (non-pruned) evaluation and reduce the performance gap between disjunctive (OR) and conjunctive (AND) queries. We also propose a linear programming optimization that can further improve the I/O, decompression and computation efficiency of Max-Score.Third, we elaborate on caching in Web search engines in two independent contributions. First, we present an analytical model that finds the optimal split in a static memory-based two-level cache. Second, we present several strategies for selecting, ordering and scheduling prefetch queries and demonstrate that these can improve the efficiency and effectiveness of Web search engines.We carefully evaluate our ideas either using a real implementation or by simulation using real-world text collections and query logs. Most of the proposed techniques are found to",
        "Predictive caching and prefetching of query results in search engines. We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.",
        "Caching for Realtime Search. Modern search engines feature real-time indices, which incorporate changes to content within seconds. As search engines also cache search results for reducing user latency and back-end load, without careful real-time management of search results caches, the engine might return stale search results to users despite the efforts invested in keeping the underlying index up to date. A recent paper proposed an architectural component called CIP -the cache invalidation predictor. CIPs invalidate supposedly stale cache entries upon index modifications. Initial evaluation showed the ability to keep the performance benefits of caching without sacrificing much the freshness of search results returned to users. However, it was conducted on a synthetic workload in a simplified setting, using many assumptions. We propose new CIP heuristics, and evaluate them in an authentic environment -on the real evolving corpus and query stream of a large commercial news search engine. Our CIPs operate in conjunction with realistic cache settings, and we use standard metrics for evaluating cache performance. We show that a classical cache replacement policy, LRU, completely fails to guarantee freshness over time, whereas our CIPs serve 97% of the queries with fresh results. Our policies incur a negligible impact on the baseline's cache hit rate, in contrast with traditional age-based invalidation, which must severely reduce the cache performance in order to achieve the same freshness. We demonstrate that the computational overhead of our algorithms is minor, and that they even allow reducing the cache's memory footprint.",
        "A machine learning approach for result caching in web search engines a b s t r a c tA commonly used technique for improving search engine performance is result caching. In result caching, precomputed results (e.g., URLs and snippets of best matching pages) of certain queries are stored in a fast-access storage. The future occurrences of a query whose results are already stored in the cache can be directly served by the result cache, eliminating the need to process the query using costly computing resources. Although other performance metrics are possible, the main performance metric for evaluating the success of a result cache is hit rate. In this work, we present a machine learning approach to improve the hit rate of a result cache by facilitating a large number of features extracted from search engine query logs. We then apply the proposed machine learning approach to static, dynamic, and static-dynamic caching. Compared to the previous methods in the literature, the proposed approach improves the hit rate of the result cache up to 0.66%, which corresponds to 9.60% of the potential room for improvement.",
        "Compact Snippet Caching for Flash-based Search Engines. In response to a user query, search engines return the topk relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding document. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to reduce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memory the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve comparable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching.",
        "Cost-Aware Strategies for Query Result Caching in Web Search Engines Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.",
        "Cache Design of SSD-Based Search Engine Architectures: An Experimental Study Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines.",
        "The impact of caching on search engines. In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
        "Online result cache invalidation for real-time web search. Caches of results are critical components of modern Web search engines, since they enable lower response time to frequent queries and reduce the load to the search engine backend. Results in long-lived cache entries may become stale, however, as search engines continuously update their index to incorporate changes to the Web. Consequently, it is important to provide mechanisms that control the degree of staleness of cached results, ideally enabling the search engine to always return fresh results.In this paper, we present a new mechanism that identifies and invalidates query results that have become stale in the cache online. The basic idea is to evaluate at query time and against recent changes if cache hits have had their results have changed. For enhancing invalidation efficiency, the generation time of cached queries and their chronological order with respect to the latest index update are used to early prune unaffected queries. We evaluate the proposed approach using documents that change over time and query logs of the Yahoo! search engine. We show that the proposed approach ensures good query results (50% fewer stale results) and high invalidation accuracy (90% fewer unnecessary invalidations) compared to a baseline approach that makes invalidation decisions off-line. More importantly, the proposed approach induces less processing overhead, ensuring an average throughput 73% higher than that of the baseline approach.",
        "A five-level static cache architecture for web search engines a b s t r a c tCaching is a crucial performance component of large-scale web search engines, as it greatly helps reducing average query response times and query processing workloads on backend search clusters. In this paper, we describe a multi-level static cache architecture that stores five different item types: query results, precomputed scores, posting lists, precomputed intersections of posting lists, and documents. Moreover, we propose a greedy heuristic to prioritize items for caching, based on gains computed by using items' past access frequencies, estimated computational costs, and storage overheads. This heuristic takes into account the inter-dependency between individual items when making its caching decisions, i.e., after a particular item is cached, gains of all items that are affected by this decision are updated. Our simulations under realistic assumptions reveal that the proposed heuristic performs better than dividing the entire cache space among particular item types at fixed proportions.",
        "Three-Level Caching for Efficient Query Processing in Large Web Search Engines. Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395",
        "How Caching Improves Efficiency and Result Completeness for Querying Linked Data Link traversal based query execution is a novel query approach which enables applications that exploit the Web of Data to its full potential. This approach makes use of the characteristics of Linked Data: During query execution it traverses data links to discover data that may contribute to query results. Once retrieved from the Web, the data can be cached and reused for subsequent queries. We expect such a reuse to be beneficial for two reasons: First, it may improve query performance because it reduces the need to retrieve data multiple times; second, it may provide for additional query results, calculated based on cached data that would not be discoverable by a link traversal based execution alone. However, no systematic analysis exist that justifies the application of caching strategies based on these assumptions. In this paper we evaluate the potential of caching to improve efficiency and result completeness in link traversal based query execution systems. We conceptually analyze the potential benefit of keeping and reusing retrieved data. Furthermore, we verify the theoretical impact of caching by conducting a comprehensive experiment that is based on a real-world application scenario.",
        "Three-level caching for efficient query processing in large Web search engines. Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level.We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.",
        "ResIn: a combination of results caching and index pruning for high-performance web search engines. Results caching is an efficient technique for reducing the query processing load, hence it is commonly used in real search engines. This technique, however, bounds the maximum hit rate due to the large fraction of singleton queries, which is an important limitation. In this paper we propose ResIn -an architecture that uses a combination of results caching and index pruning to overcome this limitation.We argue that results caching is an inexpensive and efficient way to reduce the query processing load and show that it is cheaper to implement compared to a pruned index. At the same time, we show that index pruning performance is fundamentally affected by the changes in the query traffic that the results cache induces. We experiment with real query logs and a large document collection, and show that the combination of both techniques enables efficient reduction of the query processing costs and thus is practical to use in Web search engines.",
        "Exploiting query term correlation for list caching in web search engines. Caching technologies have been widely employed to boost the performance of Web search engines. Motivated by the correlation between terms in query logs from a commercial search engine, we explore the idea of a caching scheme based on pairs of terms, rather than individual terms (which is the typical approach used by search engines today). We propose an inverted list caching policy, based on the Least Recently Used method, in which the co-occurring correlation between terms in the query stream is accounted for when deciding on which terms to keep in the cache. We consider not only the term co-occurrence within the same query but also the cooccurrence between separate queries. Experimental results show that the proposed approach can improve not only the cache hit ratio but also the overall throughput of the system when compared to existing list caching algorithms.",
        "A refreshing perspective of search engine caching. Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!.",
        "Topical result caching in web search engines",
        "The impact of solid state drive on search engine cache management. Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSDbased search engines.",
        "A metric cache for similarity search. Similarity search in metric spaces is a general paradigm that can be used in several application fields. It can also be effectively exploited in content-based image retrieval systems, which are shifting their target towards the Web-scale dimension. In this context, an important issue becomes the design of scalable solutions, which combine parallel and distributed architectures with caching at several levels.To this end, we investigate the design of a similarity cache that works in metric spaces. It is able to answer with exact and approximate results: even when an exact match is not present in cache, our cache may return an approximate result set with quality guarantees. By conducting tests on a collection of one million high-quality digital photos, we show that the proposed caching techniques can have a significant impact on performance, like caching on text queries has been proved effective for traditional Web search engines.",
        "Second Chance: A Hybrid Approach for Dynamic Result Caching and Prefetching in Search Engines Web search engines are known to cache the results of previously issued queries. The stored results typically contain the document summaries and some data that is used to construct the final search result page returned to the user. An alternative strategy is to store in the cache only the result document IDs, which take much less space, allowing results of more queries to be cached. These two strategies lead to an interesting trade-off between the hit rate and the average query response latency. In this work, in order to exploit this trade-off, we propose a hybrid result caching strategy where a dynamic result cache is split into two sections: an HTML cache and a docID cache. Moreover, using a realistic cost model, we evaluate the performance of different result prefetching strategies for the proposed hybrid cache and the baseline HTML-only cache. Finally, we propose a machine learning approach to predict singleton queries, which occur only once in the query stream. We show that when the proposed hybrid result caching strategy is coupled with the singleton query predictor, the hit rate is further improved."
    ],
    "Consumer Product reviews": [
        "Helpfulness of online consumer reviews: A multi-perspective approach",
        "A framework for fake review detection in online consumer electronics retailers A B S T R A C TThe impact of online reviews on businesses has grown significantly during last years, being crucial to determine business success in a wide array of sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some users use unethical means to improve their online reputation by writing fake reviews of their businesses or competitors. Previous research has addressed fake review detection in a number of domains, such as product or business reviews in restaurants and hotels. However, in spite of its economical interest, the domain of consumer electronics businesses has not yet been thoroughly studied. This article proposes a feature framework for detecting fake reviews that has been evaluated in the consumer electronics domain. The contributions are fourfold: (i) Construction of a dataset for classifying fake reviews in the consumer electronics domain in four different cities based on scraping techniques; (ii) definition of a feature framework for fake review detection; (iii) development of a fake review classification method based on the proposed framework and (iv) evaluation and analysis of the results for each of the cities under study. We have reached an 82% F-Score on the classification task and the Ada Boost classifier has been proven to be the best one by statistical means according to the Friedman test.",
        "When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance. We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase.",
        "Review recommendation with graphical model and EM algorithm. Automatically assessing the quality and helpfulness of consumer reviews is more and more desirable with the evolutionary development of online review systems. Existing helpfulness assessment methodologies make use of the positive vote fraction as a benchmark and heuristically find a \"best guess\" to estimate the helpfulness of review documents. This benchmarking methodology ignores the voter population size and treats the the same positive vote fraction as the same helpfulness value. We propose a review recommendation approach that make use of the probability density of the review helpfulness as the benchmark and exploit graphical model and Expectation Maximization (EM) algorithm for the inference of review helpfulness. The experimental results demonstrate that the proposed approach is superior to existing approaches.",
        "Consumer health information needs: A systematic review of instrument development",
        "Do consumers always follow \"useful\" reviews? The interaction effect of review valence and review usefulness on consumers' purchase decisions",
        "Addressing Complex and Subjective Product-Related Queries with Customer Reviews. Online reviews are often our first port of call when considering products and purchases online. When evaluating a potential purchase, we may have a specific query in mind, e.g. 'will this baby seat fit in the overhead compartment of a 747?' or 'will I like this album if I liked Taylor Swift's 1989?'. To answer such questions we must either wade through huge volumes of consumer reviews hoping to find one that is relevant, or otherwise pose our question directly to the community via a Q/A system.In this paper we hope to fuse these two paradigms: given a large volume of previously answered queries about products, we hope to automatically learn whether a review of a product is relevant to a given query. We formulate this as a machine learning problem using a mixture-of-experts-type framework-here each review is an 'expert' that gets to vote on the response to a particular query; simultaneously we learn a relevance function such that 'relevant' reviews are those that vote correctly. At test time this learned relevance function allows us to surface reviews that are relevant to new queries on-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million questions (and answers) and 13 million reviews. We show quantitatively that it is effective at addressing both binary and open-ended queries, and qualitatively that it surfaces reviews that human evaluators consider to be relevant.",
        "Online consumer reviews and sales: Examining the chicken-egg relationships",
        "Subjective perception patterns of online reviews: A comparison of utilitarian and hedonic values A R T I C L E I N F O Keywords:Online reviews Fuzzy-set qualitative comparative analysis (fsQCA) Utilitarian value Hedonic value A B S T R A C T Online reviews are very important for the decision making of tourists, and such importance continues to grow as these reviews affect not only the tourism industry but also the operators of review platforms. Therefore, those factors that affect the perceived value of online reviews need to be investigated. Although the criteria and viewpoints for judging the value of these reviews may differ from one person to another, the existing studies on online reviews have not considered such differences. Therefore, this study aims to identify the differences in the patterns of the perceptions toward the utilitarian and hedonic values of online reviews. Building on a theoretical discussion of how information is processed during the decision making of individuals, this study develops a conceptual model that comprises three factors (i.e., the heuristic-reviewer, heuristicreview, and systematic-review factors) related to online reviews. For the data analysis, 2616 cases collected from Yelp.com were analyzed by using the fuzzy-set qualitative comparative analysis approach. The four patterns of perceptions toward the utilitarian value of online reviews (i.e., administrative reader, analytic reader, synthetic reader, and collaborative reader patterns) were derived. The three patterns of perceptions toward the hedonic value of these reviews (i.e., administrative-expertise-, synthetic-reviewer-, and nurturer-authenticity-focused readers) were also extracted. The results revealed some differences among those factors that influence the recognition of the utilitarian and hedonic values of online reviews.",
        "Examining the Influence of Emotional Expressions in Online Consumer Reviews on Perceived Helpfulness"
    ],
    "Limitations machine learning": [
        "Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution. Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.",
        "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy.",
        "Learning to join everything. Text, speech, images, video, DNA sequences provide information about entities that people can recognize when looking at a particular instance. But those entities and their attributes and relationships are not directly accessible to queries that join across types of sources. Information extraction methods based on supervised machine learning recognize mentions of entities and relationships of predefined types in different kinds of sources, which can then be used to answer some useful types of queries. However, supervised learning relies on hand-annotated training sets that are difficult to create and limit what types of entities and relationships can be joined for new applications. These limitations have prompted research into unsupervised extraction methods that rely on correlations among sources rather than hand-annotated training sets. While these methods are not yet as accurate as those based on supervised learning, they have the potential for a new query-by-example approach to information integration in which seed sets of query answers are expanded into ranked lists of potential answers by learning occurrence patterns from the seed answers. I will give examples of both types of methods from our research on biomedical information extraction, leading to some ideas on a possible convergence of search and databases through machine learning. Categories and Subject Descriptors: E.0 GENERAL General Terms: Experimentation Keywords: Information and Knowledge Management BioFernando Pereira is the Andrew and Debra Rachleff Professor and chair of the department of Computer and Information Science, University of Pennsylvania. He received a Ph.D. in Artificial Intelligence from the University of Edinburgh in 1982. Before joining Penn, he held industrial research and management positions at SRI International, at AT&T Labs, where he led the machine learning and information retrieval research department from September 1995 to April 2000, and at WhizBang Labs, a Web information extraction company. His main research interests are in machine-learnable models of language and other natural sequential data such as biological sequences. He made major contributions to advances in finite-state models for speech and text processing now in everyday industrial use. He has over 100 research publications on computational linguistics, speech recognition, machine learning and logic programming, and several issued and pending patents on speech recognition, language processing, and humancomputer interfaces. He was elected Fellow of the American Association for Artificial Intelligence in 1991 for his contributions to computational linguistics and logic programming, and he is a past president of the Association for Computational Linguistics.",
        "On Presuppositions of Machine Learning: A Meta Theory Machine learning (ML) has been run and applied by premising a series of presuppositions, which contributes both the great success of AI and the bottleneck of further development of ML. These presuppositions include (i) the independence assumption of loss function on dataset (Hypothesis I); (ii) the large capacity assumption on hypothesis space including solution (Hypothesis II); (iii) the completeness assumption of training data with high quality (Hypothesis III); and (iv) the Euclidean assumption on analysis framework and methodology (Hypothesis IV). We report, in this presentation, the effort and advances made by my group on how to break through these presuppositions of ML and drive ML development. For Hypothesis I, we introduce the noise modeling principle to adaptively design the loss function of ML, according to the distribution of data samples, which provides then a general way to robustlize any ML implementation. For Hypothesis II, we propose the model driven deep learning approach to define the smallest hypothesis space of deep neural networks (DNN), which yields not only the very efficient deep learning, but also a novel way of DNN design, interpretation and connection with the traditional optimization based approach. For Hypothesis III, we develop the axiomatic curriculum learning framework to learn the patterns from an incomplete dataset step by step and from easy to difficult, which then provides feasible ways to tackle very complex incomplete data sets. Finally, For Hypothesis IV, we introduce Banach space geometry in general, and XU-Roach theorem in particular, as a possibly useful tool to conduct non-Euclidean analysis of ML problems. In each case, we present the idea, principles, application examples and literatures.",
        "Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub.",
        "Machine Learning at Amazon. In this talk I will give an introduction into the field of machine learning and discuss why it is a crucial technology for Amazon.Machine learning is the science of automatically extracting patterns from data in order to make automated predictions of future data. One way to differentiate machine learning tasks is by the following two factors: (1) How much noise is contained in the data? and (2) How far into the future is the prediction task? The former presents a limit to the learnability of task -regardless which learning algorithm is used -whereas the latter has a crucial implication on the representation of the predictions: while most tasks in search and advertising typically only forecast minutes into the future, tasks in e-commerce can require predictions up to a year into the future. The further the forecast horizon, the more important it is to take account of uncertainty in both the learning algorithm and the representation of the predictions. I will discuss which learning frameworks are best suited for the various scenarios, that is, short-term predictions with little noise vs. long-term predictions with lots of noise, and present some ideas to combine representation learning with probabilistic methods.In the second half of the talk, I will give an overview of the applications of machine learning at Amazon ranging from demand forecasting, machine translation to automation of computer vision tasks and robotics. I will also discuss the importance of tools for data scientist and share learnings on bringing machine learning algorithms into products."
    ],
    "medicine related research": [
        "Learning Individual Causal Effects from Networked Observational Data The convenient access to observational data enables us to learn causal effects without randomized experiments. This research direction draws increasing attention in research areas such as economics, healthcare, and education. For example, we can study how a medicine (the treatment) causally affects the health condition (the outcome) of a patient using existing electronic health records. To validate causal effects learned from observational data, we have to control confounding bias-the influence of variables which causally influence both the treatment and the outcome. Existing work along this line overwhelmingly relies on the unconfoundedness assumption that there do not exist unobserved confounders. However, this assumption is untestable and can even be untenable. In fact, an important fact ignored by the majority of previous work is that observational data can come with network information that can be utilized to infer hidden confounders. For example, in an observational study of the individual-level treatment effect of a medicine, instead of randomized experiments, the medicine is often assigned to each individual based on a series of factors. Some of the factors (e.g., socioeconomic status) can be challenging to measure and therefore become hidden confounders. Fortunately, the socioeconomic status of an individual can be reflected by whom she is connected in social networks. With this fact in mind, we aim to exploit the network information to recognize patterns of hidden confounders which would further allow us to learn valid individual causal effects from observational data. In this work, we propose a novel causal inference framework, the network deconfounder, which learns representations to unravel patterns of hidden confounders from the network information. Empirically, we perform extensive experiments to validate the effectiveness of the network deconfounder on various datasets. CCS CONCEPTS \u2022 Mathematics of computing \u2192 Causal networks; \u2022 Networks \u2192 Online social networks; \u2022 Human-centered computing \u2192 Social network analysis.",
        "Medical Information Search Workshop (MEDIR) George Paliouras NCSR Demokritos Greece paliourg@iit.demokritos.gr WORKSHOP OVERVIEWMedical information search refers to methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval (IR). Such information is now potentially accessible from many sources including the general web, social media, journal articles, and hospital records. Health-related content is one of the most searched-for topics on the internet, and as such this is an important domain for research in information retrieval.Medical information is of interest to a wide variety of users, including patients and their families, researchers, general practitioners and clinicians, and clinicians with specific expertise such as radiologists. There are several dedicated services that seek to make this information more easily accessible, such as Health on the Net's medical search systems for the general public and medical practitioners 1 . Despite the popularity of the medical domain for users of search engines, and current interest in this topic within the information retrieval research community, development of search and access technologies remains particularly challenging. One of the central challenges in medical information search is diversity of the users of these services. These challenges can be summarized as follows:1. Varying information needs: Understanding various types of users and their information needs is one of the cornerstones of medical information search, while adapting information retrieval to best address these needs to develop effective, potentially personalized systems is one of its greatest challenges. 3. Varying language skills: Given that much of medical content is written in the English language, research to date in medical information search has predominantly focused on monolingual English retrieval. However, given the large number of non-English speakers on the Internet and the lack of content in their native language, effective support for them to search the English sources is highly desirable.The objective of this workshop is to provide a forum to enable the progression of research in medical information retrieval to provide enhanced search services for all users with interest in medical information search. This workshop aims to bring together researchers interested in medical information search with the goal of identifying specific research challenges that need to be addressed to advance the state-of-the-art and to foster interdisciplinary collaborations towards the meeting of these challenges. To enable this, we will encourage participation from researchers in all fields related to medical information search including mainstream information retrieval, but also natural language processing, multilingual text processing, and medical image analysis.Topics of interest include but are not limited to: Users and information needs; Semantics and NLP for medical IR; Reliability and trust in medical IR; Personalised search; Evaluation of medical IR; Multilingual questions in medical IR; Multimedia technologies in medical IR; The role of social media in medical IR.The workshop programme includes an invited presentation by Pr Bill Hersh, and short research and position papers describing approaches or challenges on the topics of the workshop. The programme aims to encourage interaction and discussion to develop a shared understanding of the research challenges facing the area of medical information retrieval. The workshop will seek to identify specific actions necessary to advance this field, and to hopefully foster further collaborations between the participants. AcknowledgementWe would like to thanks the European Science Foundation project ELIAS for supporting this workshop. 1243",
        "Exploring Clustering Based Knowledge Discovery towards Improved Medical Diagnosis We propose to develop a framework for an intelligent reasoner with capabilities that support complex decision making processes in medical diagnosis. Identifying the causes, reasoning the effects to explore information geometry and learning the associated factors, from medical forum information extracted, are the core aspects of this work. As part of the proposed framework, we present an approach that identifies semantically similar causes and effects for any specific disease from medical diagnosis literature using implicit semantic interconnections among the medical terms. First we crawled MedHelp 1 forum data and considered two types of information: forums data and posts data. Each forum link points to a specific disease and consists of several topics pertaining to that disease. Each topic consists of multiple posts that carry either users' queries/difficulties or doctor's feedback pertaining to the issue(s) of the users. We use graph based exploration on the terms (diseases) and their relations (in terms of causes/effects) and explore the information geometry pertaining to similar diseases. We performed a systematic evaluation to identify the relevance of the contextual information retrieved for a specific disease and similar factors across different diseases. The proposed approach looks promising in capturing similar causes and/or effects that pertain to multiple diseases. This would enable medical practitioners to have a multi-faceted view of a specific disease/condition.",
        "Medical information retrieval: an instance of domain-specific search. Due to an explosion in the amount of medical information available, search techniques are gaining importance in the medical domain. This tutorial discusses recent results on search in the medical domain, including the outcome of surveys on end user requirements, research relevant to the field, and current medical and health search applications available. Finally, the extent to which available techniques meet user requirements are discussed, and open challenges in the field are identified. Categories and Subject Descriptors TUTORIAL SUMMARYSearch techniques are becoming increasingly important in the medical domain, in particular due to an explosion in the amount of medical information available. This includes both patient-specific information (e.g. electronic health records) and knowledge-based information (e.g. scientific papers). The increasing emphasis on the use of evidence-based medicine (using the best evidence from the scientific literature in clinical decision making) also increases the importance of search in medical practice . Furthermore, the secondary use of anonymized electronic health records for facilitating research and improving quality in medical practice is also gaining in importance . This use of health records also requires IR techniques, and has been modelled in the new TREC medical records track in 2011.All parties involved in medical treatment are regularly faced with an information need that cannot be met from their own store of knowledge. Medical information search is a classic example of a domain-specific search, but the medical domain is so complex that simply limiting the target documents to those in the medical domain and indexing with a standard search engine is not sufficient. The range of sources of medical information (primary research sources, secondary research sources, web pages and popular publications, etc.) and the range of end users (members of the general public, general practitioners, specialists, researchers, etc.) lead to complex requirements. Recent work in the Khresmoi project 1 has collected end user requirements for search in the medical domain through online surveys, interviews and observation . The results of the requirement analysis are presented based on a model of domain-specific search, for which the following aspects need to be specified: information sources to restrict the search, search refinements that can improve the access to information in the domain, and end users and their tasks.Although many groups of people search for information in the medical domain, the analysis in the Khresmoi project has been restricted to three of the most important groups: search by physicians, search by members of the general public and search by radiologists (a subset of physicians for which search in images is of particular importance). Different end users have different requirements based on the technical level (accessibly or technically written), level of specificity (overview or in-depth), type of research paper (primary or secondary), language of the results, etc. Naturally, the trustworthiness of medical information is of particular importance. Analyses of search behaviour based on search engine log files have also been done .For physicians, an unmet information need has been reported as occurring for 2 of every 3 patients seen , or for 41% of the questions they pursued . This requires that they attempt to meet this information need by using available resources, which has traditionally involved searching in printed sources and asking colleagues, although searching on the Internet is of increasing importance. The Khresmoi survey has shown that the three most common sources of online information used by physicians (in decreasing order of usage) are: general search engines (e.g. Google, Bing, Yahoo!), medical research databases (e.g. Pubmed) and Wikipedia. Image search is particularly important in the medical domain, especially for radiologists . Internet image search applications are starting to appear (e.g. Goldminer, Yottalook). However, image search within Picture Archiving and Communication Systems (PACS) in hospitals is also being developed .Patients also have regular information needs, illustrated by the fact that 61% of American Adults seek out health advice online . The Khresmoi survey of the general public 1 http://khresmoi.eu 1191",
        "DUTIR at TREC 2019: Precision Medicine Track This paper describes the system developed for the TREC 2019 Precision Medicine Track by the Team DUTIR from Dalian University of Technology. In the system, we applied a hybrid method to score the related documents for each topic. First, we used Elasticsearch, an open-source Lucene-based full-text search engine, to obtain the initial retrieval results. Then we trained several deep models using TREC 2017 PM data. Finally, we applied the pre-trained models to reorder the initial search results. The performance of our system is above the median for the scientific abstracts subtask and below median for the clinical trials subtask.",
        "Towards Explainable Retrieval Models for Precision Medicine Literature Search In professional search tasks such as precision medicine literature search, queries often involve multiple aspects. To assess the relevance of a document, a searcher often painstakingly validates each aspect in the query and follows a task-specific logic to make a relevance decision. In such scenarios, we say the searcher makes a structured relevance judgment, as opposed to the traditional univariate (binary or graded) relevance judgment. Ideally, a search engine can support searcher's workflow and follow the same steps to predict document relevance. This approach may not only yield highly effective retrieval models, but also open up opportunities for the model to explain its decision in the same 'lingo' as the searcher. Using structured relevance judgment data from the TREC Precision Medicine track, we propose novel retrieval models that emulate how medical experts make structured relevance judgments. Our experiments demonstrate that these simple, explainable models can outperform complex, black-box learning-to-rank models.",
        "Research commentary: Intelligent systems and technology for integrative and predictive medicine: An ACP approach One of the principal goals in medicine is to determine and implement the best treatment for patients through fastidious estimation of the effects and benefits of therapeutic procedures. The inherent complexities of physiological and pathological networks that span across orders of magnitude in time and length scales, however, represent fundamental hurdles in determining effective treatments for patients. Here we argue for a new approach, called the ACP-based approach, that combines artificial (societies), computational (experiments), and parallel (execution) methods in intelligent systems and technology for integrative and predictive medicine, or more generally, precision medicine and smart health management. The advent of artificial societies that collect the clinically relevant information in prognostics and therapeutics provides a promising platform for organizing and experimenting complex physiological systems toward integrative medicine. The ability of computational experiments to analyze distinct, interactive systems such as the host mechanisms, pathological pathways, and therapeutic strategies, as well as other factors using the artificial systems, will enable control and management through parallel execution of real and arficial systems concurrently within the integrative medicine context. The development of this framework in integrative medicine, fueled by close collaborations between physicians, engineers, and scientists, will result in preventive and predictive practices of a personal, proactive, and precise nature, including rational combinatorial treatments, adaptive therapeutics, and patient-oriented disease management.",
        "Citizen Organization System for Advanced MEDical research (COSAMED). Analyzing true effect of medicine or functional food is major issue in associated research area. In order to achieve that, gathering of massive clinical trial data is required. There are three major concepts for clinical trial data collection: Citizen Science, Health 2.0 and Crowdsourcing. Citizen Science, which uses web 2.0 technologies, is web-based service for health care. Health 2.0 uses non-professionally trained individuals to conduct science-related activities. Lastly, Crowdsourcing is an online distributed problemsolving and production model. Following systems have tried to process data based on above concepts. PatientsLikeme attempted to find potential benefits from clinical outcomes within longitudinal evaluation of online data-sharing platforms. ResearchKit is about to create apps that could revolutionize medical studies. However, these systems do not have reliable protocols to obtain credible results. In addition, they mainly focus on diseases with a medicine, not on effect with a functional food.Hereby, we are developing a novel system to solve the issues: Citizen Organization System for Advanced MEDical research(COSAMED). We are looking forward to find true effect information of a functional food with our new system. COSAMED is made of five steps to design a reliable protocol. (1) Target Item Selection, to select a target effect and effect related items. (2) Preparation of Research, to select designed clinical trial protocol on user demand with automated scientific criteria. (3) Recruiting Participants, to recruit participants from linked SNS friends or from other systems. (4) Data Collection, to collect effect information from various sources. Analysis, to analyze the results by web-bases statistical tools, transfer results to a data warehouse and calculate credibility rate. Finally, the protocol is developed with product DB and clinical trial protocol snapshot DB on COSAMED. In future, we will integrate it with Openmhealth architecture to connect related systems easily and build it with user friendly interfaces to collect big data.COSAMED will be available at www.cosamed.org, and it will be a cornerstone of first citizen based clinical trial system.",
        "Medical search and classification tools for recommendation EXTENDED. As an increasing number of medical professionals move their patients' records from paper to computer, enormous amounts of electronic medical records (EMR) have become available for medical research. Some of the EMR data are well-structured, for which traditional database management systems can provide effective retrieval and management functions. However, most of the EMR data (such as progress notes and consultation letters) are in free text formats. How to effectively and efficiently retrieve and discover useful information from the vast amount of such semi-structured data is a challenge faced by medical professionals. Without proper tools, the rich information and knowledge buried in the medical health records are unavailable for clinical research and decision-making.The objective of our research is to develop text analytics tools that are capable of parsing clinical medical data so that predefined search subjects that correspond to a list of medical diagnoses can be extracted. In addition to this particular core functionality, it is also desired that several important assets should be present within the text-analytics tools in order to improve its overall ability to be used as recommendation tools.In this research, we work with research scientists at the Institute for Clinical Evaluative Sciences (ICES) in Toronto and examine a number of techniques for structuring and processing free text documents in order to effectively and efficiently search and analyze vast amount of medical records. We implement several powerful medical text analytics tools for clinical data searching and classification. For data classification, our tools sort through a great amount of patient records to identify the likelihood of a patient having myocardial infarction (MI) or hypertension (HTN), and classify the patients accordingly. Our tools can also identify the likelihood of a patient being a smoker, previous smoker or non-smoker based on the text data of medical records. All the algorithms and domain knowledge implemented in these tools were provided by medical doctors and domain experts Copyright is held by the author/owner. SIGIR '10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07. working in the fields. System evaluations have been conducted and the performance results have been shown to be promising. AN INTEGRATED GUI-BASED TOOLA GUI-based interface is designed and implemented for the text-analytics tool under a point-and-click environment. Clicking the button of ICES search tool, we can get the starting to the free-text search as shown in . The free-text search tool is designed to retrieve records based on keywords such as \"MI\". The search results display the key sentences in the medical records containing the keywords. Clicking the button of ICES classification tool, we can use three classification tools summarize the records of each patient in . Three classification tools classify patients based on their likelihoods of having MI or hypertension, or on their smoking status.",
        "MedSearch: a specialized search engine for medical information retrieval. People are thirsty for medical information. Existing Web search engines often cannot handle medical search well because they do not consider its special requirements. Often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology. Therefore, he sometimes prefers to pose long queries, describing his symptoms and situation in plain English, and receive comprehensive, relevant information from search results. This paper presents MedSearch, a specialized medical Web search engine, to address these challenges. MedSearch uses several key techniques to improve its usability and the quality of search results. First, it accepts queries of extended length and reforms long queries into shorter queries by extracting a subset of important and representative words. This not only significantly increases the query processing speed but also improves the quality of search results. Second, it provides diversified search results. Lastly, it suggests related medical phrases to help the user quickly digest search results and refine the query. We evaluated MedSearch using medical questions posted on medical discussion forums. The results show that MedSearch can handle various medical queries effectively and efficiently.",
        "The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine. Despite its intuitive appeal, the hypothesis that retrieval at the level of \"concepts\" should outperform purely term-based approaches remains unverified empirically. In addition, the use of \"knowledge\" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for \"conceptual retrieval\" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.",
        "Customizing a Variant Annotation-Support Tool: an Inquiry into Probability Ranking Principles for TREC Precision Medicine The TREC 2017 Precision Medicine Track aims at building systems providing meaningful precision medicine-related information to clinicians in the field of oncology. The track includes two tasks: 1) retrieving scientific abstracts addressing treatment effect and prognosis of a disease and 2) retrieving clinical trials for which a patient is eligible. The SIB Text Mining group participated in both tasks. Regarding the retrieval of scientific abstracts, we designed a set of different queries with decreasing levels of specificity. The idea was to start initiating a very specific query, from which less specific queries will be inferred. We may thus consider as relevant abstracts that did not mention all critical aspects of the complete query but could still be of interest. Therefore, the main component of our approach was a large query generation module (e.g. disease + gene + variant; disease + gene; gene + variant)with each generated query being differentially weighted. To increase the scope of the queries, we applied query expansion strategies. In particular, a single nucleotide variant (SNV) generator was developed to recognize standard nomenclature as described by the Human Genome Variation Society (HGVS) as well as non-standard formats frequently found in the literature. We thus expect to retrieve a maximum of relevant abstracts. We then applied different strategies to favor relevant abstracts by re-ranking them based on more general criteria. First, we assumed that an abstract with a high frequency of drug names is more probably relevant to support our task. Therefore, we pre-annotated all the collection with DrugBank, thus enabling to retrieve the number of occurrences of drug names per abstract. Second, we assumed that the presence of some specific keywords (e.g. \"treat\") in the abstract should increase the relevance of the paper, while the presence of some other keywords (e.g. \"marker\") should decrease its relevance. Third, we assumed that some publications, such as clinical trials, should receive higher relevance for this task. Regarding the retrieval of clinical trials, we investigated for the competition different combinations of filtering and information retrieval strategies, mostly based on the exploitation of ontologies. Our preliminary analysis of the collection showed that : (1) demographic features (age and gender) are stored in a perfectly-structured form in clinical trials, thus this feature can be easily handled with strict filtering ; (2) the trials contain very few mentions of the requested genes and variants ; (3) diseases are stored in very inconsistent forms, as they are free text entities and can be mentioned in different fields such as condition, keywords, summary, criteria, etc. Thus, we assumed that identifying clinical trials dealing with the correct disease was the most challenging issue for this competition. For such a task, we perform Name Entity Recognition with the NCI thesaurus in order to recognize mentions of diseases in topics and in different fields of the clinical trials. This strategy handles several issues of free text descriptions, such as synonyms (\"Cancer\" and \"Neoplasm\" are equivalent) and hierarchies (\"Colon carcinoma\" is a subtype of \"Colorectal carcinoma\"). Then, for each topic, we apply different strategies of trials filteringaccording to fields where the disease was identifiedand hierarchies. Finally, classical information retrieval is performed with genes and variants as queries. The strictest filtering leads to an average of 62 retrieved trials per topic and tends to favor high precision, while the most relaxed filtering leads to an average of 379 retrieved trials per topic and tends to favor high recall. Yet, results show that the Precision values are poorly impacted by these strategies, while runs that favor Recall showed a better general behavior for this task.",
        "Msuggest: a semantic recommender framework for traditional chinese medicine book search engine. Learning traditional Chinese medicine knowledge from the digital library is becoming more and more important these days in China. In medicine learning, many readers want to find out the intrinsic relation between two medicines or among thousands of medicines. A semantic recommender system is useful for readers to understand something quickly by means of analogy which is a cognitive process of transferring information from a particular subject to another if they are similar in some aspects. In view of these above, we present a novel recommender framework called Msuggest to give the diverse semantic recommended medicine terminologies and book pages when a reader searching for medicine information in digital library. Users can choose various aspects including medicine property, efficacy, clinical application, place of origin, book provenance and etc. to see different recommended results. We evaluate Msuggest under the t-test on the samples from random sampling. The result shows that Msuggest is effective and efficient in giving the recommended words and book pages.",
        "Integrative Database for Exploring Compound Combinations of Natural Products for Medical Effects. Natural products used in dietary supplements, complementary and alternative medicine (CAM) and conventional medicine are composites of multiple chemical compounds. These chemical compounds potentially offer an extensive source for drug discovery with accumulated knowledge of efficacy and safety. However, existing natural product related databases have drawbacks in both standardization and structuralization of information. Therefore, in this work, we construct an integrated database of natural products by mapping the prescription, herb, compound, and phenotype information to international identifiers and structuralizing the efficacy information through database integration and text-mining methods. We expect that the constructed database could serve as a fundamental resource for the natural products research.",
        "Health Misinformation in Search and Social Media. People regularly use web search and social media to investigate health related issues.is type of Internet data might contain misinformation i.e. incorrect information which contradicts current established medical understanding. If people are in uenced by the presented misinformation in these sources, they can make harmful decisions about their health. My research goal is to investigate the e ect of Internet data on people's health. Working with my colleagues on this topic, our current ndings suggest that there is a potential for people to be harmed by search engine results. Furthermore, we successfully built a high precision approach to track misinformation in social media. In this paper, I brie y discuss my current work including background key references. erea er, I propose a research plan to understand possible mechanisms of misinformation's e ect on people and its possible impacts on public health. Later, I will explain the suggested research methodology to achieve the research plan. RESEARCH PROPOSALUsing the massive available amount of online data, we can potentialy help improve people's health in a direct and quantitative way. In the past, researchers used to rely on medical reports from hospitals and health clinics in order to conduct health-related research. It was not until the 1960s that steps towards automating data collection took place . As much as the old data resources seem to be reliable and focused, this kind of data does not cover all public health aspects but focuses more on the sickness conditions of the health-domain [5, p. 8].e purpose of this work is to use Internet data (search engine result pages, social media) in order to understand how information and, more speci cally, misinformation a ect people's health where misinformation is de ned as a piece of information spreading in Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan \u00a9 2017 Copyright held by the owner/author(s). 978-1-4503-5022-8/17/08. DOI: 10.1145/3077136.3084153 the media con rmed to be false by reliable sources Coady . e dissertation work shall focus on two research questions regarding e ort. RQ1: How search engine in uences people's health-care decision and possible reasons behind this in uence .We are planning to design controlled laboratory studies in order to understand the reasons behind people being in uenced by online search results. People might be in uenced because of a large number of factors, such as: prior-knowledge, the trustworthiness of sources, visual display aids. For space limitation reasons, we will only explain the experimental setup of one user study that looks into the a ect of the prior-knowledge factor. During this speci c study, we will ask about the e cacy of two types of medical treatments: treatments that are popular for a speci c medical condition, assuming people already have a prior knowledge about them, and rare treatments for medical condition that few people know about. Later, we will ask participants to decide whether the treatment is helpful or not for the speci c medical condition a er being exposed to correct and incorrect search result pages. Based on the participants' answers, if there is a statistical signi cance between popular and non-popular treatments, then prior-knowledge has an impact on people's healthcare decisions. RQ2: How do health rumors in social media e ect the belavior of people susceptible rumors i.e. are they more likely to have some speci c event/outcome/personal experience. To achieve this, we will rst build a medical rumor classi er that identi es who is spreading rumors. Next, we aim to understand how the detected rumors a ect people's health-care decisions. Inspired by the work done by Olteanu et al. [4], we plan to use strati ed propensity score matching analysis in order to understand possible personal experiences rumor cohorts might have. We de ne the treatment group as people who spread rumors about a speci c medical treatment for a speci c illness and the control group as people who share general information about the medical illness. Possible outcomes we might be looking at are: word likelihood/distribution explaning personal experiences behavior [2], anxiety level, possible medical related outcomes such as immunization decisions, social network behavior such as the tweeting rates.",
        "UTD HLTRI at TREC 2017: Precision Medicine Track In this paper, we describe the system designed for the TREC 2017 Precision Medicine track by the University of Texas at Dallas (UTD) Human Language Technology Research Institute (HLTRI). Our system incorporates an aspect-based retrieval paradigm wherein each of the four structured components of the topic is cast as a separate aspect, along with two \"hidden\" aspects encoding the need that retrieved documents be within the domain of precision medicine and that retrieved documents have a focus on treatment. To this end, we construct knowledge graph encoding the relationships between drugs, genes, and mutations. Our experiments reveal that the aspect-based approach leads to improved quality of retrieved scientific articles and clinical trials.",
        "Lexicon-free and context-free drug names identification methods using hidden markov models and pointwise mutual information. The paper concerns the issue of extraction of medicine names from free text documents written in Polish. Using lexiconbased approaches, it is impossible to identify unknown or misspelled medicine names. In this paper, we present the results of experimentation on two methods: Hidden Markov Model (HMM) and Pointwise Mutual Information (PMI)-based approach. The experiment was to identify the medicine names without the use of lexicon or contextual information. The experimentation results show, that HMM may be used as one of several steps in drug names' identification (with F-score slightly below 70% for the test set), while the PMI can help in increasing the precision of results achieved using HMM, but with significant loss in recall.",
        "Bridging East and West: An Integration of TCM and Western Medicine in Medical Text Mining Traditional Chinese Medicine (TCM) has been used by practitioners for millennia to prevent and treat disease, but has struggled to gain broad acceptance in the West. In 2019, the World Health Organisation officially recognized TCM as a form of medical treatment, a step towards internationalizing TCM and integrating it with Western medicine (WM). The proposed dissertation research aims to bridge eastern and western medical philosophies by applying named entity recognition (NER) and information retrieval (IR) models supported by medical and cross lingual knowledge graphs, to enhance the retrieval performance as well as to increase the model explainability.",
        "Interactions between health searchers and search engines. The Web is an important resource for understanding and diagnosing medical conditions. Based on exposure to online content, people may develop undue health concerns, believing that common and benign symptoms are explained by serious illnesses. In this paper, we investigate potential strategies to mine queries and searcher histories for clues that could help search engines choose the most appropriate information to present in response to exploratory medical queries. To do this, we performed a longitudinal study of health search behavior using the logs of a popular Web search engine. We found that query variations which might appear innocuous (e.g. \"bad headache\" vs \"severe headache\") may hold valuable information about the searcher which could be used by search engines to improve performance. Furthermore, we investigated how medically-concerned users respond differently to search engine result pages (SERPs) and find that their disposition for clicking on concerning pages is pronounced, potentially leading to a self-reinforcement of concern. Finally, we studied to which degree variations in the SERP impact future search and real-world healthseeking behavior and obtained some surprising results (e.g., viewing concerning pages may lead to a short-term reduction of in-world healthcare utilization).",
        "The University of Padua IMS Research Group at TREC 2018 Precision Medicine Track We report on the participation of the Information Management System (IMS) Research Group of the University of Padua in the second task of the Precision Medicine Track at TREC 2018: the Clinical Trials task. We designed a procedure to: i) expand query terms iteratively, based on knowledge bases, to increase the probability of finding relevant trials by adding neoplasm, gene, and protein term variants to the initial query; ii) filter out trials based on demographic data. We submitted three runs: a plain BM25 using the provided textual fields <gene> and <disease> as query, a BM25 with a first knowledge-based query expansion, and another BM25 with an additional knowledge-based query expansion. This initial set of experiments lays the ground for a deeper study on the effectiveness of (automatic) knowledge-based expansion techniques in the context of precision medicine.",
        "Scaling up all pairs similarity search. Given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. We propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. We show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches.",
        "Deep Learning Approach for the Precision Medicine Track he paper describes the system presented by the University of L'Aquila in collaboration with the University of Havana-team named UNIVAQ-to the TREC 2019 Precision Medicine Track. The proposed solution, maps any kind of documents-Scientific. , Clinical trials, and Topics-into a multi-dimensional common general representation. Each document is described by five primitive features. The values of each feature are extracted from the original documents using deep learning and machine learning text processing based techniques. To recognize Genes and Diseases, we have trained our models using the PubTator annotated corpus. Instead, to derive demographics information, we have trained the employed deep learning models using the documents-obtained from the Relevance and Raw judgements of the past edition of TREC Precision Medicine / Clinical Decision Support Track 2018-considered \"relevant\" or \"partially relevant\". The results of the Track clearly show that applying a system (as our) made solely by a tagging based approach to the Precision Medicine task, is not sufficient to achieve the performances gained by other systems presented in the TREC Precision Medicine Track 2019.",
        "Learning About Health and Medicine from Internet Data. Surveys show that around 70% of US Internet users consult the Internet when they require medical information. People seek this information using both traditional search engines and via social media. The information created using the search process offers an unprecedented opportunity for applications to monitor and improve the quality of life of people with a variety of medical conditions. In recent years, research in this area has addressed public-health questions such as the effect of media on development of anorexia, developed tools for measuring influenza rates and assessing drug safety, and examined the effects of health information on individual wellbeing. This tutorial will show how Internet data can facilitate medical research, providing an overview of the state-of-the-art in this area. During the tutorial we will discuss the information which can be gleaned from a variety of Internet data sources, including social media, search engines, and specialized medical websites. We will provide an overview of analysis methods used in recent literature, and show how results can be evaluated using publicly-available health information and online experimentation. Finally, we will discuss ethical and privacy issues and possible technological solutions. This tutorial is intended for researchers of user generated content who are interested in applying their knowledge to improve health and medicine.",
        "Analyzing the Landscape of Anti-Cancer Drug Research in Pancreatic Cancer. Due to the importance of anti-cancer drug development, there has been a surge of publications pertinent to the field of pancreatic cancer. Therefore, a need for analysis of drug research trend is recognized to minimize risk of new drug testing and understand the anti-cancer drug research area. The purpose of this paper is to identify the landscape of pancreatic cancer drug research by analyzing the anti-cancer drugs extracted from PubMed records and clinical trials datasets. We conduct drug cluster analysis by using the topic modeling technique and network analysis of drug clusters. Comparison of two distinct datasets, scientific publications vs. clinical trials, gives us a new insight into identifying the different portrait of anti-cancer drug research in pure research and clinical settings.The results show that two distinct research trends are observed by ClinicalTrials dataset and PubMed records. It is the major difference with other bibliometric studies in cancer research. Using two different data sources, we can identify different drug research patterns. First, the assumption that drug research published in PubMed is preceded by clinical trials is statistically confirmed in our study. Second, a research trend of new drug testing with various targets is observed in clinical data. On the other hand, we identify that diverse chemicals (e.g. sodium, phosphatidylinositols, and calcium) together with standard therapeutic agent are studied in scientific publications in PubMed. ACKNOWLEDGMENTS",
        "Workshop on health search and discovery: helping users and advancing medicine. This workshop brings together researchers and practitioners from industry and academia to discuss search and discovery in the medical domain. The event focuses on ways to make medical and health information more accessible to laypeople (including enhancements to ranking algorithms and search interfaces), and how we can discover new medical facts and phenomena from information sought online, as evidenced in query streams and other sources such as social media. This domain also offers many opportunities for applications that monitor and improve quality of life of those affected by medical conditions, by providing tools to support their healthrelated information behavior. Categories and Subject Descriptors KeywordsHealth seeking; Medical discovery; Data mining; Social media. BACKGROUND AND MOTIVATIONHealth-related information is one of the most widely searched domains on the Web. A recent survey by the Pew Internet and American Life Project found that 59% of U.S. adults have looked online for health information in the past year, with 35% of respondents attempting to diagnose a medical condition online . Health information seeking is conducted using both search systems such as Internet search engines and via social media (e.g., advice from friends). Research has indicated that cyberspace can be a dangerous place for those with no medical training. Web search engines can surface alarming content that can cause unwarranted anxiety .Such challenges necessitate new information retrieval (IR) methods-both backend and frontend-to significantly improve the search experience for health information seeking. These methods can, for example, translate layman and technical terminology, and consider domain knowledge and interpretability and health-related reliability or authority during ranking or crawling . This domain also presents unprecedented opportunities for the development of applications that monitor and improve the quality of life of people affected by a variety of medical conditions via tools to support their health-oriented information behavior. Mining aspects of that behavior (e.g., queries and social media interactions) with consent in the aggregate across many users has potential to assist in medical discoveries and enhance public health monitoring .The monitoring and use of health-related online behavior also brings into sharper focus important tradeoffs between privacy and benefit, which need to be explored in more detail. We envisage that the expertise of the SIGIR community will provide helpful insights on matters of privacy, as well as retrieval, personalization, expertise modeling, search interface design, data mining, and others; all of which are critical to enabling advances this area. Through presentations and discussions at the workshop we hope to outline how the IR community can play a central role the future of health search and discovery, and bring benefit to health seekers everywhere. WORKSHOP OVERVIEWThe event comprises discussions (including brainstorming and breakouts), keynotes, and other presentations on papers submitted in response to an open call. The themes of the workshop include:\u2022 Predictive analysis from large-scale data such as behavioral logs, microblogging, and other social media; \u2022 Mining large-scale content (e.g., logs, Web crawls) for insights and links between conditions, drugs, and cohorts; \u2022 Establishing the reliability of online health content;\u2022 Diagnostic search by patients and health practitioners and its cognitive impact; \u2022 Modeling the effect of domain expertise on health seeking; \u2022 Query formulation for medical retrieval (including query expansion, UMLS, concept-hierarchies, etc.); \u2022 Visualization and exploration of medical information; \u2022 Privacy issues on the storage and retrieval of medical content;\u2022 Personalization of short-and long-term health interests;\u2022 Platforms for storing and maintaining health and fitness information (e.g., HealthVault), and; \u2022 Social networks for sharing and monitoring health-related content (e.g., PatientsLikeMe).Discussions at the event will target many of these topics. One area of particular interest is challenges in improving health search for laypeople. Websites such as WebMD.com, eHealthme.com, and PatientsLikeMe.com provide medical information or peer support to health seekers. We will discuss improvements to health search via, for example, synthesizing health content from many sources, and considering base rates and source reliability in result ranking.",
        "A Text Mining Approach for Identifying Herb-chemical Relationships from Biomedical Articles. BACKGROUNDHerbs are natural products that human consume in many different ways such as food and medicine. They have a long empirical history to treat diseases and have relatively less side effects in comparison to Western medicine. Due to such strengths of herbs, many studies related to medicinal herbs have been conducted all over the world. For example, researches proving the medical efficacy of herbs in target diseases and developing new drugs for the treatment of various diseases have been performed. In this aspect, identifying chemicals in herbs is a crucial task because, although several existing databases related to traditional Chinese medicine currently provide herb-chemical relationships, the number of provided herbs is limited. Recently, with the accumulation of a large amount of biomedical articles on the Web site such as PubMed, it becomes possible to extract relationship from texts using a text-mining method. Thus, in this study, we constructed a herb-chemical corpus and developed a rule-based text mining model for identifying herb-chemical relationships from PubMed abstracts. METHODS AND RESULTSA text corpus plays an important role in linguistics because they are intended to be representative of a very wide range of language use in many domains. In the field of text mining, the corpus generally contains information about locations of each entity mention and relation types between annotated mentions. Since there is no corpus available that covers the same domain and the relationship between herbs and chemicals, we manually curated a corpus of 265 herbchemical relationship that consists of 128 positive sentences and 137 negative sentences. To achieve this, we annotated herb and chemical mentions in 13,408,621 PubMed abstracts using named entity recognition (NER) tools; ChemSpot for chemical terms ; LingPipe dictionary-based exact matching for herb terms . Since there is no herb annotation Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). CONCLUSION AND DISCUSSIONIn this study, we presented a corpus of the herb-chemical relationship and a rule based model to predict the relationship. We expect that the herb-chemical corpus will be a building block for many researches using herbs in the textmining domain. Also, extracted relationships from PubMed could be used for various studies that require information about new herb-chemical relationships that are not available in existing databases.",
        "The Knowledge and Language Gap in Medical Information Seeking. Interest in medical information retrieval has risen significantly in the last few years. The Internet has become a primary source for consumers looking for health information and advice; however, their lack of expertise causes a language and knowledge gap that affects their ability to properly formulate their information needs. Health experts also struggle to efficiently search the large amount of medical literature available to them, which impacts their ability of integrating the latest research findings in clinical practice. In this dissertation, I propose several methods to overcome these challenges, thus improving search outcomes.For queries issued by lay users, I introduce query clarification, a technique to identify the most appropriate expert expression that describes their information need; such expression is then used to expand the query . I experiment with three existing synonym mappings, and show that the best one leads to a 7.3% improvement over non-clarified queries. When a classifier that predicts the most appropriate mapping for each query is used, an additional 5.2% improvement over non-clarified queries is achieved.Furthermore, I introduce a set of features to capture semantic similarity between consumer queries and retrieved documents, which are then exploited by a learning to rank framework . This approach yields a 26.6% improvement over the best known results on a dataset designed to evaluate medical information retrieval for lay users.To improve literature search for medical professionals, I propose and evaluate two query reformulation techniques that expand complex medical queries with relevant latent and explicit medical concepts . The first is an unsupervised system that combines a statistical query expansion with a medical terms filter, while the second is a supervised neural convolutional model that predicts which terms to add to medical queries. Both approaches are competitive with the state of the art, achieving up to 8% improvement in inferred nDCG.Finally, I conclude my dissertation by showing how the convolutional model can be adapted to reduce clinical notes that contain significant noise, such as medical abbreviations, incomplete sentences, and redundant information . This approach outperforms the best query reformulation system for this task by 27% in inferred nDCG."
    ],
    "Natural Language Processing": [
        "An emotion-based responding model for natural language conversation. As an important task of artificial intelligence, natural language conversation has attracted wide attention of researchers in natural language processing. Existing works in this field mainly focus on consistency of neural response generation whilst ignoring the effect of emotion state on the response generation. In this paper, we propose an Emotion-based natural language Responding Model (ERM) to address the challenging issue in conversation. ERM encodes the emotion state of the conversation as distributed embedding into the process of response generation, redefines an objective function that jointly trains our model and introduces a novel re-rank function to select the appropriate response. Experimental results on Chinese conversation dataset show that our method yields qualitative performance improvements in the Perplexity (PPL), Word Error-rate (WER) and Bilingual Evaluation Understudy (BLEU) compared with the baseline sequence-to-sequence (Seq2Seq) model, and achieves better performance than the state-of-the-art in terms of emotion and content consistency of the response.",
        "Assessing the influence of personal preferences on the choice of vocabulary for natural language generation a b s t r a c tReferring expression generation is the part of natural language generation that decides how to refer to the entities appearing in an automatically generated text. Lexicalization is the part of this process which involves the choice of appropriate vocabulary or expressions to transform the conceptual content of a referring expression into the corresponding text in natural language. This problem presents an important challenge when we have enough knowledge to allow more than one alternative. In those cases, we need some heuristics to decide which alternatives are more appropriate in a given situation. Whereas most work on natural language generation has focused on a generic way of generating language, in this paper we explore personal preferences as a type of heuristic that has not been properly addressed. We empirically analyze the TUNA corpus, a corpus of referring expression lexicalizations, to investigate the influence of language preferences in how people lexicalize new referring expressions in different situations. We then present two corpus-based approaches to solve the problem of referring expression lexicalization, one that takes preferences into account and one that does not. The results show a decrease of 50% in the similarity error against the reference corpus when personal preferences are used to generate the final referring expression.",
        "Natural language processing and information retrieval",
        "Proceedings of the 3rd Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018) co-located with the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018), Ann Arbor, USA, July 12, 2018",
        "The Function of Semantics in Automated Language Processing. This paper is a survey of some of the major semantic models that have been developed for automated semantic analysis of natural language.Current approaches to semantic analysis and logical inference are based mainly on models of human cognitive processes such as Quillian's semantic memory, Simmon's Protosynthex III and others.All existing systems and/or models, more or less experimental, were applied to a small subset of English.They are highly tentative because the definitions of semantic processes and semantically structured lexicon~s are not formulated rigorously. This is due mainly to the fact that it is unknown whether a unique, consistent hierarchization of the semantic features of language is possible. However, the models described are significant contributions to an unexplored field called semantics.The progressive development of a sophisticated, semantically based system for automated processing of natural language is a realistic goal. It should not be neglected, despite the fact that it is difficult to predict when this goal will be achieved. KEY WORDS AND PHRASESnatural language processing, grammars, semantics, computational linguistics I.",
        "Chinese-English mixed text normalization. Along with the expansion of globalization, multilingualism has become a popular social phenomenon. More than one language may occur in the context of a single conversation. This phenomenon is also prevalent in China. A huge variety of informal Chinese texts contain English words, especially in emails, social media, and other user generated informal contents. Since most of the existing natural language processing algorithms were designed for processing monolingual information, mixed multilingual texts cannot be well analyzed by them. Hence, it is of critical importance to preprocess the mixed texts before applying other tasks. In this paper, we firstly analyze the phenomena of mixed usage of Chinese and English in Chinese microblogs. Then, we detail the proposed two-stage method for normalizing mixed texts. We propose to use a noisy channel approach to translate in-vocabulary words into Chinese. For better incorporating the historical information of users, we introduce a novel user aware neural network language model. For the out-of-vocabulary words (such as pronunciations, informal expressions and et al.), we propose to use a graphbased unsupervised method to categorize them. Experimental results on a manually annotated microblog dataset demonstrate the effectiveness of the proposed method. We also evaluate three natural language parsers with and without using the proposed method as the preprocessing step. From the results, we can see that the proposed method can significantly benefit other NLP tasks in processing mixed text.",
        "HITS@FIRE task 2015: Twitter based Named Entity Recognizer for Indian Languages Natural Language processing (NLP) in its pure sense, is a platform that provides the ability for transforming natural language text to useful information. Named Entity Recognition (NER) is a key task in NLP for classification of named entities in natural languages. Though, there are several algorithms for named entity classification, identifying named entities in twitter data is a demanding task. Loads of information are being shared by people in twitter on a daily basis. This information is unstructured and often contains important information about organizations, politics, disasters, promotional advertisements etc. In this paper, we provide a NER that can effectively classify named entities in twitter data for Indian Languages such as English, Hindi and Tamil. POS, Chunk, Suffix, Prefix information has been used for training in Conditional Random Fields (CRF) based NER Model. CRF is a popular model for labeling and classification in text mining. Performance analysis was done using n-fold validation and F-measure. A maximum precision of 93.82 for English, 92.28 for Hindi and 86.94 for Tamil twitter data was achieved through N fold validation. Results provided by ESM-IL share task in terms of precision for English is 50.48, for Hindi is 81.49 and for Tamil 70.42. The proposed algorithm has a higher classification accuracy and it is achieved through n-fold validation. CCS Concepts \u2022 Human centered computing\u279d Human machine interaction\u279d Collaborative and social computing \u2022 Applied Computing \u279dDocument managing and text computing methodologies\u279d Artificial Intelligence and Machine Learning.",
        "An Approach to Natural Language Processing for Document Retrieval. Document retrieval systems have been restricted, by the nature of tile task, to techniques that can be used with large numbers of documents and broad domains. The most effective techniques that have been developed are based on the statistics of word occurrences ill text. In this paper, we describe an approach to using natural language processiag (NLP) techniques for what is essentially a natural language problem -the comparison of a request text with the text of document titles and abstracts. The proposed NLP techniques are used to develop a request model based on \"conceptual case frames\" and to compare this model with the texts of candidate documents. The request model is also used to provide information to statistical search techniques that identify the candidate documents. As part of a preliminary evaluation of this approach, case frame representations of a set of requests from the CACM collection were constructed. Statistical searches carried out using dependency and relative importance information derived from the request models indicate that performance benefits can be obtained.",
        "Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach.",
        "Distributional phrasal paraphrase generation for statistical machine translation Paraphrase generation has been shown useful for various natural language processing tasks, including statistical machine translation. A commonly used method for paraphrase generation is pivoting , which benefits from linguistic knowledge implicit in the sentence alignment of parallel texts, but has limited applicability due to its reliance on parallel texts. Distributional paraphrasing ] has wider applicability, is more language-independent, but doesn't benefit from any linguistic knowledge. Nevertheless, we show that using distributional paraphrasing can yield greater gains in translation tasks. We report method improvements leading to higher gains than previously published, of almost 2 BLEU points, and provide implementation details, complexity analysis, and further insight into this method.",
        "Natural language processing: Word recognition without segmentation",
        "ASE@DPIL-FIRE2016: Hindi Paraphrase Detection using Natural Language Processing Techniques & Semantic Similarity Computations The paper reports the approaches utilized and results achieved for our system in the shared task (in FIRE-2016) for paraphrase identification in Indian languages (DPIL). Since Indian languages have a complex inherent nature, paraphrase identification in these languages becomes a challenging task. In the DPIL task, the challenge is to detect and identify whether a given sentence pairs paraphrased or not. In the proposed work, natural language processing with semantic concept extractions is explored for paraphrase detection in Hindi. Stop word removal, stemming and part of speech tagging are employed. Further similarity computations between the sentence pairs are done by extracting semantic concepts using WordNet lexical database. Initially, the proposed approach is evaluated over the given training sets using different machine learning classifiers. Then testing phase is used to predict the classes using the proposed features. The results are found to be promising, which shows the potency of natural language processing techniques and semantic concept extractions in detecting paraphrases.",
        "Word classification and hierarchy using co-occurrence word information. By the development of the computer in recent years, calculating a complex advanced processing at high speed has become possible. Moreover, a lot of linguistic knowledge is used in the natural language processing (NLP) system for improving the system. Therefore, the necessity of co-occurrence word information in the natural language processing system increases further and various researches using co-occurrence word information are done. Moreover, in the natural language processing, dictionary is necessary and indispensable because the ability of the entire system is controlled by the amount and the quality of the dictionary. In this paper, the importance of co-occurrence word information in the natural language processing system was described. The classification technique of the co-occurrence word (receiving word) and the co-occurrence frequency was described and the classified group was expressed hierarchically. Moreover, this paper proposes a technique for an automatic construction system and a complete thesaurus. Experimental test operation of this system and effectiveness of the proposal technique is verified.",
        "Learning information intent via observation. Workers in organizations frequently request help from assistants by sending request messages that express information intent: an intention to update data in an information system. Human assistants spend a significant amount of time and effort processing these requests. For example, human-resource assistants process requests to update personnel records, and executive assistants process requests to schedule conference rooms or to make travel reservations. To process the intent of a request, an assistant reads the request and then locates, completes, and submits a form that corresponds to the expressed intent. Automatically or semiautomatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work.For a well-understood domain, a straightforward application of natural-language-processing techniques can be used to build an intelligent form interface to semi-automatically process information-intent request messages. However, high performance parsers are based on machine-learning algorithms that require a large corpus of examples that have been labeled by an expert. The generation of a labeled corpus of requests is a major barrier to the construction of a parser. In this paper, we investigate the construction of a natural-language-processing system and an intelligent form system that observes an assistant processing requests. The intelligent form system then generates a labeled training corpus by interpreting the observations. This paper reports on the measurement of the performance of the machinelearning algorithms based on real data. The combination of observations, machine learning, and interaction design produces an effective intelligent form interface based on natural language processing.",
        "Foundations of Statistical Natural Language Processing",
        "Applying semantic knowledge to the automatic processing of temporal expressions and events in natural language a b s t r a c tThis paper addresses the problem of the automatic recognition and classification of temporal expressions and events in human language. Efficacy in these tasks is crucial if the broader task of temporal information processing is to be successfully performed. We analyze whether the application of semantic knowledge to these tasks improves the performance of current approaches. We therefore present and evaluate a data-driven approach as part of a system: TIPSem. Our approach uses lexical semantics and semantic roles as additional information to extend classical approaches which are principally based on morphosyntax. The results obtained for English show that semantic knowledge aids in temporal expression and event recognition, achieving an error reduction of 59% and 21%, while in classification the contribution is limited. From the analysis of the results it may be concluded that the application of semantic knowledge leads to more general models and aids in the recognition of temporal entities that are ambiguous at shallower language analysis levels. We also discovered that lexical semantics and semantic roles have complementary advantages, and that it is useful to combine them. Finally, we carried out the same analysis for Spanish. The results obtained show comparable advantages. This supports the hypothesis that applying the proposed semantic knowledge may be useful for different languages.",
        "Processing natural language for an expert system using a sub-language approach",
        "WWW 2008 workshop: NLPIX2008 summary. The amount of information available on the Web has increased rapidly, reaching levels that few would ever have imagined possible. We live in what could be called the \"informationexplosion era,\" and this situation poses new problems for computer scientists. Users demand useful and reliable information from the Web in the shortest time possible, but the obstacles to fulfilling this demand are many including language barriers and the so-called \"long tail.\" Even worse, users may provide only vague specifications of the information that they actually want, so that a more concrete specification must somehow be inferred by Web access tools. Natural language processing (NLP) is one of the key technologies for solving the above Web usability problems. Almost all the Web page provide with the essential information in the form of natural language texts, and the amount of these text information is huge. In order to offer solutions to these problems we must perform searching and extracting information from the Web texts using NLP technologies. The aim of this workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008) is to bring researchers and practitioners together in order to discuss our most pressing needs with respect to accessing information on the Web, and to discuss new ideas in NLP technologies that might offer viable solutions for those issues.",
        "NLP4REC: The WSDM 2020 Workshop on Natural Language Processing for Recommendations Natural language processing is becoming more and more important in recommender systems. This half day workshop explores challenges and potential research directions in Recommender Systems (RSs) combining Natural Language Processing (NLP). The focus will be on stimulating discussions around how to combine natural language processing technologies with recommendation. We welcome theoretical, experimental, and methodological studies that leverage NLP technologies to advance recommender systems, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating the interaction between NLP and RSs to develop more intelligent RSs.",
        "Transporting the Linguistic String Project System from a Medical to a Navy Domain The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems.",
        "Natural language and the information layer My talk has three parts: on the first phase of natural language processing research and its lessons; on subsequent developments up to the present and their lessons; and on where we are now and what I think are the wider implications for the future. Words, classification, and retrievalWhen I began research in computing nearly fifty years ago, people were very excited about what could be done with computers, challenged by how to do it, and pushing applications that offered new opportunities in dealing with information. One of these areas was NLP -natural language processing -or NLIP -processing information conveyed in natural language. At that time, most researchers thought primarily of the efficiency gains that could be Copyright is held by the author/owner(s). SIGIR'07, July 23-27, 2007, Amsterdam, The Netherlands. ACM 978-1-59593-597-7/07/0007. made by emulating or supporting people, e.g. in translation or document retrieval, though earlier visionaries like Warren Weaver had seen how computers could open up quite novel opportunities, and a little later Doug Englebart would illustrate some of these.My own research focussed on automatic classification (nowadays called unsupervised machine learning). It was clear that humans rely in NLP on the use of general conceptual classifications of words -like a thesaurus -to resolve ambiguity in language (in research practice, text). Individual words are ambiguous, and so are the structures represented by word strings. These ambiguities are resolved by the fact that discourse is coherent, and effective because concepts and topics are repeated, and repeated enough to get them across. Repeated individual concepts and standard conceptual patterns enable us to select the right meanings for words and structural relationships for word sequences. This very general idea fairly obviously applies to translation, but it also applies to document retrieval: different words in a query and a document may still stand for the same concept and thus be allowed to match.The question is, where to get the lexical classification and stock of text patterns from. The obvious answer is, from text. If words tend to co-occur in many texts this similar behaviour suggests they are conceptually related. Thus in principle one should be able to build a thesaurus automatically from a vast text corpus, and analogously to extract repeating conceptual patterns.I was interested in building a thesaurus for translation, but of course had no corpus. I managed to finesse this by exploiting some limited dictionary (not thesaurus) data, for a pilot demonstration. For retrieval the situation is easier. The collection of documents or texts from which you want to retrieve supplies the corpus to build the classification. This is the line we followed, with some success in retrieval performance, particularly when relative word frequency was adequately factored in and eventually captured by word weighting. Documents score not just by the number of words matching, but by the sum of their weights.All of this was essentially statistical in character. Facts about word occurrences and co-occurrences were used to capture meaning in a way that could be manipulated without needing to know what that meaning was. An unusually frequent word in a document is a good indicator of an important topic in the document, so if the query uses the word, the document is likely to be relevant to your information need.One very simple word weighting formula captures this effectively. If a word occurs in many documents most of these -other things being equal -are unlikely to be relevant to the user's information need. Weighting by inverse document frequency, idf , relatively favours less common and more discriminating terms. If you SIGIR 2007 Proceedings ACM Athena Award Lecture 3",
        "Understanding Human Language: Can NLP and Deep Learning Help?. There is a lot of overlap between the core problems of information retrieval (IR) and natural language processing (NLP). An IR system gains from understanding a user need and from understanding documents, and hence being able to determine whether a document has information that satisfies the user need. Much of NLP is about the same thing: Natural language understanding aims to understand the meaning of questions and documents and meaning relationships. The exciting recent application of deep learning approaches in NLP has brought new tools for effectively understanding language semantics. In principle, there should be a lot of synergy, though in practice the concerns of IR on large systems and macro-scale understanding have tended to contrast with the emphasis in NLP on language structure and micro-scale understanding.My talk will emphasize the two topics of how NLP can contribute to understanding textual relationships and how deep learning approaches substantially aid in this goal. One basic -and very successful tool -has been the new generation of distributed word representations: neural word embeddings. However, beyond just word meanings, we need to understand how to compose the meanings of larger pieces of text. Two requirements for that are good ways to understand the structure of human language utterances and ways to compose their meanings. Deep learning methods can help for both tasks. Finally, we need to understand relationships between pieces of text, to be able to do tasks such as Natural Language Inference (or Recognizing Textual Entailment) and Question Answering, and I will look at some of our recent work in these areas, both with and without the help of neural networks. KeywordsNatural language processing, deep learning, word vectors, compositionality, natural language inference, recognizing textual entailment, question answering Short BiographyChristopher Manning is a professor of computer science and linguistics at Stanford University. His Ph.D. is from Stanford in 1995, and he held faculty positions at Carnegie Mellon University and the University of Sydney before returning to Stanford. His research goal is computers that can intelligently process, understand, and generate human language material. Manning concentrates on machine learning approaches to computational linguistic problems, including syntactic parsing, computational semantics and pragmatics, textual inference, machine translation, and using deep learning for NLP. He is an ACM Fellow, a AAAI Fellow, and an ACL Fellow, and has coauthored leading textbooks on statistical natural language processing and information retrieval. He is a member of the Stanford NLP group (@stanfordnlp).",
        ". s of the Papers to be presented at the Sixth Annual International ACM SIGIR Conference 1983 Natural language processing techniques developed for Artificial Intelligence programs can aid in constructing powerful information retrieval systems in at least two areas. Automatic construction of new concepts allows a large body of information to be organized compactly and in a manner that allows a wide range of queries to be answered. Also, using natural language processing techniques to conceptually analyze the documents being stored in a system greatly expands the effectiveness of queries about given pieces of text. However, only robust conceptual analysis methods are adequate for such systems. This paper will discuss approaches to both concept learning, in the form of Generalization-Based Memory, and powerful, robust text processing achieved by Memory-Based Understanding.These techniques have been implemented in the computer systems IPP, a program that reads, remembers and generalizes from news stories about terrorism, and RESEARCHER, currently in the prototype stage, that operates in a very different domain (technical texts, patent abstracts in particular). The City University, LondonWe are concerned with the problem of making automated information retrieval (IR) systems directly accessible by end users, without recourse to human intermediaries. Thus, we are concerned with automating at least some of the functions performed by human intermediaries in IR interaction",
        "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019",
        "LDC-1: A Transportable, Knowledge-Based Natural Language Processor for Office Environments During the 1970s, a number of systems providing limited English-language processing capabilities were developed to permit computer access by casual or untrained users. Our interest is in adapting and extending techniques developed for these systems, especially those used in database query systems and our own English-language programming language system (NLC), for use in office environments. This paper describes the Layered Domain Class system (LDC), a state-of-the-art natural language processor whose major goals are (1) to provide English-language retrieval capabilities for mediumsized office domains that have been stored on the computer as text-edited files, as opposed to more restrictive database structures; and (2) to eliminate the need to call in the system designer when extensions into new domains are desired, without sacrificing the depth or reliability of the interface. In this paper we (a) provide an overview of LDC, including sample inputs; (b) briefly discuss the role of each module of the system, with special attention to provisions for users to adapt the system to deal with new types of data; and (c) consider the relation of our system to other formal and natural language interfaces that are in use or under development.",
        "Retrieval from Captioned Image Databases Using Natural Language Processing. A t rst sight, it might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat di erent. The ANVIL system uses a natural language tec hnique to obtain high accuracy retriev al of images which h a ve been annotated with a descriptive textual caption. The natural language techniques also allow additional con textual information to be derived from the relation betw een the query and thecaption, which c a n h e l p users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system.",
        "Natural language processing in support of decision-making: phrases and part-of-speech tagging",
        "Deep Natural Language Processing for Search and Recommendation Search and recommender systems process rich natural language text data such as user queries and documents (e.g., articles, profiles, transcripts, comments, posts). Achieving high-quality search and recommendation results requires processing and understanding such information effectively and efficiently, where natural language processing (NLP) technologies are widely deployed. Natural language data are represented as a sequence of words. Understanding such sequential information is generally a nontrivial task in traditional methods, with challenges on both data sparsity and data generalization. Deep learning models provide an opportunity to effectively extract the representative relevant information, thus better understanding complicated semantics and underlying user intention. In recent years, the rapid development of deep learning technology has been proven successful for improving various NLP tasks, indicating their great potential of promoting search and recommender systems. Developing deep learning models for NLP [4, 11, 19, 21, 28, 29] in search and recommender systems [6, 7] involves various fundamental components including 1). query and document understanding that extracts and infers relevant information, such as intent prediction [18], entity tagging and disambiguation [15, 26], topic understanding [1, 20], and opinion mining [13, 16]; 2). retrieval and ranking methodologies designed with strong latency restrictions [8, 10, 17] and various matching strategies [3, 12, 14]; and 3). language generation techniques designed to proactively guide/interact with users to further resolve ambiguity,"
    ],
    "graph based ranking": [
        "Effective latent space graph-based re-ranking model with global consistency. Recently the re-ranking algorithms have been quite popular for web search and data mining. However, one of the issues is that those algorithms treat the content and link information individually. Inspired by graph-based machine learning algorithms, we propose a novel and general framework to model the re-ranking algorithm, by regularizing the smoothness of ranking scores over the graph, along with a regularizer on the initial ranking scores (which are obtained by the base ranker). The intuition behind the model is the global consistency over the graph: similar entities are likely to have the same ranking scores with respect to a query. Our approach simultaneously incorporates the content with other explicit or implicit link information in a latent space graph. Then an effective unified re-ranking algorithm is performed on the graph with respect to the query. To illustrate our methodology, we apply the framework to literature retrieval and expert finding applications on DBLP bibliography data. We compare the proposed method with the initial language model method and another PageRankstyle re-ranking method. Also, we evaluate the proposed method with varying graphs and settings. Experimental results show that the improvement in our proposed method is consistent and promising.",
        "Ranking objects by following paths in entity-relationship graphs. In this paper, we propose an object ranking method for search and recommendation. By selecting schema-level paths and following them in an entity-relationship graph, it can incorporate diverse semantics existing in the graph. Utilizing this kind of graph-based data models has been recognized as a reasonable way for dealing with heterogeneous data. However, previous work on ranking models using graphs has some limitations. In order to utilize a variety of semantics in multiple types of data, we define a schema path as a basic component of the ranking model. By following the path or a combination of paths, relevant objects could be retrieved or recommended. We present some preliminary experiments to evaluate our method. In addition, we discuss several interesting challenges that can be considered in future work.",
        "Personalized recommendation via user preference matching A B S T R A C TGraph-based recommendation approaches use a graph model to represent the relationships between users and items, and exploit the graph structure to make recommendations. Recent graphbased recommendation approaches focused on capturing users' pairwise preferences and utilized a graph model to exploit the relationships between different entities in the graph. In this paper, we focus on the impact of pairwise preferences on the diversity of recommendations. We propose a novel graph-based ranking oriented recommendation algorithm that exploits both explicit and implicit feedback of users. The algorithm utilizes a user-preference-item tripartite graph model and modified resource allocation process to match the target user with users who share similar preferences, and make personalized recommendations. The principle of the additional preference layer is to capture users' pairwise preferences, provide detailed information of users for further recommendations. Empirical analysis of four benchmark datasets demonstrated that our proposed algorithm performs better in most situations than other graph-based and ranking-oriented benchmark algorithms.",
        "Graph Analytical Re-Ranking for Entity Search Entity search is a fundamental task in Linked Data (LD). The task is, given a keyword search query, to retrieve a set of entities in LD which are relevant to the query. The state-ofthe-art approaches for entity search are based on information retrieval technologies such as TF-IDF vectorization and ranking models. This paper examines the approaches by applying a traditional evaluation metrics, recall@k, and shows ranking qualities still room left for improvements. In order to improve the ranking qualities, this paper explores possibilities of graph analytical methods. LD is regarded as a large graph, graph analytical approaches are therefore appropriate for this purpose. Since query-based graph analytical approaches fit to entity search tasks, this paper proposes a personalized PageRankbased re-ranking method, PPRSD (Personalized PageRank based Score Distribution), for retrieved results by the state-of-the-art. The experimental evaluation recognizes improvements but its results are not satisfactory, yet. For further improvements, this paper reports investigations about relationship between queries and entities in terms of path lengths on the graph, and discusses future directions for graph analytical approaches.",
        "Computational Social Indicators: A Case Study of Chinese University Ranking. Many professional organizations produce regular reports of social indicators to monitor social progress. Despite their reasonable results and societal value, early e orts on social indicator computing su er from three problems: 1) labor-intensive data gathering, 2) insu cient data, and 3) expert-relied data fusion. Towards this end, we present a novel graph-based multi-channel ranking scheme for social indicator computation by exploring the rich multi-channel Web data. For each channel, this scheme presents the semi-structured and unstructured data with simple graphs and hypergraphs, respectively. It then groups the channels into di erent clusters according to their correlations. A er that, it uses a uni ed model to learn the cluster-wise common spaces, perform ranking separately upon each space, and fuse these rankings to produce the nal one. We take Chinese university ranking as a case study and validate our scheme over a real-world dataset. It is worth emphasizing that our scheme is applicable to computation of other social indicators, such as Educational a ainment.",
        "Graph Databases for Information Retrieval Graph models have been deployed in the context of information retrieval for many years. Computations involving the graph structure are often separated from computations related to the base ranking. In recent years, graph data management has been a topic of interest in database research. We propose to deploy graph database management systems to implement existing and novel graph-based models for information retrieval. For this a unifying mapping from a graph query language to graph based retrieval models needs to be developed; extending standard graph database operations with functionality for keyword search. We also investigate how data structures and algorithms for ranking should change in presence of continuous database updates. We want to investigate how temporal decay can affect ranking when data is continuously updated. Finally, can databases be deployed for efficient two-stage retrieval approaches?",
        "Supervised Nested PageRank. Graph-based ranking plays a key role in many applications, such as web search and social computing. Pioneering methods of ranking on graphs (e.g., PageRank and HITS) computed ranking scores relying only on the graph structure. Recently proposed methods, such as Semi-Supervised PageRank, take into account both the graph structure and the metadata associated with nodes and edges in a unified optimization framework. Such approaches are based on initializing the underlying random walk models with prior weights of nodes and edges that in turn depend on their individual properties. While in those models the prior weights of nodes and edges depend only on their own features, one can also assume that such weights may also depend or be related to the prior weights of their neighbors. This paper addresses the problem of weighting nodes and edges according to this intuition by realizing it in a general ranking model and an efficient algorithm of tuning the parameters of that model.",
        "Fusion of Heterogeneous Information in Graph-Based Ranking for Query-Biased Summarization We propose a graph-based ranking method for query-biased summarization in a three-layer graph model consisting of document, sentence and word-layers. The model has a representation that fuses three kinds of heterogeneous information: part-whole relationships between different linguistic units, similarity using the overlap of the Basic Elements (BEs) in the statements, and semantic similarity between words. In an experiment using the text summarization test collection of Nakano et al., our proposed method achieved the best result of the five considered methods, which were based on other graph models with an average R-Precision of 0.338.",
        "HyperSum: hypergraph based semi-supervised sentence ranking for query-oriented summarization. Graph based sentence ranking algorithms such as PageRank and HITS have been successfully used in query-oriented summarization. With these algorithms, the documents to be summarized are often modeled as a text graph where nodes represent sentences and edges represent pairwise similarity relationships between two sentences. A deficiency of conventional graph modeling is its incapability of naturally and effectively representing complex group relationships shared among multiple objects. Simply squeezing complex relationships into pairwise ones will inevitably lead to loss of information which can be useful for ranking and learning. In this paper, we propose to take advantage of hypergraph, i.e. a generalization of graph, to remedy this defect. In a text hypergraph, nodes still represent sentences, yet hyperedges are allowed to connect more than two sentences. With a text hypergraph, we are thus able to integrate both group relationships formulated among multiple sentences and pairwise relationships formulated between two sentences in a unified framework. As essential work, it is first addressed in the paper that how a text hypergraph can be built for summarization by applying clustering techniques. Then, a hypergraph based semisupervised sentence ranking algorithm is developed for queryoriented extractive summarization, where the influence of query is propagated to sentences through the structure of the constructed text hypergraph. When evaluated on DUC data sets, performance of the proposed approach is remarkable.",
        "A Graph-based model for context-aware recommendation using implicit feedback data. Recommender systems have been successfully dealing with the problem of information overload. However, most recommendation methods suit to the scenarios where explicit feedback, e.g. ratings, are available, but might not be suitable for the most common scenarios with only implicit feedback. In addition, most existing methods only focus on user and item dimensions and neglect any additional contextual information, such as time and location. In this paper, we propose a graph-based generic recommendation framework, which constructs a Multi-Layer Context Graph (MLCG) from implicit feedback data, and then performs ranking algorithms in MLCG for context-aware recommendation. Specifically, MLCG incorporates a variety of contextual information into a recommendation process and models the interactions between users and items. Moreover, based on MLCG, two novel ranking methods are developed: Context-aware Personalized Random Walk (CPRW) captures user preferences and current situations, and Semantic Path-based Random Walk (SPRW) incorporates semantics of paths in MLCG into random walk model for recommendation. The experiments on two real-world datasets demonstrate the effectiveness of our approach.",
        "Respect my authority!: HITS without hyperlinks, utilizing cluster-based language models. We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them.We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based re-ranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents.",
        "Unsupervised graph-based rank aggregation for improved retrieval A R T I C L E I N F O Keywords:Rank aggregation Content-based retrieval Multimodal retreival Graph-based fusion A B S T R A C T This paper presents a robust and comprehensive graph-based rank aggregation approach, used to combine results of isolated ranker models in retrieval tasks. The method follows an unsupervised scheme, which is independent of how the isolated ranks are formulated. Our approach is able to combine arbitrary models, defined in terms of different ranking criteria, such as those based on textual, image or hybrid content representations.We reformulate the ad-hoc retrieval problem as a document retrieval based on fusion graphs, which we propose as a new unified representation model capable of merging multiple ranks and expressing inter-relationships of retrieval results automatically. By doing so, we claim that the retrieval system can benefit from learning the manifold structure of datasets, thus leading to more effective results. Another contribution is that our graph-based aggregation formulation, unlike existing approaches, allows for encapsulating contextual information encoded from multiple ranks, which can be directly used for ranking, without further computations and postprocessing steps over the graphs. Based on the graphs, a novel similarity retrieval score is formulated using an efficient computation of minimum common subgraphs. Finally, another benefit over existing approaches is the absence of hyperparameters.A comprehensive experimental evaluation was conducted considering diverse well-known public datasets, composed of textual, image, and multimodal documents. Performed experiments demonstrate that our method reaches top performance, yielding better effectiveness scores than state-of-the-art baseline methods and promoting large gains over the rankers being fused, thus demonstrating the successful capability of the proposal in representing queries based on a unified graph-based model of rank fusions.",
        "Fast Item Ranking under Neural Network based Measures Recently, plenty of neural network based recommendation models have demonstrated their strength in modeling complicated relationships between heterogeneous objects (i.e., users and items). However, the applications of these fine trained recommendation models are limited to the off-line manner or the re-ranking procedure (on a pre-filtered small subset of items), due to their time-consuming computations. Fast item ranking under learned neural network based ranking measures is largely still an open question. In this paper, we formulate ranking under neural network based measures as a generic ranking task, Optimal Binary Function Search (OBFS), which does not make strong assumptions for the ranking measures. We first analyze limitations of existing fast ranking methods (e.g., ANN search) and explain why they are not applicable for OBFS. Further, we propose a flexible graph-based solution for it, Binary Function Search on Graph (BFSG). It can achieve approximate optimal efficiently, with accessible conditions. Experiments demonstrate effectiveness and efficiency of the proposed method, in fast item ranking under typical neural network based measures.",
        "Text Representation, Retrieval, and Understanding with Knowledge Graphs. This dissertation aims to improve text representation, retrieval, and understanding with knowledge graphs. Previous information retrieval systems were mostly built upon bag-ofwords representations and frequency-based retrieval models. Effective as they are, wordbased representations and frequency signals only provide shallow text understanding and have various intrinsic challenges. Utilizing entities and their structured semantics from knowledge graphs, this dissertation goes beyond bag-of-words and improves search with richer text representations, customized semantic structures, sophisticated ranking models and neural networks.This thesis research starts by enriching query representations with entities and their textual attributes. It first presents query expansion methods that better represent the query with words from entity descriptions. Then it develops a supervised latent-space ranking model that connects query and documents through related entities from the knowledge graph. It also provides a novel supervised related entity finding technique in the entity search setup.Then this dissertation presents our entity-oriented search framework that represents query and documents with entity-based text representations and matches them in the entity space. We construct a bag-of-entities model that represents texts using automatically linked entities with a customized linking strategy. Ranking with bag-of-entities can be done either solely with discrete match-as in classic retrieval models-or by our Explicit Semantic Ranking approach that soft matches the query and documents with continuous knowledge graph embeddings. The entity-based text representations are then combined with word-based representations in a word-entity duet representation method. In the duet, query and documents are represented by both bag-of-words and bag-of-entities; the ranking of them goes through both in-space matches and cross-space matches which together incorporates various types of semantics from knowledge graphs. The duet framework also introduces a hierarchical ranking model that learns the linking of entities and the ranking of documents jointly from relevance labels.This thesis research concludes with a neural entity salience estimation model that provides a deeper text understanding capability. We developed a Kernel Entity Salience Model that better estimates the importance of entities in text with distributed representations and kernelbased interactions. Not only does it improve the salience estimation accuracy, it can also",
        "Random surfer with back step. We present a novel link-based ranking algorithm RBS, which may be viewed as an extension of PageRank by back-step feature. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering General TermsAlgorithms,Experimentation KeywordsRanking Algorithms, PageRank, Back Step THE PROBLEMIn this paper we report development of a link-based ranking algorithm which is built on a random surfer model reflecting back steps made by a Web surfer.Link analysis of the Web turned out to be a powerful approach in the context of exponentially growing Web with documents of extremely varying type, quality and size, where traditional text information retrieval techniques are not robust enough.Link-based ranking algorithms are used for ordering results of search queries passed to search engines. More precisely, each document is assigned some ranking score which is computed by a ranking algorithm according to some intuitive model, and then documents are presented in non-increasing order of their ranking scores. Since the number of documents returned by a search engine may easily be far too large to be seen by a user, such ordering is extremely important.Link-based ranking algorithms proved their value in practice. The most famous example is perhaps Google's PageRank [1] which is successfully applied in this leading search engine. Traditional Random Surfer ModelPageRank is based on the random surfer model [1]. The model has a parameter 0 \u2264 a \u2264 1. More precisely, in each time step t a surfer, currently visiting a document p(t), with probability a follows uniformly picked out-link or, with the remaining probability 1 \u2212 a (called here a decay factor), randomly jumps to another page with uniform distribution over pages.Unfortunately, the traditional Random Surfer model does not reflect some very common aspect of Web surfing i.e reverting to a Copyright is held by the author/owner(s). page visited in the previous step. Reverting to previous pages constitutes a substantial part of Web surfing behavior, and, in our opinion, it should be reflected in ranking schemes. WWW2004,There are many ways of implementing revisiting Web pages in Web browsers -one of the most common is via back and forward buttons. We build our model on a simplification of this approachi.e. we assume that there is only a back button available.In this paper we describe incorporating back step into ranking scheme by properly extending the random surfer model and report experimental results of RBS algorithm, which is our implementation of the proposed novel model. PREVIOUS WORKThere have been proposed some link-based ranking algorithms before (Hits, \"Unified framework\" or Salsa), which incorporate the notion of backward flow of rank, but in order to compute so-called hub score rather than to model back step.On the other hand, Fagin et al. analyse random walks with back steps in the context of Web surfing, but approach presented in that paper differs from ours. Authors do not aim at developing link-based ranking algorithm for Web search. Furthermore, they define their main model so that the result is equivalent to the result obtained in the traditional random surfer model, because they allow for back-step after any forward step, which is not realistic in our opinion. Due to this, our back step is formulated slightly differently. Most importantly, the computational methods proposed in [3] would be of prohibitively high time complexity when applied to large scale Web page collections, despite they are polynomial.In contrast, our approach results in really effective method of computing rank score which reflects back-step and produces different ranking than PageRank. RANDOM SURFER WITH BACK STEPWe extend the traditional random surfer model by introducing another parameter 0 \u2264 b \u2264 1, satisfying a + b \u2264 1, which represents the back-step probability. Random surfer is visiting pages in discrete time steps, so that in each step while visiting a page p they may perform one of the following actions:",
        "Meta-Path-Based Ranking with Pseudo Relevance Feedback on Heterogeneous Graph for Citation Recommendation. The sheer volume of scholarly publications available online significantly challenges how scholars retrieve the new information available and locate the candidate reference papers. While classical text retrieval and pseudo relevance feedback (PRF) algorithms can assist scholars in accessing needed publications, in this study, we propose an innovative publication ranking method with PRF by leveraging a number of meta-paths on the heterogeneous bibliographic graph. Different meta-paths on the graph address different ranking hypotheses, whereas the pseudo-relevant papers (from the retrieval results) are used as the seed nodes on the graph. Meanwhile, unlike prior studies, we propose \"restricted meta-path\" facilitated by a new context-rich heterogeneous network extracted from full-text publication content along with citation context. By using learning-to-rank, we integrate 18 different meta-path-based ranking features to derive the final ranking scores for candidate cited papers. Experimental results with ACM full-text corpus show that meta-path-based ranking with PRF on the new graph significantly (p < 0.0001) outperforms text retrieval algorithms with text-based or PageRank-based PRF.",
        "Efficient manifold ranking for image retrieval. Manifold Ranking (MR), a graph-based ranking algorithm, has been widely applied in information retrieval and shown to have excellent performance and feasibility on a variety of data types. Particularly, it has been successfully applied to content-based image retrieval, because of its outstanding ability to discover underlying geometrical structure of the given image database. However, manifold ranking is computationally very expensive, both in graph construction and ranking computation stages, which significantly limits its applicability to very large data sets. In this paper, we extend the original manifold ranking algorithm and propose a new framework named Efficient Manifold Ranking (EMR). We aim to address the shortcomings of MR from two perspectives: scalable graph construction and efficient computation. Specifically, we build an anchor graph on the data set instead of the traditional k-nearest neighbor graph, and design a new form of adjacency matrix utilized to speed up the ranking computation. The experimental results on a real world image database demonstrate the effectiveness and efficiency of our proposed method. With a comparable performance to the original manifold ranking, our method significantly reduces the computational time, makes it a promising method to large scale real world retrieval problems.",
        "Towards an effective and unbiased ranking of scientific literature through mutual reinforcement. It is important to help researchers find valuable scientific papers from a large literature collection containing information of authors, papers and venues. Graph-based algorithms have been proposed to rank papers based on networks formed by citation and co-author relationships. This paper proposes a new graph-based ranking framework MutualRank that integrates mutual reinforcement relationships among networks of papers, researchers and venues to achieve a more synthetic, accurate and fair ranking result than previous graph-based methods. MutualRank leverages the network structure information among papers, authors, and their venues available from a literature collection dataset and sets up a unified mutual reinforcement model that involves both intraand inter-network information for ranking papers, authors and venues simultaneously. To evaluate, we collect a set of recommended papers from websites of graduate-level computational linguistics courses of 15 top universities as the benchmark and apply different methods to estimate paper importance. The results show that MutualRank greatly outperforms the competitors including PageRank, HITS and CoRank in ranking papers as well as researchers. The experimental results also demonstrate that venues ranked by MutualRank are reasonable.",
        "An efficient page ranking approach based on vector norms using sNorm(p) algorithm In the whole world, the internet is exercised by millions of people every day for information retrieval. Even for a small to smaller task like fixing a fan, to cook food or even to iron clothes persons opt to search the web. To fulfill the information needs of people, there are billions of web pages, each having a different degree of relevance to the topic of interest (TOI), scattered throughout the web but this huge size makes manual information retrieval impossible. The page ranking algorithm is an integral part of search engines as it arranges web pages associated with a queried TOI in order of their relevance level. It, therefore, plays an important role in regulating the search quality and user experience for information retrieval. PageRank, HITS, and SALSA are well-known page ranking algorithm based on link structure analysis of a seed set, but ranking given by them has not yet been efficient. In this paper, we propose a variant of SALSA to give sNorm(p) for the efficient ranking of web pages. Our approach relies on a p-Norm from Vector Norm family in a novel way for the ranking of web pages as Vector Norms can reduce the impact of low authority weight in hub weight calculation in an efficient way. Our study, then compares the rankings given by PageRank, HITS, SALSA, and sNorm(p) to the same pages in the same query. The effectiveness of the proposed approach over state of the art methods has been shown using performance measurement technique, Mean Reciprocal Rank (MRR), Precision, Mean Average Precision (MAP), Discounted Cumulative Gain (DCG) and Normalized DCG (NDCG). The experimentation is performed on a dataset acquired after pre-processing of the results collected from initial few pages retrieved for a query by the Google search engine. Based on the type and amount of in-hand domain expertise 30 queries are designed. The extensive evaluation and result analysis are performed using MRR, Precision@k, MAP, DCG, and NDCG as the performance measuring statistical metrics. Furthermore, results are statistically verified using a significance test. Findings show that our approach outperforms state of the art methods by attaining 0.8666 as MRR value, 0.7957 as MAP value. Thus contributing to the improvement in the ranking of web pages more efficiently as compared to its counterparts."
    ],
    "medical studies that use information retrieval ?": [
        "Practice-based Evidence in Medicine: Where Information Retrieval Meets Data Mining",
        "Multi-modal Relevance Feedback for Medical Image Retrieval Medical image retrieval can assist physicians in finding information supporting their diagnosis. Systems that allow searching for medical images need to provide tools for quick and easy navigation and query refinement as the time for information search is often short. Relevance feedback is a powerful tool in information retrieval. This study evaluates relevance feedback techniques with regard to the content they use. A novel relevance feedback technique that uses both text and visual information of the results is proposed. Results show the potential of relevance feedback techniques in medical image retrieval and the superiority of the proposed algorithm over commonly used approaches. Future steps include integrating semantics into relevance feedback techniques to benefit of the structured knowledge of ontologies and experimenting on the fusion of text and visual information.",
        "Modelling Relevance towards Multiple Inclusion Criteria when Ranking Patients. In the medical domain, information retrieval systems can be used for identifying cohorts (i.e. patients) required for clinical studies. However, a challenge faced by such search systems is to retrieve the cohorts whose medical histories cover the inclusion criteria specified in a query, which are often complex and include multiple medical conditions. For example, a query may aim to find patients with both 'lupus nephritis' and 'thrombotic thrombocytopenic purpura'. In a typical best-match retrieval setting, any patient exhibiting all of the inclusion criteria should naturally be ranked higher than a patient that only exhibits a subset, or none, of the criteria. In this work, we extend the two main existing models for ranking patients to take into account the coverage of the inclusion criteria by adapting techniques from recent research into coverage-based diversification. We propose a novel approach for modelling the coverage of the query inclusion criteria within the records of a particular patient, and thereby rank highly those patients whose medical records are likely to cover all of the specified criteria. In particular, our proposed approach estimates the relevance of a patient, based on the mixture of the probability that the patient is retrieved by a patient ranking model for a given query, and the likelihood that the patient's records cover the query criteria. The latter is measured using the relevance towards each of the criteria stated in the query, represented in the form of sub-queries. We thoroughly evaluate our proposed approach using the test collection provided by the TREC 2011 and 2012 Medical Records track. Our results show significant improvements over existing strong baselines.",
        "Medical Information Search Workshop (MEDIR) George Paliouras NCSR Demokritos Greece paliourg@iit.demokritos.gr WORKSHOP OVERVIEWMedical information search refers to methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval (IR). Such information is now potentially accessible from many sources including the general web, social media, journal articles, and hospital records. Health-related content is one of the most searched-for topics on the internet, and as such this is an important domain for research in information retrieval.Medical information is of interest to a wide variety of users, including patients and their families, researchers, general practitioners and clinicians, and clinicians with specific expertise such as radiologists. There are several dedicated services that seek to make this information more easily accessible, such as Health on the Net's medical search systems for the general public and medical practitioners 1 . Despite the popularity of the medical domain for users of search engines, and current interest in this topic within the information retrieval research community, development of search and access technologies remains particularly challenging. One of the central challenges in medical information search is diversity of the users of these services. These challenges can be summarized as follows:1. Varying information needs: Understanding various types of users and their information needs is one of the cornerstones of medical information search, while adapting information retrieval to best address these needs to develop effective, potentially personalized systems is one of its greatest challenges. 3. Varying language skills: Given that much of medical content is written in the English language, research to date in medical information search has predominantly focused on monolingual English retrieval. However, given the large number of non-English speakers on the Internet and the lack of content in their native language, effective support for them to search the English sources is highly desirable.The objective of this workshop is to provide a forum to enable the progression of research in medical information retrieval to provide enhanced search services for all users with interest in medical information search. This workshop aims to bring together researchers interested in medical information search with the goal of identifying specific research challenges that need to be addressed to advance the state-of-the-art and to foster interdisciplinary collaborations towards the meeting of these challenges. To enable this, we will encourage participation from researchers in all fields related to medical information search including mainstream information retrieval, but also natural language processing, multilingual text processing, and medical image analysis.Topics of interest include but are not limited to: Users and information needs; Semantics and NLP for medical IR; Reliability and trust in medical IR; Personalised search; Evaluation of medical IR; Multilingual questions in medical IR; Multimedia technologies in medical IR; The role of social media in medical IR.The workshop programme includes an invited presentation by Pr Bill Hersh, and short research and position papers describing approaches or challenges on the topics of the workshop. The programme aims to encourage interaction and discussion to develop a shared understanding of the research challenges facing the area of medical information retrieval. The workshop will seek to identify specific actions necessary to advance this field, and to hopefully foster further collaborations between the participants. AcknowledgementWe would like to thanks the European Science Foundation project ELIAS for supporting this workshop. 1243",
        "Why Assessing Relevance in Medical IR is Demanding This study investigates if and why assessing relevance of clinical records for a clinical retrieval task is cognitively demanding. Previous research has highlighted the challenges and issues information retrieval systems are faced with when determining the relevance of documents in this domain, e.g., the vocabulary mismatch problem. Determining if this assessment imposes cognitive load on human assessors, and why this is the case, may shed lights on what are the (cognitive) processes that assessors use for determining document relevance (in this domain). High cognitive load may impair the ability of the user to make accurate relevance judgements and hence the design of IR mechanisms may need to take this into account in order to reduce the load.",
        "Applying Information Retrieval to the Electronic Health Record for Cohort Discovery and Rare Disease Detection The last decade has seen rapid adoption of electronic health records in the United States and elsewhere. This has resulted in vast amounts of data that can be re-used for other purposes such as clinical research. However, most of this data is non-standardized and unstructured, making retrieval and other uses challenging. This talk will describe recent research applying and evaluating information retrieval techniques to two use cases: discovering cohorts for clinical research studies and detecting rare diseases. .",
        "Medical information retrieval: an instance of domain-specific search. Due to an explosion in the amount of medical information available, search techniques are gaining importance in the medical domain. This tutorial discusses recent results on search in the medical domain, including the outcome of surveys on end user requirements, research relevant to the field, and current medical and health search applications available. Finally, the extent to which available techniques meet user requirements are discussed, and open challenges in the field are identified. Categories and Subject Descriptors TUTORIAL SUMMARYSearch techniques are becoming increasingly important in the medical domain, in particular due to an explosion in the amount of medical information available. This includes both patient-specific information (e.g. electronic health records) and knowledge-based information (e.g. scientific papers). The increasing emphasis on the use of evidence-based medicine (using the best evidence from the scientific literature in clinical decision making) also increases the importance of search in medical practice . Furthermore, the secondary use of anonymized electronic health records for facilitating research and improving quality in medical practice is also gaining in importance . This use of health records also requires IR techniques, and has been modelled in the new TREC medical records track in 2011.All parties involved in medical treatment are regularly faced with an information need that cannot be met from their own store of knowledge. Medical information search is a classic example of a domain-specific search, but the medical domain is so complex that simply limiting the target documents to those in the medical domain and indexing with a standard search engine is not sufficient. The range of sources of medical information (primary research sources, secondary research sources, web pages and popular publications, etc.) and the range of end users (members of the general public, general practitioners, specialists, researchers, etc.) lead to complex requirements. Recent work in the Khresmoi project 1 has collected end user requirements for search in the medical domain through online surveys, interviews and observation . The results of the requirement analysis are presented based on a model of domain-specific search, for which the following aspects need to be specified: information sources to restrict the search, search refinements that can improve the access to information in the domain, and end users and their tasks.Although many groups of people search for information in the medical domain, the analysis in the Khresmoi project has been restricted to three of the most important groups: search by physicians, search by members of the general public and search by radiologists (a subset of physicians for which search in images is of particular importance). Different end users have different requirements based on the technical level (accessibly or technically written), level of specificity (overview or in-depth), type of research paper (primary or secondary), language of the results, etc. Naturally, the trustworthiness of medical information is of particular importance. Analyses of search behaviour based on search engine log files have also been done .For physicians, an unmet information need has been reported as occurring for 2 of every 3 patients seen , or for 41% of the questions they pursued . This requires that they attempt to meet this information need by using available resources, which has traditionally involved searching in printed sources and asking colleagues, although searching on the Internet is of increasing importance. The Khresmoi survey has shown that the three most common sources of online information used by physicians (in decreasing order of usage) are: general search engines (e.g. Google, Bing, Yahoo!), medical research databases (e.g. Pubmed) and Wikipedia. Image search is particularly important in the medical domain, especially for radiologists . Internet image search applications are starting to appear (e.g. Goldminer, Yottalook). However, image search within Picture Archiving and Communication Systems (PACS) in hospitals is also being developed .Patients also have regular information needs, illustrated by the fact that 61% of American Adults seek out health advice online . The Khresmoi survey of the general public 1 http://khresmoi.eu 1191",
        "Medical search and classification tools for recommendation EXTENDED. As an increasing number of medical professionals move their patients' records from paper to computer, enormous amounts of electronic medical records (EMR) have become available for medical research. Some of the EMR data are well-structured, for which traditional database management systems can provide effective retrieval and management functions. However, most of the EMR data (such as progress notes and consultation letters) are in free text formats. How to effectively and efficiently retrieve and discover useful information from the vast amount of such semi-structured data is a challenge faced by medical professionals. Without proper tools, the rich information and knowledge buried in the medical health records are unavailable for clinical research and decision-making.The objective of our research is to develop text analytics tools that are capable of parsing clinical medical data so that predefined search subjects that correspond to a list of medical diagnoses can be extracted. In addition to this particular core functionality, it is also desired that several important assets should be present within the text-analytics tools in order to improve its overall ability to be used as recommendation tools.In this research, we work with research scientists at the Institute for Clinical Evaluative Sciences (ICES) in Toronto and examine a number of techniques for structuring and processing free text documents in order to effectively and efficiently search and analyze vast amount of medical records. We implement several powerful medical text analytics tools for clinical data searching and classification. For data classification, our tools sort through a great amount of patient records to identify the likelihood of a patient having myocardial infarction (MI) or hypertension (HTN), and classify the patients accordingly. Our tools can also identify the likelihood of a patient being a smoker, previous smoker or non-smoker based on the text data of medical records. All the algorithms and domain knowledge implemented in these tools were provided by medical doctors and domain experts Copyright is held by the author/owner. SIGIR '10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07. working in the fields. System evaluations have been conducted and the performance results have been shown to be promising. AN INTEGRATED GUI-BASED TOOLA GUI-based interface is designed and implemented for the text-analytics tool under a point-and-click environment. Clicking the button of ICES search tool, we can get the starting to the free-text search as shown in . The free-text search tool is designed to retrieve records based on keywords such as \"MI\". The search results display the key sentences in the medical records containing the keywords. Clicking the button of ICES classification tool, we can use three classification tools summarize the records of each patient in . Three classification tools classify patients based on their likelihoods of having MI or hypertension, or on their smoking status.",
        "Concept-based Medical Document Retrieval: THCIB at CLEF eHealth Lab 2013 Task 3 We describe our participation in the task 3 of ShARe/CLEF eHealth Lab 2013: information retrieval to address questions patient may have when reading clinical reports. In our experiments, we focus mainly on two levels of analysis, namely query analysis and document analysis, to disclose the relevance between query and documents. In terms of query analysis, we first observe each medical-oriented query to find its identical or related UMLS concepts derived from the query, which may help to induce relevant results that refer to the same thing but are represented in other surface forms. In such manner, we extend the query based on the medical concepts so as to achieve a bigger coverage. In terms of document analysis, we leverage different scores (e.g., relevance score, PageRank score, HITS score and layout score) as feature to re-rank the documents of search results. With those two levels of analysis, we implement a concept-based method and a topic-based method to accomplish the task of medical document retrieval. Experiments indicate that the proposed method is effective.",
        "Overview of the ImageCLEFmed 2006 Medical Retrieval and Medical Annotation Tasks",
        "Medical Image Retrieval: ISSR at CLEF 2010 This is the second participation of Institute of Statistical Studies and Research (ISSR) group in CLEF 2010-Medical image retrieval track. This paper describes our experiments in monolingual and multilingual tasks. First, we test Paragraph Extraction (PE) and Sentence Selection (SS) approaches on the classical medical retrieval task (Ad-hoc), as well as on Case-based retrieval. Second, we compare between three Cross Language Information Retrieval (CLIR) methods. These methods are Machine Translation (MT), dictionary translation as well as translating via thesauri. For indexing and retrieval, we used the Lemur toolkit. Regarding ad-hoc retrieval task best results obtained when image caption and title used only, and for case-based task, there is no significant difference between adding extra text to the article and using its title and its image captions. For multilingual task, there is no significant difference between the three methods.",
        "Knowledge Representations and Inference Techniques for Medical Question Answering Answering medical questions related to complex medical cases, as required in modern Clinical Decision Support (CDS) systems, imposes (1) access to vast medical knowledge and (2) sophisticated inference techniques. In this article, we examine the representation and role of combining medical knowledge automatically derived from (a) clinical practice and (b) research findings for inferring answers to medical questions. Knowledge from medical practice was distilled from a vast Electronic Medical Record (EMR) system, while research knowledge was processed from biomedical articles available in PubMed Central. The knowledge automatically acquired from the EMR system took into account the clinical picture and therapy recognized from each medical record to generate a probabilistic Markov network denoted as a Clinical Picture and Therapy Graph (CPTG). Moreover, we represented the background of medical questions available from the description of each complex medical case as a medical knowledge sketch. We considered three possible representations of medical knowledge sketches that were used by four different probabilistic inference methods to pinpoint the answers from the CPTG. In addition, several answer-informed relevance models were developed to provide a ranked list of biomedical articles containing the answers. Evaluations on the TREC-CDS data show which of the medical knowledge representations and inference methods perform optimally. The experiments indicate an improvement of biomedical article ranking by 49% over state-of-the-art results.",
        "Biomedical Information Retrieval Retrieving relevant information from biomedical text data is a new challenging area of research. Thousands of articles are being added into biomedical literature each year and this large collection of publications offer an excellent opportunity for discovering hidden biomedical knowledge by applying information retrieval (IR) and Natural Language Processing (NLP) technologies. Biomedical Text processing is different from others. It requires special kind of processing as it has complex medical terminologies. Medical entity identification and normalization itself is a research problem. Relationships among medical entities have the impact on any system. The Clinical Decision Support systems are aimed to provide assistance to the decision-making tasks in biomedical domain. The medical knowledge have the potential to impact considerably on the quality of care provided by clinicians. Medical field has various types of queries: short questions, medical case reports, medical case narratives, verbose medical queries, community questioning, semi-structured queries, etc. These diverse nature of medical data demands special kind of attention from IR and NLP.",
        "Medical Image Retrieval: Applications and Resources",
        "Evaluating medical information retrieval. This paper presents a framework for evaluating information retrieval of medical records. We use the BLULab corpus, a large collection of real-world de-identified medical records. The collection has been hand coded by clinical terminologists using the ICD-9 medical classification system. The ICD codes are used to devise queries and relevance judgements for this collection. Results of initial test runs using a baseline IR system show that there is room for improvement in medical information retrieval. Queries and relevance judgements are made available at http://aehrc.com/med_eval.",
        "Understanding the retrieval effectiveness of collaborative tags and author keywords in different retrieval environments: An experimental study on medical collections",
        "Improving Text Retrieval in Medical Collections Through Automatic Categorization. A current and important research issue is the retrieval of relevant medical information. In fact, while the medical knowledge expands at a rate never observed before, its diffusion is slow. One of the main reasons is the difficulty in locating the relevant information in the modern and large medical text collections of today. In this work, we introduce a framework, based on Bayesian networks, that allows combining information derived from the text of the medical documents with information on the diseases related to these documents (obtained from an automatic categorization method). This leads to a new ranking formula which we evaluate using a medical reference collection, the OHSUMED collection. Our results indicate that this combination of evidences might yield considerable gains in retrieval performance. When the queries are strongly related to diseases, these gains might be as high as 84%. This shows that information generated by an automatic categorization procedure can be used effectively to improve the quality of the answers provided by an information retrieval (IR) system specialized in the medical domain.",
        "An adaptive evidence weighting method for medical record search. In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence in multiple medical reports (from different hospital departments or from different tests within the same department) of a patient. Furthermore, we explore several informative features for learning our retrieval model.",
        "IBI at TREC 2018: Precision Medicine Track Nowadays, the growing amount of biomedical scientific literature that can be accessed online represents a valuable source of information useful to tailor medical decisions to a specific clinical case. With this respect, Information Retrieval tools play an essential role in enabling physicians to automatically analyze huge amounts of publications so as to retrieve relevant recent information related to the treatment, prevention or prognosis of specific clinical conditions and traits. In this paper, we present and discuss the biomedical scientific Information Retrieval strategy we developed in the context of our participation to the Precision Medicine Track of the Text Retrieval Conference 2018. Given the description of a clinical case, we describe how we retrieve from PubMed a ranked list of scientific abstracts that discuss medical care that may be relevant for the patient under consideration. To this purpose we rely on the query formulation capabilities provided by Elasticsearch, a full-text search engine, complemented by data processing steps useful both to properly build search queries and to refine the ranking of search results proposed by Elasticsearch.",
        "Automated Object Extraction for Medical Image Retrieval Using the Insight Toolkit (ITK). Visual information retrieval is an emerging domain in the medical field as it has been in computer vision for more than ten years. It has the potential to help better managing the rising amount of visual medical data. One of the most frequent application fields for contentbased medical image retrieval (CBIR) is diagnostic aid. By submitting an image showing a certain pathology to a CBIR system, the medical expert can easily find similar cases. A major problem is the background surrounding the object in many medical images. System parameters of the imaging modalities are stored around the images in text as well as patient name or a logo of the institution. With such noisy input data, image retrieval often rather finds images where the object appears in the same area and is surrounded by similar structures. Whereas in specialised application domains, segmentation can focus the research on a particular area, PACS-like (Picture Archiving and Communication System) databases containing a large variety of images need a more general approach. This article describes an algorithm to extract the important object of the image to reduce the amount of data to be analysed for CBIR and focuses analysis to the important object. Most current solutions index the entire image without making a difference between object and background when using varied PACS-like databases or radiology teaching files. Our requirement is to have a fully automatic algorithm for object extraction. Medical images have the advantage to normally have one particular object more or less in the centre of the image. The database used for evaluating this task is taken from a radiology teaching file called casimage and the retrieval component is an open source retrieval engine called medGIFT.",
        "Medical Image Retrieval: ISSR at CLEF 2009 This paper represents the first participation of the Institute of Statistical Studies and Research at Cairo University group in CLEF 2009-Medical image retrieval track. Our system uses Lemur toolkit for text retrieval. The main objective is to carry out retrieving medical image depending on associated image text. We experimented with different text features such as article title, image caption and the article paragraph(s) denoting to the image. We propose a simple and effective extraction method to find relevant paragraphs based on the structure of HTML files. Automatic translation of queries in different languages other than collection language is also experimented. In this paper the results of 9 runs are presented in order to compare retrieval based on different text features and the effect of stop word lists and the use of relevance feedback.",
        "A query and patient understanding framework for medical records search. Electronic medical records (EMRs) are being increasingly used worldwide to facilitate improved healthcare services . They describe the clinical decision process relating to a patient, detailing the observed symptoms, the conducted diagnostic tests, the identified diagnoses and the prescribed treatments. However, medical records search is challenging, due to the implicit knowledge inherent within the medical records -such knowledge may be known by medical practitioners, but hidden to an information retrieval (IR) system . For instance, the mention of a treatment such as a drug may indicate to a practitioner that a particular diagnosis has been made even if this was not explicitly mentioned in the patient's EMRs. Moreover, the fact that a symptom has not been observed by a clinician may rule out some specific diagnoses.Our work focuses on searching EMRs to identify patients with medical histories relevant to the medical condition(s) stated in a query. The resulting system can be beneficial to healthcare providers, administrators, and researchers who may wish to analyse the effectiveness of a particular medical procedure to combat a specific disease . During retrieval, a healthcare provider may indicate a number of inclusion criteria to describe the type of patients of interest. For example, the used criteria may include personal profiles (e.g. age and gender) or some specific medical symptoms and tests, allowing to identify patients that have EMRs matching the criteria.To attain effective retrieval performance, we hypothesise that, in such a medical IR system, both the information needs and patients should be modelled based on how the medical process is developed. Specifically, our thesis states that since the medical decision process typically encompasses four aspects (symptom, diagnostic test, diagnosis, and treatment), a medical search system should take into account these aspects and apply inferences to recover possible implicit knowledge. We postulate that considering these aspects and their derived implicit knowledge at different levels of the retrieval process (namely, sentence, record, and inter-record level) enhances the retrieval performance. Indeed, we propose to build a query and patient understanding framework that can gain insights from EMRs and queries, by modelling and reasoning during retrieval in terms of Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR'13, July 28-August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07. the four aforementioned aspects (symptom, diagnostic test, diagnosis, and treatment) at three different levels of the retrieval process. Firstly, at the sentence level, a medical negation detection tool is used to identify the context (negative/positive) of terms, which leads to an accurate representation of the medical conditions both in the EMRs and the queries. Handling negated language is challenging in medical records search, since it is commonly used by medical practitioners to indicate that a patient does not possess a particular medical condition . In particular, we improve the representation of both EMRs and queries by representing terms along with their context, in order to prevent EMRs containing terms with the opposite context to the query's intent from being retrieved, since they are unlikely to be relevant. For example, preventing the EMRs stating \"no history of dementia\" from being retrieved for a query searching for patients with \"dementia\". Secondly, at the record level, the semantic type and relationships of medical terms extracted from knowledge-based resources (e.g. ontologies, websites) are leveraged to infer the wider medical history of the patient in terms of the four above aspects. Indeed, we propose to represent EMRs and queries within the four medical aspects, and infer the relevance of an EMR by exploiting association rules extracted from the semantic relationships of medical terms within these aspects. For example, patients with a medical history of having CABG surgery (treatment) can be inferred as relevant to a query searching for a patient suffering from heart disease (diagnosis), since a CABG surgery is a treatment of heart disease.Thirdly, at the inter-record level, we exploit knowledge about how the four medical aspects are handled by different hospital departments to gain further understanding about the appropriateness of EMRs from different departments for a given query. Specifically, we propose to aggregate EMRs at the department level (i.e. inter-record level) to extract implicit medical knowledge (i.e. expertise of each department) and model this department's expertise, while ranking EMRs. For instance, patients having EMRs from the cardiology department are likely to be relevant to a query such as \"find patients suffering from heart attack\".We evaluate our work using standard test collections provided by the TREC Medical Records track , which represent the scenario of practitioners searching for patients based on the relevance of their EMRs.",
        "Methods for Combining Content-Based and Textual-Based Approaches in Medical Image Retrieval",
        "IRIT at ImageCLEF 2010: Medical Retrieval Track We reported some experiments conducted by our members in the SIG team at the IRIT laboratory in the CLEF medical retrieval task, namely ImageCLEFmed. In 2010, we are particularly interested in the case-based retrieval task. Our information retrieval approach integrates a hybrid method of concept extraction for enhancing the semantics of the document as well as of the query. More precisely, we applied a knowledge-based concept extraction method combined with statistical information obtained by scoring identified terms denoting concepts both in the document and query. The experiments carried out on the Image-CLEF 2010 show that our information retrieval approach based on the proposed method of concept extraction show an improvement of 7.07% in terms of MAP (for the best configuration trained on ImageCLEF 2010) over the baseline.",
        "Using Exploration and Learning for Medical Records Search: An Experiment in Identifying Cohorts for Comparative Effectiveness Research This paper describes an experiment performed on a medical record data set, using an information retrieval (IR) tool that applies the techniques of exploration and learning, to assist a researcher in identifying the most relevant cohorts. The paper presents some brief background on exploration and learning, how they are incorporated in the IR tool, and an instantiation of exploration and learning used for selecting cohorts for a research population. The research problem addressed in this paper is the TREC 2012 Medical Track task: How to provide contentbased access to free-text fields of electronic medical records? The stated goal of the task is to \"find a population over which comparative effectiveness studies can be done.\"",
        "Studies of the onset and persistence of medical concerns in search logs. The Web provides a wealth of information about medical symptoms and disorders. Although this content is often valuable to consumers, studies have found that interaction with Web content may heighten anxiety and stimulate healthcare utilization. We present a longitudinal log-based study of medical search and browsing behavior on the Web. We characterize how users focus on particular medical concerns and how concerns persist and influence future behavior, including changes in focus of attention in searching and browsing for health information. We build and evaluate models that predict transitions from searches on symptoms to searches on health conditions, and escalations from symptoms to serious illnesses. We study the influence that the prior onset of concerns may have on future behavior, including sudden shifts back to searching on the concern amidst other searches. Our findings have implications for refining Web search and retrieval to support people pursuing diagnostic information.",
        "IRIT at ImageCLEF 2011: Medical Retrieval Task In this paper, we reported some experiments conducted by our members in the SIG team at the IRIT laboratory in the University of Toulouse within the context of the medical information retrieval (IR) task. As in our previous participation in ImageCLEF, in 2011, our research focuses on the case-based retrieval task. We compared the performance of different state-of-the-art term weighting models for retrieving patient cases that might best suit the clinical information need. Furthermore, we also combined term scores obtained by two state-of-the-art weighting models using a particular data fusion technique. Finally, a state-of-the-art query expansion (QE) technique is used for improving biomedical IR performance.",
        "Workshop on health search and discovery: helping users and advancing medicine. This workshop brings together researchers and practitioners from industry and academia to discuss search and discovery in the medical domain. The event focuses on ways to make medical and health information more accessible to laypeople (including enhancements to ranking algorithms and search interfaces), and how we can discover new medical facts and phenomena from information sought online, as evidenced in query streams and other sources such as social media. This domain also offers many opportunities for applications that monitor and improve quality of life of those affected by medical conditions, by providing tools to support their healthrelated information behavior. Categories and Subject Descriptors KeywordsHealth seeking; Medical discovery; Data mining; Social media. BACKGROUND AND MOTIVATIONHealth-related information is one of the most widely searched domains on the Web. A recent survey by the Pew Internet and American Life Project found that 59% of U.S. adults have looked online for health information in the past year, with 35% of respondents attempting to diagnose a medical condition online . Health information seeking is conducted using both search systems such as Internet search engines and via social media (e.g., advice from friends). Research has indicated that cyberspace can be a dangerous place for those with no medical training. Web search engines can surface alarming content that can cause unwarranted anxiety .Such challenges necessitate new information retrieval (IR) methods-both backend and frontend-to significantly improve the search experience for health information seeking. These methods can, for example, translate layman and technical terminology, and consider domain knowledge and interpretability and health-related reliability or authority during ranking or crawling . This domain also presents unprecedented opportunities for the development of applications that monitor and improve the quality of life of people affected by a variety of medical conditions via tools to support their health-oriented information behavior. Mining aspects of that behavior (e.g., queries and social media interactions) with consent in the aggregate across many users has potential to assist in medical discoveries and enhance public health monitoring .The monitoring and use of health-related online behavior also brings into sharper focus important tradeoffs between privacy and benefit, which need to be explored in more detail. We envisage that the expertise of the SIGIR community will provide helpful insights on matters of privacy, as well as retrieval, personalization, expertise modeling, search interface design, data mining, and others; all of which are critical to enabling advances this area. Through presentations and discussions at the workshop we hope to outline how the IR community can play a central role the future of health search and discovery, and bring benefit to health seekers everywhere. WORKSHOP OVERVIEWThe event comprises discussions (including brainstorming and breakouts), keynotes, and other presentations on papers submitted in response to an open call. The themes of the workshop include:\u2022 Predictive analysis from large-scale data such as behavioral logs, microblogging, and other social media; \u2022 Mining large-scale content (e.g., logs, Web crawls) for insights and links between conditions, drugs, and cohorts; \u2022 Establishing the reliability of online health content;\u2022 Diagnostic search by patients and health practitioners and its cognitive impact; \u2022 Modeling the effect of domain expertise on health seeking; \u2022 Query formulation for medical retrieval (including query expansion, UMLS, concept-hierarchies, etc.); \u2022 Visualization and exploration of medical information; \u2022 Privacy issues on the storage and retrieval of medical content;\u2022 Personalization of short-and long-term health interests;\u2022 Platforms for storing and maintaining health and fitness information (e.g., HealthVault), and; \u2022 Social networks for sharing and monitoring health-related content (e.g., PatientsLikeMe).Discussions at the event will target many of these topics. One area of particular interest is challenges in improving health search for laypeople. Websites such as WebMD.com, eHealthme.com, and PatientsLikeMe.com provide medical information or peer support to health seekers. We will discuss improvements to health search via, for example, synthesizing health content from many sources, and considering base rates and source reliability in result ranking.",
        "Overview of the ImageCLEFmed 2007 Medical Retrieval and Medical Annotation Tasks"
    ],
    "information retrieval on different language sources ?": [
        "An Investigation of Cross-Language Information Retrieval for User-Generated Internet Video. Increasing amounts of user-generated video content are being uploaded to online repositories. This content is often very uneven in quality and topical coverage in different languages. The lack of material in individual languages means that cross-language information retrieval (CLIR) within these collections is required to satisfy the user's information need. Search over this content is dependent on available metadata, which includes user-generated annotations and often noisy transcripts of spoken audio. The effectiveness of CLIR depends on translation quality between query and content languages. We investigate CLIR effectiveness for the blip10000 archive of user-generated Internet video content. We examine the retrieval effectiveness using the title and free-text metadata provided by the uploader and automatic speech recognition (ASR) generated transcripts. Retrieval is carried out using the Divergence From Randomness models, and automatic translation using Google translate. Our experimental investigation indicates that different sources of evidence have different retrieval effectiveness and in particular differing levels of performance in CLIR. Specifically, we find that the retrieval effectiveness of the ASR source is significantly degraded in CLIR. Our investigation also indicates that for this task the Title source provides the most robust source of evidence for CLIR, and performs best when used in combination with other sources of evidence. We suggest areas for investigation to give most effective and robust CLIR performance for user-generated content.",
        "Translating cross-lingual spelling variants using transformation rules. Technical terms and proper names constitute a major problem in dictionary-based cross-language information retrieval (CLIR). However, technical terms and proper names in different languages often share the same Latin or Greek origin, being thus spelling variants of each other. In this paper we present a novel two-step fuzzy translation technique for cross-lingual spelling variants. In the first step, transformation rules are applied to source words to render them more similar to their target language equivalents. The rules are generated automatically using translation dictionaries as source data. In the second step, the intermediate forms obtained in the first step are translated into a target language using fuzzy matching. The effectiveness of the technique was evaluated empirically using five source languages and English as a target language. The two-step technique performed better, in some cases considerably better, than fuzzy matching alone. Even using the first step as such showed promising results.",
        "Report on CLEF-2005 Evaluation Campaign: Monolingual, Bilingual and GIRT Information Retrieval For our fifth participation in the CLEF evaluation campaigns, the first objective was to propose an effective and general stopword list along with a light stemming procedure for the Hungarian, Bulgarian and Portuguese (Brazilian) languages. Our second objective was to obtain a better picture of the relative merit of various search engines when processing documents in those languages. To do so we evaluated our scheme using two probabilistic models and nine vectorprocessing approaches. In the bilingual track, we evaluated both the machine translation and bilingual dictionary approaches to automatically translate a query submitted in English into various target languages. This year we explored new freely available translation sources, together with a combined query translation approach in order to obtain a better translation of the user's information need. Finally, using the GIRT corpora (available in English, German and Russian), we investigated variations in retrieval effectiveness when including or excluding manually assigned keywords attached to bibliographic records (mainly comprising a title and an abstract).",
        "Cross-Language Information Retrieval (CLIR) Track Overview A cross-language retrieval track was offered for the third time at TREC-8. The main task was the same as that of the previous year: the goal was for groups to use queries written in a single language in order to retrieve documents from a multilingual pool of documents written in many different languages. Compared to the usual definition of cross-language information retrieval, where systems work with a single language pair, retrieving documents in a language L1 using queries in language L2, this is a slightly more comprehensive task, and we feel one that more closely meets the demands of real world applications. The document languages used were the same as for TREC-7: English, German, French and Italian. The queries were available in all of these languages. Monolingual non-English retrieval was offered to new participants who preferred to begin with an easier task. However, all the groups which did not tackle the full task opted for limited cross-language rather than monolingual runs. These experiments were evaluated by NIST and are published as unofficial (\"alternate\") runs. We also offered a subtask, working with documents from the field of social sciences. This collection (known as \"GIRT\") has some very interesting features, such as controlled vocabulary terms, title translations, and an associated multilingual thesaurus. The track was coordinated at Eurospider Information Technology AG in Zurich. Due to its multilingual nature, the topic creation and relevance assessment tasks were distributed over four sites in different countries: NIST (English), IZ Bonn (German), IEI-CNR (Italian) and University of Zurich (French). The University of Hildesheim invested considerable effort into rendering the topics homogeneous and consistent over languages. The participating groups experimented with a wide variety of strategies, ranging machine translation, corpus-, and dictionary-based approaches. Some results are given in Section 4. There were, however, also some striking similarities between many of the runs, such as the choice of English as topic language the majority, and the use of Systran by a lot of groups. Some implications of these findings are discussed in Section 5. The main goal of the TREC CLIR activities has been the creation of a multilingual test collection that is re-usable for a wide range of evaluation experiments. This means that the quality of the relevance assessments is very important. The Twenty-One group conducted an interesting analysis with respect to the completeness of the assessments and the impact of this on the pool. We address some of their findings in Section 5. The paper concludes with an indication of our plans for the future of the cross-language track, which will bring substantial changes to the format and coordination of the activities.",
        "Concept unification of terms in different languages via web mining for Information Retrieval a b s t r a c tFor historical and cultural reasons, English phases, especially proper nouns and new words, frequently appear in Web pages written primarily in East Asian languages such as Chinese, Korean, and Japanese. Although such English terms and their equivalences in these East Asian languages refer to the same concept, they are often erroneously treated as independent index units in traditional Information Retrieval (IR). This paper describes the degree to which the problem arises in IR and proposes a novel technique to solve it. Our method first extracts English terms from native Web documents in an East Asian language, and then unifies the extracted terms and their equivalences in the native language as one index unit. For Cross-Language Information Retrieval (CLIR), one of the major hindrances to achieving retrieval performance at the level of Mono-Lingual Information Retrieval (MLIR) is the translation of terms in search queries which can not be found in a bilingual dictionary. The Web mining approach proposed in this paper for concept unification of terms in different languages can also be applied to solve this well-known challenge in CLIR. Experimental results based on NTCIR and KT-Set test collections show that the high translation precision of our approach greatly improves performance of both Mono-Lingual and Cross-Language Information Retrieval.",
        "Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings",
        "A Comparative Study on Different Translation Approaches for Query Formation in the Source Retrieval Task. The text reuse detection among documents in comparable corpora has become an important research topic due to its usages ranging from document linking to plagiarism detection. A text reuse detection system typically computes similarity between source document and the possibly reused document. Considering the real-world scenario where exhaustive comparison is not possible, the system must first retrieve a subset of documents that serves as the source of similarity computation. But the task becomes more challenging when the language of reused document differs from the source document. In this paper we present a comparative study of different translation approaches used to map the language barrier for cross-lingual retrieval of possible sources of text reuse. We perform our experiments on CL!NSS 2013 dataset. It is observed that machine translation based query formation approach retrieves sources with highest recall. Our system also outperforms the existing systems in the literature.",
        "Medical Information Search Workshop (MEDIR) George Paliouras NCSR Demokritos Greece paliourg@iit.demokritos.gr WORKSHOP OVERVIEWMedical information search refers to methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval (IR). Such information is now potentially accessible from many sources including the general web, social media, journal articles, and hospital records. Health-related content is one of the most searched-for topics on the internet, and as such this is an important domain for research in information retrieval.Medical information is of interest to a wide variety of users, including patients and their families, researchers, general practitioners and clinicians, and clinicians with specific expertise such as radiologists. There are several dedicated services that seek to make this information more easily accessible, such as Health on the Net's medical search systems for the general public and medical practitioners 1 . Despite the popularity of the medical domain for users of search engines, and current interest in this topic within the information retrieval research community, development of search and access technologies remains particularly challenging. One of the central challenges in medical information search is diversity of the users of these services. These challenges can be summarized as follows:1. Varying information needs: Understanding various types of users and their information needs is one of the cornerstones of medical information search, while adapting information retrieval to best address these needs to develop effective, potentially personalized systems is one of its greatest challenges. 3. Varying language skills: Given that much of medical content is written in the English language, research to date in medical information search has predominantly focused on monolingual English retrieval. However, given the large number of non-English speakers on the Internet and the lack of content in their native language, effective support for them to search the English sources is highly desirable.The objective of this workshop is to provide a forum to enable the progression of research in medical information retrieval to provide enhanced search services for all users with interest in medical information search. This workshop aims to bring together researchers interested in medical information search with the goal of identifying specific research challenges that need to be addressed to advance the state-of-the-art and to foster interdisciplinary collaborations towards the meeting of these challenges. To enable this, we will encourage participation from researchers in all fields related to medical information search including mainstream information retrieval, but also natural language processing, multilingual text processing, and medical image analysis.Topics of interest include but are not limited to: Users and information needs; Semantics and NLP for medical IR; Reliability and trust in medical IR; Personalised search; Evaluation of medical IR; Multilingual questions in medical IR; Multimedia technologies in medical IR; The role of social media in medical IR.The workshop programme includes an invited presentation by Pr Bill Hersh, and short research and position papers describing approaches or challenges on the topics of the workshop. The programme aims to encourage interaction and discussion to develop a shared understanding of the research challenges facing the area of medical information retrieval. The workshop will seek to identify specific actions necessary to advance this field, and to hopefully foster further collaborations between the participants. AcknowledgementWe would like to thanks the European Science Foundation project ELIAS for supporting this workshop. 1243",
        "Addressing the lack of direct translation resources for cross-language retrieval. Most cross language information retrieval research concentrates on language pairs for which direct, rich, and often multiple translation resources already exist. However, for most language pairs, translation via an intermediate language is necessary. Two distinct methods for dealing with the additional ambiguity introduced by the extra translation step have been proposed and individually, shown to improve retrieval effectiveness. Two previous works indicated that in combination, the methods were ineffective. This paper provides strong empirical evidence that the methods can be combined to produce consistent and often significant improvements in retrieval effectiveness. The improvement is shown across a number of different intermediate languages and test collections.",
        "ITC-irst at CLEF 2003: Monolingual, Bilingual and Multilingual Information Retrieval This paper reports on the participation of ITC-irst in the Cross Language Evaluation Forum 2003; in particular, in the monolingual, bilingual, small multilingual, and spoken document retrieval tracks. Considered languages were English, French, German, Italian, and Spanish. With respect to our CLEF 2002 system, the statistical models for bilingual document retrieval have been improved, more languages have been considered, and a novel multilingual information retrieval system has been developed, which combines several bilingual retrieval models into a statistical framework. As in the last CLEF, bilingual models integrate retrieval and translation scores over the set of N-best translations of the source query.",
        "Study of cross lingual information retrieval using on-line translation systems. Typical cross language retrieval requires special linguistic resources, such as bilingual dictionaries and parallel corpus. In this study, we focus on the cross lingual retrieval problem that only uses online translation systems. We compare two approaches: a translation-based approach that directly translates queries into the language of documents and then applies traditional information retrieval techniques; and a model-based approach that first learns a statistical translation model from the translations acquired from an online translation system and then applies the learned statistical model to cross lingual information retrieval. Our empirical study with ImageCLEF has shown the model-based approach performs significantly better than the translation-based approach.",
        "Cross-lingual Information Retrieval based on Multiple Indexes In this paper we present the technical details of the retrieval system with which we participated at the CLEF09 Ad-hoc TEL task. We present a retrieval approach based on multiple indexes for different languages which is combined with a conceptbased retrieval approach based on Explicit Semantic Analysis. In order to create the language-specific indices for each language, a language detection approach is applied as preprocessing step. We combine the different indices through rank aggregation and present our experimental results with different rank aggregation strategies. Our results show that the use of multiple indices (one for each language) does not improve upon a baseline index containing documents in all languages. The combination with concept based retrieval, however, results in better retrieval performance in some of the cases considered. For the bilingual tasks the final retrieval results of our system were the 5th best results on the BL dataset and the second best on the BNF dataset.",
        "Cross-Language Information Retrieval and Evaluation, Workshop of Cross-Language Evaluation Forum, CLEF 2000, Lisbon, Portugal, September 21-22, 2000, Revised Papers This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are liable for prosecution under the German Copyright Law. PrefaceThe first evaluation campaign of the Cross-Language Evaluation Forum (CLEF) for European languages was held from January to September 2000. The campaign culminated in a two-day workshop in Lisbon, Portugal, 21-22 September, immediately following the fourth European Conference on Digital Libraries (ECDL 2000). The first day of the workshop was open to anyone interested in the area of Cross-Language Information Retrieval (CLIR) and addressed the topic of CLIR system evaluation. The goal was to identify the actual contribution of evaluation to system development and to determine what could be done in the future to stimulate progress. The second day was restricted to participants in the CLEF 2000 evaluation campaign and to their experiments. This volume constitutes the proceedings of the workshop and provides a record of the campaign. CLEF is currently an activity of the DELOS Network of Excellence for Digital Libraries, funded by the EC Information Society Technologies to further research in digital library technologies. The activity is organized in collaboration with the US National Institute of Standards and Technology (NIST). The support of DELOS and NIST in the running of the evaluation campaign is gratefully acknowledged.I should also like to thank the other members of the Workshop Steering Committee for their assistance in the organization of this event. April 2001Carol Peters The objective of the Cross-Language Evaluation Forum (CLEF) is to develop and maintain an infrastructure for the testing and evaluation of information retrieval systems operating on European languages, in both monolingual and cross-language contexts, and to create test-suites of reusable data that can be employed by system developers for benchmarking purposes. The first CLEF evaluation campaign started in early 2000 and ended with a workshop in Lisbon, Portugal, 22-23 September 2000. This volume constitutes the proceedings of the workshop and also provides a record of the results of the campaign. It consists of two parts and an appendix. The first part reflects the presentations and discussions on the topic of evaluation for crosslanguage information retrieval systems during the first day of the workshop, whereas the second contains papers from the individual participating groups reporting their experiments and analysing their results. The appendix presents the evaluation techniques and measures used to derive the results and provides the run statistics. The aim of this Introduction is to present the main issues discussed at the workshop and also to provide the reader with the necessary background to the experiments through a description of the tasks set for CLEF 2000. In conclusion, our plans for future CLEF campaigns are outlined. CLEF 2000 Workshop Steering Committee Evaluation for CLIR SystemsThe first two papers in Part I of the proceedings describe the organization of crosslanguage evaluation campaigns for text retrieval systems. CLEF is a continuation and expansion of the cross-language system evaluation activity for European languages begun in 1997 with the track for Cross-Language Information Retrieval (CLIR) in the Text REtrieval Conference (TREC) series. The paper by Harman et al. gives details on how the activity was organized, the various issues that had to be addressed, and the results obtained. The difficulties experienced during the first year, in which the track was coordinated centrally at NIST (US National Institute for Standards and Technology) led to the setting up of a distributed coordination in four countries (USA, Germany, Italy and Switzerland) with native speakers being responsible for the preparation of topics (structured statements of possible information needs) and relevance judgments (assessment of the relevance of the ranked lists of results submitted by participating systems). A natural consequence of this distributed coordination was the 2 Carol Peters decision, in 1999, to transfer the activity to Europe and set it up independently as CLEF. The infrastructure and methodology adopted in CLEF is based on the experience of the CLIR tracks at TREC.The second paper by Kando presents the NTCIR Workshops, a series of evaluation workshops for text retrieval systems operating on Asian languages. The 2000-2001 campaign conducted by NTCIR included cross-language system evaluation for Japanese-English and Chinese-English. Although both CLEF and NTCIR have a common basis in TREC there are interesting differences between the methodology adopted by the two campaigns. In particular, NTCIR employs multigrade relevance judgments rather than the binary system used by CLEF and inherited from TREC. Kando motivates this decision and discusses the effects.The CLEF campaign provides participants with the possibility to test their systems on both general-purpose texts (newspapers and newswires) and domain-specific collections. The third paper by Kluck and Gey examines the domain-specific task, begun in TREC and continued in CLEF, and describes the particular document collection used: the GIRT database for social sciences.The rest of the papers in the first part of this volume focus on some of the main issues that were discussed during the first day of the workshop. These included the problem of resources, the transition from the evaluation of cross-language text retrieval systems to systems running on other media, the need to consider the user perspective rather than concentrating attention solely on system performance, and the importance of being able to evaluate single system components rather than focusing on overall performance. A further point for discussion was the addition of new languages to the multilingual document collection.The problem of resources has always been seen as crucial in cross-language system development. In order to be able to match queries against documents, some kind of lexical resource is needed to provide the transfer mechanism, e.g. bilingual or multilingual dictionaries, thesauri, or corpora. In order to be able to process a number of different languages, suitable language processing tools are needed, e.g. languagespecific tokenizers, stemmers, morphologies, etc.. It is generally held that the quality of the resource used considerably affects system performance. This question was discussed at length during the workshop. The paper by Gonzalo presents a survey on the different language resources used by the CLEF 2000 participants. Many of the resources listed were developed by the participants themselves, thus showing that an evaluation exercise of this type is not only evaluating systems but also the resources used by the systems. The need for more pooling and sharing of resources between groups in order to optimize effort emerges clearly from this survey. Gonzalo concludes with some interesting proposals for the introduction of additional tasks, aimed at measuring the effect of the resources used on overall system performance, in a future campaign.The papers by Oard and by Jones both discuss CLIR from the user perspective. Oard focuses on the document selection question: how the users of a CLIR system can correctly identify the -for them -most useful documents from a ranked list of results when they cannot read the language of the target collection. He advocates the advantages of an interactive CLIR evaluation and makes a proposal as to how an evaluation of this type could be included in CLEF. Jones also supports the extension of evaluation exercises in order to assess the usefulness of techniques that can assist the user with Introduction 3 relevance judgment and information extraction. In this respect, he mentions the importance of document summarization -already included in the NTCIR evaluation programme. In addition, Jones talks about work in cross-language multimedia information retrieval and suggests directions for future research. He asserts that specifically-developed standard test collections are needed to advance research in this area.In the final paper in Part I, Gey lists several areas in which research could lead to improvement in cross-language information retrieval including resource enrichment, the use of pivot languages and phonetic transliteration. In particular, he discusses the need for post-evaluation failure analysis and shows how this could provide important feedback resulting in improved system design and performance. CLEF provides the research community with the necessary infrastructure for studies of this type. The CLEF 2000 ExperimentsThere were several reasons behind the decision to coordinate the cross-language system evaluation activity for European languages independently and to move it to Europe. One was the desire to extend the number of languages covered, another was the intention to offer a wider range of retrieval tasks to better meet the needs of the multilingual information retrieval research community.As can be seen from the descriptions of the experiments in Part II of this volume, CLEF 2000 included four separate evaluation tracks:\u2022 multilingual information retrieval \u2022 bilingual information retrieval \u2022 monolingual (non-English) information retrieval \u2022 cross-language domain-specific information retrieval The main task -inherited from TREC -required searching a multilingual document collection, consisting of national newspapers in four languages (English, French, German and Italian) of the same time period, in order to retrieve relevant documents. Forty topics were developed on the basis of the contents of the multilingual collection -ten topics for each collection -and complete topic sets were produced in all four languages. Topics are structured statements of hypothetical user needs. Each topic consisted of three fields: a brief title statement; a one-sentence description; a more complex narrative specifying the relevance assessment criteria. Queries are constructed using one of more of these fields. Additional topic sets were then created for Dutch, Finnish, Spanish and Swedish, in each case translating from the original. The main requirement was that, for each language, the topic set should be as linguistically representative as possible, i.e. using the terms that would naturally be expected to represent the set of topic concepts in the given language. The methodology followed was that described in the paper by Harman et al..A bilingual system evaluation task was also offered, consisting of querying the English newspaper collection using any topic language (except English). Many newcomers to cross-language system evaluation prefer to begin with the simpler bilingual task before moving on to tackle the additional issues involved in truly multilingual retrieval. Carol PetersOne of the aims of the CLEF activity is to encourage the development of tools to manipulate and process languages other than English. Different languages present different problems. Methods that may be efficient for certain language typologies may not be so effective for others. Issues that have to be catered for include word order, morphology, diacritic characters, language variants. For this reason, CLEF 2000 included a track for French, German and Italian monolingual information retrieval.The cross-language domain-specific task has been offered since TREC-7. The rationale of this subtask is to test retrieval on another type of document collection, serving a different kind of information need. The implications are discussed in the paper by Kluck and Gey in the first part of this volume.The papers in Part II describe the various experiments by the participating groups with these four tasks. Both traditional and innovative approaches to CLIR were experimented, and different query expansion techniques were tried. All kinds of source to target transfer mechanisms were employed, including both query and document translation. Commercial and in-house resources were used and included machine translation, dictionary and corpus-based methods. The strategies used varied from traditional IR to a considerable employment of natural language processing techniques. Different groups focused on different aspects of the overall problem, ranging from the development of language-independent tools such as stemmers to much work on language-specific features like morphology and compounding. Many groups compared different techniques in different runs in order to evaluate the effect of a given technique on performance. Overall, CLEF 2000 offered a very good picture of current issues and approaches in CLIR.The first paper in this part by Martin Braschler provides an overview and analysis of all the results, listing the most relevant achievements and comparing them with those of previous years in the CLIR track at TREC. As one of the main objectives of CLEF is to produce evaluation test-suites that can be used by the CLIR research community, Braschler also provides an analysis of the test collection resulting from the CLEF 2000 campaign, demonstrating its validity for future system testing, tuning and development activities. The appendix presents the evaluation results for each group, run by run. CLEF in the FutureThe CLEF 2001 campaign is now under way. The main tasks are similar to those of the first campaign. There are, however, some extensions and additions. In particular the multilingual corpus has been considerably enlarged and Spanish (news agency) and Dutch (national newspaper) collections for 1994 have been added. The multilingual task in CLEF 2001 involves querying collections in five languages (English, French, German, Italian and Spanish) and there will be two bilingual tracks: searching either the English or the Dutch collections. Spanish and Dutch have also been included in the monolingual track. There will be seven official topic languages, including Japanese. Additional topics will be provided in a number of other European languages, including Finnish, Swedish and Russian, and also in Chinese and Thai.Introduction 5 CLEF 2000 concentrated on the traditional metrics of recall and precisionhowever these have limitations in what they tell us about the usefulness of a retrieval system to the user. CLEF 2001 will thus also include an experimental track designed to test interactive CLIR systems and to establish baselines against which future research progress can be measured. The introduction of this track is a direct result of discussions which began in the workshop with the presentations by Oard and by Jones, and of the proposal by Oard reported in Part I of this volume.Two main issues must be considered when planning future CLEF campaigns: the addition of more languages, and the inclusion of new tasks.The extension of language coverage, discussed considerably at the workshop, depends on two factors: the demand from potential participants and the existence of sufficient resources to handle the requirements of new language collections. It was decided that Spanish and Dutch met these criteria for CLEF . CLEF 2002 and 2003 will be mainly funded by a contract from the European Commission (IST-2000-31002) but it is probable that, in the future, it will be necessary to seek support from national funding agencies as well if more languages are to be included. The aim will be to cover not only the major European languages but also some representative samples of minority languages, including members from each major group: e.g. Germanic, Romance, Slavic, and Ugro-Finnic languages. Furthermore, building on the experience of CLEF 2001, we intend to continue to provide topics in Asian languages.CLEF 2000 concentrated on cross-language text retrieval and on measuring overall system performance. However, in the future, we hope to include tracks to evaluate CLIR systems working on media other than text. We are now beginning to examine the feasibility of organizing a spoken CLIR track in which systems would have to process and match spoken queries in more than one language against a spoken document collection. Another important innovation would be to devise methods that enable the assessment of single system components, as suggested in the paper by Gonzalo.CLIR system development is still very much in the experimental stage and involves expertise from both the natural language processing and the information retrieval fields. The CLEF 2000 Workshop provided an ideal opportunity for a number of key players, with very different backgrounds, to come together and exchange ideas and compare results on the basis of a common experience: participation in the CLEF evaluation campaign. CLEF is very much a collaborative effort between organizers and participants with the same common goal: the improvement of CLIR system performance. The discussions at the workshop have had considerable impact on the organization of the 2001 campaign. The success of future campaigns will depend on the continuation and strengthening of this collaboration.More information on the organization of the current CLEF campaign and instructions on how to contact us can be found at: http://www.clef-campaign.org/. AcknowledgementsTo a large extent, CLEF depends on voluntary work. I should like to acknowledge the generous collaboration of a number of people and organizations. First of all, I wish to It is not easy to set up an infrastructure that meets the needs of a large number of languages. I should like to thank the following organisations who voluntarily engaged translators to provide topic sets in Dutch, Finnish and Swedish, working on the basis of the set of source topics:\u2022 the DRUID project for the Dutch topics;\u2022 the Department of Information Studies, University of Tampere, Finland, engaged the UTA Language Centre for the Finnish topics; \u2022 SICS Human Computer Interaction and Language Engineering Laboratory for the Swedish topics. The support of all the data providers and copyright holders is also gratefully acknowledged, and in particular:\u2022 The Los Angeles Times, for the English data collection;\u2022 Le Monde S.A. and ELDA: European Language Resources Distribution Agency, for the French data.\u2022 Frankfurter Rundschau, Druck und Verlagshaus Frankfurt am Main; Der Spiegel, Spiegel Verlag, Hamburg, for the German newspaper collections.\u2022 InformationsZentrum Sozialwissenschaften, Bonn, for the GIRT database.\u2022 Hypersystems Srl, Torino and La Stampa, for the Italian data.\u2022 Schweizerische Depeschenagentur (SDA) and Associated Press (AP) for the newswire data of the training collection. Without their help, this evaluation activity would be impossible.Last, but not least, I thank Julio Gonzalo for his help and encouragement in the preparation of this volume. CLIR Evaluation at TREC paraic@textwise.com. . Starting in 1997, the National Institute of Standards and Technology conducted 3 years of evaluation of cross-language information retrieval systems in the Text REtrieval Conference (TREC). Twentytwo participating systems used topics (test questions) in one language to retrieve documents written in English, French, German, and Italian. A large-scale multilingual test collection has been built and a new technique for building such a collection in a distributed manner was devised.",
        "Cross-Language Information Retrieval using Dutch Query Translation This paper describes a hastily carried out, rather limited bilingual information retrieval experiment. The experiment takes Dutch topics to retrieve relevant English documents using Microsoft SQL Server version 7.0. In order to cross the language barrier between query and document, the researchers use query translation by means of a machine-readable dictionary. The Dutch run was void of the typical natural language processing techniques such as parsing, stemming, or part of speech tagging. A monolingual run was carried out for comparison purposes. Due to limitations in time, retrieval system, translation method, and test collection, there is only a preliminary analysis of the results.",
        "Query Reformulation Patterns of Mixed Language Queries in Different Search Intents. With the increasing number of multilingual resources on the Internet, cross-language information retrieval has become an important research topic. In cultures where people speak both Chinese and English, using mixed language in oral speaking and web searching is a common phenomenon. While queries are the key element of information retrieval process, mixed-language queries have not yet been adequately studied. This study use query log analysis to examine the query reformulation patterns regarding Chinese-English mixed language queries, and how search intents may affect the query reformulation types users employ. The results can inform IR system designers to enhance cross-language controlled vocabularies and develop discovery platforms for multilingual content, and improve search engines to provide users with more relevant, personalized search results. The findings could also be expanded to other language combinations that would improve search engine designs.",
        "Multilingual Information Retrieval Using English and Chinese Queries",
        "Information Retrieval with Hindi, Bengali, and Marathi Languages: Evaluation and Analysis. Our first objective in participating in FIRE evaluation campaigns is to analyze the retrieval effectiveness of various indexing and search strategies when dealing with corpora written in Hindi, Bengali and Marathi languages. As a second goal, we have developed new and more aggressive stemming strategies for both Marathi and Hindi languages during this second campaign. We have compared their retrieval effectiveness with both light stemming strategy and n-gram language-independent approach. As another languageindependent indexing strategy, we have evaluated the trunc-n method in which the indexing term is formed by considering only the first n letters of each word. To evaluate these solutions we have used various IR models including models derived from Divergence from Randomness (DFR), Language Model (LM) as well as Okapi, or the classical tf idf vector-processing approach.For the three studied languages, our experiments tend to show that IR models derived from Divergence from Randomness (DFR) paradigm tend to produce the best overall results. For these languages, our various experiments demonstrate also that either an aggressive stemming procedure or the trunc-n indexing approach produces better retrieval effectiveness when compared to other word-based or n-gram language-independent approaches. Applying the Z-score as data fusion operator after a blind-query expansion tends also to improve the MAP of the merged run over the best single IR system.",
        "Creating and exploiting a comparable corpus in cross-language information retrieval We present a method for creating a comparable text corpus from two document collections in different languages. The collections can be very different in origin. In this study, we build a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. The keys with best resolution power were extracted from the documents of one collection, the source collection, by using the relative average term frequency (RATF) value. The keys were translated into the language of the other collection, the target collection, with a dictionary-based query translation program. The translated queries were run against the target collection and an alignment pair was made if the retrieved documents matched given date and similarity score criteria. The resulting comparable collection was used as a similarity thesaurus to translate queries along with a dictionary-based translator. The combined approaches outperformed translation schemes where dictionary-based translation or corpus translation was used alone.",
        "Finnish as Source Language in Bilingual Question Answering. This paper presents a bilingual question answering system that has Finnish as its source language and English as its target language. The system was evaluated in the QA@CLEF 2004 evaluation campaign. It is the only officially evaluated QA system that takes Finnish as input. The system is based on question classification and analysis, translation of important query terms, document retrieval, answer pattern instantiation and answer selection. The system achieves an accuracy of 10,88%.",
        "Multilingual Information Retrieval Using Machine Translation, Relevance Feedback and Decompounding",
        "CLARIT TREC-8 CLIR Experiments In the TREC-8 cross-language information retrieval (CLIR) track, we adopted the approach of using machine translation to prepare a source-language query for use in a target-language retrieval task. We empirically evaluated (1) the effect of pseudo relevance feedback on retrieval performance with two feedback vector length control methods in CLIR and (2) the effect of multilingual data merging either before or after retrieval. Our experiments show that, in general, pseudo relevance feedback significantly improves cross-language retrieval performance, and that postretrieval merging of retrieval results can outperform pre-retrieval merging of multilingual data collections.",
        "Named entity transliteration for cross-language information retrieval using compressed word format mapping algorithm. Transliteration of named entities in user queries is a vital step in any Cross-Language Information Retrieval (CLIR) system. Several methods for transliteration have been proposed till date based on the nature of the languages considered. In this paper, we present a transliteration algorithm for mapping English named entities to their proper Tamil equivalents. Our algorithm employs a grapheme-based model, in which transliteration equivalents are identified by mapping the source language names to their equivalents in a target language database, instead of generating them. The basic principle is to compress the source word into its minimal form and align it across an indexed list of target language words to arrive at the top n-equivalents based on the edit distance. We compare the performance of our approach with a statistical generation approach using Microsoft Research India (MSRI) transliteration corpus. Our approach has proved very effective in terms of accuracy and time.",
        "Bootstrapping dictionaries for cross-language information retrieval. The bottleneck for dictionary-based cross-language information retrieval is the lack of comprehensive dictionaries, in particular for many different languages. We here introduce a methodology by which multilingual dictionaries (for Spanish and Swedish) emerge automatically from simple seed lexicons. These seed lexicons are automatically generated, by cognate mapping, from (previously manually constructed) Portuguese and German as well as English sources. Lexical and semantic hypotheses are then validated and new ones iteratively generated by making use of co-occurrence patterns of hypothesized translation synonyms in parallel corpora. We evaluate these newly derived dictionaries on a large medical document collection within a cross-language retrieval setting.",
        "Multilingual Information Retrieval Mechanism Using VLSI. Requirements and Approaches for Information Retrieval Systems in the Computer-Aided Software Engineering and Document Processing Environment",
        "On the mono- and cross-language detection of text reuse and plagiarism. Plagiarism, the unacknowledged reuse of text, has increased in recent years due to the large amount of texts readily available. For instance, recent studies claim that nowadays a high rate of student reports include plagiarism, making manual plagiarism detection practically infeasible.Automatic plagiarism detection tools assist experts to analyse documents for plagiarism. Nevertheless, the lack of standard collections with cases of plagiarism has prevented accurate comparing models, making differences hard to appreciate. Seminal efforts on the detection of text reuse have fostered the composition of standard resources for the accurate evaluation and comparison of methods.The aim of this PhD thesis is to address three of the main problems in the development of better models for automatic plagiarism detection: (i) the adequate identification of good potential sources for a given suspicious text; (ii) the detection of plagiarism despite modifications, such as words substitution and paraphrasing (special stress is given to crosslanguage plagiarism); and (iii) the generation of standard collections of cases of plagiarism and text reuse in order to provide a framework for accurate comparison of models.Regarding difficulties (i) and (ii) , we have carried out preliminary experiments over the METER corpus . Given a suspicious document dq and a collection of potential source documents D, the process is divided in two steps. First, a small subset of potential source documents D * \u2282 D is retrieved. The documents d \u2208 D * are the most related to dq and, therefore, the most likely to include the source of the plagiarised fragments in it. We performed this stage on the basis of the Kullback-Leibler distance, over a subsample of document's vocabularies. Afterwards, a detailed analysis is carried out comparing dq to every d \u2208 D * in order to identify potential cases of plagiarism and their source. This comparison was made on the basis of word n-grams, by considering n = {2, 3}. These n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications . The result is offered to the user to take the final decision. Further experiments were done in both stages in order to compare other similarity measures, such as the cosine measure, the Jaccard coefficient and diverse fingerprinting and probabilistic models. One of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism. Approaching the detection of this kind of plagiarism is of high relevance, as the most of the information published is written in English, and authors in other languages may find it attractive to make use of direct translations.Our experiments, carried out over parallel and a comparable corpora, show that models of \"standard\" cross-language information retrieval are not enough. In fact, if the analysed source and target languages are related in some way (common linguistic ancestors or technical vocabulary), a simple comparison based on character n-grams seems to be the option. However, in those cases where the relation between the implied languages is weaker, other models, such as those based on statistical machine translation, are necessary .We plan to perform further experiments, mainly to approach the detection of cross-language plagiarism. In order to do that, we will use the corpora developed under the framework of the PAN competition on plagiarism detection (cf. PAN@CLEF: http://pan.webis.de). Models that consider cross-language thesauri and comparison of cognates will also be applied.",
        "Cross-lingual search over 22 european languages. In this paper we present a system for cross-lingual information retrieval, which can handle tens of languages and millions of documents. Functioning of the system is demonstrated on corpus of European Legislation (22 languages, more than 400,000 documents per language). The system uses an interactive webinterface, which can take advantage of a predefined thesaurus allowing the user to dynamically re-rank the retrieval results based on the mapping onto a predefined thesaurus.",
        "Experiments in the Retrieval of Unsegmented Japanese Text at the NTCIR-2 Workshop Our work with the Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) system has made use of overlapping character n-grams in the indexing and retrieval of text. In previous experiments with Western European languages we have shown that longer length n-grams (e.g., n=6) are capable of providing an effective form of alinguistic term normalization. We have wanted to investigate whether these methods could be adapted to processing unsegmented languages such as Japanese. To that end we participated in the Japanese and English portion of the NTCIR-2 evaluation. This paper describes results in monolingual Japanese and English retrieval and in cross-language retrieval using each language as a source language for the other. We found that 6-grams performed comparably with English words and that 2-grams and 3-grams perform equally well in Japanese text. A combination of runs using each tokenization method resulted in only a marginal improvement over runs using a single approach. These two trends were consistent regardless of query length or source language.",
        "Resolving Translation Ambiguity using Monolingual Corpora. A Report on Clairvoyance CLEF-2002 Experiments Choosing the correct target words is a difficult problem for machine translation. In cross-language information retrieval, this problem of choice is mitigated since more than one translation alternative can be retained in the target query. Between choosing just one word as a translation and keeping all the possible translations for each source word, one can apply a range of filtering techniques for eliminating some words and keeping others. In the bilingual track of CLEF 2002, focusing on word translation ambiguity, we experimented with several techniques for choosing the best target translation for each source query word by using co-occurrence statistics in a reference corpus consisting of documents in the target language. One of two distinct corpora was used, the target-language test corpus or the World Wide Web. Our techniques give one best translation per source query word. We also experimented with combining these word choice results (providing up to three translations for each word) in the final translated query. The source query languages were Spanish and Chinese; the target language documents were in English. We submitted four automatic runs for each language pair. When the methods were combined, mixing results obtained with different reference corpora, the recall and average precision of Spanish-to-English retrieval reached 95% and 97%, respectively, of the recall and average precision of an English monolingual retrieval run. For Chinese-to-English text retrieval, the recall and average precision reached 89% and 60%, respectively, of the English run.",
        "(CLSCR) Cross Language Source Code Reuse Detection Using Intermediate Language In today's digital era information access is just a click away. so computer science students also have easy access to all the source codes from different websites thus it has become difficult for academicians to detect source code reuse in students programming assignments. The new trend in the area of source code reuse is using the source code by translating it in another programming language popularly known as cross language plagiarism. Our CLSCR addresses this problem. CLSCR mainly has two components: A compiler that compiles and translates the language specific source code into a tool specific internal format and The Similarity calculator that computes similarity between internal formats of different programs.",
        "Scalable Multilingual Information Access The third Cross-Language Evaluation Forum workshop (CLEF-2002) provides the unprecedented opportunity to evaluate retrieval in eight different languages using a uniform set of topics and assessment methodology. This year the Johns Hopkins University Applied Physics Laboratory participated in the monolingual, bilingual, and multilingual retrieval tasks. We contend that information access in a plethora of languages requires approaches that are inexpensive in developer and run-time costs. In this paper we describe a simplified approach that seems suitable for retrieval in many languages; we also show how good retrieval is possible over many languages, even when translation resources are scarce, or when query-time translation is infeasible. In particular, we investigate the use of character n-grams for monolingual retrieval, pre-translation expansion as a technique to mitigate errors due to limited translation resources, and translation of document representations to an interlingua for computationally efficient retrieval against multiple languages.",
        "Transfer Learning for Cross-Lingual Sentiment Classification with Weakly Shared Deep Neural Networks. Cross-lingual sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of data in a label-scarce target language by exploiting labeled data from a label-rich language. The fundamental challenge of cross-lingual learning stems from a lack of overlap between the feature spaces of source language data and that of target language data. To address this challenge, previous studies have been performed to make use of the translated resources for sentiment classification in the target language, and the classification performance is far from satisfactory because of the language gap between the source language and the translated target language.In this paper, to address the above challenge, we present a novel deep neural network structure, called Weakly Shared Deep Neural Networks (WSDNNs), to transfer the crosslingual information from a source language to a target language. To share the sentiment labels between two languages, we build multiple weakly shared layers of features. It allows to represent both shared inter-language features and language-specific ones, making this structure more flexible and powerful in capturing the feature representations of bilingual languages jointly. We conduct a set of experiments with cross-lingual sentiment classification tasks on multilingual Amazon product reviews. The empirical results show that our proposed approach significantly outperforms the stateof-the-art methods for cross-lingual sentiment classification, especially when label data is scarce.",
        "Caption and Query Translation for Cross-Language Image Retrieval. For many cross-language retrieval tasks, the predominant approach is to translate the query into the language of the document collection (target language). This often gives results as good as, if not better, than translating the document collection into the query language (source language). In this paper, we evaluate query versus document translation for the ImageCLEF 2004 bilingual ad hoc retrieval task. Image retrieval is achieved through matching textual queries to associated image captions for the following languages: French, German, Spanish and Italian using commercially and publicly available resources. On average, we find query translation to outperform document translation (77% of English MAP compared to 65% respectively) but this varies widely across language and query. Combining document and query translation we achieve an average MAP of 85% of English."
    ],
    "papers that compare multiple information retrieval methods": [
        "Modeling representation uncertainty in concept-based multimedia retrieval. Representing multimedia documents by means of concepts labels attached to parts of these documents has great potential for improving retrieval performance. The reason is that concepts are independent from how users refer to them and from the modality in which they occur. For example, a Flower and une Fleur refers to the same concept and a singing bird can appear in an image or an audio recording. The question whether a concept occurs in a multimedia document is answered by a concept detector. However, as building concept detectors is difficult the current detection performance is low which causes the retrieval engine to be uncertain about the actual document representation.This thesis proposes the Uncertain Document Representation Ranking (URR) Framework which deals with this uncertainty by transferring the principles of the Portfolio Selection Theory in finance where the future win of a share is uncertain to the concept-based retrieval problem. Similarly to the distribution of future wins, the retrieval framework considers multiple possible concept-based document representations for each document resulting in multiple possible scores, which is the main scientific contribution of this thesis. Given an existing retrieval function for a certain representation, documents are ranked by the expected score plus an expression of the score's variance.From the general URR framework, we derive ranking models for shot and video segment retrieval. The shot retrieval and the video segment model re-use the probability of relevance and a language modeling ranking function respectively, basing themselves on decades of text retrieval research. We show in experiments that the models significantly improve performance over several strong baselines in five TRECVid collections.Furthermore, current performance of concept-based multimedia retrieval is low. A major reason for this is the performance of the concept detectors on which the simulation is based. Therefore, we predict the influence of improved concept detectors on general concept-based retrieval performance using Monte Carlo simulations. We find that more effort is needed to improve concept detectors, but it is realistic for concept-based retrieval to reach performance suitable for large-scale, real-life applications in the future.. Representing multimedia documents by means of concepts labels attached to parts of these documents has great potential for improving retrieval performance. The reason is that concepts are independent from how users refer to them and from the modality in which they occur. For example, a Flower and une Fleur refers to the same concept and a singing bird can appear in an image or an audio recording. The question whether a concept occurs in a multimedia document is answered by a concept detector. However, as building concept detectors is difficult the current detection performance is low which causes the retrieval engine to be uncertain about the actual document representation.This thesis proposes the Uncertain Document Representation Ranking (URR) Framework which deals with this uncertainty by transferring the principles of the Portfolio Selection Theory in finance where the future win of a share is uncertain to the concept-based retrieval problem. Similarly to the distribution of future wins, the retrieval framework considers multiple possible concept-based document representations for each document resulting in multiple possible scores, which is the main scientific contribution of this thesis. Given an existing retrieval function for a certain representation, documents are ranked by the expected score plus an expression of the score's variance.From the general URR framework, we derive ranking models for shot and video segment retrieval. The shot retrieval and the video segment model re-use the probability of relevance and a language modeling ranking function respectively, basing themselves on decades of text retrieval research. We show in experiments that the models significantly improve performance over several strong baselines in five TRECVid collections.Furthermore, current performance of concept-based multimedia retrieval is low. A major reason for this is the performance of the concept detectors on which the simulation is based. Therefore, we predict the influence of improved concept detectors on general concept-based retrieval performance using Monte Carlo simulations. We find that more effort is needed to improve concept detectors, but it is realistic for concept-based retrieval to reach performance suitable for large-scale, real-life applications in the future.",
        "Applying Multiple Characteristics and Techniques to Obtain High Levels of Performance in Information Retrieval Our information retrieval system which achieves its goals by taking advantage of numerous characteristics of the information and applying numerous sophisticated techniques is described. Robertson's 2-Poisson model and Rocchio's formula, both of which are known to be effective, have been applied in the system. Characteristics of newspapers such as locational information were applied. We give examples of this method's effectiveness in retrieval from collections of newspaper articles, such as the document set for NTCIR 3. We present our application of Fujita's method, where longer terms are used in retrieval by the system but de-emphasized relative to the emphasis on the shortest terms; this allows us to use both compound and single-word terms. The statistical test used in expanding queries through an automatic feedback process is described. The method gives us terms which have been statistically confirmed to be related to the top-ranked documents that were obtained in the first retrieval. We describe the success of the system in four tasks (Korean, Japanese, English, and Chinese) of monolingual information retrieval at NTCIR 3; i.e., the highest scores for precision on all tasks, except for average precision on the \"rigid\" CC task, where its score was second highest. In terms of the other evaluated measures and monolingual information retrieval in other languages, the system obtained both the best average precision and the best R-precision.",
        "Clustering for Photo Retrieval at Image CLEF 2008 This paper presents the first participation of the University of Ottawa group in the Photo Retrieval task at Image CLEF 2008. Our system uses the following components: Lucene for text indexing and LIRE for image indexing. We experiment with several clustering methods in order to retrieve images from diverse clusters. The clustering methods are: k-means clustering, hierarchical clustering, and our own method based on WordNet. We present results for thirteen submitted runs, in order to compare retrieval based on text description, to image-only retrieval, and to merged retrieval, and to compare results for the different clustering methods.",
        "Analyses of Multiple-Evidence Combinations for Retrieval Strategies",
        "Mixture model with multiple centralized retrieval algorithms for result merging in federated search. Result merging is an important research problem in federated search for merging documents retrieved from multiple ranked lists of selected information sources into a single list. The state-of-the-art result merging algorithms such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) try to map document scores retrieved from different sources to comparable scores according to a single centralized retrieval algorithm for ranking those documents. Both SSL and SAFE arbitrarily select a single centralized retrieval algorithm for generating comparable document scores, which is problematic in a heterogeneous federated search environment, since a single centralized algorithm is often suboptimal for different information sources.Based on this observation, this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithms. One simple approach is to learn a set of combination weights for multiple centralized retrieval algorithms (e.g., logistic regression) to compute comparable document scores. The paper shows that this simple approach generates suboptimal results as it is not flexible enough to deal with heterogeneous information sources. A mixture probabilistic model is thus proposed to learn more appropriate combination weights with respect to different types of information sources with some training data. An extensive set of experiments on three datasets have proven the effectiveness of the proposed new approach.",
        "Retrieving Information from Multiple Sources. The Web has several information sources on which an ongoing event is discussed. To get a complete picture of the event, it is important to retrieve information from multiple sources. We propose a novel neural network based model which integrates the embeddings from multiple sources, and thus retrieves information from them jointly, as opposed to combining multiple retrieval results. The importance of the proposed model is that no document-aligned comparable data is needed. Experiments on posts related to a particular event from three different sources -Facebook, Twitter and WhatsApp -exhibit the efficacy of the proposed model.",
        "Pairwise statistical significance of local sequence alignment using multiple parameter sets. Accurate estimation of statistical significance of a pairwise alignment is an important problem in sequence comparison. Recently, a comparative study of pairwise statistical significance with database statistical significance was conducted. In this paper, we extend the earlier work on pairwise statistical significance by incorporating with it the use of multiple parameter sets. Preliminary results for a knowledge discovery application such as homology detection reveal that using multiple parameter sets for pairwise statistical significance estimates gives significantly better coverage than using a single parameter set, at least at some error levels. Also, the fact that the performance does not degrade when using multiple parameter sets is a strong evidence that the assumption that the score distribution follows an extreme value distribution is valid even when using multiple parameter sets. Results of pairwise statistical significance using multiple parameter sets are further shown to be significantly better than database statistical significance estimates reported by BLAST and PSI-BLAST, and comparable and at times significantly better than SSEARCH.",
        "A Comparative Study of Diversity Methods for Hybrid Text and Image Retrieval Approaches",
        "Methods for combining Content-based and Textual-based Approaches in Medical Image Retrieval This paper describes our participation in the Medical Image Retrieval task of Image CLEF 2008. Our aim was to evaluate different combination methods for purely textual and visual approaches. Our most interesting conclusion is that combining results provided by both methods using classical combination function allows to obtain higher retrieval accuracy in terms of MAP. MAP values than combination according to query type. Moreover, it is more reliable than using only textual retrieval or using only visual retrieval.",
        "NovaSearch at TREC 2014 Clinical Decision Support Track This paper describes the participation of the NovaSearch group at TREC Clinical Decision Support 2014. As this is the first edition of the track, we decided to assess the performance of multiple information retrieval techniques: retrieval functions, re-ranking, query expansion and classification of medical articles into question categories. The best performing run was based on an ensemble of state-of-the-art retrieval algorithms combined with unsupervised fusion. Our best run was based on the late fusion of runs using MeSH query expansion, pseudo-relevance feedback with terms from top retrieved results and multiple retrieval functions (BM25L, BM25+, TF-IDF and Language Models) combined with RRF fusion algorithm. We also tested an algorithm to measure article relevance to the target medical questions (diagnosis, test and treatment articles), based on the frequency of words to some categories. An additional experiment was based on pseudo relevance feedback based on each article's journal reputation. Although some techniques did not increase our baseline performance, we are satisfied with our global performance.",
        "Improving retrieval feedback with multiple term-ranking function combination In this article we consider methods for automatic query expansion from top retrieved documents (i.e., retrieval feedback) that make use of various functions for scoring expansion terms within Rocchio's classical reweighting scheme. An analytical comparison shows that the retrieval performance of methods based on distinct term-scoring functions is comparable on the whole query set but differs considerably on single queries, consistent with the fact that the ordered sets of expansion terms suggested for each query by the different functions are largely uncorrelated. Motivated by these findings, we argue that the results of multiple functions can be merged, by analogy with ensembling classifiers, and present a simple combination technique based on the rank values of the suggested terms. The combined retrieval feedback method is effective not only with respect to unexpanded queries but also to any individual method, with notable improvements on the system's precision. Furthermore, the combined method is robust with respect to variation of experimental parameters and it is beneficial even when the same information needs are expressed with shorter queries.",
        "Combining Multiple Resources, Evidences and Criteria for Genomic Information Retrieval We participated in the passage retrieval and aspect retrieval subtasks of the TREC 2006 Genomics Track. This paper describes the methods developed for these two subtasks. For passage retrieval, our query expansion method utilizes multiple external biomedical resources to extract acronyms, aliases, and synonyms, and we propose a post-processing step which combines the evidence from multiple scoring methods to improve relevance-based passage rankings. For aspect retrieval, our method estimates the topical aspects of the retrieved passages and generates passage rankings by considering both topical relevance and topical novelty. Empirical results demonstrate the effectiveness of these methods.",
        "Patent Retrieval Based on Multiple Information Resources. Query expansion methods have been proven to be effective to improve the average performance of patent retrieval, and most of query expansion methods use single source of information for query expansion term selection. In this paper, we propose a method which exploits external resources for improving patent retrieval. Google search engine and Derwent World Patents Index were used as external resources to enhance the performance of query expansion methods. LambdaRank was employed to improve patent retrieval performance by combining different query expansion methods with different text fields weighting strategies of different resources. Experiments on TREC data sets showed that our combination of multiple information sources for query formulation was more effective than using any single source to improve patent retrieval performance.",
        "Information Retrieval Features for Personality Traits This paper describes the methods employed to solve the Author Profiling task at PAN-2015. The main goal was to test the use of features derived from Information Retrieval to identify the personality traits of the author of a given text. This paper describes the features, the classification algorithms employed, and how the experiments were run. Also, I provide a comparative analysis of my results compared to those of other groups.",
        "Cross-lingual Information Retrieval based on Multiple Indexes In this paper we present the technical details of the retrieval system with which we participated at the CLEF09 Ad-hoc TEL task. We present a retrieval approach based on multiple indexes for different languages which is combined with a conceptbased retrieval approach based on Explicit Semantic Analysis. In order to create the language-specific indices for each language, a language detection approach is applied as preprocessing step. We combine the different indices through rank aggregation and present our experimental results with different rank aggregation strategies. Our results show that the use of multiple indices (one for each language) does not improve upon a baseline index containing documents in all languages. The combination with concept based retrieval, however, results in better retrieval performance in some of the cases considered. For the bilingual tasks the final retrieval results of our system were the 5th best results on the BL dataset and the second best on the BNF dataset.",
        "Low-Complexity Supervised Rank Fusion Models. Combining multiple retrieval functions can lead to notable gains in retrieval performance. Learning to Rank (LETOR) techniques achieve outstanding retrieval results, by learning models with no bounds on model complexity. Often, minor retrieval gains are attained at a significant cost in model complexity. This paper focuses on the research question: can less complex models achieve results comparable to LETOR models? In this paper, we investigate an approach for the selection and fusion of rank lists with low-complexity models. The described Learning to Fuse (L2F) algorithm, is a supervised rank fusion procedure that controls the model complexity by discarding rank lists that bring minor improvements to final rank. Evaluation results, on two different datasets, show that it is indeed possible to achieve a retrieval performance comparable to LETOR methods, using only 3-5% of the rank lists of the number of rank lists used by LETOR methods.",
        "Applying Multiple Characteristics and Techniques to Obtain High Levels of Performance in Information Retrieval at NTCIR-4 Our information retrieval system takes advantage of numerous characteristics of the information and applies numerous sophisticated techniques. Robertson's 2-Poisson model and Rocchio's formula, both of which are known to be effective, have been applied in the system. Characteristics of newspapers such as locational information were applied. We present our application of Fujita's method, where longer terms are used in retrieval by the system but de-emphasized relative to the emphasis on the shortest terms; this allows us to use both compound and single-word terms. The statistical test used in expanding queries through an automatic feedback process is described. The method gives us terms which have been statistically confirmed to be related to the top-ranked documents that were obtained in the first retrieval. We also used a numerical term QIDF, which is an IDF term for queries. It has a function to decrease the scores for stop words that occur in many queries. It can be very useful for foreign languages for which we cannot examine stop words. We participated in three tasks (Korean, Japanese, and English) of monolingual information retrieval at NTCIR 4. We obtained relatively higher precisions in all the tasks in which we participated. In particular, we obtained the best precision in Korean description-based monolingual information retrieval.",
        "Reranking Hypotheses of Machine-Translated Queries for Cross-Lingual Information Retrieval. Machine Translation (MT) systems employed to translate queries for Cross-Lingual Information Retrieval typically produce a single translation with maximum translation quality. This, however, might not be optimal with respect to retrieval quality and other translation variants might lead to better retrieval results. In this paper, we explore a method using multiple translations produced by an MT system, which are reranked using a supervised machine-learning method trained to directly optimize retrieval quality. We experiment with various types of features and the results obtained on the medical-domain test collection from the CLEF eHealth Lab series show significant improvement of retrieval quality compared to a system using single translation provided by MT.",
        "Rapid detection of similar peer-reviewed scientific papers via constant number of randomized fingerprints a b s t r a c tThis research is concerned with the detection of similar academic papers. Given a tested paper from a given corpus of 10,099 peer-reviewed scientific papers, a two-stage process was activated. During the first stage, most of the papers were filtered out using a fast filter method. In the second stage, in order to detect similar papers we applied 23 heuristic variants derived from 3 novel prototype methods using various parameter settings. The three novel prototype methods are: CT-TR -Constant Number of randomized T fingerprints, compared to each one-third of R (first/middle/last) fingerprints, CT-AR: Constant Number of randomized T fingerprints, compared to all R fingerprints, and CDT-AR: Constant Number of divided randomized T fingerprints compared, to all R fingerprints. Results achieved by the new methods are superior to those of previous heuristic methods, which were approximations of the \"Full Fingerprint\" (FF) method, currently considered the best heuristic method. The order of this new methods' run-time, (n), is far more efficient than the order of the FF method run-time, (n 2 ) (after removing short documents from the corpus).",
        "Fusion in Information Retrieval: SIGIR 2018 Half-Day Tutorial. Fusion is an important and central concept in Information Retrieval. The goal of fusion methods is to merge different sources of information so as to address a retrieval task. For example, in the adhoc retrieval setting, fusion methods have been applied to merge multiple document lists retrieved for a query. The lists could be retrieved using different query representations, document representations, ranking functions and corpora. MOTIVATIONFusion is a classic technique used for more than twenty years in Information Retrieval, specifically adhoc (query-based) retrieval, that allows multiple sources of information to be combined into a single result set . Fusion can be collection-based, system-based ( multiple ranking algorithms), content-based, and even query-based when many similar queries express the same information need . The real power of fusion comes from the fact that even simple aggregation functions have the potential to provide enhanced retrieval effectiveness by exploiting the chorus effect .In this tutorial, we will show that advances in fusion are directly applicable to current open problems in Information Retrieval, and that much can be learned from these models as machine learning becomes even more prominent in modern search solutions. In particular we draw parallels between unsupervised fusion and ensembles of classifiers in supervised learning .We focus on retrieval settings where a single corpus is used, and different factors that affect retrieval vary; e.g., queries used to represent the information need, document and/or query representations, ranking functions, etc. We briefly discuss the setting of retrieval over several corpora (a.k.a., federated or distributed search ); Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s specifically, we survey several state-of-the-art techniques for fusing lists retrieved from different corpora. We believe that federated search deserves a tutorial in its own right which covers the three main challenges: resource representation, resource selection and results merging . Finally, it is important for everyone in the community to understand just how effective simple fusion techniques can be. and compare three state-of-the-art retrieval systems on 100 adhoc queries in the ClueWeb12B UQV100 collection. The three systems being compared are BM25, a field-based SDM model (the exact configuration is identical to the one described by Gallagher et al. ), a LambdaMART learning-to-rank (LTR) model (here lightGBM is used with 459 features), and double unsupervised fusion 18] (RRF [29] over all UQV query variations and two systems -SDM-Field and BM25). shows the three strong baselines as a difference in NDCG@10 score w.r.t. a BM25 bagof-words run. We can clearly see that not only does fusion make more queries better on average, as shown in , it is also far less likely to make queries worse. This can clearly be seen when comparing Wins, Ties, and Losses (W/T/L) in the table. So, there is much to be learned from fusion baselines when doing exploratory failure analysis on the robustness of new ranking algorithms. TUTORIAL OBJECTIVES\u2022 Highlight the important role of fusion in Information Retrieval.\u2022 Provide a methodological view of the numerous fusion methods.\u2022 Provide an overview of the theoretical foundations of various fusion approaches.\u2022 Introduce the audience to various tasks and challenges for which fusion has been applied and can be applied.\u2022 Discuss parallels with, and more generally pointers to, relevant, related work in machine learning and computational social choice theory.\u2022 Discuss open questions and challenges.",
        "Applying Multiple Characteristics and Techniques in the NICT Information Retrieval System in NTCIR-5 Our information retrieval system takes advantage of numerous characteristics of information and uses numerous sophisticated techniques. Robertson's 2-Poisson model and Rocchio's formula, both of which are known to be effective, are used in the system. Characteristics of newspapers such as locational information are used. We present our application of Fujita's method, where longer terms are used in retrieval by the system but de-emphasized relative to the emphasis on the shortest terms; this allows us to use both compound and single-word terms. The statistical test used in expanding queries through an automatic feedback process is described. The method gives us terms that have been statistically shown to be related to the top-ranked documents that were obtained in the first retrieval. We also used a numerical term, QIDF, which is an IDF term for queries. It decreases the scores for stop words that occur in many queries. It can be very useful for foreign languages for which we cannot determine stop words. We participated in three monolingual information retrieval tasks (Korean, Japanese, and English) and two bilingual information retrieval tasks (Japanese-English and English-Japanese) in NTCIR-5. We obtained high precision in all the tasks in which we participated compared to other participants. In particular, we obtained the best precision in the Korean title-based monolingual information retrieval and the Japanese-English bilingual information retrieval.",
        "Cluster-Based Document Retrieval with Multiple Queries The merits of using multiple queries representing the same information need to improve retrieval effectiveness have recently been demonstrated in several studies. In this paper we present the first study of utilizing multiple queries in cluster-based document retrieval; that is, using information induced from clusters of similar documents to rank documents. Specifically, we propose a conceptual framework of retrieval templates that can adapt cluster-based document retrieval methods, originally devised for a single query, to leverage multiple queries. The adaptations operate at the query, document list and similarity-estimate levels. Retrieval methods are instantiated from the templates by selecting, for example, the clustering algorithm and the cluster-based retrieval method. Empirical evaluation attests to the merits of the retrieval templates with respect to very strong baselines: state-of-the-art cluster-based retrieval with a single query and highly effective fusion of document lists retrieved for multiple queries. In addition, we present findings about the impact of the effectiveness of queries used to represent an information need on (i) cluster hypothesis test results, (ii) percentage of relevant documents in clusters of similar documents, and (iii) effectiveness of state-of-the-art cluster-based retrieval methods.",
        "A Multiple Criteria Approach for Information Retrieval",
        "Overall Comparison at the Standard Levels of Recall of Multiple Retrieval Methods with the Friedman Test",
        "Methods for Combining Content-Based and Textual-Based Approaches in Medical Image Retrieval",
        "Unsupervised Visual and Textual Information Fusion in CBMIR Using Graph-Based Methods Multimedia collections are more than ever growing in size and diversity. Effective multimedia retrieval systems are thus critical to access these datasets from the end-user perspective and in a scalable way. We are interested in repositories of image/text multimedia objects and we study multimodal information fusion techniques in the context of content-based multimedia information retrieval. We focus on graphbased methods, which have proven to provide state-of-the-art performances. We particularly examine two such methods: cross-media similarities and random-walk-based scores. From a theoretical viewpoint, we propose a unifying graph-based framework, which encompasses the two aforementioned approaches. Our proposal allows us to highlight the core features one should consider when using a graph-based technique for the combination of visual and textual information. We compare cross-media and random-walk-based results using three different real-world datasets. From a practical standpoint, our extended empirical analyses allow us to provide insights and guidelines about the use of graph-based methods for multimodal information fusion in content-based multimedia information retrieval. Additional Key Words and Phrases: Content-based multimedia information retrieval, information fusion, graph-based methods, cross-media similarity, random walk, Visual reranking ACM Reference Format:Julien Ah-Pine, Gabriela Csurka, and St\u00e9phane Clinchant. 2015. Unsupervised visual and textual information fusion in CBMIR using graph-based methods.",
        "Concept integration of document databases using different indexing languages. An integrated information retrieval system generally contains multiple databases that are inconsistent in terms of their content and indexing. This paper proposes a rough set-based transfer (RST) model for integration of the concepts of document databases using various indexing languages, so that users can search through the multiple databases using any of the current indexing languages. The RST model aims to effectively create meaningful transfer relations between the terms of two indexing languages, provided a number of documents are indexed with them in parallel. In our experiment, the indexing concepts of two databases respectively using the Thesaurus of Social Science (IZ) and the Schlagwortnormdatei (SWD) are integrated by means of the RST model. Finally, this paper compares the results achieved with a cross-concordance method, a conditional probability based method and the RST model.",
        "Simple and Effective Approach to Score Standardisation. Webber, Moffat and Zobel proposed score standardization for information retrieval evaluation with multiple test collections. Given a topic-by-run raw score matrix in terms of some evaluation measure, each score can be standardised using the topic's sample mean and sample standard deviation across a set of past runs so as to quantify how different a system is from the \"average\" system in standard deviation units. Using standardised scores, researchers can compare systems across different test collections without worrying about topic hardness or normalisation. While Webber et al. mapped the standardised scores to the [0, 1] range using a standard normal cumulative density function, the present study demonstrates that linear transformation of the standardised scores, a method widely used in educational research, can be a simple and effective alternative. We use three TREC robust track data sets with graded relevance assessments and official runs to compare these methods by means of leave-one-out tests, discriminative power, swap rate tests, and topic set size design. In particular, we demonstrate that our method is superior to the method of Webber et al. in terms of swap rates and topic set size design: put simply, our method ensures pairwise system comparisons that are more consistent across different data sets, and is arguably more convenient for designing a new test collection from a statistical viewpoint.",
        "Robust result merging using sample-based score estimates In federated information retrieval, a query is routed to multiple collections and a single answer list is constructed by combining the results. Such metasearch provides a mechanism for locating documents on the hidden Web and, by use of sampling, can proceed even when the collections are uncooperative. However, the similarity scores for documents returned from different collections are not comparable, and, in uncooperative environments, document scores are unlikely to be reported. We introduce a new merging method for uncooperative environments, in which similarity scores for the sampled documents held for each collection are used to estimate global scores for the documents returned per query. This method requires no assumptions about properties such as the retrieval models used. Using experiments on a wide range of collections, we show that in many cases our merging methods are significantly more effective than previous techniques.",
        "Improving query translation in English-Korean cross-language information retrieval. Query translation is a viable method for cross-language information retrieval (CLIR), but it suffers from translation ambiguities caused by multiple translations of individual query terms. Previous research has employed various methods for disambiguation, including the method of selecting an individual target query term from multiple candidates by comparing their statistical associations with the candidate translations of other query terms. This paper proposes a new method where we examine all combinations of target query term translations corresponding to the source query terms, instead of looking at the candidates for each query term and selecting the best one at a time. The goodness value for a combination of target query terms is computed based on the association value between each pair of the terms in the combination. We tested our method using the NTCIR-3 English-Korean CLIR test collection. The results show some improvements regardless of the association measures we used.",
        "Analyses of Multiple Evidence Combination. It hsa been known that different representations of a query ret rieve different sets of documents.Recent work suggests that significant improvements in retrieval performance can be achieved by combining multiple representations of an information need.However, little effort has been made to understand the reason why combining multiple sources of evidence improves retrieval effectiveness. In this paper we analyze why improvements can be achieved with evidence combination, and investigate how evidence should be combined. We describe a rationale for multiple evidence combination, and propose a combining method whose properties coincide wit h the rationale. We also investigate e the effect of using rank instead of similarity on retrieval effectiveness.",
        "Ranking with multiple hyperplanes. The central problem for many applications in Information Retrieval is ranking and learning to rank is considered as a promising approach for addressing the issue. Ranking SVM, for example, is a state-of-the-art method for learning to rank and has been empirically demonstrated to be effective. In this paper, we study the issue of learning to rank, particularly the approach of using SVM techniques to perform the task. We point out that although Ranking SVM is advantageous, it still has shortcomings. Ranking SVM employs a single hyperplane in the feature space as the model for ranking, which is too simple to tackle complex ranking problems. Furthermore, the training of Ranking SVM is also computationally costly. In this paper, we look at an alternative approach to Ranking SVM, which we call \"Multiple Hyperplane Ranker\" (MHR), and make comparisons between the two approaches. MHR takes the divide-and-conquer strategy. It employs multiple hyperplanes to rank instances and finally aggregates the ranking results given by the hyperplanes. MHR contains Ranking SVM as a special case, and MHR can overcome the shortcomings which Ranking SVM suffers from. Experimental results on two information retrieval datasets show that MHR can outperform Ranking SVM in ranking.",
        "Meta-Analysis for Retrieval Experiments Involving Multiple Test Collections. Traditional practice recommends that information retrieval experiments be run over multiple test collections, to support, if not prove, that gains in performance are likely to generalize to other collections or tasks. However, because of the pooling assumptions, evaluation scores are not directly comparable across different test collections. We present a widely-used statistical tool, meta-analysis, as a framework for reporting results from IR experiments using multiple test collections. We demonstrate the meta-analytical approach through two standard experiments on stemming and pseudorelevance feedback, and compare the results to those obtained from score standardization. Meta-analysis incorporates several recent recommendations in the literature, including score standardization, reporting effect sizes rather than score differences, and avoiding a reliance on null-hypothesis statistical testing, in a unified approach. It therefore represents an important methodological improvement over using these techniques in isolation. CCS CONCEPTS\u00b7 Information systems \u2192 Evaluation of retrieval results; KEYWORDS meta-analysis, score standardization, effect sizes ACM Reference Format:"
    ],
    " risks of information retrieval in social media ?": [
        "LTL-INAOE's Participation at eRisk 2019: Detecting Anorexia in Social Media through Shared Personal Information Detecting mental-health risk behaviours at primary stages is crucial to bring help to people and to avoid undesired consequences. In this paper, we describe our participation at the eRisk 2019 shared task. The proposed approach mainly relies on analysing the sentences that include personal information, i.e., fragments of texts reflecting user's interests, concerns, beliefs, personality traits and psychological state. The obtained results are very competitive, validating the important role of personal information for early detection of traces related to anorexia.",
        "Detecting Social Media Icebergs by Their Tips: Rumors, Persuasion Campaigns, and Information Needs. Online activities of more than one billion social media users all over the world form a resourceful ocean of data. Many social media mining techniques try to explore this ocean and extract different types of resources. In this thesis, we present a framework that can detect different types of meaningful social media phenomena. They usually can be viewed as a group of online activities from many social media users with a common or similar objective, such as spreading of rumors, bursting information needs on events and products, or asking for support of an action. These different types of social media phenomena are relatively rare but can be very influential.Detecting them is challenging according to its characteristics. Each phenomenon contains a collection of activities that usually take variety of forms. Taking the spreading of rumor in social media as an example, one rumor may be spread in different forms of statements and expressions. And it can be very hard to distinguish them from statements from trustful sources. Existing work of detecting different types of social media phenomena usually adopts classifiers trained on features of a single activity or cluster of activities . However, the features from single activity are not sufficient for many detection tasks. And the features from cluster of activities will not be significant until that cluster becomes large enough, which cannot be used in early stage detection .In this thesis, we propose to detect meaningful social media phenomena by signal user behaviors observed at an early stage. Just like spotting icebergs in the ocean by their tips, in our case, the tip of a social media iceberg is a small proportion of activities that exist only in social media icebergs. And they can be found even at the early stage. Therefore, we design our detection framework to first detect these specific signal activities. Then we will use them to understand the characteristic of the entire collection of activities from social media phenomena . What we learned can be used to train accurate classifiers to identify whether a collection of activities containing signal activities is a target social media phenomenon or not. This framework is generic and can be applied on detecting many different types of collective activities in social media. We apply our framework on detecting three types of meaningful soPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). cial media phenomena, i.e., emerging information needs, trending rumors, and persuasion campaigns.To detect emerging information needs, we train a classifier to detect user asking question behaviors as signals. We analyze all the questions detected by this classifier and extract keywords from their content to identify emerging information needs. We find out that as signal activities, the questions being asked are substantially different from other types of activities. The keywords extracted from those questions have a considerable power of predicting the trends of Google queries .In our work of detecting trending rumors[3], we find that when there is a rumor, even though most posts do not raise questions about it, there may be a few that do. These questions suspecting whether a piece of information is true or not can help us identify controversial and unconfirmed statements, such as social media rumors. Therefore, we adopt this type enquiry activities as signal to detect rumors. Experiment results show that our rumor detection approach can detect social media rumors at early stage effectively and efficiently.At last, we propose to apply and improve our framework to detect another very important type of social media phenomena, i.e., persuasion campaigns. We will first study and provide a formal definition of social media persuasion campaigns. Then we will implement our detection framework and experiment it with different signal activities. We also propose to develop an algorithm to discover activities opposing the detected persuasion campaigns. We will conduct experiments on Twitter to check the effectiveness and efficiency of our method.",
        "Privacy-Preserving IR 2016: Differential Privacy, Search, and Social Media. Due to lack of mature techniques in privacy-preserving information retrieval (IR), concerns about information privacy and security have become serious obstacles that prevent valuable user data to be used in IR research such as studies on query logs, social media, and medical record retrieval. In SIGIR 2014 and SIGIR 2015, we have run the privacy-preserving IR workshops exploring and understanding the privacy and security risks in information retrieval. This year, we continue the efforts of connecting the two disciplines of IR and privacy/ security by organizing this workshop. We target on three themes, differential privacy and IR dataset release, privacy in search and browsing, and privacy in social media. The workshop include panels with researchers from both fields on these three themes, as well as invite industry speakers for real-world challenges. The goals of this workshop include (1) bringing together the two research fields, and (2) yielding fruitful collaborations. KeywordsPrivacy-Preserving Information Retrieval MOTIVATIONInformation retrieval and information privacy/security are two fast-growing computer science disciplines. Information retrieval provides a set of information seeking, organization, analysis, and decision-making techniques. Information privacy/security defends information from unauthorized or malicious use, disclosure, modification, attack, and destruction. The two disciplines often appear as two areas with opposite goals: one is to seek information from large amounts of materials, the other is to protect (sensitive) information from being found out. On the other hand, there are many synPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ergies and connections between these two disciplines. For example, information retrieval researchers or practitioners often need to consider privacy or security issues in designing solutions for information processing and management, while researchers in information privacy and security often utilize information retrieval techniques when they build the adversary models to simulate how the adversary can actively seek sensitive information. However, there have been only limited efforts to connect the two important disciplines. SIGIR '16 July 17-21, 2016, Pisa, ItalyDue to lack of mature techniques in privacy-preserving information retrieval, concerns about information privacy and security have become serious obstacles that prevent valuable user data to be used in IR research such as studies about query logs, social media, tweets, session analysis, and medical record retrieval. For instance, the recent TREC Medical Record Retrieval Tracks are halted because of the privacy issue and the TREC Microblog Tracks could not provide participants with a standard testbed of tweets for system development. The situation needs to be improved in a timely manner.In addition, with the emergence of online social networks and the growing popularity of digital communication, more and more information about individuals is becoming available on the Internet. While much of this information is not sensitive, it is not uncommon for users to publish sensitive information online, especially on social networking sites.",
        "Social media and SMS in the haiti earthquake. We describe some first results of an empirical study describing how social media and SMS were used in coordinating humanitarian relief after the Haiti Earthquake in January 2010. Current information systems for crisis management are increasingly incorporating information obtained from citizens transmitted via social media and SMS. This information proves particularly useful at the aggregate level. However it has led to some problems: information overload and processing difficulties, variable speed of information delivery, managing volunteer communities, and the high risk of receiving inaccurate or incorrect information.",
        "All our messages are belong to us: usable confidentiality in social networks. Current online social networking (OSN) sites pose severe risks to their users' privacy. Facebook in particular is capturing more and more of a user's past activities, sometimes starting from the day of birth. Instead of transiently passing on information between friends, a user's data is stored persistently and therefore subject to the risk of undesired disclosure. Traditionally, a regular user of a social network has little awareness of her privacy needs in the Web or is not ready to invest a considerable effort in securing her online activities. Furthermore, the centralised nature of proprietary social networking platforms simply does not cater for end-to-end privacy protection mechanisms.In this paper, we present a non-disruptive and lightweight integration of a confidentiality mechanism into OSNs. Additionally, direct integration of visual security indicators into the OSN UI raise the awareness for (un)protected content and thus their own privacy. We present a fully-working prototype for Facebook and an initial usability study, showing that, on average, untrained users can be ready to use the service in three minutes.",
        "A Micromodule Approach for Building Real-Time Systems with Python-Based Models: Application to Early Risk Detection of Depression on Social Media",
        "Towards Ordinal Suicide Ideation Detection on Social Media The rising ubiquity of social media presents a platform for individuals to express suicide ideation, instead of traditional, formal clinical settings. While neural methods for assessing suicide risk on social media have shown promise, a crippling limitation of existing solutions is that they ignore the inherent ordinal nature across finegrain levels of suicide risk. To this end, we reformulate suicide risk assessment as an Ordinal Regression problem, over the Columbia-Suicide Severity Scale. We propose SISMO, a hierarchical attention model optimized to factor in the graded nature of increasing suicide risk levels, through soft probability distribution since not all wrong risk-levels are equally wrong. We establish the face value of SISMO for preliminary suicide risk assessment on real-world Reddit data annotated by clinical experts. We conclude by discussing the empirical, practical, and ethical considerations pertaining to SISMO in a larger picture, as a human-in-the-loop framework. 1 CCS CONCEPTS \u2022 Computing methodologies \u2192 Natural language processing; \u2022 Human-centered computing \u2192 Social media.",
        "Utilizing Online Social Media for Disaster Relief: Practical Challenges in Retrieval. In recent years, several disaster events (e.g., earthquakes in NepalIndia and Italy, terror attacks in Paris and Brussels) have proven the crucial role of Online Social Media (OSM) in providing actionable situational information. However, in such media, the crucial information is typically obscured by a lot of insignificant information (e.g., personal opinions, prayers for victims). Moreover, when time is critical, owing to the rapid speed and huge volume of microblogs, it is infeasible for human subjects to go through all the tweets posted. Hence, automated IR methods are needed to extract the relevant information from the deluge of posts. Though several methodologies have been developed for tasks like classification, summarization, etc. of social media data posted during disasters , there are still several research challenges that need to be addressed for effectively utilising social media data (e.g., microblogs) for aiding disaster relief operations. Research challenges:We have identified the following challenges in developing IR systems for OSM text posted during disasters. (i) Dealing with noisy vocabulary of OSM content: Microblogs often contains various spellings of the same word, that includes both English words (like 'epicentre' and 'epicenter') as well as non-English words (like 'gurudwara' and 'gurdwara'). Moreover, on microblogging sites like Twitter, words are often shortened arbitrarily owing to the strict restriction on the length of microblogs (e.g., 'operation' shortened to 'oper' or 'ops'). Hence IR methodologies need to be able to handle such arbitrary variations in spelling.(ii) Need for improved models for retrieval and ranking: The context and vocabulary of the microblogs are time-variant. For example, at the initial phase of the 2015 Nepal-India earthquake, terms like 'send' and 'airlifted' were being used to indicate availability of resources (e.g. \"Haryana Govt. airlifted 20,000 food packets\"). However, in the later phases, terms like 'distribute' and 'reach' were used to describe the availability of resources (e.g. \"ISKON Kathmandu distributes food\"). Thus retrieval schemes need to dynamically adapt to the fast-changing context and vocabulary for effective retrieval from OSM. (iii) Need for better evaluation measures: From our discussions with the NGOs who participate in relief works, we understood that a binary notion of relevance of OSM posts is not apt in disaster scenarios. For example, consider the two tweets \"Urgent : Blood shortage Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan \u00a9 2017 , however the first one is much more important and actionable than the second one. Developing benchmark collections containing such graded relevance is also a challenge. (iv) Need for retrieval from multiple sources: When a disaster strikes, responding authorities need to identify actionable information from multiple sources such as crowdsourced data from Twitter, Facebook, closed group communication data like WhatsApp chats among relief workers, and so on, for a comprehensive analysis of the situation. Further, data from different sources can be in different languages, e.g., English, and local languages of the region where the disaster has occurred. Hence, developing a common IR framework to extract data from such heterogeneous data sources is a challenge that is yet to be addressed. Work done: The main objective of this PhD work is to address the aforementioned practical IR problems. As initial efforts towards understanding the challenges, we have developed two datasets for retrieval and summarisation of microblogs related to disasters, and made them available to the research community . We have also developed a context-specific stemming algorithm for noisy microblogs, that enabled significantly better retrieval of microblogs (in English) as compared to the well-known Porter stemmer . In another ongoing work, we have proposed word embedding based techniques for identifying tweets informing about resource needs / availabilities [1] -our proposed methodologies outperform prior pattern matching based techniques. We will continue to improve our proposed techniques in future.",
        "Early Mental Health Risk Assessment through Writing Styles, Topics and Neural Models This paper describes the participation of the RELAI team in the eRisk 2020 tasks. The 2020 edition of eRisk proposed two tasks: (T1) Early assessment of risk of self-harm and (T2) Signs of depression in social media users. The second task focused on automatically filling a depression questionnaire given user writing history. The RELAI team participated in both tasks, and addressed them using topic modeling algorithms (LDA and Anchor), neural models with three different architectures (Deep Averaging Networks (DANs), Contextualizers, and Recurrent Neural Networks (RNNs)), and an approach based on writing styles. For the second task related to early detection of depression, the system based on LDA performed well according to all the evaluation metrics, and achieved the best performance among participants according to the Average Difference between Overall Depression Levels (ADODL) with a score of 83.15%. Overall, the submitted systems achieved promising results, and suggest that evidence extracted from social media could be useful for early mental health risk assessment.",
        "Social Information Retrieval Systems: Emerging Technologies and Applications for Searching the Web Effectively",
        "DataMirror: Reflecting on One's Data Self: A Tool for Social Media Users to Explore Their Digital Footprints Small pieces of data that are shared online, over time and across multiple social networks, have the potential to reveal more cumulatively than a person intends. This could result in harm, loss or detriment to them depending what information is revealed, who can access it, and how it is processed. But how aware are social network users of how much information they are actually disclosing? And if they could examine all their data, what cumulative revelations might be found that could potentially increase their risk of various online threats (social engineering, fraud, identify theft, loss of face, etc.)? In this paper, we present DataMirror, an initial prototype tool, that enables social network users to aggregate their online data so that they can search, browse and visualise what they have put online. The aim of the tool is to investigate and explore people's awareness of their data self that is projected online; not only in terms of the volume of information that they might share, but what it may mean when combined together, what pieces of sensitive information may be gleaned from their data, and what machine learning may infer about them given their data.",
        "Bots and Gender Profiling on Twitter This article describes the contribution of the Natural Language Processing Lab of CIC-IPN, Mexico in task bots and gender profiling at PAN-19 evaluation lab. The escalation in the use of social media facilities and proliferation in the fame of online social media websites such as Twitter, Facebook, LinkedIn, etc. directed to the growth of unwanted social bots as automatic social performers. Performers like this can perform numerous nasty acts comprising human discussions inflators, cheaters, stock market exploiters and so on. The risk is even higher when the motive is a political party. Moreover, bots are generally related to spreading fake news. So, it is essential to deal with the classification of social-bots from an author profiling point of view from the perspective of the marketing field, security, and forensics. We cannot deny the importance of bots on social media. Due to the high influence of bots, it is necessary to uncover the possible threats of social media bots. The goal of the article is to detect (A) if the author of a Tweet is a bot or a human, (B) if human, identify the gender of that particular author. We participated in the English language only. In the proposed approach, we used a well-known bag of words model with different preprocessing actions (stemming, stop words removal, lowercase, etc.). On provided development corpora, we got 87.12 (accuracy) on task A (binary class) by using Logistic Regression and 68.99 on task B (multiclass) by using Decision Tree classifier. In the evaluation phase on TIRA, we obtained 86.29 accuracy for task A and 68.37 for task B.",
        "A General Framework for People Retrieval in Social Media with Multiple Roles",
        "CEN@Amrita: Information Retrieval on CodeMixed Hindi-English Tweets Using Vector Space Models One of the major challenges nowadays is Information retrieval from social media platforms. Most of the information on these platforms is informal and noisy in nature. It makes the Information retrieval task more challenging. The task is even more difficult for twitter because of its character limitation per tweet. This limitation bounds the user to express himself in condensed set of words. In the context of India, scenario is little more complicated as users prefer to type in their mother tongue but lack of input tools force them to use Roman script with English embeddings. This combination of multiple languages written in the Roman script makes the Information retrieval task even harder. Query processing for such CodeMixed content is a difficult task because query can be in either of the language and it need to be matched with the documents written in any of the language. In this work, we dealt with this problem using Vector Space Models which gave significantly better results than the other participants. The Mean Average Precision (MAP) for our system was 0.0315 which was second best performance for the subtask.",
        "BioInfo@UAVR at eRisk 2019: delving into Social Media Texts for the Early Detection of Mental and Food Disorders This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the shared tasks of CLEF eRisk 2019 1. The objective of the eRisk initiative is to encourage research in the area of information retrieval for the automatic detection of risk situations on the internet. The challenge was organized in three tasks, focused on the early detection of anorexia (T1), self-harm (T2) and severity of depression (T3). We addressed these tasks using a mix approach that combines machine learning with psycholinguistics and behavioural patterns. The results obtained validate the use of such patterns in the context of social media mining and motivate future research into this field.",
        "Finding people and their utterances in social media. Since its introduction, social media, \"a group of internet-based applications that (. . . ) allow the creation and exchange of user generated content\" , has attracted more and more users. Over the years, many platforms have arisen that allow users to publish information, communicate with others, connect to like-minded, and share anything a users wants to share. Text-centric examples are mailing lists, forums, blogs, community question answering, collaborative knowledge sources, social networks, and microblogs, with new platforms starting all the time. Given the volume of information available in social media, ways of accessing this information intelligently are needed; this is the scope of my research.Why should we care about information in social media? Here are three examples that motivate my interest. (A) Viewpoint research; someone wants to take note of the viewpoints on a particular issue. (B) Answers to problems; many problems have been encountered before, and people have shared solutions. (C) Product development; gaining insight into how people use a product and what features they wish for, eases the development of new products. Looking at these examples of information need in social media, we observe that they revolve not just around relevance in the traditional sense (i.e., objects relevant to a given topic), but also around criteria like credibility, authority, viewpoints, expertise, and experiences. However, these additional aspects are typically conditioned on the topical relevance of information objects.In social media, \"information objects\" come in several types but many are utterances created by people (blog posts, emails, questions, answers, tweets). People and their utterances offer two natural entry points to information contained in social media: utterances that are relevant and people that are of interest. I focus on three tasks in which the interaction between the two is key.The first task concerns finding utterances in social media. Although this resembles a traditional ad hoc retrieval task, the lack of top-down rules and editors in social media entails the use of unexpected language: spelling and grammar errors are not corrected, Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07. and the vocabulary is unrestricted, unlike edited content. In my research, I use two features of social media to overcome the problem of unexpected language: (i) I translate several credibility indicators to measurable features, and implement these in the blog post retrieval process to enhance retrieval effectiveness [2]; (ii) Utterances are surrounded by their environment, and this influences their content. I use this observation to introduce a model for query modeling using external collections , and investigate which context levels are useful in email retrieval .The second task, finding people in social media, is operationalized in blog feed search: identify blogs that show a recurring interest in a given topic. This task shows similarities with expert finding, and models from this field have been successfully adopted. Blogbased models build and rank representations of bloggers based on their utterances, whereas post-based models rank utterances and aggregate scores to construct a ranking of bloggers. The former capture the centrality of the topic to the blog, but are not very efficient; the latter can identify interesting posts and are efficient. In I introduce a two-stage model that ranks utterances and constructs models for the blogs these utterances belong to and ranks these blog models. My two-stage model is more efficient than blog-based models, and more effective than post-based models.The third and final task builds on the previous two, and focuses on finding utterances using people. Applied to search in email archives, personal profiles can be constructed from people's utterances. On top of these personal profiles, communication profiles are built, containing information extracted from threads, quotes and replies as well as linguistic clues. Communication profiles can indicate the role of people in a topic-dependent way. For a given topic, I use both communication and personal profiles of people to find utterances that are relevant to the topic .",
        "Towards Trust\u00ad-based Decentralized Ad-Hoc Social Networks. Social Media has transformed modern day society. It can be argued that one of the main drivers behind this transformation are novel ways to effectively distribute content in a highly targeted fashion and at scale. Recently, this effectiveness has come under attack based on new phenomena known as Fake News, Filter Bubble and Echo Chambers. The public debate about the impact of these phenomena on modern day society ranges from demanding a complete social media shutdown to government intervention and censorship. Furthermore, it appears that Social Media Platform providers are not sure what countermeasures are needed to address these new challenges. The main concern is that Black Mirror like scenarios will emerge simply by allowing privately held companies decide what content is conforming to public norms leading to a distortion of values. This paper presents an alternative solution by focusing on empowering meaningful relationships and not content engagement. The main motivation behind the proposed solution is to create social networks that follow a 'Trust by Design' paradigm. This paper introduces and discusses the above-mentioned challenges and presents a novel new social media concept seeking to overcome current challenges. CCS CONCEPTS Social Media Application has become an important element in modern day society. They equally serve as communication platform and content distribution platform. Based on their mostly centralized design the social media platform is able to facilitate both aspects in large scale. Two further advantages of this central design are that (1) the user has access to vast amounts of content and (2) access to the overall social graph making it easy to connect with others. The social media platform provider, therefore, can be viewed as a proxy between content and people with a substantial commercial advantage due to being able to distribute premium content in a highly targeted fashion. The underlying principle of content distribution within social networks is based on content engagement, also known as reactions exhibited by users in the form of views, likes, comments and shares. The underlying assumption is that the more engagement a certain piece of content . Filter bubbles are the result of engagement-based content filtering. The underlying principle is to show the user content that relates to the content the user has previously engaged on. The result is a content stream that lacks diversification of topics and opinions. Echo Chambers are a result of content recommendations that are based on interests of friends and peers. This results in a content feed that is strongly biased towards grouped opinion (e.g. Group Think). Finally, fake news and related expressions of the same, such as alternative facts, related to gaming or misusing the underlying greedy algorithm. The result is an increase in the spread of mostly promoted and highly opinionated content. The main approach is to create a viral effect by using fake accounts that increase the engagement resulting in a wider spread of the content . Current algorithms that are responsible for detecting content that is showing high engagement (i.e. is viral) are not able to assess if both the content and the engagement is trustworthy (i.e. not fake). This paper argues that the concept of trust (or the lack of the same) is the driving force behind all three challenges with the first relating to trusting the social media platform managing the distribution of the content (filter bubble). The second challenge relates to trusting friends and peers (echo chamber), and finally the third relates to trusting oneself and ones own judgment (fake news). Based on the centralized concept of most social media platforms, trusting the platform is by far the most common challenge. It can be argued that users simply don't have the control or even leverage to change or understand how content is distributed and what decisions have led to this. Moreover, users are left completely in the dark about what behaviour or engagement has led to content being shown in their content stream. This handover of trust is a critical element for social media applications. In essence it is completely at the discretion of the social network provider how the user's data is used and what content is delivered. However, even though social media platform providers consider the decision over content distribution to be fully theirs, the question of responsibility, in the case of questionable content being spread, is passed on to the content providers and/or users [8]. This argument is based on the claim that social media applications, similar to search indexing engines, only facilitate the distribution and cannot be responsible for the nature of the content itself. This argument, however, is questioned has been questioned by social protection agencies and policymakers leading to the 'Right to be Forgotten' legislation forcing companies to take down content and/or content links . Furthermore, based on the intensity of the public debate around Fake News Social Media Platform providers have introduced questionable countermeasures that include censorship, content monitoring and account banning. The most worrying aspect of applying these measures is that privately held companies are making decisions without reference to any ethical guideline. This can lead to a Black Mirror like scenario in which private corporations or individuals controlling the same, can influence the public debate towards their agenda. Moreover, the current development creates a separate yet connected concern in which individuals (and in this case not content) are assessed and scored for their usefulness. This concern has been highlighted by the US Government in a recent report indicating that Social Scoring can lead to widespread discrimination [10]. Overall, it can be concluded that all challenges introduced above are related to trust and in most cases the lack of the same. To overcome the current 'Trust Crisis' in Social Media Applications this paper introduces a novel social media concept following a 'Trust-by-Design' paradigm. The presented approach (including a draft architecture) seeks to motivate ongoing research in the web community to develop novel approaches that minimize risk to society by Social Media Applications, such as the ones depicted in the TV show Black Mirror. Proposed SolutionThe below-introduced solution forms a conceptual base for social network applications to be designed with trust in mind. For this several technological elements are connected to create the overall architecture. This section introduces a use case and all required concepts together with the challenges they address. After this, a preliminary architecture is presented, followed by a discussion of open challenges. BackgroundSocial Media Applications are based on the concept of connecting people. This resembles the basic idea of the WWW which in its core was invented to connect information. Recently, the concept of linking information via hyperlink has been extended towards linking knowledge represented by the Semantic Web . Social Media Applications have added a new layer to this core concept of linking by connecting users and distributing content based on the behaviour users emit when consuming content. The main difference however between the WWW and Social Media Applications is that the later follows a centralized architecture which increases the effectiveness of content distribution but also limits control over the same. This shift of control furthermore increases the required trust a user has to have in the distribution mechanisms of the central platform.This paper introduces a concept that seeks to shift the control and with it the responsibility, back to the individual user. At the centre of this approach lies the ad-hoc creation of social networks based on creating meaningful relationships within the environment of the user that are trusted. To utilize the environment effectively proximity technology is discussed, such as IoT based smart environments. Furthermore, a concept of computational trust is introduced that overlays and the user's social networks. The underlying assumption is that it is easier for an individual to trust people and objects that are proximity. In effect, this means that social media application need to go out into the wild and out of the confined centrally managed box they are sitting at the moment.",
        "Real time discussion retrieval from twitter. While social media receive a lot of attention from the scientific community in general, there is little work on high recall retrieval of messages relevant to a discussion. Hash tag based search is widely used for data retrieval from social media. This work shows limitations of this approach, because the majority of the relevant messages do not even contain any hash tag, and unpredictable hash tags are used as the conversation evolves in time. To overcome these limitations, we propose an alternative retrieval method. Given an input stream of messages as an example of the discussion, our method extracts the most relevant words from it and queries the social network for more messages with these words. Our method filters messages that do not belong to the discussion using an LDA topic model. We demonstrate this concept on manually built collections of tweets about major sport and music events.",
        "Mobile App Retrieval for Social Media Users via Inference of Implicit Intent in Social Media Text",
        "Discovering socially similar users in social media datasets based on their socially important locations A B S T R A C TSocially similar social media users can be defined as users whose frequently visited locations in their social media histories are similar. Discovering socially similar social media users is important for several applications, such as, community detection, friendship analysis, location recommendation, urban planning, and anomaly user and behavior detection. Discovering socially similar users is challenging due to dataset size and dimensions, spam behaviors of social media users, spatial and temporal aspects of social media datasets, and location sparseness in social media datasets. In the literature, several studies are conducted to discover similar social media users out of social media datasets using spatial and temporal information. However, most of these studies rely on trajectory pattern mining methods or take into account semantic information of social media datasets. Limited number of studies focus on discovering similar users based on their social media location histories. In this study, to discover socially similar users, frequently visited or socially important locations of social media users are taken into account instead of all locations that users visited. A new interest measure, which is based on Levenshtein distance, was proposed to quantify user similarity based on their socially important locations and two algorithms were developed using the proposed method and interest measure. The algorithms were experimentally evaluated on a real-life Twitter dataset. The results show that the proposed algorithms could successfully discover similar social media users based on their socially important locations.",
        "A Neural Network Approach to Early Risk Detection of Depression and Anorexia on Social Media Text In recent years, people actively write text messages on social media platforms like Twitter and Reddit. The text shared on social media drives various applications including influenza detection, suicide detection, and mental illness detection. This work presents our approach to early risk detection of depression and anorexia on social media in CLEF eRisk 2018. For the two mental illnesses, our models combine TF-IDF information and convolutional neural networks (CNNs) to identify the articles written by potential patients. The official evaluation shows our models achieve ERDE5 of 10.81%, ERDE50 of 9.22%, and F-score of 0.37 in depression detection and ERDE5 of 13.65%, ERDE50 of 11.14%, and F-score of 0.67 in anorexia detection.",
        "Detecting Early Risk of Depression from Social Media User-generated Content This paper presents the systems developed by the UQAM team for the CLEF eRisk Pilot Task 2017. The goal was to predict as early as possible the risk of mental health issues from user-generated content in social media. Several approaches based on supervised learning and information retrieval methods were used to estimate the risk of depression for a user given the content of its posts in reddit. Among the five systems evaluated, the experiments show that combining information retrieval and machine learning approaches gives the best results.",
        "Sub-event discovery and retrieval during natural hazards on social media data. Social media sites contain a considerable amount of data for natural calamities events, such as earthquakes, snowstorms, mud-rock flows. With the increasing amount of social media data, an important task is to discover and retrieve sub-events over time. Especially in emergency situations, rescue and relief activities can be enhanced by identifying and retrieving sub-events of a natural hazard event. However, the existing event detection techniques in news-related reports cannot effectively work for social media data due to the unstructured of social network data. In this paper, we propose a new natural hazard subevents discovery model SED (Sub-Events Discovery), which adopts multifarious features to detect sub-events. Moreover, in order to retrieve the sub-events over a specific event, we introduce a novel SER (Sub-Event Retrieval) algorithm from time-stamped social media data. Our novel approach SER makes use of automatically obtained messages from external search engines in the entire process. For purpose of determining the periodical convergence time for natural hazard event, our method provides online sub-events retrieval and sub-events discovery to meet the further needs. Next the improved estimation standards with timestamp are utilized in our experiments to verify the effectiveness and efficiency of SED model and SER algorithm.",
        "The importance of being socially-savvy: quantifying the influence of social networks on microblog retrieval. Social media users create virtual connections for various reasons: personal and professional. While significant research efforts have been spent on exploring the dynamics of creation of social network connections, little is known about how those connections influence the content generated by social media users. In this work, we quantitatively evaluate the influence of social networks on social media content providers. Additionally, we propose several document expansion methods, which leverage the content generated by the social networks of the authors of social media documents and compare their effectiveness. Experimental results on a large sample of Twitter data indicate that retrieval models discriminatively leveraging social network content for document expansion outperform both traditional, sociallyunaware retrieval models and retrieval models that indiscriminatively utilize all social connections."
    ],
    "actual experiments that strengthen theoretical knowledge": [
        "Ontology for cultural variations in interpersonal communication: Building on theoretical models and crowdsourced knowledge",
        "A Nonparametric Sequential Test for Online Randomized Experiments. We propose a nonparametric sequential test that aims to address two practical problems pertinent to online randomized experiments: (i) how to do a hypothesis test for complex metrics; (ii) how to prevent type 1 error inflation under continuous monitoring. The proposed test does not require knowledge of the underlying probability distribution generating the data. We use the bootstrap to estimate the likelihood for blocks of data followed by mixture sequential probability ratio test. We validate this procedure on data from a major online e-commerce website. We show that the proposed test controls type 1 error at any time, has good power, is robust to misspecification in the distribution generating the data, and allows quick inference in online randomized experiments.",
        "Spatial Representations of Knowledge: Validity and Applications to Information Science. This research examined the relationship between subjects' knowledge of an area and their similarity ratings of pairs of concepts.Knowledge was operationalized as (i) test score performance, and (2) correspondence with an expert's similarity judgments.Subjects (n=55) were exposed to a three-week module on social psychology as part of their introductory psychology course.At the end of this course segment they were administered an examination and, later, a term similarity questionnaire.Students' similarity ratings related to both exam score and similarity ratings of the expert.Furthermore, as expected, test performance was strongly related (r=+.55) to the correlation with the expert's similarity judgments. A model based on term similarity judgments was proposed as a theoretical explanation of how term dependence models influence information retrieval system performance.A critical shortcoming of research in informarion retrieval is a failure to identify the basic mechanisms that influence the performance of retrieval systems Williams and Kim, 1975).Nevertheless, an understanding of these basic mechanisms is prerequisite to the construction of a theory that may eventually lead to Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.(~)1981 ACM 0-89791-052-4/81/0500-0113 $00.75an improvement in the performance of information retrieval systems. Specifically, a theory that explains and predicts the performance of an information Retrieval System (IRS) must ultimately relate to how people make relevance judgments.That is, an effective system must be able to distinguish between those documents that users wish to receive and those that they do not wish to be Nevertheless, what remains to be explained is why an improvement in this correspondence should result in improved retrieval. We feel that \"knowledge\" provides an explanation as to why these changes in the IRS's system of concepts leads to 113",
        "Algorithms for String Searching: A Survey. We survey several algorithms for searching a string in a piece of text. We include theoretical and empirical results, as well as the actual code of each algorithm. An extensive bibliography is also included.",
        "Articulating information needs in XML query languages Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML documents comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test suite of the INEX XML Retrieval Evaluation Initiative. Theoretically, we create two mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language.Our main experimental findings are: First, while structure is used in varying degrees of complexity, two-thirds of the queries can be expressed in a fielded-search-like format which does not use the hierarchical structure of the documents. Second, three-quarters of the queries use constraints on the context of the elements to be returned; these contextual constraints cannot be captured by ordinary keyword queries. Third, structure is used as a search hint, and not as a strict requirement, when judged against the underlying information need. Fourth, the use of structure in queries functions as a precision enhancing device.",
        "Strategic foundation of computational social science. For decades, scholars of various disciplines have been fretted over strategic interactions, presenting theoretical insights and empirical observations . Despite the central role played by strategic interactions in creating values in the Internet environment, our ability to understand them scientifically and to manage them in practice has remained limited. While engineering communities suffer from not having enough theoretical resource to formalize such phenomena, economics and social sciences lack adequate technology to properly operationalize their theoretical insights, thereby demanding an integrative solution. This project aims to develop a rational-choice-theory-driven framework for computational social science, focusing on social interactions on the Internet. In order to suggest theoretical foundations, validation of the predictions in a controlled environment, and verification of the results in actual platforms, general approaches and a few examples of ongoing research are presented.",
        "Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs Knowledge graphs have been widely adopted to improve recommendation accuracy. The multi-hop user-item connections on knowledge graphs also endow reasoning about why an item is recommended. However, reasoning on paths is a complex combinatorial optimization problem. Traditional recommendation methods usually adopt brute-force methods to find feasible paths, which results in issues related to convergence and explainability. In this paper, we address these issues by better supervising the path finding process. The key idea is to extract imperfect path demonstrations with minimum labeling efforts and effectively leverage these demonstrations to guide path finding. In particular, we design a demonstration-based knowledge graph reasoning framework for explainable recommendation. We also propose an ADversarial Actor-Critic (ADAC) model for the demonstration-guided path finding. Experiments on three real-world benchmarks show that our method converges more quickly than the state-of-the-art baseline and achieves better recommendation accuracy and explainability. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems; \u2022 Computing methodologies \u2192 Reinforcement learning.",
        "Ranking the NTCIR Systems Based on Multigrade Relevance. At NTCIR-4, new retrieval effectiveness metrics called Qmeasure and R-measure were proposed for evaluation based on multigrade relevance. This paper shows that Q-measure inherits both the reliability of noninterpolated Average Precision and the multigrade relevance capability of Average Weighted Precision through a theoretical analysis, and then verify the above claim through experiments by actually ranking the systems submitted to the NTCIR-3 CLIR Task. Our experiments confirm that the Q-measure ranking is very highly correlated with the Average Precision ranking and that it is more reliable than Average Weighted Precision.",
        "Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion. Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications.",
        "The Effect of training on biologists acceptance of bioinformatics tools: A field experiment",
        "Deep Adaptive Feature Aggregation in Multi-task Convolutional Neural Networks RDF2vec is an embedding technique for representing knowledge graph entities in a continuous vector space. In this paper, we investigate the effect of materializing implicit A-box axioms induced by subproperties, as well as symmetric and transitive properties. While it might be a reasonable assumption that such a materialization before computing embeddings might lead to better embeddings, we conduct a set of experiments on DBpedia which demonstrate that the materialization actually has a negative effect on the performance of RDF2vec. In our analysis, we argue that despite the huge body of work devoted on completing missing information in knowledge graphs, such missing implicit information is actually a signal, not a defect, and we show examples illustrating that assumption.",
        "The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine. Despite its intuitive appeal, the hypothesis that retrieval at the level of \"concepts\" should outperform purely term-based approaches remains unverified empirically. In addition, the use of \"knowledge\" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for \"conceptual retrieval\" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.",
        "A Knowledge-Enhanced Recommendation Model with Attribute-Level Co-Attention Deep neural networks (DNNs) have been widely employed in recommender systems including incorporating attention mechanism for performance improvement. However, most of existing attentionbased models only apply item-level attention on user side, restricting the further enhancement of recommendation performance. In this paper, we propose a knowledge-enhanced recommendation model ACAM, which incorporates item attributes distilled from knowledge graphs (KGs) as side information, and is built with a co-attention mechanism on attribute-level to achieve performance gains. Specifically, each user and item in ACAM are represented by a set of attribute embeddings at first. Then, user representations and item representations are augmented simultaneously through capturing the correlations between different attributes by a co-attention module. Our extensive experiments over two realistic datasets show that the user representations and item representations augmented by attribute-level co-attention gain ACAM's superiority over the state-of-the-art deep models.",
        "On Link Prediction in Knowledge Bases: Max-K Criterion and Prediction Protocols. Building knowledge base embedding models for link prediction has achieved great success. We however argue that the conventional top-k criterion used for evaluating the model performance is inappropriate. This paper introduces a new criterion, referred to as max-k. Through theoretical analysis and experimental study, we show that the top-k criterion is fundamentally inferior to max-k. We also introduce two prediction protocols for the max-k criterion. These protocols are strongly justified theoretically. Various insights concerning the max-k criterion and the two protocols are obtained through extensive experiments. CCS CONCEPTS\u2022 Computing methodologies \u2192 Reasoning about belief and knowledge; Statistical relational learning; \u2022 Information systems \u2192 Top-k retrieval in databases;",
        "Strengthening collaborative data analysis and decision making in web communities",
        "A Fast and Provable Method for Estimating Clique Counts Using Tur\u00e1n's Theorem. Clique counts reveal important properties about the structure of massive graphs, especially social networks. The simple setting of just 3-cliques (triangles) has received much attention from the research community. For larger cliques (even, say 6-cliques) the problem quickly becomes intractable because of combinatorial explosion.Most methods used for triangle counting do not scale for large cliques, and existing algorithms require massive parallelism to be feasible.We present a new randomized algorithm that provably approximates the number of k-cliques, for any constant k. The key insight is the use of (strengthenings of) the classic Tur\u00e1n's theorem: this claims that if the edge density of a graph is sufficiently high, the k-clique density must be non-trivial. We define a combinatorial structure called a Tur\u00e1n shadow, the construction of which leads to fast algorithms for clique counting.We design a practical heuristic, called Tur\u00e1n-shadow, based on this theoretical algorithm, and test it on a large class of test graphs. In all cases, Tur\u00e1n-shadow has less than 2% error, and runs in a fraction of the time used by well-tuned exact algorithms. We do detailed comparisons with a range of other sampling algorithms, and find that Tur\u00e1n-shadow is generally much faster and more accurate. For example, Tur\u00e1n-shadow estimates all clique numbers up to size 10 in social network with over a hundred million edges. This is done in less than three hours on a single commodity machine.",
        "Application of aboutness to functional benchmarking in information retrieval Experimental approaches are widely employed to benchmark the performance of an information retrieval (IR) system. Measurements in terms of recall and precision are computed as performance indicators. Although they are good at assessing the retrieval effectiveness of an IR system, they fail to explore deeper aspects such as its underlying functionality and explain why the system shows such performance. Recently, inductive (i.e., theoretical) evaluation of IR systems has been proposed to circumvent the controversies of the experimental methods. Several studies have adopted the inductive approach, but they mostly focus on theoretical modeling of IR properties by using some metalogic. In this article, we propose to use inductive evaluation for functional benchmarking of IR models as a complement of the traditional experiment-based performance benchmarking. We define a functional benchmark suite in two stages: the evaluation criteria based on the notion of \"aboutness,\" and the formal evaluation methodology using the criteria. The proposed benchmark has been successfully applied to evaluate various well-known classical and logic-based IR models. The functional benchmarking results allow us to compare and analyze the functionality of the different IR models.",
        "A comparison of the optimality of statistical significance tests for information retrieval evaluation. Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.",
        "Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization. The lack of sufficient labeled Web pages in many languages, especially for those uncommonly used ones, presents a great challenge to traditional supervised classification methods to achieve satisfactory Web page classification performance. To address this, we propose a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification, which is based on the following two important observations. First, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. Second, we also observe that the associations between word clusters and Web page classes are a more reliable carrier than raw words to transfer knowledge across languages. With these recognitions, we attempt to transfer knowledge from the auxiliary language, in which abundant labeled Web pages are available, to target languages, in which we want classify Web pages, through two different paths: word cluster approximations and the associations between word clusters and Web page classes. Due to the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. We evaluate the proposed approach in extensive experiments using a real world crosslanguage Web page data set. Promising results demonstrate the effectiveness of our approach that is consistent with our theoretical analyses.",
        "Information flow and analysis: Theory, simulation, and experiments. I. Basic theoretical and conceptual development",
        "Stochastic Query Covering for Fast Approximate Document Retrieval We design algorithms that, given a collection of documents and a distribution over user queries, return a small subset of the document collection in such a way that we can efficiently provide high-quality answers to user queries using only the selected subset. This approach has applications when space is a constraint or when the query-processing time increases significantly with the size of the collection. We study our algorithms through the lens of stochastic analysis and prove that even though they use only a small fraction of the entire collection, they can provide answers to most user queries, achieving a performance close to the optimal. To complement our theoretical findings, we experimentally show the versatility of our approach by considering two important cases in the context of Web search. In the first case, we favor the retrieval of documents that are relevant to the query, whereas in the second case we aim for document diversification. Both the theoretical and the experimental analysis provide strong evidence of the potential value of query covering in diverse application scenarios.",
        "Document selection for tiered indexing in commerce search. A search engine aims to return a set of relevant documents in response to a query, while minimizing the response time. This has led to the use of a tiered index, where the search engine maintains a small cache of documents that can serve a large fraction of queries. We give a novel algorithm for the selection of documents in a tiered index for commerce search (i.e. users searching for products on the web) that effectively exploits the superior structural characteristics of commerce search queries. This is in sharp contrast to previous approaches to tiered indexing that were aimed at general web search where queries are typically unstructured. We theoretically analyze our algorithms and give performance guarantees even in worst-case scenarios. We then complement and strengthen our theoretical claims by performing exhaustive experiments on realworld commerce search data, and show that our algorithm outperforms state-of-the-art tiered indexing techniques that were developed for general web search.",
        "Coupled nominal similarity in unsupervised learning. The similarity between nominal objects is not straightforward, especially in unsupervised learning. This paper proposes coupled similarity metrics for nominal objects, which consider not only intra-coupled similarity within an attribute (i.e., value frequency distribution) but also inter-coupled similarity between attributes (i.e. feature dependency aggregation). Four metrics are designed to calculate the intercoupled similarity between two categorical values by considering their relationships with other attributes. The theoretical analysis reveals their equivalent accuracy and superior efficiency based on intersection against others, in particular for large-scale data. Substantial experiments on extensive UCI data sets verify the theoretical conclusions. In addition, experiments of clustering based on the derived dissimilarity metrics show a significant performance improvement.",
        "Theoretical Understandings of Product Embedding for E-commerce Machine Learning Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.",
        "Make It a Chorus: Knowledge- and Time-aware Item Modeling for Sequential Recommendation Traditional recommender systems mainly aim to model inherent and long-term user preference, while dynamic user demands are also of great importance. Typically, a historical consumption will have impacts on the user demands for its relational items. For instance, users tend to buy complementary items together (iPhone and Airpods) but not substitutive items (Powerbeats and Airpods), although substitutes of the bought one still cater to his/her preference. To better model the effects of history sequence, previous studies introduce the semantics of item relations to capture user demands for recommendation. However, we argue that the temporal evolution of the effects caused by different relations cannot be neglected. In the example above, user demands for headphones can be promoted after a long period when a new one is needed. To model dynamic meanings of an item in different sequence contexts, a novel method Chorus is proposed to take both item relations and corresponding temporal dynamics into consideration. Chorus aims to derive the embedding of target item in a knowledge-aware and time-aware way, where each item will get its basic representation and relation-related ones. Then, we devise temporal kernel functions to combine these representations dynamically, according to whether there are relational items in history sequence as well as the elapsed time. The enhanced target item embedding is flexible to work with various algorithms to calculate the ranking score and generate recommendations. According to extensive experiments in three real-world datasets, Chorus gains significant improvements compared to state-of-the-art baseline methods. Furthermore, the time-related parameters are highly interpretable and hence can strengthen the explainability of recommendation.",
        "Optimising topical query decomposition. Topical query decomposition (TQD) is a paradigm recently introduced in , which, given a query, returns to the user a set of queries that cover the answer set of the original query. The TQD problem was studied as a variant of the set-cover problem and solved by means of a greedy algorithm.This paper aims to strengthen the original formulation by introducing a new global objective function, and thus formalising the problem as a combinatorial optimisation one. Such a reformulation defines a common framework allowing a formal evaluation and comparison of different approaches to TQD. We apply simulated annealing, a sub-optimal metaheuristic, to the problem of topical query decomposition and we show, through a large experimentation on a data sample extracted from an actual query log, that such meta-heuristic achieves better results than the greedy algorithm.",
        "Hyper Questions: Unsupervised Targeting of a Few Experts in Crowdsourcing. Quality control is one of the major problems in crowdsourcing. One of the primary approaches to rectify this issue is to assign the same task to different workers and then aggregate their answers to obtain a reliable answer. In addition to simple aggregation approaches such as majority voting, various sophisticated probabilistic models have been proposed. However, given that most of the existing methods operate by strengthening the opinions of the majority, these models often fail when the tasks require highly specialized knowledge and the ability of a large majority of the workers is inadequate. In this paper, we focus on an important class of answer aggregation problems in which majority voting fails and propose the concept of hyper questions to devise effective aggregation methods. A hyper question is a set of single questions, and our key idea is that experts are more likely to provide correct answers to all of the single questions included in a hyper question than nonexperts. Thus, experts are more likely to reach consensus on the hyper questions than non-experts, which strengthen their influences. We incorporate the concept of hyper questions into existing answer aggregation methods. The results of our experiments conducted using both synthetic datasets and real datasets demonstrate that our simple and easily usable approach works effectively in cases where only a few experts are available.",
        "The Unreusability of Diversified Search Test Collections Traditional \"ad hoc\" test collections, typically built based on depth-100 pools, are often used a posteriori by non-contributors, i.e., research groups that did not contribute the pools. The Leave One Out (LOO) test is useful for testing whether the test collections are actually reusable: that is, whether the non-contributors can be evaluated fairly relative to the contributors' official performances. In contrast, at the recent web search result diversification tasks of TREC and NTCIR, diversity test collections have been built using shallow pools: the pool depths lie between 20 and 40. Thus it is unlikely that these diversity test collections are reusable: in fact, the organisers of these diversity tasks never claimed that they are. Nevertheless, these collections are also used a posteriori by non-contributors. In light of this, Sakai et al. [21] demonstrated by means of LOO tests that the NTCIR-9 INTENT-1 Chinese diversity test collection is not reusable, and also showed that condensed-list evaluation metrics generally provide better estimates of the noncontributors' true performances than raw evaluation metrics. This paper generalises and strengthens their findings through LOO tests with the latest TREC 2012 diversity test collection.",
        "Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval",
        "Improved Theoretical and Practical Guarantees for Chromatic Correlation Clustering. We study a natural generalization of the correlation clustering problem to graphs in which the pairwise relations between objects are categorical instead of binary. This problem was recently introduced by Bonchi et al. under the name of chromatic correlation clustering, and is motivated by many real-world applications in data-mining and social networks, including community detection, link classification, and entity de-duplication.Our main contribution is a fast and easy-to-implement constant approximation framework for the problem, which builds on a novel reduction of the problem to that of correlation clustering. This result significantly progresses the current state of knowledge for the problem, improving on a previous result that only guaranteed linear approximation in the input size. We complement the above result by developing a linear programming-based algorithm that achieves an improved approximation ratio of 4. Although this algorithm cannot be considered to be practical, it further extends our theoretical understanding of chromatic correlation clustering. We also present a fast heuristic algorithm that is motivated by real-life scenarios in which there is a groundtruth clustering that is obscured by noisy observations. We test our algorithms on both synthetic and real datasets, like social networks data. Our experiments reinforce the theoretical findings by demonstrating that our algorithms generally outperform previous approaches, both in terms of solution cost and reconstruction of an underlying ground-truth clustering.",
        "RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. To address the sparsity and cold start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve recommendation performance. This paper considers the knowledge graph as the source of side information. To address the limitations of existing embeddingbased and path-based methods for knowledge-graph-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the knowledge graph into recommender systems. Similar to actual ripples propagating on the water, RippleNet stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user's potential interests along links in the knowledge graph. The multiple \"ripples\" activated by a user's historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item, which could be used for predicting the final clicking probability. Through extensive experiments on real-world datasets, we demonstrate that RippleNet achieves substantial gains in a variety of scenarios, including movie, book and news recommendation, over several state-of-the-art baselines."
    ],
    "fake news detection": [
        "Fake News Detection in Social Networks via Crowd Signals. Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, Detective, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.",
        "Multilingual Detection of Fake News Spreaders via Sparse Matrix Factorization Fake news is an emerging problem in online news and social media. Efficient detection of fake news spreaders and spurious accounts across multiple languages is becoming an interesting research problem, and is the key focus of this paper. Our proposed solution to PAN 2020 fake news spreaders challenge models the accounts responsible for spreading the fake news by accounting for different types of textual features, decomposed via sparse matrix factorization, to obtain easy-to-learn-from, compact representations, including the information from multiple languages. The key contribution of this work is the exploration of how powerful and scalable matrix factorization-based classification can be in a multilingual setting, where the learner is presented with the data from multiple languages simultaneously. Finally, we explore the joint latent space, where patterns from individual languages are maintained. The proposed approach scored second on the 2020 PAN shared task for identification of fake news spreaders.",
        "EvolutionTeam at CLEF2020 - CheckThat! lab : Integration of Linguistic and Sentimental Features in a Fake News Detection Approach Misinformation is a growing problem around the web. The spread of such a phenomenon may impact public opinions. Hence fake news detection is indispensable. The first step for fact-checking is the selection of check worthy tweets for a certain topic, then ranking sentences from related web pages according to the carried evidence. Afterward, the claim will be verified according to evident sentences. At CLEF2020-CheckThat! lab, three tasks run in Arabic, namely check-worthiness on tweets, evidence retrieval, and claim verification that corresponds respectively to task1, task3, and task4. We participated in the three tasks. We integrated manual sentiment features as well as named entities to detect fake news. The integration of sentiment information in the first task caused result degradation since there may be an overlap between check worthy and not check worthy tweets. For the second task, we explored the effect of sentiment presence and we used cosine similarity as a similarity measure between the claim and a specific snippet. The third task is a classification task based on sentiment and linguistic features to compute the overlap and the contradiction between the claim and the detected check worthy sentences. The results of task1 and task3 leave large room for improvement, whereas the results of task 4 are promising since our system reached 0.55 of F1-measure.",
        "Assembly of Polarity, Emotion and User Statistics for Detection of Fake Profiles The explosive growth of fake news on social networks has aroused great interest from researchers in different disciplines. To achieve efficient and effective detection of fake news requires scientific contributions from various disciplines, such as computational linguistics, artificial intelligence, and sociology. Here we illustrate how polarity, emotion, and user statistics can be used to detect fake profiles on Twitter's social network. This paper presents a novel strategy for the characterization of the Twitter profile based on the generation of an assembly of polarity, emotion, and user statistics characteristics that serve as input to a set of classifiers. The results are part of our participation in the PAN 2020 in the CLEF in the task of Profiling Fake News Spreaders on Twitter.",
        "Fake News: Fundamental Theories, Detection Strategies and Challenges. The explosive growth of fake news and its erosion to democracy, justice, and public trust increased the demand for fake news detection. As an interdisciplinary topic, the study of fake news encourages a concerted effort of experts in computer and information science, political science, journalism, social science, psychology, and economics. A comprehensive framework to systematically understand and detect fake news is necessary to attract and unite researchers in related areas to conduct research on fake news. This tutorial aims to clearly present (1) fake news research, its challenges, and research directions; (2) a comparison between fake news and other related concepts (e.g., rumours); (3) the fundamental theories developed across various disciplines that facilitate interdisciplinary research; (4) various detection strategies unified under a comprehensive framework for fake news detection; and (5) the state-of-the-art datasets, patterns, and models. We present fake news detection from various perspectives, which involve news content and information in social networks, and broadly adopt techniques in data mining, machine learning, natural language processing, information retrieval and social search. Facing the upcoming 2020 U.S. presidential election, challenges for automatic, effective and efficient fake news detection are also clarified in this tutorial.",
        "Sadness and Fear: Classification of Fake NewsSpreaders Content on Twitter The vast amount of accurate and inaccurate information circulating on the internet requires computational methodologies to detect low-quality content. This kind of content often constitutes fake news, as in the PAN @ CLEF 2020 competition Profiling Fake News Spreaders on Twitter. This competition asks for systems that identify possible fake news spreaders on social media as a first step to prevent fake news from being propagated among online users. In this paper, the methodology used for this classification task is reported. Preprocessing of the data and the features extracted to classify fake news spreaders is explained. A regression-as-classification approach that enables the representation of being a fake news spreader as a gradable one is proposed. The performance (accuracy) on the training and the test set with the different sets of features is reported.",
        "FakeFinder: Twitter Fake News Detection on Mobile",
        "RMIT at PAN-CLEF 2020: Profiling Fake News Spreaders on Twitter Automatic detection of fake news in social media has become a prominent research topic due to its widespread, adverse effect on not only the society and public health but also on economy and democracy. The computational approaches towards automatic detection of fake news span from analyzing the source credibility, user credibility, as well as social network structure and the news content. However, the studies on user credibility in this context have largely focused on the frequency and times of engaging in a fake news propagation rather than profiling users based on the content of their tweets. In this paper, we approach this challenge through extracting linguistic and sentiment features from users' tweet feed as well as retrieving the presence of emojis, hashtags and political bias in their tweets. These features are then used to classify users into spreaders or non-spreaders of fake news. Our proposed approach achieves 72% accuracy, being among the top-4 results obtained by systems for the task in the English language.",
        "Beyond News Contents: The Role of Social Context for Fake News Detection",
        "Joint Estimation of User And Publisher Credibility for Fake News Detection",
        "Detecting Fake News Spreaders on Twitter Using Universal Sentence Encoder In the present attempt, we have developed a framework to detect the fake news spreaders on twitter by utilizing their tweets. Here, we have employed the pre-trained sentence embedding of Google and fed this embedding to a Long Short Term Memory (LSTM) based deep learning framework. Finally, the embedding is passed through an attention layer and predicts whether an author is prone to spread fake news or not. We have built models for two languages-English and Spanish. We have achieved 72% accuracy in this fake news spreader detection task.",
        "Challenges of computational verification in social multimedia. Fake or misleading multimedia content and its distribution through social networks such as Twitter constitutes an increasingly important and challenging problem, especially in the context of emergencies and critical situations. In this paper, the aim is to explore the challenges involved in applying a computational verification framework to automatically classify tweets with unreliable media content as fake or real. We created a data corpus of tweets around big events focusing on the ones linking to images (fake or real) of which the reliability could be verified by independent online sources. Extracting content and user features for each tweet, we explored the fake prediction accuracy performance using each set of features separately and in combination. We considered three approaches for evaluating the performance of the classifier, ranging from the use of standard cross-validation, to independent groups of tweets and to cross-event training. The obtained results included a 81% for tweet features and 75% for user ones in the case of cross-validation. When using different events for training and testing, the accuracy is much lower (up to %58) demonstrating that the generalization of the predictor is a very challenging issue.",
        "Fake News Detection: An Interdisciplinary Research",
        "FACE-KEG: Fact Checking Explained using KnowledgE Graphs In recent years, a plethora of fact checking and fact verification techniques have been developed to detect the veracity or factuality of online information text for various applications. However, limited efforts have been undertaken to understand the interpretability of such veracity detection, i.e. explaining why a particular piece of text is factually correct or incorrect. In this work, we seek to bridge this gap by proposing a technique, FACE-KEG, to automatically perform explainable fact checking. Given an input fact or claim, our proposed model constructs a relevant knowledge graph for it from a large-scale structured knowledge base. This graph is encoded via a novel graph transforming encoder. Our model also simultaneously retrieves and encodes relevant textual context about the input text from the knowledge base. FACE-KEG then jointly exploits both the concept-relationship structure of the knowledge graph as well as semantic contextual cues in order to (i) detect the veracity of an input fact, and (ii) generate a human-comprehensible natural language explanation justifying the fact's veracity. We conduct extensive experiments on three large-scale datasets, and demonstrate the effectiveness of FACE-KEG while performing fact checking. Automatic and human evaluations further show that FACE-KEG significantly outperforms competitive baselines in learning concise, coherent and informative explanations for the input facts.",
        "Mining Dual Emotion for Fake News Detection",
        "Detecting Fake News Spreaders in Social Networks via Linguistic and Personality Features This paper addresses the problem of automatically detecting fake news spreaders in social networks such as Twitter. We model the problem as a binary classification task and consider several groups of features, including writing style, word and char n-grams, BERT semantic embedding, and sentiment analysis, which are computed from a set of tweets each user authored. Our proposed approach is evaluated on the dataset made available by the PAN at CLEF 2020 shared task on profiling fake news spreader, which provided labeled data in both English and Spanish. Experimental results show that we can detect fake news spreaders with an accuracy of 0.73 in English and 0.77 in Spanish when our approach is evaluated with 10-fold cross-validation on the provided training set, and with an accuracy of 0.71 in English and 0.76 in Spanish when the model is trained on the whole training set and tested on the provided test set. We also investigate the role of psycho-linguistic (LIWC) and personality features to detect fake news spreaders and find out that personality features do have a significant impact in user sharing behavior, achieving an accuracy of 0.72 in English and 0.80 in Spanish when evaluated with 10-fold cross-validation on the provided training set.",
        "LSACoNet: A Combination of Lexical and Conceptual Features for Analysis of Fake News Spreaders on Twitter Fake news detection on social medial has attracted a huge body of research as one of the most important tasks of social analysis in recent years. In this task, given a Twitter feed, the goal is to identify fake/real news authors or spreaders. We assume fake news authors mostly like to play with the semantic aspect of news rather than trying to add specific changes to their styles. However, making a change into the semantic aspect of news can cause unwanted changes in style. We hypothesize, by relying on news content, a combination of semantic and coarse-grained features may lead us to common information about the author's style while reviewing the conceptual aspect of author documents. In this paper, we propose the LSACoNet representation using a fully connected neural network (FCNN) classifier that combines different levels of document representation to investigate this hypothesis. Experimental results presented in this paper showed that a combination of representations plays an important role in identifying fake/real news spreaders. Finally, we achieved accuracies of 72.5% and 74.5% in the English and Spanish test datasets, respectively, using presented LSACoNet representation and FCNN classifier.",
        "ULMFiT for Twitter Fake News Spreader Profiling 21 st century is named as the age of information technologies. Social applications such as Facebook, Twitter, Instagram, etc. have become a quick and huge media for spreading news over the internet. At the same time, the ability for the wide spread of news that is of low quality with intentionally false information is creating havocs causing damage to the extent of losing lives in the society. Such news is termed as fake news and detecting the fake news spreader is drawing more attention these days as fake news can manipulate communities' minds and also social trust. Until date, many studies have been done in this area and most of them are based on Machine Learning and Deep Learning approaches. In this paper, we have proposed a Universal Language Model Fine-Tuning model based on Transfer Learning to detect potential fake news spreaders on Twitter. The proposed model collects wiki text data to train the Language Model to capture general features of the language and this knowledge is transferred to build a classifier using fake news spreaders dataset provided by PAN 2020 to identify the fake news spreader. The results obtained on PAN 2020 fake news dataset are encouraging.",
        "Detecting fake news stories via multimodal analysis",
        "Convolutional neural network with margin loss for fake news detection",
        "MVAE: Multimodal Variational Autoencoder for Fake News Detection",
        "CSI: A Hybrid Deep Model for Fake News Detection. e topic of fake news has drawn a ention both from the public and the academic communities. Such misinformation has the potential of a ecting public opinion, providing an opportunity for malicious parties to manipulate the outcomes of public events such as elections. Because such high stakes are at play, automatically detecting fake news is an important, yet challenging problem that is not yet well understood. Nevertheless, there are three generally agreed upon characteristics of fake news: the text of an article, the user response it receives, and the source users promoting it. Existing work has largely focused on tailoring solutions to one particular characteristic which has limited their success and generality.In this work, we propose a model that combines all three characteristics for a more accurate and automated prediction. Speci cally, we incorporate the behavior of both parties, users and articles, and the group behavior of users who propagate fake news. Motivated by the three characteristics, we propose a model called CSI which is composed of three modules: Capture, Score, and Integrate. e rst module is based on the response and text; it uses a Recurrent Neural Network to capture the temporal pa ern of user activity on a given article. e second module learns the source characteristic based on the behavior of users, and the two are integrated with the third module to classify an article as fake or not. Experimental analysis on real-world data demonstrates that CSI achieves higher accuracy than existing models, and extracts meaningful latent representations of both users and articles.",
        "Tracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate. When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classication of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classication accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.",
        "Fake News Spreader Detection on Twitter using Character N-Grams The authors of fake news often use facts from verified news sources and mix them with misinformation to create confusion and provoke unrest among the readers. The spread of fake news can thereby have serious implications on our society. They can sway political elections, push down the stock price or crush reputations of corporations or public figures. Several websites have taken on the mission of checking rumors and allegations, but are often not fast enough to check the content of all the news being disseminated. Especially social media websites have offered an easy platform for the fast propagation of information. Towards limiting fake news from being propagated among social media users, the task of this year's PAN 2020 challenge lays the focus on the fake news spreaders. The aim of the task is to determine whether it is possible to discriminate authors that have shared fake news in the past from those that have never done it. In this notebook, we describe our profiling system for the fake news detection task on Twitter. For this, we conduct different feature extraction techniques and learning experiments from a multilingual perspective, namely English and Spanish. Our final submitted systems use character n-grams as features in combination with a linear SVM for English and Logistic Regression for the Spanish language. Our submitted models achieve an overall accuracy of 73% and 79% on the English and Spanish official test set, respectively. Our experiments show that it is difficult to differentiate solidly fake news spreaders on Twitter from users who share credible information leaving room for further investigations. Our model ranked 3rd out of 72 competitors.",
        "dEFEND: A System for Explainable Fake News Detection",
        "Combining Neural, Statistical and External Features for Fake News Stance Identification. Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and hand crafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.",
        "FANG: Leveraging Social Context for Fake News Detection Using Graph Representation",
        "Analyzing User Profiles for Detection of Fake News Spreaders on Twitter The massive spread of digital information to which our society is subjected nowadays has led to a great amount of false or extremely biased information being shared and consumed by Internet users every day. Disinformation, including misleading and even false information, is a major issue for our current society. The impact of fake news on politics, economy and even public health is yet to be specified. Internet users must face a high amount of false information in digital media such as rumours, fake news, and extremely biased news. Given the crucial role that the spread of fake news plays in our current society, it is becoming essential to design tools to automatically verify the veracity of online information. In order to address this issue, the PAN@CLEF 2020 competition has proposed a task focused on the detection of fake news spreaders on Twitter. In this paper, we offer a detailed description of the system developed for this competition. Our system relies on psychological features for modelling the behaviour of users.",
        "Truth be Told: Fake News Detection Using User Reactions on Reddit"
    ],
    " multimedia retrieval ": [
        "Multimedia Document Presentation, Information Extraction, and Document Formation in MINOS: A Model and a System MINOS is an object-oriented multimedia information system that provides integrated facilities for creating and managing complex multimedia objects. In this paper the model for multimedia documents supported by MINOS and its implementation is described. Described in particular are functions provided in MINOS that exploit the capabilities of a modern workstation equipped with image and voice input-output devices to accomplish an active multimedia document presentation and browsing within documents. These functions are powerful enough to support a variety of office applications. Also described are functions provided for the extraction of information from multimedia documents that exist in a large repository of information (multimedia document archiver) and functions that select and transform this information. Facilities for information sharing among objects of the archiver are described; an interactive multimedia editor that is used for the extraction and interactive creation of new information is outlined; finally, a multimedia document formatter that is used to synthesize a new multimedia document from extracted and interactively generated information is presented. This prototype system runs on a SUN-3 workstation running UNIX'\". An Instavox, directly addressable, analog device is used to store voice segments.",
        "Web search engine multimedia functionality. Web search engines are beginning to offer access to multimedia searching, including audio, video and image searching. In this paper we report findings from a study examining the state of multimedia search functionality on major general and specialized Web search engines. We investigated 102 Web search engines to examine: (1) how many Web search engines offer multimedia searching, (2) the type of multimedia search functionality and methods offered, such as ''query by example'', and (3) the supports for personalization or customization which are accessible as advanced search. Findings include:(1) few major Web search engines offer multimedia searching and (2) multimedia Web search functionality is generally limited. Our findings show that despite the increasing level of interest in multimedia Web search, those few Web search engines offering multimedia Web search, provide limited multimedia search functionality. Keywords are still the only means of multimedia retrieval, while other methods such as ''query by example'' are offered by less than 1% of Web search engines examined.",
        "Classifier-specific intermediate representation for multimedia tasks",
        "Automatic query-time generation of retrieval expert coefficients for multimedia retrieval. Content-based Multimedia Information Retrieval can be defined as the task of matching a multi-modal information need against various components of a multimedia corpus and retrieving relevant elements. Generally the matching and retrieval takes place across multiple 'features' which can either be visual or audio, or can be high-level or low-level, and each of which can be seen to be an independent retrieval expert. The task of answering a query can thus be formulated as a combination of experts data fusion problem. Depending on the query, each expert may perform differently and so retrieval coefficients can be used to weight each expert to increase overall performance. Previous approaches to expert coefficient generation have included query-independent coefficients, identification of query-classes and machine learning methods, to name a few .The approach I propose is different, as it seeks to dynamically create expert coefficients which are query-dependent. This approach is based upon earlier experiments where an initial correlation was observed between the score distribution of a retrieval expert, and its relative performance when compared against other experts for that query. I have created a basic method which leverages these observations to create query-time coefficients which achieve comparable performance to oracle-determined query-independent weights, for the experts and collections used in the aforementioned experiment. Previous research which examined score distribution did so with respect to relevance, whereas this work seeks to compare expert scores for a given query to determine relative performance.In my work I aim to explore this correlation by eliminating potential bias from the data collections, the retrieval experts and the queries used in experiments to obtain more robust observations. Using and extending previous investigations into data fusion [2], I will explore where data fusion succeeds in multimedia retrieval, and where it does not. I then aim to refine and extend my existing techniques for automatic coefficient generation to incorporate the new observations, so as to improve performance. Finally I will combine this approach with existing data fusion methods, such as query-class coefficients, with each approach complimenting the other to achieve further performance improvements.",
        "Interlinking Multimedia: How to Apply Linked Data Principles to Multimedia Fragments In this paper, we introduce interlinking multimedia (iM), a pragmatic way to apply the linked data principles to fragments of multimedia items. We report on use cases showing the need for retrieving and describing multimedia fragments. We then introduce the principles for interlinking multimedia in the Web of Data, discussing potential solutions which sometimes highlight controversial debates regarding what the various representations of a Web resource span. We finally present methods for enabling a widespread use of interlinking multimedia.",
        "Beyond audio and video retrieval: towards multimedia summarization",
        "Proceedings of the 1st International Workshop on Environnmental Multimedia Retrieval co-located with ACM International Conference on Multimedia Retrieval, EMR@ICMR 2014, Glasgow, UK, April 1, 2014",
        "G. Stamou and S. Kollias, Editors, Multimedia Content and the Semantic Web: Methods, Standards and Tools, John Wiley & Sons, Ltd., West Sussex, England (2005) In this book, Stamou and Kollias share interesting insights into the Semantic Web by reviewing multimedia content, organizational methods, and its standards. The editors have assembled many interesting authors ad hoc to examine topics such as structured identification in audiovisual documents, object-based video indexing, and speech and image processing methods.This compilation of authors is one of the first books to introduce multimedia content technology and its tools. The book can be very useful to researchers and developers in the semantic multimedia domain. Analysis of different methodologies can provide ideas to experienced researchers in academia. Various multimedia content organization tools and applications can make this book a useful guide for developers and business organizations.The book is organized into three parts. Part one and two cover ideas in organizing the Web and multimedia content analysis. Part three introduces multimedia content management tools and semantic web applications. The editors have done a superb job of integrating emerging ideas and grouping related papers into meaningful clusters.In Part one, Chapter 1, MPEG-7 and MPEG-21 standards are introduced as a layout for representing multimedia and the Semantic Web. MPEG-21 was standardized with users in mind, since its technology relies on description tools. Chapter 2, Ontology Representation and Querying for Realizing Semantics Driven Applications, discusses the representation in ontology, querying methods and implementation. Chapter 3 describes the 'what' and 'how' of MPEG-7 standard; the web ontology language to define its semantics; and ways the ontology can be used on the semantic web. The last chapter in Part one is the only chapter authored by the two publishers of the book. It applies fuzzy logic to knowledge-based systems for multimedia analysis and retrieval.Part two focuses on fundamental multimedia content analysis technologies in five respective chapters. Chapter 5, Structure Identification in an Audiovisual Documents, presents gathered work on shot-segmentation and its tools. In Chapter 6, entitled Object-based Indexing of Multimedia Contents, the authors examine heterogeneous data as objects and develop an overview of the trends in object based indexing. Chapter 7 on automatic extraction and analysis of visual objects information reviews main concepts of the semantic class model and an algorithm for object detection in images. Chapter 8 introduces a hybrid framework for modeling concepts and context using MPEG-7 standards. Chapter 9 is dedicated to graphical models and multimedia understanding in machine learning.Five papers on multimedia content managements systems and semantic web applications are grouped in Part three. The authors introduce cases to show how the technology can be adapted to related fields of research and business. Chapters 10, 12, and 13 focus on multimedia content indexing and retrieval and technology specific to speech, texts and image processing methods, Chapters 10, 11, and 14 illustrate cases about systems and applications in academia and business.Ontology and multimedia content management systems mentioned in this book could serve as part of the digital pathway to the semantic web. However, creating multimedia descriptors and difficulties in standardization are considered persistent issues in organizing multimedia contents. In that sense, introducing more real cases to applied fields would have shed more optimism for readers.",
        "Multimedia retrieval in social networks for photo book creation",
        "Effective music tagging through advanced statistical modeling. Music information retrieval (MIR) holds great promise as a technology for managing large music archives. One of the key components of MIR that has been actively researched into is music tagging. While significant progress has been achieved, most of the existing systems still adopt a simple classification approach, and apply machine learning classifiers directly on low level acoustic features. Consequently, they suffer the shortcomings of (1) poor accuracy, (2) lack of comprehensive evaluation results and the associated analysis based on large scale datasets, and (3) incomplete content representation, arising from the lack of multimodal and temporal information integration.In this paper, we introduce a novel system called MMTagger that effectively integrates both multimodal and temporal information in the representation of music signal. The carefully designed multilayer architecture of the proposed classification framework seamlessly combines Multiple Gaussian Mixture Models (GMMs) and Support Vector Machine (SVM) into a single framework. The structure preserves more discriminative information, leading to more accurate and robust tagging. Experiment results obtained with two large music collections highlight the various advantages of our multilayer framework over state of the art techniques.",
        "Music Retrieval and Recommendation: A Tutorial Overview. In this tutorial, we give an introduction to the field of and state of the art in music information retrieval (MIR). The tutorial particularly spotlights the question of music similarity, which is an essential aspect in music retrieval and recommendation. Three factors play a central role in MIR research: (1) the music content, i.e., the audio signal itself, (2) the music context, i.e., metadata in the widest sense, and (3) the listeners and their contexts, manifested in user-music interaction traces. We review approaches that extract features from all three data sources and combinations thereof and show how these features can be used for (large-scale) music indexing, music description, music similarity measurement, and recommendation. These methods are further showcased in a number of popular music applications, such as automatic playlist generation and personalized radio stationing, location-aware music recommendation, music search engines, and intelligent browsing interfaces. Additionally, related topics such as music identification, automatic music accompaniment and score following, and search and retrieval in the music production domain are discussed.",
        "Jointly exploiting visual and non-visual information for event-related social media retrieval",
        "ObjectSense: a scalable multi-objects recognition system based on partial-duplicate image retrieval",
        "Unlocking the semantics of multimedia presentations in the web with the multimedia metadata ontology. The semantics of rich multimedia presentations in the web such as SMIL, SVG, and Flash cannot or only to a very limited extend be understood by search engines today. This hampers the retrieval of such presentations and makes their archival and management a difficult task. Existing metadata models and metadata standards are either conceptually too narrow, focus on a specific media type only, cannot be used and combined together, or are not practically applicable for the semantic description of rich multimedia presentations.In this paper, we propose the Multimedia Metadata Ontology (M3O) for annotating rich, structured multimedia presentations. The M3O provides a generic modeling framework for representing sophisticated multimedia metadata. It allows for integrating the features provided by the existing metadata models and metadata standards. Our approach bases on Semantic Web technologies and can be easily integrated with multimedia formats such as the W3C standards SMIL and SVG. With the M3O, we unlock the semantics of rich multimedia presentations in the web by making the semantics machine-readable and machine-understandable. The M3O is used with our SemanticMM4U framework for the multi-channel generation of semantically-rich multimedia presentations.",
        "Active learning in multimedia annotation and retrieval: A survey Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation.",
        "Interactive Search in Video & Lifelogging Repositories. Due to increasing possibilities to create digital video, we are facing the emergence of large video archives that are made accessible either online or offline. Though a lot of research has been spent on video retrieval tools and methods, which allow for automatic search in videos, still the performance of automatic video retrieval is far from optimal.At the same time, the organization of personal data is receiving increasing research attention due to the challenges that are faced in gathering, enriching, searching and visualizing this data. Given the increasing quantities of personal data being gathered by individuals, the concept of a heterogeneous personal digital libraries of rich multimedia and sensory content for every individual is becoming a reality.Despite the differences between video archives and personal lifelogging libraries, we are facing very similar challenges when accessing these multimedia repositories. For example, users will struggle to find the information they are looking for in either collection if they are not able to formulate their search needs through a query. In this tutorial we discussed (i) proposed solutions for improved video & lifelog content navigation, (ii) typical interaction of content-based querying features, and (iii) advanced content visualization methods. Moreover, we discussed and demonstrate interactive video & lifelog search systems and ways to evaluate their performance. MOTIVATIONThis tutorial addressed two separate, yet similar challenges, namely interactive search in video and lifelogging repositories. In Section 1.1, we first introduce current challenges in the video retrieval domain. Section 1.2 then introduces open research challenges when dealing with lifelogging material. Interactive Video SearchOver the last two decades, there has been a lot of research on content-based video retrieval to solve the problem Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).CHIIR '17 March 07-11, 2017, Oslo, Norway of finding the proper scene of interest in a large video archive (e.g., ). Video retrieval tools use content-based indexing methods to perform automatic annotation with content descriptors for color, texture, shape, and semantic concepts. The extracted information is used to provide retrieval functions through different querying modes : query-by-text, query-by-example image (or example clip), query-by-sketch, and by query-by-filtering (e.g. through semantic concepts). The user first formulates a query, initiates the retrieval engine that looks for matching video segments, and then browses the results, which are typically lists of keyframes extracted from the result segments.Retrieval approaches commonly use interactive relevance feedback methods to keep the human in the loop after the query and to learn which results are relevant in order to adapt the search in another iteration to the user's needs ([3, 12, 24]). However, the actual search process with video retrieval tools is mostly automatic and a black box for the user. Interaction is limited to the query-formulation and results-browsing phases. Research on video retrieval mainly focuses on improving the performance of the retrieval engine (i.e., the querying itself), with less focus on the user and her interaction with the system .There are well-known issues with video retrieval applications. First, there is the usability gap. A user is often not able to express her needs and thoughts through text, a problem that is already apparent for an image (\"An image is worth a thousand words.\") but much worse for a video segment consisting of many images. In fact, most video retrieval tools operate on images (i.e., keyframes of shots) and provide image query features. Additionally, the issue of polysemy is a challenging problem in text-based search, which can only be partially solved through relevance feedback. Similarly, an automatic retrieval tool cannot easily determine the user-dependent relative importance (i.e., weight) of a query term and hence often returns too many irrelevant results. When considering the query-by-example approach, it turns out that users rarely have a good example image or example clip at hand, which is similar to the target scene they are looking for. The query-by-sketch approach is also not very convenient for users, because most users typically cannot draw a sufficiently good sketch for a scene they want to find, although they would immediately recognize it when they see it. Query-by-filtering (e.g., for a semantic concept like 'car ', for example) in a large video collection is often also not very helpful for a user, since the returned result list is typically way too long and the confidence of concept detection is still not good enough. 421",
        "A Spatial Match Representation Scheme for Indexing and Querying in Iconic Image Databases A. In multimedia information retrieval applications, contentbased image retrieval is essential for retrieving relevant mnltimedia documents. The purpose of our paper is to provide both effective representation and efficient retrieval of images when a pixel-level original image is antomaticahy or mannally transformed into its iconic image containing meaningful graphic descriptions, called icon objects. For this, we propose a new spatial match representation scheme to describe spatial relationships between icon objects preciseIy by using accurate positional operators as well as by expressing objects as rectangles, rather than pointer. In order to accelerate image searching, we also design an efficient retrieval method using a signature file technique. Finally, we show from our experiments that our representation scheme achieves better retrieval effectiveness than the 9-DLT scheme. Recently, much attention has been paid to Multimedia Information Retrieval(MIR) because we have had so many applications that should be supported by handling muhimedia data, such as text, image, video, audio, and animation.The applications include digital libraries, advertisements, medical information, remote sensing and astronomy, cartography, digita newspapers, and architectural design. So far, text attributes in multimedia documents have mainly been used for supporting queries by content. The approach using text content(e.g., captions and keywords) has a couple of problems, First, the original keywords do not allow for unanticipated searching. The other problem is that the caption is not adequate to describe the layout, sketch, and shape of the image. Therefore, in order to support MIR.applications effectively, content-based image retrieval is essential because it plays an important roIe in retrieving relevant multimedia documents. 169-.-.32-A r ---,F. ,y 9",
        "OCTOPUS: aggressive search of multi-modality data using multifaceted knowledge base. An important trend in Web information processing is the support of multimedia retrieval. However, the most prevailing paradigm for multimedia retrieval, content-based retrieval (CBR), is a rather conservative one whose performance depends on a set of specifically defined low-level features and a carefully chosen sample object. In this paper, an aggressive search mechanism called Octopus is proposed which addresses the retrieval of multimodality data using multifaceted knowledge. In particular, Octopus promotes a novel scenario in which the user supplies seed objects of arbitrary modality as the hint of his information need, and receives a set of multi-modality objects satisfying his need. The foundation of Octopus is a multifaceted knowledge base constructed on a layered graph model (LGM), which describes the relevance between media objects from various perspectives. Link analysis based retrieval algorithm is proposed based on the LGM. A unique relevance feedback technique is developed to update the knowledge base by learning from user behaviors, and to enhance the retrieval performance in a progressive manner. A prototype implementing the proposed approach has been developed to demonstrate its feasibility and capability through illustrative examples.",
        "PythiaSearch: a multiple search strategy-supportive multimedia retrieval system",
        "Experiments with Segmentation Strategies for Passage Retrieval in Audio-Visual Documents",
        "Feature propagation on image webs for enhanced image retrieval",
        "When textual and visual information join forces for multimedia retrieval",
        "Investigating the document structure as a source of evidence for multimedia fragment retrieval a b s t r a c tMultimedia objects can be retrieved using their context that can be for instance the text surrounding them in documents. This text may be either near or far from the searched objects. Our goal in this paper is to study the impact, in term of effectiveness, of text position relatively to searched objects. The multimedia objects we consider are described in structured documents such as XML ones. The document structure is therefore exploited to provide this text position in documents. Although structural information has been shown to be an effective source of evidence in textual information retrieval, only a few works investigated its interest in multimedia retrieval. More precisely, the task we are interested in this paper is to retrieve multimedia fragments (i.e. XML elements having at least one multimedia object). Our general approach is built on two steps: we first retrieve XML elements containing multimedia objects, and we then explore the surrounding information to retrieve relevant multimedia fragments. In both cases, we study the impact of the surrounding information using the documents structure.Our work is carried out on images, but it can be extended to any other media, since the physical content of multimedia objects is not used. We conducted several experiments in the context of the Multimedia track of the INEX evaluation campaign. Results showed that structural evidences are of high interest to tune the importance of textual context for multimedia retrieval. Moreover, the proposed approach outperforms state of the art approaches.",
        "A RELIEF-based modality weighting approach for multimodal information retrieval",
        "Sketch-Based Image Retrieval via Adaptive Weighting",
        "Fast nonrigid 3D retrieval using modal space transform",
        "Unsupervised Visual and Textual Information Fusion in CBMIR Using Graph-Based Methods Multimedia collections are more than ever growing in size and diversity. Effective multimedia retrieval systems are thus critical to access these datasets from the end-user perspective and in a scalable way. We are interested in repositories of image/text multimedia objects and we study multimodal information fusion techniques in the context of content-based multimedia information retrieval. We focus on graphbased methods, which have proven to provide state-of-the-art performances. We particularly examine two such methods: cross-media similarities and random-walk-based scores. From a theoretical viewpoint, we propose a unifying graph-based framework, which encompasses the two aforementioned approaches. Our proposal allows us to highlight the core features one should consider when using a graph-based technique for the combination of visual and textual information. We compare cross-media and random-walk-based results using three different real-world datasets. From a practical standpoint, our extended empirical analyses allow us to provide insights and guidelines about the use of graph-based methods for multimodal information fusion in content-based multimedia information retrieval. Additional Key Words and Phrases: Content-based multimedia information retrieval, information fusion, graph-based methods, cross-media similarity, random walk, Visual reranking ACM Reference Format:Julien Ah-Pine, Gabriela Csurka, and St\u00e9phane Clinchant. 2015. Unsupervised visual and textual information fusion in CBMIR using graph-based methods.",
        "DOLORES: A System for Logic-Based Retrieval of Multimedia Objects. We describe the design and implementation of a system for logic-baaed multimedia retrieval. As highlevel logic for retrieval of hypermedia documents, we have developed a probabilistic object-oriented logic (POOL) which supports aggregated objects, different kinds of propositions (terms, classifications and attributes) and even rules as being contained in objects.Based on a probabilistic four-valued logic, POOL uses an implicit open world assumption, allows for closed world assumptions and is able to deal with inconsistent knowledge. POOL programs and queries are translated into probabilistic Datalog programs which can be interpreted by the HySpirit inference engine. For storing the multimedia data, we have developed a new basic IR system which yields physical data abstraction.The overall architecture and the flexibility of each layer supports logic-based methods for multimedia information retrieval.",
        "Proceedings of the 1st International Workshop on Social Multimedia and Storytelling co-located with ACM International Conference on Multimedia Retrieval (ICMR 2014), Glasgow, Scotland, UK, April 1, 2014",
        "3D Visualization Technique for Retrieval of Documents and Images. This paper describes a new method of multimedia information retrieval which shows multimedia information on a screen in a multi-directional way, using perspective, during the process of recursive interactive retrieval. This method supports a graphical, sensitive and individually tailored interaction between a user and multimedia data. The original work here in is the support of interaction between a user and multimedia data in a way that shows as many as multimedia data in a multi-directional manner on a screen. This work does not involve analytical techniques, although much current research into multimedia information retrieval focuses on processing and handling multimedia data. The prototype system INMUL(Interactive Multidirectional Information Displaying System) is now being implemented.",
        "A semantic model for cross-modal and multi-modal retrieval",
        "Multimedia information retrieval. :This tutorial is concerned with creating the best possible multimedia search experience. The intriguing bit here is that the query itself can be a multimedia excerpt: For example, when you walk around in an unknown place and stumble across an interesting landmark, would it not be great if you could just take a picture with your mobile phone and send it to a service that finds a similar picture in a database and tells you more about the building -and about its significance for that matter? The ideas for this type of search have been around for a decade, but this tutorial will look at recent successes and take stock of the state-of-the-art. It examines the full matrix of a variety of query modes versus document types. How do you retrieve a music piece by humming? What if you want to find news video clips on forest fires using a still image? The tutorial discusses underlying techniques and common approaches to facilitate multimedia search engines: metadata driven search; piggy-back text search where automated processes create text surrogates for multimedia; automated image annotation; content-based search. The latter is studied in more depth looking at features and distances, and how to effectively combine them for efficient retrieval, to a point where the participants have the ingredients and recipe in their hands for building their own visual search engines. Supporting users in their resource discovery mission when hunting for multimedia material is not a technological indexing problem alone. We will briefly look at interactive ways of engaging with repositories through browsing and relevance feedback, roping in geographical context, and providing visual summaries for videos. The tutorial emphasises state-of-the-art research in the area of multimedia information retrieval, which gives an indication of the research and development trends and, thereby, a glimpse of the future world.",
        "ADAM: a system for jointly providing ir and database queries in large-scale multimedia retrieval. The tremendous increase of multimedia data in recent years has heightened the need for systems that not only allow to search with keywords, but that also support content-based retrieval in order to effectively and efficiently query large collections. In this paper, we introduce ADAM, a system that is able to store and retrieve multimedia objects by seamlessly combining aspects from databases and information retrieval. ADAM is able to work with both structured and unstructured data and to jointly provide Boolean retrieval and similarity search. To efficiently handle large volumes of data it makes use of a signature-based indexing and the distribution of the collection to multiple shards that are queried in a MapReduce style. We present ADAM in the setting of a sketch-based image retrieval application using the ImageNet collection containing 14 million images.",
        "A System Framework for Concept- and Credibility-Based Multimedia Retrieval"
    ],
    "processing natural language for information retrieval": [
        "Natural language processing for information retrieval: the time is ripe (again)",
        "Assessing the influence of personal preferences on the choice of vocabulary for natural language generation a b s t r a c tReferring expression generation is the part of natural language generation that decides how to refer to the entities appearing in an automatically generated text. Lexicalization is the part of this process which involves the choice of appropriate vocabulary or expressions to transform the conceptual content of a referring expression into the corresponding text in natural language. This problem presents an important challenge when we have enough knowledge to allow more than one alternative. In those cases, we need some heuristics to decide which alternatives are more appropriate in a given situation. Whereas most work on natural language generation has focused on a generic way of generating language, in this paper we explore personal preferences as a type of heuristic that has not been properly addressed. We empirically analyze the TUNA corpus, a corpus of referring expression lexicalizations, to investigate the influence of language preferences in how people lexicalize new referring expressions in different situations. We then present two corpus-based approaches to solve the problem of referring expression lexicalization, one that takes preferences into account and one that does not. The results show a decrease of 50% in the similarity error against the reference corpus when personal preferences are used to generate the final referring expression.",
        "Does word sense disambiguation improve information retrieval?. A basic form of semantic annotation is to label a word in a document with its correct sense based on the context in which the word occurs, thus providing the disambiguated sense of the word. Performing this task automatically is known as word sense disambiguation, which has been extensively studied in the natural language processing literature. Will semantic annotation of word senses improve information retrieval? This paper provides some thoughts on this question, which lies at the intersection of natural language processing and information retrieval.",
        "Natural language processing and information retrieval",
        "Edge-Guided Natural Language Text Compression. We describe a novel compression technique for natural language text collections which takes advantage of the information provided by edges when a graph is used to model the text. This technique is called edge-guided compression. We propose an algorithm that allows the text to be transformed in agreement with the edge-guided technique in conjunction with the spaceless words transformation. The result of these transformations is a PPM-friendly byte-stream that has to be codified with a PPM family encoder. The comparison with state-of-art compressors shows that our proposal is a competitive choice for medium and large natural language text collections.",
        "The Function of Semantics in Automated Language Processing. This paper is a survey of some of the major semantic models that have been developed for automated semantic analysis of natural language.Current approaches to semantic analysis and logical inference are based mainly on models of human cognitive processes such as Quillian's semantic memory, Simmon's Protosynthex III and others.All existing systems and/or models, more or less experimental, were applied to a small subset of English.They are highly tentative because the definitions of semantic processes and semantically structured lexicon~s are not formulated rigorously. This is due mainly to the fact that it is unknown whether a unique, consistent hierarchization of the semantic features of language is possible. However, the models described are significant contributions to an unexplored field called semantics.The progressive development of a sophisticated, semantically based system for automated processing of natural language is a realistic goal. It should not be neglected, despite the fact that it is difficult to predict when this goal will be achieved. KEY WORDS AND PHRASESnatural language processing, grammars, semantics, computational linguistics I.",
        "Natural Language Processing for Text Mining and Decision Making",
        "Fast and Effective Neural Networks for Translating Natural Language into Denotations",
        "An Approach to Natural Language Processing for Document Retrieval. Document retrieval systems have been restricted, by the nature of tile task, to techniques that can be used with large numbers of documents and broad domains. The most effective techniques that have been developed are based on the statistics of word occurrences ill text. In this paper, we describe an approach to using natural language processiag (NLP) techniques for what is essentially a natural language problem -the comparison of a request text with the text of document titles and abstracts. The proposed NLP techniques are used to develop a request model based on \"conceptual case frames\" and to compare this model with the texts of candidate documents. The request model is also used to provide information to statistical search techniques that identify the candidate documents. As part of a preliminary evaluation of this approach, case frame representations of a set of requests from the CACM collection were constructed. Statistical searches carried out using dependency and relative importance information derived from the request models indicate that performance benefits can be obtained.",
        "HITS@FIRE task 2015: Twitter based Named Entity Recognizer for Indian Languages Natural Language processing (NLP) in its pure sense, is a platform that provides the ability for transforming natural language text to useful information. Named Entity Recognition (NER) is a key task in NLP for classification of named entities in natural languages. Though, there are several algorithms for named entity classification, identifying named entities in twitter data is a demanding task. Loads of information are being shared by people in twitter on a daily basis. This information is unstructured and often contains important information about organizations, politics, disasters, promotional advertisements etc. In this paper, we provide a NER that can effectively classify named entities in twitter data for Indian Languages such as English, Hindi and Tamil. POS, Chunk, Suffix, Prefix information has been used for training in Conditional Random Fields (CRF) based NER Model. CRF is a popular model for labeling and classification in text mining. Performance analysis was done using n-fold validation and F-measure. A maximum precision of 93.82 for English, 92.28 for Hindi and 86.94 for Tamil twitter data was achieved through N fold validation. Results provided by ESM-IL share task in terms of precision for English is 50.48, for Hindi is 81.49 and for Tamil 70.42. The proposed algorithm has a higher classification accuracy and it is achieved through n-fold validation. CCS Concepts \u2022 Human centered computing\u279d Human machine interaction\u279d Collaborative and social computing \u2022 Applied Computing \u279dDocument managing and text computing methodologies\u279d Artificial Intelligence and Machine Learning.",
        "Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach.",
        "Editorial for the 2nd Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL) at SIGIR 2017",
        "ASE@DPIL-FIRE2016: Hindi Paraphrase Detection using Natural Language Processing Techniques & Semantic Similarity Computations The paper reports the approaches utilized and results achieved for our system in the shared task (in FIRE-2016) for paraphrase identification in Indian languages (DPIL). Since Indian languages have a complex inherent nature, paraphrase identification in these languages becomes a challenging task. In the DPIL task, the challenge is to detect and identify whether a given sentence pairs paraphrased or not. In the proposed work, natural language processing with semantic concept extractions is explored for paraphrase detection in Hindi. Stop word removal, stemming and part of speech tagging are employed. Further similarity computations between the sentence pairs are done by extracting semantic concepts using WordNet lexical database. Initially, the proposed approach is evaluated over the given training sets using different machine learning classifiers. Then testing phase is used to predict the classes using the proposed features. The results are found to be promising, which shows the potency of natural language processing techniques and semantic concept extractions in detecting paraphrases.",
        "Cross-Language Information Retrieval and Evaluation, Workshop of Cross-Language Evaluation Forum, CLEF 2000, Lisbon, Portugal, September 21-22, 2000, Revised Papers This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are liable for prosecution under the German Copyright Law. PrefaceThe first evaluation campaign of the Cross-Language Evaluation Forum (CLEF) for European languages was held from January to September 2000. The campaign culminated in a two-day workshop in Lisbon, Portugal, 21-22 September, immediately following the fourth European Conference on Digital Libraries (ECDL 2000). The first day of the workshop was open to anyone interested in the area of Cross-Language Information Retrieval (CLIR) and addressed the topic of CLIR system evaluation. The goal was to identify the actual contribution of evaluation to system development and to determine what could be done in the future to stimulate progress. The second day was restricted to participants in the CLEF 2000 evaluation campaign and to their experiments. This volume constitutes the proceedings of the workshop and provides a record of the campaign. CLEF is currently an activity of the DELOS Network of Excellence for Digital Libraries, funded by the EC Information Society Technologies to further research in digital library technologies. The activity is organized in collaboration with the US National Institute of Standards and Technology (NIST). The support of DELOS and NIST in the running of the evaluation campaign is gratefully acknowledged.I should also like to thank the other members of the Workshop Steering Committee for their assistance in the organization of this event. April 2001Carol Peters The objective of the Cross-Language Evaluation Forum (CLEF) is to develop and maintain an infrastructure for the testing and evaluation of information retrieval systems operating on European languages, in both monolingual and cross-language contexts, and to create test-suites of reusable data that can be employed by system developers for benchmarking purposes. The first CLEF evaluation campaign started in early 2000 and ended with a workshop in Lisbon, Portugal, 22-23 September 2000. This volume constitutes the proceedings of the workshop and also provides a record of the results of the campaign. It consists of two parts and an appendix. The first part reflects the presentations and discussions on the topic of evaluation for crosslanguage information retrieval systems during the first day of the workshop, whereas the second contains papers from the individual participating groups reporting their experiments and analysing their results. The appendix presents the evaluation techniques and measures used to derive the results and provides the run statistics. The aim of this Introduction is to present the main issues discussed at the workshop and also to provide the reader with the necessary background to the experiments through a description of the tasks set for CLEF 2000. In conclusion, our plans for future CLEF campaigns are outlined. CLEF 2000 Workshop Steering Committee Evaluation for CLIR SystemsThe first two papers in Part I of the proceedings describe the organization of crosslanguage evaluation campaigns for text retrieval systems. CLEF is a continuation and expansion of the cross-language system evaluation activity for European languages begun in 1997 with the track for Cross-Language Information Retrieval (CLIR) in the Text REtrieval Conference (TREC) series. The paper by Harman et al. gives details on how the activity was organized, the various issues that had to be addressed, and the results obtained. The difficulties experienced during the first year, in which the track was coordinated centrally at NIST (US National Institute for Standards and Technology) led to the setting up of a distributed coordination in four countries (USA, Germany, Italy and Switzerland) with native speakers being responsible for the preparation of topics (structured statements of possible information needs) and relevance judgments (assessment of the relevance of the ranked lists of results submitted by participating systems). A natural consequence of this distributed coordination was the 2 Carol Peters decision, in 1999, to transfer the activity to Europe and set it up independently as CLEF. The infrastructure and methodology adopted in CLEF is based on the experience of the CLIR tracks at TREC.The second paper by Kando presents the NTCIR Workshops, a series of evaluation workshops for text retrieval systems operating on Asian languages. The 2000-2001 campaign conducted by NTCIR included cross-language system evaluation for Japanese-English and Chinese-English. Although both CLEF and NTCIR have a common basis in TREC there are interesting differences between the methodology adopted by the two campaigns. In particular, NTCIR employs multigrade relevance judgments rather than the binary system used by CLEF and inherited from TREC. Kando motivates this decision and discusses the effects.The CLEF campaign provides participants with the possibility to test their systems on both general-purpose texts (newspapers and newswires) and domain-specific collections. The third paper by Kluck and Gey examines the domain-specific task, begun in TREC and continued in CLEF, and describes the particular document collection used: the GIRT database for social sciences.The rest of the papers in the first part of this volume focus on some of the main issues that were discussed during the first day of the workshop. These included the problem of resources, the transition from the evaluation of cross-language text retrieval systems to systems running on other media, the need to consider the user perspective rather than concentrating attention solely on system performance, and the importance of being able to evaluate single system components rather than focusing on overall performance. A further point for discussion was the addition of new languages to the multilingual document collection.The problem of resources has always been seen as crucial in cross-language system development. In order to be able to match queries against documents, some kind of lexical resource is needed to provide the transfer mechanism, e.g. bilingual or multilingual dictionaries, thesauri, or corpora. In order to be able to process a number of different languages, suitable language processing tools are needed, e.g. languagespecific tokenizers, stemmers, morphologies, etc.. It is generally held that the quality of the resource used considerably affects system performance. This question was discussed at length during the workshop. The paper by Gonzalo presents a survey on the different language resources used by the CLEF 2000 participants. Many of the resources listed were developed by the participants themselves, thus showing that an evaluation exercise of this type is not only evaluating systems but also the resources used by the systems. The need for more pooling and sharing of resources between groups in order to optimize effort emerges clearly from this survey. Gonzalo concludes with some interesting proposals for the introduction of additional tasks, aimed at measuring the effect of the resources used on overall system performance, in a future campaign.The papers by Oard and by Jones both discuss CLIR from the user perspective. Oard focuses on the document selection question: how the users of a CLIR system can correctly identify the -for them -most useful documents from a ranked list of results when they cannot read the language of the target collection. He advocates the advantages of an interactive CLIR evaluation and makes a proposal as to how an evaluation of this type could be included in CLEF. Jones also supports the extension of evaluation exercises in order to assess the usefulness of techniques that can assist the user with Introduction 3 relevance judgment and information extraction. In this respect, he mentions the importance of document summarization -already included in the NTCIR evaluation programme. In addition, Jones talks about work in cross-language multimedia information retrieval and suggests directions for future research. He asserts that specifically-developed standard test collections are needed to advance research in this area.In the final paper in Part I, Gey lists several areas in which research could lead to improvement in cross-language information retrieval including resource enrichment, the use of pivot languages and phonetic transliteration. In particular, he discusses the need for post-evaluation failure analysis and shows how this could provide important feedback resulting in improved system design and performance. CLEF provides the research community with the necessary infrastructure for studies of this type. The CLEF 2000 ExperimentsThere were several reasons behind the decision to coordinate the cross-language system evaluation activity for European languages independently and to move it to Europe. One was the desire to extend the number of languages covered, another was the intention to offer a wider range of retrieval tasks to better meet the needs of the multilingual information retrieval research community.As can be seen from the descriptions of the experiments in Part II of this volume, CLEF 2000 included four separate evaluation tracks:\u2022 multilingual information retrieval \u2022 bilingual information retrieval \u2022 monolingual (non-English) information retrieval \u2022 cross-language domain-specific information retrieval The main task -inherited from TREC -required searching a multilingual document collection, consisting of national newspapers in four languages (English, French, German and Italian) of the same time period, in order to retrieve relevant documents. Forty topics were developed on the basis of the contents of the multilingual collection -ten topics for each collection -and complete topic sets were produced in all four languages. Topics are structured statements of hypothetical user needs. Each topic consisted of three fields: a brief title statement; a one-sentence description; a more complex narrative specifying the relevance assessment criteria. Queries are constructed using one of more of these fields. Additional topic sets were then created for Dutch, Finnish, Spanish and Swedish, in each case translating from the original. The main requirement was that, for each language, the topic set should be as linguistically representative as possible, i.e. using the terms that would naturally be expected to represent the set of topic concepts in the given language. The methodology followed was that described in the paper by Harman et al..A bilingual system evaluation task was also offered, consisting of querying the English newspaper collection using any topic language (except English). Many newcomers to cross-language system evaluation prefer to begin with the simpler bilingual task before moving on to tackle the additional issues involved in truly multilingual retrieval. Carol PetersOne of the aims of the CLEF activity is to encourage the development of tools to manipulate and process languages other than English. Different languages present different problems. Methods that may be efficient for certain language typologies may not be so effective for others. Issues that have to be catered for include word order, morphology, diacritic characters, language variants. For this reason, CLEF 2000 included a track for French, German and Italian monolingual information retrieval.The cross-language domain-specific task has been offered since TREC-7. The rationale of this subtask is to test retrieval on another type of document collection, serving a different kind of information need. The implications are discussed in the paper by Kluck and Gey in the first part of this volume.The papers in Part II describe the various experiments by the participating groups with these four tasks. Both traditional and innovative approaches to CLIR were experimented, and different query expansion techniques were tried. All kinds of source to target transfer mechanisms were employed, including both query and document translation. Commercial and in-house resources were used and included machine translation, dictionary and corpus-based methods. The strategies used varied from traditional IR to a considerable employment of natural language processing techniques. Different groups focused on different aspects of the overall problem, ranging from the development of language-independent tools such as stemmers to much work on language-specific features like morphology and compounding. Many groups compared different techniques in different runs in order to evaluate the effect of a given technique on performance. Overall, CLEF 2000 offered a very good picture of current issues and approaches in CLIR.The first paper in this part by Martin Braschler provides an overview and analysis of all the results, listing the most relevant achievements and comparing them with those of previous years in the CLIR track at TREC. As one of the main objectives of CLEF is to produce evaluation test-suites that can be used by the CLIR research community, Braschler also provides an analysis of the test collection resulting from the CLEF 2000 campaign, demonstrating its validity for future system testing, tuning and development activities. The appendix presents the evaluation results for each group, run by run. CLEF in the FutureThe CLEF 2001 campaign is now under way. The main tasks are similar to those of the first campaign. There are, however, some extensions and additions. In particular the multilingual corpus has been considerably enlarged and Spanish (news agency) and Dutch (national newspaper) collections for 1994 have been added. The multilingual task in CLEF 2001 involves querying collections in five languages (English, French, German, Italian and Spanish) and there will be two bilingual tracks: searching either the English or the Dutch collections. Spanish and Dutch have also been included in the monolingual track. There will be seven official topic languages, including Japanese. Additional topics will be provided in a number of other European languages, including Finnish, Swedish and Russian, and also in Chinese and Thai.Introduction 5 CLEF 2000 concentrated on the traditional metrics of recall and precisionhowever these have limitations in what they tell us about the usefulness of a retrieval system to the user. CLEF 2001 will thus also include an experimental track designed to test interactive CLIR systems and to establish baselines against which future research progress can be measured. The introduction of this track is a direct result of discussions which began in the workshop with the presentations by Oard and by Jones, and of the proposal by Oard reported in Part I of this volume.Two main issues must be considered when planning future CLEF campaigns: the addition of more languages, and the inclusion of new tasks.The extension of language coverage, discussed considerably at the workshop, depends on two factors: the demand from potential participants and the existence of sufficient resources to handle the requirements of new language collections. It was decided that Spanish and Dutch met these criteria for CLEF . CLEF 2002 and 2003 will be mainly funded by a contract from the European Commission (IST-2000-31002) but it is probable that, in the future, it will be necessary to seek support from national funding agencies as well if more languages are to be included. The aim will be to cover not only the major European languages but also some representative samples of minority languages, including members from each major group: e.g. Germanic, Romance, Slavic, and Ugro-Finnic languages. Furthermore, building on the experience of CLEF 2001, we intend to continue to provide topics in Asian languages.CLEF 2000 concentrated on cross-language text retrieval and on measuring overall system performance. However, in the future, we hope to include tracks to evaluate CLIR systems working on media other than text. We are now beginning to examine the feasibility of organizing a spoken CLIR track in which systems would have to process and match spoken queries in more than one language against a spoken document collection. Another important innovation would be to devise methods that enable the assessment of single system components, as suggested in the paper by Gonzalo.CLIR system development is still very much in the experimental stage and involves expertise from both the natural language processing and the information retrieval fields. The CLEF 2000 Workshop provided an ideal opportunity for a number of key players, with very different backgrounds, to come together and exchange ideas and compare results on the basis of a common experience: participation in the CLEF evaluation campaign. CLEF is very much a collaborative effort between organizers and participants with the same common goal: the improvement of CLIR system performance. The discussions at the workshop have had considerable impact on the organization of the 2001 campaign. The success of future campaigns will depend on the continuation and strengthening of this collaboration.More information on the organization of the current CLEF campaign and instructions on how to contact us can be found at: http://www.clef-campaign.org/. AcknowledgementsTo a large extent, CLEF depends on voluntary work. I should like to acknowledge the generous collaboration of a number of people and organizations. First of all, I wish to It is not easy to set up an infrastructure that meets the needs of a large number of languages. I should like to thank the following organisations who voluntarily engaged translators to provide topic sets in Dutch, Finnish and Swedish, working on the basis of the set of source topics:\u2022 the DRUID project for the Dutch topics;\u2022 the Department of Information Studies, University of Tampere, Finland, engaged the UTA Language Centre for the Finnish topics; \u2022 SICS Human Computer Interaction and Language Engineering Laboratory for the Swedish topics. The support of all the data providers and copyright holders is also gratefully acknowledged, and in particular:\u2022 The Los Angeles Times, for the English data collection;\u2022 Le Monde S.A. and ELDA: European Language Resources Distribution Agency, for the French data.\u2022 Frankfurter Rundschau, Druck und Verlagshaus Frankfurt am Main; Der Spiegel, Spiegel Verlag, Hamburg, for the German newspaper collections.\u2022 InformationsZentrum Sozialwissenschaften, Bonn, for the GIRT database.\u2022 Hypersystems Srl, Torino and La Stampa, for the Italian data.\u2022 Schweizerische Depeschenagentur (SDA) and Associated Press (AP) for the newswire data of the training collection. Without their help, this evaluation activity would be impossible.Last, but not least, I thank Julio Gonzalo for his help and encouragement in the preparation of this volume. CLIR Evaluation at TREC paraic@textwise.com. . Starting in 1997, the National Institute of Standards and Technology conducted 3 years of evaluation of cross-language information retrieval systems in the Text REtrieval Conference (TREC). Twentytwo participating systems used topics (test questions) in one language to retrieve documents written in English, French, German, and Italian. A large-scale multilingual test collection has been built and a new technique for building such a collection in a distributed manner was devised.",
        "Lab Report Special Section: Natural Language Processing and Information Retrieval Group Information Access and User Interfaces Division National Institute of Standards and Technology OverviewThe Natural Language Processing and Information Retrieval Group was formed in 1994 at the National Institute of Standards and Technology (NIST) in recognition of the importance of managing the ever-increasing amount of electronically available text. The formal objective of the group is \"to work with industry, academia and other government agencies to promote the use of more effective and efficient techniques for manipulating (largely) unstructured textual information, especially the browsing, searching, and presentation of that information\". The group carries on the work in text retrieval started in 1988.Because of the initial work in text retrieval, the majority of the current projects involve improving the transfer of better text retrieval technology into commercial systems. Two approaches are being followed. The first approach (started in 1988) is to build a large-scale prototype retrieval system (the PRISE system) capable of handling over three gigabytes of data. This system uses natural language input and state-of-the-art statistical ranking mechanisms. The prototype has become the focus for continued research by the group into more effective and efficient retrieval systems using natural language queries. Additionally it serves as a vehicle for work in the NISO Z39.50 Information Retrieval standard and for in-house usability testing.The second approach (started in 1992) is to conduct a conference attracting international participation from researchers in information retrieval. The Text REtrieval Conference (TREC) is now starting its fifth year. Participating groups work with a large test collection built at NIST, submit their results for a common evaluation, and meet for a three-day workshop to compare techniques and results. The conference is starting to serve as a major technology-transfer mechanism in the area of information retrieval. New methods of evaluating retrieval technology are also being developed based on work for this conference.Several other projects represent a broadening of group's interests to include additional areas of natural language processing and evaluation. Projects are being pursued in the area of improved handling of large text files, including how to automatically create hypertext links and how to facilitate machine-aided editing of massive text files. Also planned are projects in the area of human computer interaction techniques for information access, including advanced interfaces, i.e., mixed mode and multimedia, for information retrieval.6",
        "Word classification and hierarchy using co-occurrence word information. By the development of the computer in recent years, calculating a complex advanced processing at high speed has become possible. Moreover, a lot of linguistic knowledge is used in the natural language processing (NLP) system for improving the system. Therefore, the necessity of co-occurrence word information in the natural language processing system increases further and various researches using co-occurrence word information are done. Moreover, in the natural language processing, dictionary is necessary and indispensable because the ability of the entire system is controlled by the amount and the quality of the dictionary. In this paper, the importance of co-occurrence word information in the natural language processing system was described. The classification technique of the co-occurrence word (receiving word) and the co-occurrence frequency was described and the classified group was expressed hierarchically. Moreover, this paper proposes a technique for an automatic construction system and a complete thesaurus. Experimental test operation of this system and effectiveness of the proposal technique is verified.",
        "Experiments on Statistical Approaches to Compensate for Limited Linguistic Resources. Information Retrieval systems can benefit from advanced linguistic resources when carrying out tasks such as word-stemming or query translation. The main goal of our experiments has been the development of methodologies that minimize the human labor needed for creating linguistic resources for new languages. For this purpose, we have applied statistical techniques to extract information directly from the collections.",
        "Processing natural language for an expert system using a sub-language approach",
        "Kernel methods, syntax and semantics for relational text categorization. Previous work on Natural Language Processing for Information Retrieval has shown the inadequateness of semantic and syntactic structures for both document retrieval and categorization. The main reason is the high reliability and effectiveness of language models, which are sufficient to accurately solve such retrieval tasks. However, when the latter involve the computation of relational semantics between text fragments simple statistical models may result ineffective. In this paper, we show that syntactic and semantic structures can be used to greatly improve complex categorization tasks such as determining if an answer correctly responds to a question. Given the high complexity of representing semantic/syntactic structures in learning algorithms, we applied kernel methods along with Support Vector Machines to better exploit the needed relational information. Our experiments on answer classification on Web and TREC data show that our models greatly improve on bagof-words.",
        "Describing and Predicting Online Items with Reshare Cascades via Dual Mixture Self-exciting Processes A massive amount of news is being shared online by individuals and news agencies, making it difficult to take advantage of these news and analyse them in traditional ways. In view of this, there is an urgent need to use recent technologies to analyse all news relevant information that is being shared in natural language and convert it into forms that can be more easily and precisely processed by computers. Knowledge Graphs (KGs) offer offer a good solution for such processing. Natural Language Processing (NLP) offers the possibility for mining and lifting natural language texts to knowledge graphs allowing to exploit its semantic capabilities, facilitating new possibilities for news analysis and understanding. However, the current available techniques are still away from perfect. Many approaches and frameworks have been proposed to track and analyse news in the last few years. The shortcomings of those systems are that they are static and not updateable, are not designed for largescale data volumes, did not support real-time processing, dealt with limited data resources, used traditional lifting pipelines and supported limited tasks, or have neglected the use of knowledge graphs to represent news into a computer-processable form. Therefore, there is a need to better support lifting natural language into a KG. With the continuous development of NLP techniques, the design of new dynamic NLP lifters that can cope with all the previous shortcomings is required. This paper introduces a general NLP lifting architecture for automatically lifting and processing news reports in real-time based on the recent development of the NLP methods.",
        "Effective language identification of forum texts based on statistical approaches a b s t r a c tThis investigation deals with the problem of language identification of noisy texts, which could represent the primary step of many natural language processing or information retrieval tasks. Language identification is the task of automatically identifying the language of a given text. Although there exists several methods in the literature, their performances are not so convincing in practice.In this contribution, we propose two statistical approaches: the high frequency approach and the nearest prototype approach. In the first one, 5 algorithms of language identification are proposed and implemented, namely: character based identification (CBA), word based identification (WBA), special characters based identification (SCA), sequential hybrid algorithm (HA1) and parallel hybrid algorithm (HA2). In the second one, we use 11 similarity measures combined with several types of character N-Grams.For the evaluation task, the proposed methods are tested on forum datasets containing 32 different languages. Furthermore, an experimental comparison is made between the proposed approaches and some referential language identification tools such as: LIGA, NTC, Google translate and Microsoft Word. Results show that the proposed approaches are interesting and outperform the baseline methods of language identification on forum texts.",
        "Document Expansion for Speech Retrieval. Advances in automatic speech recognition allow us to search large speech collections using traditional information retrieval methods. The problem of \\aboutness\" for documents | is a document about a certain concept | has been at the core of document indexing for the entire history of IR. This problem is more di cult for speech indexing since automatic speech transcriptions often contain mistakes. In this study we s h o w t h a t document expansion can be successfully used to alleviate the e ect of transcription mistakes on speech r etrieval. The loss of retrieval e ectiveness due to automatic transcription errors can be reduced by document expansion from 15{27% relative t o r e t r i e v al from human transcriptions to only about 7{13%, even for automatic transcriptions with word error rates as high as 65%. For good automatic transcriptions (25% word error rate), retrieval e ectiveness with document expansion is indistinguishable from retrieval from human transcriptions. This makes speech retrieval from automatic transcriptions, even poor ones, competitive with retrieval from perfect transcriptions.1 Introduction Increasing amounts of spoken communication are stored in digital form for archival purposes (for instance, broadcasts) or as a byproduct of modern communications technology (voice-mail for instance). Furthermore, multimedia documents and databases are becoming increasingly popular, for example on the Web. It would be therefore desirable to have tools searching spoken information that complement the existing methods for searching textual information.With advances in automatic speech recognition (ASR) technology, it is now possible to automatically transcribe speech with reasonable accuracy 17]. Once the contents of a speech database or the audio portions of a multimedia database are transcribed using a speech recognition system, traditional information retrieval tech n i q u e s c a n b e u s e d t o search the database. However, inaccuracies in the automatic transcriptions pose several new problems in use of IR technology for speech r e t r i e v al.",
        "Applying Light Natural Language Processing to Ad-Hoc Cross Language Information Retrieval In the CLEF 2005 Ad-Hoc Track we experimented with language-specific morphosyntactic processing and light Natural Language Processing (NLP) for the retrieval of Bulgarian, French, Italian, English and Greek.",
        "Proceedings of the 2nd Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017) co-located with the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017), Tokyo, Japan, August 11, 2017",
        "NLP4REC: The WSDM 2020 Workshop on Natural Language Processing for Recommendations Natural language processing is becoming more and more important in recommender systems. This half day workshop explores challenges and potential research directions in Recommender Systems (RSs) combining Natural Language Processing (NLP). The focus will be on stimulating discussions around how to combine natural language processing technologies with recommendation. We welcome theoretical, experimental, and methodological studies that leverage NLP technologies to advance recommender systems, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating the interaction between NLP and RSs to develop more intelligent RSs.",
        "Natural-Language Retrieval of Images Based on Descriptive Captions We describe a prototype intelligent information retrieval system that uses natural-language understanding to efficiently locate captioned data. Multimedia data generally require captions to explain their features and significance. Such descriptive captions often rely on long nominal compounds (strings of consecutive nouns) which create problems of disambiguating word sense. In our system, captions and user queries are parsed and interpreted to produce a logical form, using a detailed theory of the meaning of nominal compounds. A fine-grain match can then compare the logical form of the query to the logical forms for each caption. To improve system efficiency, we first perform a coarse-grain match with index files, using nouns and verbs extracted from the query. Our experiments with randomly selected queries and captions from an existing image library show an increase of 30% in precision and 50% in recall over the keyphrase approach currently used. Our processing times have a median of seven seconds as compared to eight minutes for the existing system, and our system is much easier to use.",
        "NLPIR: a Theoretical Framework for Applying Natural Language Processing to Information Retrieval",
        "A deep learning based Part-of-Speech (POS) tagger for Sanskrit language by embedding character level features. Part-of-Speech (POS) tagging is an important task in Natural Language Processing and numerous taggers have been developed for POS tagging in several languages. In Sanskrit also, one of the oldest languages in the world, many POS taggers were developed. However, less attention was given to the machine learning based POS tagging. In this paper, various deep learning algorithms are used for implementing a POS tagger for Sanskrit. This problem is framed as a sequence labeling problem at the character level. Therefore, a word to be POS tagged is considered as a sequence of characters and the sequential relationship among the characters in a word is captured with the deep learning algorithms such as Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) networks, Gate Recurrent Unit (GRU) and their bidirectional versions. The character level formulation of the problem reduces the memory requirement compared to the word level implementations and also increases the accuracy of labeling. The performance of the labeling task was analyzed with the different combinations of hyperparameters. We obtained the accuracy score of 97.86% with Bidirectional GRU. The character level implementations of both uni and bidirectional forms of RNN, LSTM and GRU outperformed all world level implementations in terms of accuracy, number of trainable parameters and the storage requirement. CCS CONCEPTS\u2022 Theory of computation \u2192 Theory and algorithms for application domains; \u2022 Computing methodologies \u2192 Machine learning; Neural networks;",
        "Speech-Driven Text Retrieval: Using Target IR Collections for Statistical Language Model Adaptation in Speech Recognition",
        "Generating Keyword Queries for Natural Language Queries to Alleviate Lexical Chasm Problem. In recent years, the task of reformulating natural language queries has received considerable attention from both industry and academic communities. Because of the lexical chasm problem between natural language queries and web documents, if we directly use natural language queries as inputs for retrieval, the results are usually unsatisfactory. In this work, we formulated the task as a translation problem to convert natural language queries into keyword queries. Since the nature language queries users input are diverse and multi-faceted, general encoder-decoder models cannot effectively handle low-frequency words and out-ofvocabulary words. We propose a novel encoder-decoder method with two decoders: the pointer decoder firstly extracts query terms directly from the source text via copying mechanism, then the generator decoder generates query terms using two attention modules simultaneously considering the source text and extracted query terms. For evaluation and training, we also proposed a semi-automatic method to construct a large-scale dataset about natural language query-keyword query pairs. Experimental results on this dataset demonstrated that our model could achieve better performance than the previous state-of-the-art methods.",
        "On the Construction of Effective Vocabularies for Information Retrieval. Natural language query formulations exhibit advantages over artificial language statements since they permit the user to approach the retrieval environment without prior training and without using intermediaries. To obtain adequate retrieval output, it is however necessary to emphasize the good terms and to deemphasize the bad ones. The usefulness of the terms in a natural language vocabulary is first characterized in terms of their frequency distribution over the documents of a collection. The construction of \"good\" natural language vocabularies is then described, and methods are given for improving the vocabulary by transforming terms that operate poorly for retrieval purposes into better ones.",
        "Document lndexing and Retrieval using Natural Language Processing",
        "Intelligent Information Systems. Natural language processing techniques developed for Artificial Intelligence programs can aid in constructing powerful information retrieval systems in at least, two areas. Automatic construction of new concepts allows a large body of information to be organized compactly and in a manner that allows a wide range of queries to be answered.Also, using natural language processing techniques to conceptually analyze the documents being stored in a system greatly expands the effectiveness of queries about given pieces of text. However, only robust conceptual analysis methods are adequate for such systems. This paper will discuss approaches to b,)th concept learning, in the form of Generalization-Ba~s'ed Memory, and powerful, robust text processing achieved by Alemory-Based Understandin 9. These techniques have been intplemented in the computer systems IPP, a program that reads, remembers and generalizes from ne~s storms about terrorism, and RESEARCIIER, currently il~ the prototype stage, that operates in a very differez~t domain (technical texts, patent abstracts in particular).",
        "Transporting the Linguistic String Project System from a Medical to a Navy Domain The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems.",
        "Natural language processing of the texts of chemical patent abstracts",
        "Improving the Precision of RDF Question/Answering Systems: A Why Not Approach. Given a natural language question qNL over an RDF dataset D, an RDF Question/Answering (Q/A) system first translates qNL into a SPARQL query graph Q and then evaluates Q over the underlying knowledge graph to figure out the answers Q(D). However, due to the challenge of understanding natural language questions and the complexity of linking phrases with specific RDF items (e.g., entities and predicates), the translated query graph Q may be incorrect, leading to some wrong or missing answers. In order to improve the system's precision, we propose a self-learning solution based on the users' feedback over Q(D). Specifically, our method automatically refines the SPARQL query Q into a new query graph Q with minimum modifications (over the original query Q). The new query will fix the errors and omissions of the query results. Furthermore, each amendment will also be used to improve the precision in answering subsequent natural language questions. BACKGROUND AND MOTIVATIONAs more and more RDF Question/Answering (Q/A) systems over RDF knowledge graphs are designed, which allow users to express queries in natural language and translate them into SPARQL queries automatically, RDF is more widely used. Generally, RDF Q/A systems translate a user's natural language question qNL into a SPARQL query graph Q and evaluate Q over the underlying RDF knowledge graph G to find the answers. However, due to ambiguity of natural language question sentences, the translated Q may not be correct, which results in errors and omissions of the query results. According to the analysis of several RDF Q/A systems, we classify the typical errors into three categories: (1) Entity/Class Linking Error: Systems link the phrase in users' questions with a wrong entity/class, reflected in subject or object, which is chargeable on the imperfect entity-mapping dictionary. As for the question \"Which actress was born in countries in Europe?\" (see , the system mistakenly links \"countries in Europe\" with class Country instead of the correct one EuropeanCountry . (2) Relation Paraphrasing Error: Q/A systems often reply on a relation-paraphrase dictionary to extract relations in users' questions. A paraphrase is to align a natural language relation phrase with the corresponding predicate. However, due to mistakes, noise and incompleteness of the paraphrase dictionary, Q/A systems often extract imperfect relations or even wrong relations, reflected in predicate in RDF triples. As shown in , the right predicate should be birthPlace , while the system interprets the relation as deathPlace due to one error in the paraphrase dictionary, i.e., mapping \"be born (in)\" to deathPlace .",
        "Understanding Human Language: Can NLP and Deep Learning Help?. There is a lot of overlap between the core problems of information retrieval (IR) and natural language processing (NLP). An IR system gains from understanding a user need and from understanding documents, and hence being able to determine whether a document has information that satisfies the user need. Much of NLP is about the same thing: Natural language understanding aims to understand the meaning of questions and documents and meaning relationships. The exciting recent application of deep learning approaches in NLP has brought new tools for effectively understanding language semantics. In principle, there should be a lot of synergy, though in practice the concerns of IR on large systems and macro-scale understanding have tended to contrast with the emphasis in NLP on language structure and micro-scale understanding.My talk will emphasize the two topics of how NLP can contribute to understanding textual relationships and how deep learning approaches substantially aid in this goal. One basic -and very successful tool -has been the new generation of distributed word representations: neural word embeddings. However, beyond just word meanings, we need to understand how to compose the meanings of larger pieces of text. Two requirements for that are good ways to understand the structure of human language utterances and ways to compose their meanings. Deep learning methods can help for both tasks. Finally, we need to understand relationships between pieces of text, to be able to do tasks such as Natural Language Inference (or Recognizing Textual Entailment) and Question Answering, and I will look at some of our recent work in these areas, both with and without the help of neural networks. KeywordsNatural language processing, deep learning, word vectors, compositionality, natural language inference, recognizing textual entailment, question answering Short BiographyChristopher Manning is a professor of computer science and linguistics at Stanford University. His Ph.D. is from Stanford in 1995, and he held faculty positions at Carnegie Mellon University and the University of Sydney before returning to Stanford. His research goal is computers that can intelligently process, understand, and generate human language material. Manning concentrates on machine learning approaches to computational linguistic problems, including syntactic parsing, computational semantics and pragmatics, textual inference, machine translation, and using deep learning for NLP. He is an ACM Fellow, a AAAI Fellow, and an ACL Fellow, and has coauthored leading textbooks on statistical natural language processing and information retrieval. He is a member of the Stanford NLP group (@stanfordnlp).",
        ". s of the Papers to be presented at the Sixth Annual International ACM SIGIR Conference 1983 Natural language processing techniques developed for Artificial Intelligence programs can aid in constructing powerful information retrieval systems in at least two areas. Automatic construction of new concepts allows a large body of information to be organized compactly and in a manner that allows a wide range of queries to be answered. Also, using natural language processing techniques to conceptually analyze the documents being stored in a system greatly expands the effectiveness of queries about given pieces of text. However, only robust conceptual analysis methods are adequate for such systems. This paper will discuss approaches to both concept learning, in the form of Generalization-Based Memory, and powerful, robust text processing achieved by Memory-Based Understanding.These techniques have been implemented in the computer systems IPP, a program that reads, remembers and generalizes from news stories about terrorism, and RESEARCHER, currently in the prototype stage, that operates in a very different domain (technical texts, patent abstracts in particular). The City University, LondonWe are concerned with the problem of making automated information retrieval (IR) systems directly accessible by end users, without recourse to human intermediaries. Thus, we are concerned with automating at least some of the functions performed by human intermediaries in IR interaction",
        "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019",
        "Deep Learning for Information Retrieval. Recent years have observed a significant progress in information retrieval and natural language processing with deep learning technologies being successfully applied into almost all of their major tasks. The key to the success of deep learning is its capability of accurately learning distributed representations (vector representations or structured arrangement of them) of natural language expressions such as sentences, and effectively utilizing the representations in the tasks. This tutorial aims at summarizing and introducing the results of recent research on deep learning for information retrieval, in order to stimulate and foster more significant research and development work on the topic in the future.The tutorial mainly consists of three parts. In the first part, we introduce the fundamental techniques of deep learning for natural language processing and information retrieval, such as word embedding, recurrent neural networks, and convolutional neural networks. In the second part, we explain how deep learning, particularly representation learning techniques, can be utilized in fundamental NLP and IR problems, including matching, translation, classification, and structured prediction. In the third part, we describe how deep learning can be used in specific application tasks in details. The tasks are search, question answering (from either documents, database, or knowledge base), and image retrieval.",
        "Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs. The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands. We hypothesize that the support of fine-grained user interaction can greatly improve the usability of natural language interfaces. In the specific setting of natural language interfaces to web APIs, we conduct a systematic study to verify our hypothesis. To facilitate this study, we propose a novel modular sequence-to-sequence model to create interactive natural language interfaces. By decomposing the complex prediction process of a typical sequence-to-sequence model into small, highly-specialized prediction units called modules, it becomes straightforward to explain the model prediction to the user, and solicit user feedback to correct possible prediction errors at a fine-grained level. We test our hypothesis by comparing an interactive natural language interface with its non-interactive version through both simulation and human subject experiments with real-world APIs. We show that with interactive natural language interfaces, users can achieve a higher success rate and a lower task completion time, which lead to greatly improved user satisfaction.",
        "Retrieval from Captioned Image Databases Using Natural Language Processing. A t rst sight, it might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat di erent. The ANVIL system uses a natural language tec hnique to obtain high accuracy retriev al of images which h a ve been annotated with a descriptive textual caption. The natural language techniques also allow additional con textual information to be derived from the relation betw een the query and thecaption, which c a n h e l p users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system.",
        "Deep Natural Language Processing for Search and Recommendation Search and recommender systems process rich natural language text data such as user queries and documents (e.g., articles, profiles, transcripts, comments, posts). Achieving high-quality search and recommendation results requires processing and understanding such information effectively and efficiently, where natural language processing (NLP) technologies are widely deployed. Natural language data are represented as a sequence of words. Understanding such sequential information is generally a nontrivial task in traditional methods, with challenges on both data sparsity and data generalization. Deep learning models provide an opportunity to effectively extract the representative relevant information, thus better understanding complicated semantics and underlying user intention. In recent years, the rapid development of deep learning technology has been proven successful for improving various NLP tasks, indicating their great potential of promoting search and recommender systems. Developing deep learning models for NLP [4, 11, 19, 21, 28, 29] in search and recommender systems [6, 7] involves various fundamental components including 1). query and document understanding that extracts and infers relevant information, such as intent prediction [18], entity tagging and disambiguation [15, 26], topic understanding [1, 20], and opinion mining [13, 16]; 2). retrieval and ranking methodologies designed with strong latency restrictions [8, 10, 17] and various matching strategies [3, 12, 14]; and 3). language generation techniques designed to proactively guide/interact with users to further resolve ambiguity,"
    ],
    "recommendation systems": [
        "Personalised Reranking of Paper Recommendations Using Paper Content and User Behavior Academic search engines have been widely used to access academic papers, where users' information needs are explicitly represented as search queries. Some modern recommender systems have taken one step further by predicting users' information needs without the presence of an explicit query. In this article, we examine an academic paper recommender that sends out paper recommendations in email newsletters, based on the users' browsing history on the academic search engine. Specifically, we look at users who regularly browse papers on the search engine, and we sign up for the recommendation newsletters for the first time. We address the task of reranking the recommendation candidates that are generated by a production system for such users.We face the challenge that the users on whom we focus have not interacted with the recommender system before, which is a common scenario that every recommender system encounters when new users sign up. We propose an approach to reranking candidate recommendations that utilizes both paper content and user behavior. The approach is designed to suit the characteristics unique to our academic recommendation setting. For instance, content similarity measures can be used to find the closest match between candidate recommendations and the papers previously browsed by the user. To this end, we use a knowledge graph derived from paper metadata to compare entity similarities (papers, authors, and journals) in the embedding space. Since the users on whom we focus have no prior interactions with the recommender system, we propose a model to learn a mapping from users' browsed articles to user clicks on the recommendations. We combine both content and behavior into a hybrid reranking model that outperforms the production baseline significantly, providing a relative 13% increase in Mean Average Precision and 28% in Precision@1.Moreover, we provide a detailed analysis of the model components, highlighting where the performance boost comes from. The obtained insights reveal useful components for the reranking process and can be generalized to other academic recommendation settings as well, such as the utility of graph embedding similarity. Also, recent papers browsed by users provide stronger evidence for recommendation than historical ones. CCS Concepts: \u2022 Information systems \u2192 Recommender systems;X. Li is now at the National University of Defense Technology, Changsha, China. This research was partially supported by Ahold Delhaize, the China Scholarship Council, and the Innovation Center for Artificial Intelligence (ICAI). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. Authors' addresses: X. Li and Y. Chen, Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China; emails: lixinyimichael@gmail.com, y.chen4@uva.nl; B. Pettit, Elsevier, London, United Kingdom; email: b.pettit@elsevier.com; M. de Rijke, Informatics Institute, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands; email: derijke@uva.nl. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Along with the digitization of academic resources and the increasing popularity of academic information platforms, online access to academic papers has become a widely used service. Various online academic service providers have given users access to papers through their search engines, such as Google Scholar , Aminer and ScienceDirect [59], where users can enter queries to seek relevant papers in their database. In this scenario, users need to have an idea of what they are looking for, and the information needs can be formalized as queries. The search system takes a query as input and returns a ranking of relevant papers for users to examine and interact with.While such academic search engines can often fulfill user requests by catering to specific information needs represented as queries, there are cases when users' information needs are not explicitly specified. For instance, users may want to learn about new developments in their domain by looking at emerging papers that are relevant. In this case, the user may not have an idea of what queries to enter on the search engine. This is a situation where paper recommender systems can step in and recommend relevant papers without the need for a user query.Paper recommender systems have a role that is complementary to the search engine. The possible recommendation scenarios fall into three categories based on the recommendation timing:",
        "Recommender Systems: Research Direction. Recommender systems are omnipresent on the web. They aim to address the problem of information overload. Recommender systems personalizes the content to each individual user based on their preferences. Currently, these systems are used to recommend movies, music, news, products to buy etc. They essentially assist us in making decisions. This paper presents the author's plan for his PhD research in this domain and research questions to be discussed at the WSDM 2017 Doctoral Consortium.",
        "Multiobjective Pareto-Efficient Approaches for Recommender Systems Recommender systems are quickly becoming ubiquitous in applications such as e-commerce, social media channels, and content providers, among others, acting as an enabling mechanism designed to overcome the information overload problem by improving browsing and consumption experience. A typical task in many recommender systems is to output a ranked list of items, so that items placed higher in the rank are more likely to be interesting to the users. Interestingness measures include how accurate, novel, and diverse are the suggested items, and the objective is usually to produce ranked lists optimizing one of these measures. Suggesting items that are simultaneously accurate, novel, and diverse is much more challenging, since this may lead to a conflicting-objective problem, in which the attempt to improve a measure further may result in worsening other measures. In this article, we propose new approaches for multiobjective recommender systems based on the concept of Pareto efficiency-a state achieved when the system is devised in the most efficient manner in the sense that there is no way to improve one of the objectives without making any other objective worse off. Given that existing multiobjective recommendation algorithms differ in their level of accuracy, diversity, and novelty, we exploit the Pareto-efficiency concept in two distinct manners: (i) the aggregation of ranked lists produced by existing algorithms into a single one, which we call Pareto-efficient ranking, and (ii) the weighted combination of existing algorithms resulting in a hybrid one, which we call Pareto-efficient hybridization. Our evaluation involves two real application scenarios: music recommendation with implicit feedback (i.e., Last.fm) and movie recommendation with explicit feedback (i.e., MovieLens). We show that the proposed Pareto-efficient approaches are effective in suggesting items that are likely to be simultaneously accurate, diverse, and novel. We discuss scenarios where the system achieves high levels of diversity and novelty without compromising its accuracy. Further, comparison against multiobjective baselines reveals improvements in terms of accuracy (from 10.4% to 10.9%), novelty (from 5.7% to 7.5%), and diversity (from 1.6% to 4.2%).",
        "Analysis and Classification of Multi-Criteria Recommender Systems. Recent studies have indicated that the application of Multi-Criteria Decision Making (MCDM) methods in recommender systems has yet to be systematically explored. This observation partially contradicts with the fact that in related literature, there exist several contributions describing recommender systems that engage some MCDM method. Such systems, which we refer to as multi-criteria recommender systems, have early demonstrated the potential of applying MCDM methods to facilitate recommendation, in numerous application domains. On the other hand, a comprehensive analysis of existing systems would facilitate their understanding and development. Towards this direction, this paper identifies a set of dimensions that distinguish, describe and categorize multi-criteria recommender systems, based on existing taxonomies and categorizations. These dimensions are integrated into an overall framework that is used for the analysis and classification of a sample of existing multi-criteria recommender systems. The results provide a comprehensive overview of the ways current multi-criteria recommender systems support the decision of online users.",
        "Recommendation systems with complex constraints: A course recommendation perspective We study the problem of making recommendations when the objects to be recommended must also satisfy constraints or requirements. In particular, we focus on course recommendations: the courses taken by a student must satisfy requirements (e.g., take two out of a set of five math courses) in order for the student to graduate. Our work is done in the context of the CourseRank system, used by students to plan their academic program at Stanford University. Our goal is to recommend to these students courses that not only help satisfy constraints, but that are also desirable (e.g., popular or taken by similar students). We develop increasingly expressive models for course requirements, and present a variety of schemes for both checking if the requirements are satisfied, and for making recommendations that take into account the requirements. We show that some types of requirements are inherently expensive to check, and we present exact, as well as heuristic techniques, for those cases. Although our work is specific to course requirements, it provides insights into the design of recommendation systems in the presence of complex constraints found in other applications.",
        "WSDM 2021 Tutorial on Conversational Recommendation Systems Recent years have witnessed the emerging of conversational systems, including both physical devices and mobile-based applications. Both the research community and industry believe that conversational systems will have a major impact on human-computer interaction, and specifically, the IR/DM/RecSys communities have begun to explore Conversational Recommendation Systems. Conversational recommendation aims at finding or recommending the most relevant information (e.g., web pages, answers, movies, products) for users based on textual-or spoken-dialogs, through which users can communicate with the system more efficiently using natural language conversations. Due to users' constant need to look for information to support both work and daily life, conversational recommendation system will be one of the key techniques towards an intelligent web. The tutorial focuses on the foundations and algorithms for conversational recommendation, as well as their applications in real-world systems such as search engine, e-commerce and social networks. The tutorial aims at introducing and communicating conversational recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems; \u2022 Computing methodologies \u2192 Natural language processing;",
        "Activity Recommendation with Partners Recommending social activities, such as watching movies or having dinner, is a common function found in social networks or e-commerce sites. Besides certain websites which manage activity-related locations (e.g., foursquare.com), many items on product sale platforms (e.g., groupon.com) can naturally be mapped to social activities. For example, movie tickets can be thought of as activity items, which can be mapped as a social activity of \"watch a movie.\" Traditional recommender systems estimate the degree of interest for a target user on candidate items (or activities), and accordingly, recommend the top-k activity items to the user. However, these systems ignore an important social characteristic of recommended activities: people usually tend to participate in those activities with friends. This article considers this fact for improving the effectiveness of recommendation in two directions. First, we study the problem of activity-partner recommendation; i.e., for each recommended activity item, find a suitable partner for the user. This (i) saves the user's time for finding activity partners, (ii) increases the likelihood that the activity item will be selected by the user, and (iii) improves the effectiveness of recommender systems to users overall and enkindles their social enthusiasm. Our partner recommender is built upon the users' historical attendance preferences, their social context, and geographic information. Moreover, we explore how to leverage the partner recommendation to help improve the effectiveness of recommending activities to users. Assuming that users tend to select the activities for which they can find suitable partners, we propose a partner-aware activity recommendation model, which integrates this hypothesis into conventional recommendation approaches. Finally, the recommended items not only match users' interests, but also have high chances to be selected by the users, because the users can find suitable partners to attend the corresponding activities together. We conduct experiments on real data to evaluate the effectiveness of activity-partner recommendation and partner-aware activity recommendation. The results verify that (i) suggesting partners greatly improves the likelihood that a recommended activity item is to be selected by the target user and (ii) considering the existence of suitable partners in the ranking of recommended items improves the accuracy of recommendation significantly. CCS Concepts: \u2022 Information systems \u2192 Social recommendation;Additional Key Words and Phrases: Recommendation system, Location-based social network Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. In real-world recommendation applications, many items are related to activities that people like to participate in with their folks. For example, items such as movie tickets and dinner discounts are related to social activities (watching movies and dining). We call such items (social) activity items. Activity items are commonly found in real-world e-commerce websites such as Groupon (www. groupon.com) and Meituan (www.meituan.com), as shown in the examples of (1). Previous work on recommending activity items typically focused on utilizing past attendance behaviors , social links between users , and geographic information ) to predict the interests of users. Our work is the first to consider a special characteristic of social activities: people typically do not like to attend them alone. Indeed, more often than not, when considering attending a social activity, people seek partners to join them. Based on this, we extend the functionality of recommender systems in two directions that improve their effectiveness in suggesting activity items.First, assuming that a recommendation service (e.g., Groupon) promotes a set of activity items to a user, we study the problem of also recommending suitable activity partners for the items. As shows, our suggestion is to combine an activity-item promotion platform with a social network platform to find activity partners for the items which can increase the likelihood that the recommended items will be selected by the users. The rationale is that, for items that people like to participate in with their folks, if the system recommends the items alone, the user may give up attending the activity (i.e., taking up the item) if s/he cannot immediately think of someone to invite to attend the activity together. illustrates the effectiveness of recommending activity partners via an example. Consider activity item \"tickets of Bruno Mars' concert,\" for which the corresponding activity is \"watching Bruno Mars' concert.\" Imagine that you have some interest in Bruno Mars' show; however, when you see the recommendation message, you have difficulty thinking who could be suitable partners for watching the show together. This could be a good reason for you to give up attending this activity since you do not feel like watching a concert alone. On the other hand, if the recommendation also includes suggestions for possible partners, you can try inviting them and enjoy the show together. In order to evaluate our hypothesis that users prefer to take activity items if they have partners to join them, we designed a simple questionnaire to collect feedback from real Web users. The results (shown in Section 4.1) demonstrate that the great majority of Web users would favor such an approach as opposed to a simple activity item recommender. In summary, we assert that including partner recommendations not only improves the quality of recommender systems, but may also increase the positive response rate of users, therefore improving the revenue of the involved businesses. To the best of our knowledge, so far there do not exist any previous studies or applications that include this important function. This motivates us to investigate methods for activity-partner recommendation. We first explore how historical attendance preferences, social context, and geographic information can be used to recommend activity partners. Then, we propose a method that analyzes historical records of users' preferences on activity partners to predict activity partners for new recommended activities. This is a reasonable methodology, since the past users' preferences on activity partners would be available after setting up an activity-partner recommendation system.",
        "Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems. The increasing role of recommender systems in many aspects of society makes it essential to consider how such systems may impact social good. Various modifications to recommendation algorithms have been proposed to improve their performance for specific socially relevant measures. However, previous proposals are often not easily adapted to different measures, and they generally require the ability to modify either existing system inputs, the system's algorithm, or the system's outputs. As an alternative, in this paper we introduce the idea of improving the social desirability of recommender system outputs by adding more data to the input, an approach we view as as providing 'antidote' data to the system. We formalize the antidote data problem, and develop optimizationbased solutions. We take as our model system the matrix factorization approach to recommendation, and we propose a set of measures to capture the polarization or fairness of recommendations. We then show how to generate antidote data for each measure, pointing out a number of computational efficiencies, and discuss the impact on overall system accuracy. Our experiments show that a modest budget for antidote data can lead to significant improvements in the polarization or fairness of recommendations.",
        "Trust-based recommendation systems: an axiomatic approach. High-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network.In analogy to prior work on voting and ranking systems, we use the axiomatic approach from the theory of social choice. We develop a set of five natural axioms that a trustbased recommendation system might be expected to satisfy. Then, we show that no system can simultaneously satisfy all the axioms. However, for any subset of four of the five axioms we exhibit a recommendation system that satisfies those axioms. Next we consider various ways of weakening the axioms, one of which leads to a unique recommendation system based on random walks. We consider other recommendation systems, including systems based on personalized PageRank, majority of majorities, and minimum cuts, and search for alternative axiomatizations that uniquely characterize these systems.Finally, we determine which of these systems are incentive compatible, meaning that groups of agents interested in manipulating recommendations can not induce others to share their opinion by lying about their votes or modifying their trust links. This is an important property for systems deployed in a monetized environment.",
        "A Monte Carlo algorithm for cold start recommendation. Recommendation systems have been widely used in E-commerce sites, social networks, etc. One of the core tasks in recommendation systems is to predict the users' ratings on items. Although many models and algorithms have been proposed, how to make accurate prediction for new users with extremely few rating records still remains a big challenge, which is called the cold start problem. Many existing methods utilize additional information, such as social graphs, to cope with the cold start problem. However, the side information may not always be available. In contrast to such methods, we propose a more general solution to address the cold start problem based on the observed user rating records only. Specifically we define a random walk on a bipartite graph of users and items to simulate the preference propagation among users, in order to alleviate the data sparsity problem for cold start users. Then we propose a Monte Carlo algorithm to estimate the similarity between different users. This algorithm takes a precomputation approach, and thus can efficiently compute the user similarity given any new user for rating prediction. In addition, our algorithm can easily handle dynamic updates and can be parallelized naturally, which are crucial for large recommendation systems. Theoretical analysis is presented to demonstrate the efficiency and effectiveness of our algorithm, and extensive experiments also confirm our theoretical findings.",
        "How Much Novelty is Relevant?: It Depends on Your Curiosity Traditional recommendation systems (RSs) aim to recommend items that are relevant to the user's interest. Unfortunately, the recommended items will soon become too familiar to the user and hence fail to arouse her interest. Discovery-oriented recommendation systems (DORSs) complement accuracy with discover utilities (DUs) such as novelty and diversity and optimize the tradeoff between the DUs and accuracy of the recommendations. Unfortunately, DORSs ignore an important fact that different users have different appetites for DUs. That is, highly curious users can accept highly novel and diversified recommendations whereas conservative users would behave in the opposite manner. In this paper, we propose a curiosity-based recommendation system (CBRS) framework which generates recommendations with a personalized amount of DUs to fit the user's curiosity level. The major contribution of this paper is a computational model of user curiosity, called Probabilistic Curiosity Model (PCM), which is based on the curiosity arousal theory and Wundt curve in psychology research. In PCM, we model a user's curiosity with a curiosity distribution function learnt from the user's access history and compute a curiousness score for each item representing how curious the user is about the item. CBRS then selects items which are both relevant and have high curiousness score, bounded by the constraint that the amount of DUs fits the user's DU appetite. We use joint optimization and co-factorization approaches to incorporate the curiosity signal into the recommendations. Extensive experiments have been performed to evaluate the performance of CBRS against the baselines using a music dataset from last.fm. The results show that compared to the baselines CBRS not only provides more personalized recommendations that adapt to the user's curiosity level but also improves the recommendation accuracy.",
        "User Fairness in Recommender Systems. Recent works in recommendation systems have focused on diversity in recommendations as an important aspect of recommendation quality. In this work we argue that the post-processing algorithms aimed at only improving diversity among recommendations lead to discrimination among the users. We introduce the notion of user fairness which has been overlooked in literature so far and propose measures to quantify it. Our experiments on two diversification algorithms show that an increase in aggregate diversity results in increased disparity among the users.",
        "Fairness-Aware Recommendation in Multi-Sided Platforms Fairness is a critical system-level objective in recommender systems that has been the subject of extensive recent research. It is especially important in multi-sided recommendation platforms where it may be important to optimize utilities not just for the end user, but also for other entities such as item sellers or producers who desire a fair representation of their items. Existing solutions either lack the multi-sided nature of fairness in recommendations, or do not properly address various aspects of multi-sided fairness in recommendations. In this thesis, we aim at first investigating the impact of unfair recommendations on the system and how it can negatively affect major entities in the system. Then, we seek to propose a general graph-based solution that works as a post processing approach after recommendation generation to tackle the unfairness of recommendations. We plan to perform extensive experiments to evaluate the effectiveness of the proposed approach. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems.",
        "A triadic closure and homophily-based recommendation system for online social networks. Recommendation systems are popular both commercially and in the research community. For example, Online in Social Networks (OSNs) like Twitter, they are gaining an increasing attention since a lot of connection are established between users without any previous knowledge. This highlights one of the key features of a lot of OSNs: the creation of relationships between users. Therefore, it is important to find new ways to provide interesting friendships suggestions. However, mining and analyzing data from large scale Social Networks can become critical in terms of computational resources. This is particularly true in the context of ubiquitous access, where resource-constrained mobile devices are used to access the social network services. To this end, designing architectures/solutions offering the possibility of operating in a Mobile Cloud scenario is of key importance. Accordingly, we present a new recommendation system scheme that tries to find the right trade-offs between the exploitation of the already existing links/relationships and the interest affinities between users. In particular, such scheme is based on an inherently parallel Hubs And Authorities algorithm together with similarity measures that, for scalability purposes, can be easily transposed in a cloud scenario. The first one let us leverage triadic closures while the second one takes into account homophily. The proposal is supported by an extensive performance analysis on publicly available Twitter data. In particular, we proved the effectiveness World Wide Web of the proposed recommendation system by using several performance metrics available in the literature which include precision, recall, F-measure and G-measure. The results show encouraging perspectives in terms of both effectiveness and scalability, that are driving our future research efforts.",
        "When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance. We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase.",
        "Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems To address the sparsity and cold-start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve the performance of recommendation. In this article, we consider the knowledge graph (KG) as the source of side information. To address the limitations of existing embedding-based and path-based methods for KG-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the KG into recommender systems. RippleNet has two versions: (1) The outward propagation version, which is analogous to the actual ripples on water, stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user's potential interests along links in the KG. The multiple \"ripples\" activated by a user's historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item. (2) The inward aggregation version aggregates and incorporates the neighborhood information biasedly when computing the representation of a given entity. The neighborhood can be extended to multiple hops away to model high-order proximity and capture users' long-distance interests. In addition, we intuitively demonstrate how a KG assists with recommender systems in RippleNet, and we also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the KG. Through extensive experiments on real-world datasets, we demonstrate that both versions of RippleNet achieve substantial gains in a variety of scenarios, including movie, book, and news recommendations, over several state-of-the-art baselines. The explosive growth of online content and services has provided overwhelming choices for users, such as news, movies, music, restaurants, and books. Recommender systems (RSs) intend to address the information explosion by finding a small set of items for users to meet their personalized interests. Among recommendation strategies, collaborative filtering (CF), which considers users' historical interactions and makes recommendations based on their potential common preferences, has achieved great success . However, CF-based methods usually suffer from the sparsity of user-item interactions and the cold-start problem. Inspired by the success of applying KGs in a wide variety of tasks, researchers also tried to utilize KGs to improve the performance of recommender systems. As shown in , KGs can benefit the recommendation from three aspects: (1) KGs introduce semantic relatedness among items, which can help find their latent connections and improve the precision of recommended items; (2) KGs consist of relations with various types, which is helpful for extending a user's interests reasonably and increasing the diversity of recommended items; and (3) KGs connect a user's historical records and the recommended ones, thereby bringing explainability to recommender systems.In general, existing KG-aware recommendation can be classified into two categories: The first category is embedding-based methods , which preprocess a KG with knowledge graph embedding (KGE) algorithms and incorporate the learned entity embeddings into a recommendation framework. For example, Collaborative Knowledge-based Embedding (CKE) [55] combines a CF module with knowledge embedding, text embedding, and image embedding of items in a unified Bayesian framework. A Deep Knowledge-aware Network (DKN) treats entity embeddings and word embeddings as different channels, then designs a CNN framework to combine them together for news recommendation. Signed Heterogeneous Information Network Embedding (SHINE) designs deep autoencoders to embed sentiment networks, social networks, and profile (knowledge) networks for celebrity recommendations. The Knowledge-enhanced Sequential Recommender (KSR) integrates RNN with a key-value memory network and further incorporates entity embeddings learned from TransE [3] to enhance the model capacity. Embedding-based methods show high flexibility in utilizing KGs to assist with",
        "Accurate and Novel Recommendations: An Algorithm Based on Popularity Forecasting Recommender systems are in the center of network science, and they are becoming increasingly important in individual businesses for providing efficient, personalized services and products to users. Previous research in the field of recommendation systems focused on improving the precision of the system through designing more accurate recommendation lists. Recently, the community has been paying attention to diversity and novelty of recommendation lists as key characteristics of modern recommender systems. In many cases, novelty and precision do not go hand in hand, and the accuracy-novelty dilemma is one of the challenging problems in recommender systems, which needs efforts in making a trade-off between them.In this work, we propose an algorithm for providing novel and accurate recommendation to users. We consider the standard definition of accuracy and an effective self-information-based measure to assess novelty of the recommendation list. The proposed algorithm is based on item popularity, which is defined as the number of votes received in a certain time interval. Wavelet transform is used for analyzing popularity time series and forecasting their trend in future timesteps. We introduce two filtering algorithms based on the information extracted from analyzing popularity time series of the items. The popularity-based filtering algorithm gives a higher chance to items that are predicted to be popular in future timesteps. The other algorithm, denoted as a novelty and population-based filtering algorithm, is to move toward items with low popularity in past timesteps that are predicted to become popular in the future. The introduced filters can be applied as adds-on to any recommendation algorithm. In this article, we use the proposed algorithms to improve the performance of classic recommenders, including item-based collaborative filtering and Markovbased recommender systems. The experiments show that the algorithms could significantly improve both the accuracy and effective novelty of the classic recommenders.",
        "A Deep Bayesian Tensor-Based System for Video Recommendation With the availability of abundant online multi-relational video information, recommender systems that can effectively exploit these sorts of data and suggest creatively interesting items will become increasingly important. Recent research illustrates that tensor models offer effective approaches for complex multi-relational data learning and missing element completion. So far, most tensor-based user clustering models have focused on the accuracy of recommendation. Given the dynamic nature of online media, recommendation in this setting is more challenging as it is difficult to capture the users' dynamic topic distributions in sparse data settings as well as to identify unseen items as candidates of recommendation. Targeting at constructing a recommender system that can encourage more creativity, a deep Bayesian probabilistic tensor framework for tag and item recommendation is proposed. During the score ranking processes, a metric called Bayesian surprise is incorporated to increase the creativity of the recommended candidates. The new algorithm, called Deep Canonical PARAFAC Factorization (DCPF), is evaluated on both synthetic and large-scale real-world problems. An empirical study for video recommendation demonstrates the superiority of the proposed model and indicates that it can better capture the latent patterns of interactions and generates interesting recommendations based on creative tag combinations. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Relational data based personalized recommendation plays an essential role in today's online video viewing platforms. While those emerging websites provide services of millions of TV shows, movies, and music clips, they are also the main source of capturing browsing or operational data for a huge amount of users. To increase user stickiness and to enhance the overall satisfaction, services working on finding the most relevant contents and recommending unseen items to users are highly desired.As accuracy is not the only important objective of recommendation , objectives including serendipity and novelty have been widely discussed in recommender systems. However, the relation between these objectives has not been extensively studied. Hou et al. claim that an item with high novelty will not necessarily be serendipitous, and a serendipitous recommendation will not necessarily be novel. Hence, it would be helpful to design a comprehensive objective that can combine both novelty and serendipity for recommendation, which is called computational creativity.Because most large-scale digitized data nowadays can be regarded as multi-relational data, a tensor format is a natural and sound approach . Tensor decomposition is a well-known approach for representing latent relationships inherent in the multi-dimensional data. Despite their merits, traditional multi-way factor models suffer from two primary drawbacks. One is that they often fail to capture coupled and nonlinear interactions between entities [46], and are not robust to datasets containing noisy and missing values. Another is that when applying the completion mechanism to recommendation applications, the diversity of recommendation candidates is difficult to be guaranteed .Targeting these two drawbacks, through proper generative models, nonparametric Bayesian multi-way analysis algorithms (like in References [11], [46], and [38]) are especially appealing, since they provide efficient ways to deal with distinct data types as well as data with missing values and noises. Meanwhile, deep networks have been proved great empirical success in various domains . With their capability in providing more compact nonlinear representations for feature learning, it would be interesting to adopt deep learning in one or more of the tensor modes and assess its effectiveness on tensor completion.Motivated by the aforementioned considerations, this article presents a fully conjugate deep probabilistic approach for tensor decomposition. Based on the Canonical Parallel Factors (PARAFAC) decomposition, short as CP decomposition, the proposed model is capable of clustering the three-way data along each direction simultaneously. To find a more compact representation in the latent space of each mode, a multi-layer factorization is imposed on the mode factor matrix to incorporate nonlinear mapping. As a fully conjugate Bayesian model, efficient Gibbs sampling inference is facilitated, with automatic determination of core tensor rank.In order to recommend items not only considering the accuracy but also the computational creativity, we consider a new probabilistic ranking mechanism for the recommender system. The posterior likelihood of Bayesian model can also be leveraged as a probabilistic ranking generation mechanism. The framework using the proposed deep tensor for probabilistic recommendation breaks the task at hand into the following components: (1) a tensor construction stage to build user-item-tag correlation; (2) a tensor decomposition stage to learn factors for each component mode; (3) a stage of tensor completion, which computes the creativity value of tag pairs; and (4) a recommender stage to rank the candidate items according to both precision and creativity consideration. This approach is evaluated using a real-world video recommendation task, with large amounts of users, videos, and corresponding video description tags.The rest of this article is organized as follows. Section 2 reviews related work on tensor decomposition, deep Bayesian factorization, and recommendation based on beyond-accuracy metrics.",
        "Local collaborative ranking. Personalized recommendation systems are used in a wide variety of applications such as electronic commerce, social networks, web search, and more. Collaborative filtering approaches to recommendation systems typically assume that the rating matrix (e.g., movie ratings by viewers) is lowrank. In this paper, we examine an alternative approach in which the rating matrix is locally low-rank. Concretely, we assume that the rating matrix is low-rank within certain neighborhoods of the metric space defined by (user, item) pairs. We combine a recent approach for local low-rank approximation based on the Frobenius norm with a general empirical risk minimization for ranking losses. Our experiments indicate that the combination of a mixture of local low-rank matrices each of which was trained to minimize a ranking loss outperforms many of the currently used stateof-the-art recommendation systems. Moreover, our method is easy to parallelize, making it a viable approach for large scale real-world rank-based recommendation systems.",
        "Conversational Recommendation: Formulation, Methods, and Evaluation Recommender systems have demonstrated great success in information seeking. However, traditional recommender systems work in a static way, estimating user preferences on items from past interaction history. This prevents recommender systems from capturing dynamic and fine-grained preferences of users. Conversational recommender systems bring a revolution to existing recommender systems. They are able to communicate with users through natural languages during which they can explicitly ask whether a user likes an attribute or not. With the preferred attributes, a recommender system can conduct more accurate and personalized recommendations. Therefore, while they are still a relatively new topic, conversational recommender systems attract great research attention. We identify four emerging directions: (1) exploration and exploitation trade-off in the cold-start recommendation setting; (2) attributecentric conversational recommendation; (3) strategy-focused conversational recommendation; and (4) dialogue understanding and response generation. This tutorial covers these four directions, providing a review of existing approaches and progress on the topic. By presenting the emerging and promising topic of conversational recommender systems, we aim to provide take-aways to practitioners to build their own systems. We also want to stimulate more ideas and discussions with audiences on core problems of this topic such as task formalization, dataset collection, algorithm development, and evaluation, with the ambition of facilitating the development of conversational recommender systems. CCS CONCEPTS \u2022 Information systems \u2192 Users and interactive retrieval; Recommender systems; Personalization; \u2022 Human-centered computing \u2192 Interactive systems and tools.",
        "From Missing Data to Boltzmann Distributions and Time Dynamics: The Statistical Physics of Recommendation The challenge of building a good recommendation system is deeply connected to missing data-unknown features and labels to suggest the most \"valuable\" items to the user. The mysterious properties of the power law distributions that generally arises out of recommender (and social systems in general) create skewed and long-tailed consumption patterns that are often still puzzling to many of us. Missing data and skewed distributions create not just accuracy and recall problems, but also capacity allocation problems, which are at the roots of recent debate on inclusiveness and responsibility. So how do we move forward in the face of these immense conceptual and practical issues?",
        "DifRec: A Social-Diffusion-Aware Recommender System. Recommender systems used in current online social platforms make recommendations by only considering how relevant an item is to a specific user but they ignore the fact that, thanks to mechanisms like sharing or re-posting across the underlying social network, an item recommended to a user i propagates through the network and can reach another user j without needing to be explicitly recommended to j too. Overlooking this fact may lead to an inefficient use of the limited recommendation slots. These slots can instead be exploited more profitably by avoiding unnecessary duplicates and recommending other equally relevant items.In this work we take a step towards rethinking recommender systems by exploiting the anticipated social-network information diffusion and withholding recommendation of items that are expected to reach a user through sharing/reposting. We devise a novel recommender system, DifRec, by formulating the problem of maximizing the total user engagement as an allocation problem in a properly-defined neighborhoodness graph, i.e., a graph that models the conflicts of recommending an item to a user who will receive it anyway by social diffusion. We show that the problem is NP-hard and propose efficient heuristics to solve it.We assess the performance of our DifRec by involving real data from Tumblr platform. We obtain substantial improvements in overall user engagement (130-190%) over the real recommender system embedded in Tumblr and over various existing recommender systems.",
        "Deep Recommender Systems Utilizing Side Information Recommendation Systems (RS) are designed to assist users in decision making by recommending the most appropriate information or products for them. Nonetheless, many RS suffer from limitations such as data sparsity and cold-start. Side information (SI) can be integrated into a recommender system to tackle these limitations. In my Ph.D. research, I seek to build on and extend the use of SI for RS. Specifically, I propose new types and representations of SI and develop new methods to integrate SI into RS to boost its performance. This paper presents the conceptual foundation and motivation of my Ph.D. research. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems; \u2022 Computing methodologies \u2192 Neural networks; Ensemble methods.",
        "Novelty and diversity enhancement and evaluation in recommender systems and information retrieval. The development and evaluation of Information Retrieval and Recommender Systems has traditionally focused on the relevance and accuracy of retrieved documents and recommendations, respectively. However, there is an increasing realization that accuracy alone might be a sub-optimal strategy for a successful user experience . Properties such as novelty and diversity have been explored in both fields for assessing and enhancing the usefulness of search results and recommendations. In this doctoral research we study the assessment and enhancement of both properties in the confluence of Information Retrieval and Recommender Systems.In Information Retrieval, diversity is posited a quality of result lists that helps cope with the ambiguity and underspecification in users queries, whereas novelty is understood as the quality of a system that avoids redundancy. In the last years there have been proposals and significant advances for both assessing [2] and enhancing [3] novelty and diversity in search results. Recommender Systems can be seen as a particular case of personalized Information Retrieval where there is no explicit query, but just implicit information about the user's interests. Researchers have realized that improving recommendations' usefulness and user satisfaction may require more than being accurate. Recommending novel items helps avoiding recommending too obvious or popular items . Moreover, the effectiveness of recommendations can be enhanced by acknowledging the user's diverse tastes .The novelty and diversity problem has nonetheless been approached under different views and formulations in both fields, giving rise to different models, methodologies, and metrics, with little convergence between both fields in this particular area. Our research addresses the problem of modeling and enhancing novelty and diversity in the context of Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6-11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2610382 . recommender systems, seeking connections to diversity research in Information Retrieval.First, we propose the definition of a new framework of novelty and diversity metrics for Recommender Systems [4]. This framework adapts and unifies several metrics from the state of the art under a common probabilistic formulation. This formulation allows us to consider rank and relevance when assessing the novelty and diversity of recommendation lists, which has not been considered previously.Second, we research the connection of recommendation diversity to the diversity principles and techniques developed in Information Retrieval for diversity . We explore diversification techniques for recommender systems based on feature spaces and the extraction of sub-profiles, establishing an analogy with query interpretations or aspects for diversity in Information Retrieval. We study the relationship between the choice of feature spaces and their effectiveness in diversification as well.Furthermore, the thesis delves into the definition of new methods to both assess and enhance novelty and diversity in recommender systems. For instance, we analyze the formalization of diversity and the explicit role of relevance under a probabilistic perspective . We also consider the role of coverage, redundancy and size-awareness in diversity for recommender systems. Finally, we also study the popularity bias observed in many recommendation algorithms and its effect in terms of novelty and sales diversity, proposing ways to alleviate it.We conduct empirical validation and evaluation of metrics and methods in offline experiments with publicly available datasets. Furthermore, we envision to incorporate online and crowdsourced experiments to gain further insights on the effects of novel and diverse recommendations.",
        "Recommendation in Context-Rich Environment: An Information Network Analysis Approach. Recommendation has received tremendous attention recently due to its wide and successful applications across different domains. Different from traditional setting of recommendation tasks, modern recommendation tasks are usually exposed in a context-rich environment. For example, in addition to a user-item rating matrix, users and items are connected to other objects via different relationships and they are usually associated with rich attributes, such as text and spatio-temporal information. It turns out that heterogeneous information network serves a natural data model to capture the rich context of these recommendation tasks. In this tutorial, we will systematically introduce the methodologies of using heterogeneous information network mining approach to solve recommendation tasks, and demonstrate the effectiveness of such methods using different applications, ranging from collaboration recommendation in scientific research network to job recommendation in professional social network, and to drug discovery in biomedical networks. The topics to be covered in the tutorial include: (1) overall introduction; (2) recommendation in heterogeneous information networks, which introduces the general methodology of how to model the recommendation problem as a heterogeneous information network mining problem; (3) recommendation in a text-rich setting, where the information network is further enriched by refined analysis of text information; (4) recommendation with spatio-temporal information, where entities and relationships in the network are associated with spatio-temporal attributes; and (5) research frontiers for context-rich recommendation. KeywordsRecommender Systems; Information Network Mining; contexrich recommendation; Spatio-temporal recommendation ORGANIZERSYizhou Sun (Ph.D., UIUC), is an assistant professor at University of California, Los Angeles (UCLA). Her princi- Hongzhi Yin (Ph.D., Peking University), works as an ARC DECRA Fellow (Lecturer-Level) with The University of Queensland (UQ), Australia. His main research interests are in social media analysis and recommender system, especially spatio-temporal recommendation. He has published over 30 papers, with more than 10 papers and a scholar book on spatio-temporal recommendation. He received 2016 Australia Discovery Early Career Researcher Award and 2014 Distinguished Doctor Degree Thesis Award of Peking University. Dr. Yin has been the lecturer for the course \"Information Retrieval and Web Search\" in the University of Queensland and delivered many oral presentations and tutorials in major conferences. Recommendation has received tremendous attention recently due to its wide and successful applications across different domains. Different from traditional setting of recommendation tasks, modern recommendation tasks are usually exposed in a context-rich environment. Take a movie recommendation task as an example, in addition to a user-movie rating matrix, users and movies are usually connected to other objects via different relationships. For example, users are linked to other users via friendship links, and movies are 941 linked to actors/actresses, directors, and genres. The rich context can be naturally captured by heterogeneous information networks, which contain multiple types of objects and their interactions. In other words, the user-movie interaction matrix is embedded into a more complicated heterogeneous information network. By using information network mining approaches, the rich information can be fully utilized for the recommendation task, which turns out to be extremely helpful in overcoming the sparsity and cold start issue in traditional recommendation methodologies.Moreover, objects and relationships in the network are usually associated with rich text and spatio-temporal information. For example, in a citation recommendation task for scientific research, text information in papers become very critical to determine whether a paper should be considered as a reference. In another example of restaurant recommendation for users in a location-based social network service, spatio-temporal information become critical in determining the quality of the recommendation. In these cases, information networks will be further enriched by these types of information. We will then introduce how to integrate the text and spatio-temporal information into the information network, and how to utilize them to provide a better recommendation.In this tutorial, we will introduce the recent progress in recommendation algorithms that are related to information network mining approach in an organized way, which will provide researchers and practitioners in this field with a new angle. Duration and Sessions.This is one-day tutorial. The topics to be covered in the tutorial include: (1) overall introduction; (2) recommendation in heterogeneous information networks, which introduces the general methodology of how to model the recommendation problem as a heterogeneous information network mining problem; (3) recommendation in a text-rich setting, where the information network is further enriched by refined analysis of text information; (4) recommendation with spatio-temporal information, where entities and relationships in the network are associated with spatio-temporal attributes; and (5) research frontiers for context-rich recommendation. Audience and Prerequisite.The tutorial is suitable for academic and industrial researchers, graduate students, and practitioners who are interested in dealing with recommendation tasks with rich contexts including social, textual, and spatio-temporal attributes. The audience will benefit from the new perspective of solving recommendation problems in an information network mining approach and the broad applications covered in the tutorial. Only basic knowledge of data mining, information networks, and recommender systems are needed. Tutorial Material.The tutorial materials will be provided to attendees via: http://web.cs.ucla.edu/~yzsun/Tutorials/WWW2017/index. html. TUTORIAL OUTLINEThe outline of the proposed tutorial is summarized below. Part I: Preliminaries. We first introduce the preliminaries of recommendation and information network mining. Part II: Recommendation in Heterogeneous Information Networks. Heterogeneous information network that contains multiple types of objects and multiple types of relationships serves a natural data model in capturing the rich context for many recommendation tasks. In this part, we introduce three types of methodologies that can leverage the rich semantics in heterogeneous information networks for recommendation.1. Recommendation as link prediction in heterogeneous information networks. Recommendation tasks can be viewed as link prediction tasks in heterogeneous information network, where meta-path-based proximity features can be formed for each source-target pair in the network and the link prediction problem is converted to classification problem. We introduce two examples to illustrate the methodology, including collaboration recommendation in scientific research network and drug recommendation for protein target in drug discovery research [4].2. Recommendation with social network as side information. Recent studies have found that social relations that are usually captured in social networks are very helpful for the recommendation tasks. For example, friends may share similar interests and thus like similar movies. Social network information can be considered as an additional type of links that connect users, in addition to the standard user-item interactions.3. Entity recommendation with implicit feedback via meta-path propagation. In many cases, only implicit feedback is received for the recommendation task. In , the user-item implicit feedback information is designed to be propagated via meta-paths that are derived from heterogeneous information networks. Each meta-path implies an intention why a user likes an item. This leads to multiple models corresponding to different latent spaces. The best weighted combination of these latent spaces can be learned using Bayesian ranking optimization. In , a personalized model that learns a separate set of weights for each user group is further proposed.4. Entity similarity regularization for recommendation as feature selection. When facing rich context information, one of the biggest challenges is to select the most relevant information for the recommendation task. In [43], a multi-view graph-regularized matrix factorization framework is proposed to incorporate and learn the weights of heterogeneous relationships.Part III: Recommendation in a Text-Rich Setting. In the context-rich environment, there exists not only different kinds of relationships between users, items and objects of various types, but also a rich amount of text information (e.g., in the forms of short-text descriptions, documents, keywords, etc.) attached to users, items and other objects. It is of great interests to study: (1) how to overcome the cold-start problem with the help of text information, and (2) how to incorporate rich text signals to improve the recommendation in heterogeneous information networks. In this part of the tutorial, we give an overview of the problems and 942 methodologies in content-based recommendation, introduce the approaches for recommendation in text-rich information networks, and discuss about the methods for constructing structured networks from text data and performing recommendation based on the text networks so constructed.1. Content-based recommendation: An Overview. We will introduce the basics of item representation and user representation and different methods for learning user and item representations, by using literature search as an example. We then provide an overview of the state-of-theart recommendation models including keywordbased models and ontology-based models. In particular, we will discuss how user feedback can be leveraged in the models.",
        "Neural Collaborative Ranking. Recommender systems are aimed at generating a personalized ranked list of items that an end user might be interested in. With the unprecedented success of deep learning in computer vision and speech recognition, recently it has been a hot topic to bridge the gap between recommender systems and deep neural network. And deep learning methods have been shown to achieve state-of-the-art on many recommendation tasks. For example, a recent model, NeuMF, first projects users and items into some shared low-dimensional latent feature space, and then employs neural nets to model the interaction between the user and item latent features to obtain state-of-the-art performance on the recommendation tasks. NeuMF assumes that the non-interacted items are inherent negative and uses negative sampling to relax this assumption. In this paper, we examine an alternative approach which does not assume that the non-interacted items are necessarily negative, just that they are less preferred than interacted items. Specifically, we develop a new classification strategy based on the widely used pairwise ranking assumption. We combine our classification strategy with the recently proposed neural collaborative filtering framework, and propose a general collaborative ranking framework called Neural Network based Collaborative Ranking (NCR). We resort to a neural network architecture to model a user's pairwise preference between items, with the belief that neural network will effectively capture the latent structure of latent factors. The experimental results on two real-world datasets show the superior performance of our models in comparison with several state-of-the-art approaches.",
        "Recommender systems with social regularization. Although Recommender Systems have been comprehensively analyzed in the past decade, the study of social-based recommender systems just started. In this paper, aiming at providing a general method for improving recommender systems by incorporating social network information, we propose a matrix factorization framework with social regularization. The contributions of this paper are four-fold: (1) We elaborate how social network information can benefit recommender systems; (2) We interpret the differences between social-based recommender systems and trust-aware recommender systems; (3) We coin the term Social Regularization to represent the social constraints on recommender systems, and we systematically illustrate how to design a matrix factorization objective function with social regularization; and (4) The proposed method is quite general, which can be easily extended to incorporate other contextual information, like social tags, etc. The empirical analysis on two large datasets demonstrates that our approaches outperform other state-of-the-art methods.",
        "A rough set-based association rule approach for a recommendation system for online consumers a b s t r a c tIncreasing use of the Internet gives consumers an evolving medium for the purchase of products and services and this use means that the determinants for online consumers' purchasing behaviors are more important. Recommendation systems are decision aids that analyze a customer's prior online purchasing behavior and current product information to find matches for the customer's preferences. Some studies have also shown that sellers can use specifically designed techniques to alter consumer behavior. This study proposes a rough set based association rule approach for customer preference analysis that is developed from analytic hierarchy process (AHP) ordinal data scale processing. The proposed analysis approach generates rough set attribute functions, association rules and their modification mechanism. It also determines patterns and rules for e-commerce platforms and product category recommendations and it determines possible behavioral changes for online consumers.\u00a9 2016 Elsevier Ltd. All rights reserved. Research backgroundMost online businesses that are involved in the sales of products/services, such as commercial websites, are aware of the need to acquire knowledge about their online consumers. However, knowledge about online consumers, though available, is not accessible, so it is critical to analyze all of the available knowledge if online users in search for information, products, or services and then highlight potential product promotions and marketing alternatives from online firms. In this regard, recommendation systems are increasingly used by online businesses to suggest options to online consumers . A recommendation system supports users a search for information, products, or services (such as books, movies, music, digital products, Web sites and TV programs) by aggregating and analyzing suggestions from other users, reviews from various authorities and user attributes .As an information technology that supports a personalized service, recommendation systems are widely used by ecommerce practitioners and have become an important research topic in the field of information sciences and decision support systems (Liang et al., 2008;. Recommendation systems are decision aids that analyze a customer's prior online behavior and present information on products that match the customer's preferences. By analyzing the consumer's purchase history or communicating with the consumer, recommendation systems employ quantitative and qualitative methods to determine the products that best suit the customer. Most of the current recommendation systems recommend products that have a high probability of being purchased . They employ content-based filtering (CBF) Please cite this article as: S.-h. Liao, H.-k. Chang, A rough set-based association rule approach for a recommendation system for online consumers, Information Processing and Management (2016), http://dx",
        "Item-based top-N recommendation algorithms The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems-a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user-item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.",
        "A novel social network hybrid recommender system based on hypergraph topologic structure. With the advent and popularity of social network, more and more people like to share their experience in social network. However, network information is growing exponentially which leads to information overload. Recommender system is an effective way to solve this problem. The current research on recommender systems is mainly focused on research models and algorithms in social networks, and the social networks structure of recommender systems has not been analyzed thoroughly and the so-called cold start problem has not been resolved effectively. We in this paper propose a novel hybrid recommender system called Hybrid Matrix Factorization(HMF) model which uses hypergraph topology to describe and analyze the interior relation of social network in the system. More factors including contextual information, user feature, item feature and similarity of users ratings are all taken into account based on matrix factorization method. Extensive experimental evaluation on publicly available datasets demonstrate that the proposed hybrid recommender system outperforms the existing recommender systems in tackling cold start problem and dealing with sparse rating datasets. Our system also enjoys improved recommendation accuracy compared with several major existing recommendation approaches.",
        "Introduction to recommender systems: Algorithms and Evaluation Recommender systems use the opinions of members of a community to help individuals in that community identify the information or products most likely to be interesting to them or relevant to their needs. These systems, originally referred to as collaborative filtering systems, were developed to address two challenges that could not be addressed by existing keyword-based information filtering systems. First, they addressed the problem of overwhelming numbers of on-topic documents-ones which would all be selected by a keyword filterby filtering based on human judgement about the quality of those documents. Second, they addressed the problem of filtering non-text documents based on human taste. For example, the Ringo system applied collaborative filtering to recommend music to individuals and later research and commercial systems applied the same techniques to other art forms.Early research in this area focused largely on the ability of these systems to generate recommendations that were valued by the users of the system. And, indeed, these systems generated substantial enthusiasm and support from their users. In 1996, at the first of a series of workshops on collaborative filtering, it first became clear that some fairly simple algorithms (namely weighted knearest-neighbor algorithms applied to a sparse matrix of the ratings that users assigned to particular items or documents) worked well for several different research groups and application areas. This workshop also started using the term \"Recommender Systems\" and led to the publication of a special issue of Communications of the ACM on the topic (March 1997).At this point, the Recommender Systems research field diverged. Substantial commercial interest focused attention on a variety of practical questions, including the speed with which recommendations could be generated, the scale of problems that could be addressed, and the assessment of the value of recommendations to the business itself or to the customers. At the same time, a broad range of machine learning researchers (broadly defined) started applying a wide variety of techniques to recommendation problems, exploring issues of improving accuracy of algorithms, better exploiting knowledge about the Editor's address:",
        "Using control theory for stable and efficient recommender systems. The aim of a web-based recommender system is to provide highly accurate and up-to-date recommendations to its users; in practice, it will hope to retain its users over time. However, this raises unique challenges. To achieve complex goals such as keeping the recommender model upto-date over time, we need to consider a number of external requirements. Generally, these requirements arise from the physical nature of the system, for instance the available computational resources. Ideally, we would like to design a system that does not deviate from the required outcome. Modeling such a system over time requires to describe the internal dynamics as a combination of the underlying recommender model and the its users' behavior. We propose to solve this problem by applying the principles of modern control theory-a powerful set of tools to deal with dynamical systems-to construct and maintain a stable and robust recommender system for dynamically evolving environments. In particular, we introduce a design principle by focusing on the dynamic relationship between the recommender system's performance and the number of new training samples the system requires. This enables us to automate the control other external factors such as the system's update frequency. We show that, by using a Proportional-Integral-Derivative controller, a recommender system is able to automatically and accurately estimate the required input to keep the output close to a pre-defined requirements. Our experiments on a standard rating dataset show that, by using a feedback loop between system performance and training, the tradeoff between the effectiveness and efficiency of the system can be well maintained. We close by discussing the widespread applicability of our approach to a variety of scenarios that recommender systems face.",
        "SIGIR 2018 Workshop on ExplainAble Recommendation and Search (EARS 2018). Explainable recommendation and search attempt to develop models or methods that not only generate high-quality recommendation or search results, but also intuitive explanations of the results for users or system designers, which can help to improve the system transparency, persuasiveness, trustworthiness, and effectiveness, etc. This is even more important in personalized search and recommendation scenarios, where users would like to know why a particular product, web page, news report, or friend suggestion exists in his or her own search and recommendation lists. The workshop focuses on the research and application of explainable recommendation and search, and gathers researchers as well as practitioners in the field for discussions, idea communications, and research promotions. MOTIVATION AND APPROPRIATENESSThe motivation of the workshop is to promote the research and application of Explainable Recommendation and Search, under the background of Explainable AI in a more general sense. Early recommendation and search systems adopted intuitive yet easily explainable models to generate recommendation and search lists, such as user-based and item-based collaborative filtering for recommendation, which provide recommendations based on similar users or items, or TF-IDF based retrieval models for search, which provide document ranking lists according to word similarity between different documents.However, state-of-the-art recommendation and search models extensively rely on complex machine learning and latent representation models such as matrix factorization or even deep neural networks, and they work with various types of information sources such as ratings, text, images, audio or video signals. The complexity nature of state-of-the-art models make search and recommendation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8-12, 2018 systems as blank-boxes for end users, and the lack of explainability weakens the persuasiveness and trustworthiness of the system for users, making explainable recommendation and search important research issues to the IR community.In a broader sense, researchers in the whole artificial intelligence community have also realized the importance of Explainable AI, which aims to address a wide range of AI explainability problems in deep learning, computer vision, automatic driving systems, and natural language processing tasks. As an important branch of AI research, this further highlights the importance and urgency for our IR/RecSys community to address the explainability issues of various recommendation and search systems. THEME AND PURPOSEThe purpose of the workshop is to gather researchers and practitioners of recommendation and search systems to communicate the latest ideas and research achievements on explainable recommendation and search, discuss about the advantages and disadvantages of existing approaches, and share the ideas of future directions of recommendation and search in the explanation perspective. Based on this workshop, we would not only like to present the latest research achievements, but also connect researchers in the community that are interested in the explainable recommendation and search topic to promote this direction in the following years.The main themes and topics of the workshop include but are not limited to: FORMAT AND PLANED ACTIVITIESThe workshop intends to be a half day workshop with a keynote speech, several paper presentations, and a poster session to give researchers opportunity for extensive discussions. The tentative schedule of events include:\u2022 An industry keynote speech to highlight the importance of explainable recommendation and search in real-world system and business.\u2022 A mixture of 5 to 10 long and short paper presentations.\u2022 A poster session for researchers to communicate and make discussions extensively. INDUSTRY KEYNOTETitle: Shakespeare of Alibaba: Practice of Intelligent Recommendation Reason Generation in Alibaba.. : Explainable recommendation and search is a very promising topic in both academia and industry in the recent years. There are many human generated recommendation reasons for products in Alibaba Taobao to improve user experience and to increase user stickiness. However, relying on human-generated content will result in low coverage, low quality stability, and high financial expenditure. With the rapid development of deep learning technology in NLP, especially in the nature language generation field, we tried natural language generation approach in recommendation reason generation and achieved good results. We created recommendation reasons for auction and auction list, which covered millions of product categories in Taobao e-commerce, and the generated explanations were used for large-scale real-world transactions in \"2017 Double 11 Shopping Festival\" without any manual checking. Industry-level real system experiments show that it was very difficult to distinguish whether the explanations are machine-generated or manually-written, and the content generation can be controlled in multiple dimensions such as text style, text length, topics, etc. We will introduce our solution and technical details about generating free-text explanations in this keynote.Bio: Dr. Qingsong Hua has been working in Alibaba search algorithm team since 2013 and has led a lot of projects about search relevance, quality, and conversion effectiveness. He is responsible for the intelligent recommendation reason generation project, which won the biggest Alibaba technical award in \"2017 Double 11",
        "Opportunity model for e-commerce recommendation: right product; right time. Most of existing e-commerce recommender systems aim to recommend the right product to a user, based on whether the user is likely to purchase or like a product. On the other hand, the effectiveness of recommendations also depends on the time of the recommendation. Let us take a user who just purchased a laptop as an example. She may purchase a replacement battery in 2 years (assuming that the laptop's original battery often fails to work around that time) and purchase a new laptop in another 2 years. In this case, it is not a good idea to recommend a new laptop or a replacement battery right after the user purchased the new laptop. It could hurt the user's satisfaction of the recommender system if she receives a potentially right product recommendation at the wrong time. We argue that a system should not only recommend the most relevant item, but also recommend at the right time.This paper studies the new problem: how to recommend the right product at the right time? We adapt the proportional hazards modeling approach in survival analysis to the recommendation research field and propose a new opportunity model to explicitly incorporate time in an e-commerce recommender system. The new model estimates the joint probability of a user making a follow-up purchase of a particular product at a particular time. This joint purchase probability can be leveraged by recommender systems in various scenarios, including the zero-query pull-based recommendation scenario (e.g. recommendation on an e-commerce web site) and a proactive push-based promotion scenario (e.g. email or text message based marketing). We evaluate the opportunity modeling approach with multiple metrics. Experimental results on a data collected by a real-world e-commerce website(shop.com) show that it can predict a user's follow-up purchase behavior at a particular time with descent accuracy. In addition, the opportunity model significantly improves the conversion rate in pull-based systems and the user satisfaction/utility in push-based systems.",
        "A social recommendation method based on an adaptive neighbor selection mechanism a b s t r a c tRecommender systems are techniques to make personalized recommendations of items to users. In e-commerce sites and online sharing communities, providing high quality recommendations is an important issue which can help the users to make effective decisions to select a set of items. Collaborative filtering is an important type of the recommender systems that produces user specific recommendations of the items based on the patterns of ratings or usage (e.g. purchases). However, the quality of predicted ratings and neighbor selection for the users are important problems in the recommender systems. Selecting suitable neighbors set for the users leads to improve the accuracy of ratings prediction in recommendation process. In this paper, a novel social recommendation method is proposed which is based on an adaptive neighbor selection mechanism. In the proposed method first of all, initial neighbors set of the users is calculated using clustering algorithm. In this step, the combination of historical ratings and social information between the users are used to form initial neighbors set for the users. Then, these neighbor sets are used to predict initial ratings of the unseen items. Moreover, the quality of the initial predicted ratings is evaluated using a reliability measure which is based on the historical ratings and social information between the users. Then, a confidence model is proposed to remove useless users from the initial neighbors of the users and form a new adapted neighbors set for the users. Finally, new ratings of the unseen items are predicted using the new adapted neighbors set of the users and the top _ N interested items are recommended to the active user. Experimental results on three real-world datasets show that the proposed method significantly outperforms several state-of-the-art recommendation methods.",
        "Multirelational Recommendation in Heterogeneous Networks Recommender systems are key components in information-seeking contexts where personalization is sought. However, the dominant framework for recommendation is essentially two dimensional, with the interaction between users and items characterized by a single relation. In many cases, such as social networks, users and items are joined in a complex web of relations, not readily reduced to a single value. Recent multirelational approaches to recommendation focus on the direct, proximal relations in which users and items may participate. Our approach uses the framework of complex heterogeneous networks to represent such recommendation problems. We propose the weighted hybrid of low-dimensional recommenders (WHyLDR) recommendation model, which uses extended relations, represented as constrained network paths, to effectively augment direct relations. This model incorporates influences from both distant and proximal connections in the network. The WHyLDR approach raises the problem of the unconstrained proliferation of components, built from ever-extended network paths. We show that although component utility is not strictly monotonic with path length, a measure based on information gain can effectively prune and optimize such hybrids.",
        "Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an opensource Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems.",
        "Streaming Ranking Based Recommender Systems. Studying recommender systems under streaming scenarios has become increasingly important because real-world applications produce data continuously and rapidly. However, most existing recommender systems today are designed in the context of an offline setting. Compared with the traditional recommender systems, large-volume and high-velocity are posing severe challenges for streaming recommender systems. In this paper, we investigate the problem of streaming recommendations being subject to higher input rates than they can immediately process with their available system resources (i.e., CPU and memory). In particular, we provide a principled framework called as SPMF (Stream-centered Probabilistic Matrix Factorization model), based on BPR (Bayesian Personalized Ranking) optimization framework, for performing efficient ranking based recommendations in stream settings. Experiments on three real-world datasets illustrate the superiority of SPMF in online recommendations. CCS CONCEPTS\u2022 Information systems \u2192 Data mining; Retrieval models and ranking; KEYWORDS recommender systems; information retrieval; streaming data; online applications; user behaviour modeling ACM Reference Format:",
        "Meta-recommendation systems: user-controlled integration of diverse recommendations. In a world where the number of choices can be overwhelming, recommender systems help users find and evaluate items of interest. They do so by connecting users with information regarding the content of recommended items or the opinions of other individuals. Such systems have become powerful tools in domains such as electronic commerce, digital libraries, and knowledge management. In this paper, we address such systems and introduce a new class of recommender system called metarecommenders.Meta-recommenders provide users with personalized control over the generation of a single recommendation list formed from a combination of rich data using multiple information sources and recommendation techniques. We discuss experiments conducted to aid in the design of interfaces for a meta-recommender in the domain of movies. We demonstrate that meta-recommendations fill a gap in the current design of recommender systems. Finally, we consider the challenges of building real-world, usable meta-recommenders across a variety of domains.",
        "Cross-Platform App Recommendation by Jointly Modeling Ratings and Texts Over the last decade, the renaissance of Web technologies has transformed the online world into an application (App) driven society. While the abundant Apps have provided great convenience, their sheer number also leads to severe information overload, making it difficult for users to identify desired Apps. To alleviate the information overloading issue, recommender systems have been proposed and deployed for the App domain. However, existing work on App recommendation has largely focused on one single platform (e.g., smartphones), while it ignores the rich data of other relevant platforms (e.g., tablets and computers).In this article, we tackle the problem of cross-platform App recommendation, aiming at leveraging users' and Apps' data on multiple platforms to enhance the recommendation accuracy. The key advantage of our proposal is that by leveraging multiplatform data, the perpetual issues in personalized recommender systems-data sparsity and cold-start-can be largely alleviated. To this end, we propose a hybrid solution, STAR (short for \"croSs-plaTform App Recommendation\") that integrates both numerical ratings and textual content from multiple platforms. In STAR, we innovatively represent an App as an aggregation of common features across platforms (e.g., App's functionalities) and specific features that are dependent on the resided platform. In light of this, STAR can discriminate a user's preference on an App by separating the user's interest into two parts (either in the App's inherent factors or platform-aware features). To evaluate our proposal, we construct two real-world datasets that are crawled from the App stores of iPhone, iPad, and iMac. Through extensive experiments, we show that our STAR method consistently outperforms highly competitive recommendation methods, justifying the rationality of our cross-platform App recommendation proposal and the effectiveness of our solution.",
        "Building a Lifestyle Recommender System Recommender systems are new types of internet-based software tools, designed to help users find their way through today's complex on-line shops and entertainment websites. Here we provide an overview of current recommender systems, and then outline a new Lifestyle Recommender System, which employs techniques such as evolutionary search and a 3D avatar to provide tailored and friendly suggestions for users.",
        "Development and Evaluation of a Highly Scalable News Recommender System The development of highly scalable recommender systems, able to deliver recommendations in real time, is a challenging task. In contrast to traditional recommender systems, recommending news entails additional requirements. These requirements include tight response times, heavy load peaks, and continuously changing collections of users and items. In this paper we describe our participation at the CLEF-NewsREEL challenge 2015. We present our highly scalable implementation of a news recommendation algorithm. The developed approach alleviates all the specific challenges of news recommender systems. We use the Akka framework to build an asynchronous, distributable system able to run concurrently on multiple machines. Based on the framework a time window-based, most popular algorithm for recommending news articles is implemented. The evaluation shows that our system implemented using the Akka framework scales well with the restrictions and outperforms the recommendation precision of the baseline recommender."
    ],
    "personalised search in e-commerce": [
        "Data design for personalization: current challenges and emerging opportunities. There are several definitions of personalization but one that relates specifically to internet technologies is the following: Personalization technology enables the dynamic insertion, customization or suggestion of content in any format that is relevant to the individual user, based on the user's implicit behavior and preferences, and explicitly given details 1 . Personalization is central to most Internet experiences. Personalization is a data-driven process, whether the data are explicitly gathered (e.g., by asking people to fill out forms) or implicitly (e.g. through analysis of behavioral data).It is clear that designing for effective personalization poses interesting engineering and computer science challenges. However, personalization is also a user experience issue. We believe that encouraging dialogue and collaboration between data mining experts, content providers, and user-focused researchers will offer gains in the area of personalization for search and for other domains. This is increasingly the case as devices enable more forms of data to be gathered, are always on/connected and are always with users. This workshop brings researchers interested in the area of personalization to share their research, explore possibilities for collaboration, and work on defining an agenda for Data Design for Personalization.",
        "Improving Contextual Suggestions using Open Web Domain Knowledge Contextual suggestion aims at recommending items to users given their current context, such as location-based tourist recommendations. Our contextual suggestion ranking model consists of two main components: selecting candidate suggestions and providing a ranked list of personalized suggestions. We focus on selecting appropriate suggestions from the ClueWeb12 collection using tourist domain knowledge inferred from social sites and resources available on the public Web (Open Web). Specifically, we generate two candidate subsets retrieved from the ClueWeb12 collection, one by filtering the content on mentions of the location context, and one by integrating domain knowledge derived from the Open Web. The impact of these candidate selection methods on contextual suggestion effectiveness is analyzed using the test collection constructed for the TREC Contextual Suggestion Track in 2014. Our main findings are that contextual suggestion performance on the subset created using Open Web domain knowledge is significantly better than using only geographical information. Second, using a prior probability estimated from domain knowledge leads to better suggestions and improves the performance.",
        "Wisdom of Crowds or Wisdom of a Few? In this keynote we give an introduction to wisdom of crowds in the Web, the long tail of web content, and the bias involved in the generation of user generated content (UGC). This bias creates the wisdom of ad hoc crowds or the wisdom of a few. Although it is well known that user activity in most settings follows a power law, that is, few people do a lot, while most do nothing, there are few studies that characterize well this activity. In a recent analysis of social network data we corroborated that a small percentage of the active users (passive users are the majority) represent at least the 50% of the UGC. As a sub-product, we also found a lower bound for the digital desert, the content in the Web that nobody reads. These results implies that most of the wisdom comes from a few users, which is not that surprising, as the Web is a reflection of our own society, where economical or political power also is in the hands of minorities.",
        "Personalisation of Web Search: Exploring Search Query Parameters and User Information Privacy Implications-The Case of Google Personalised search adapts search results to the needs and interests of users. This is done through user data collected through various implicit and explicit methods and is used to build profiles of information needs of users. This paper highlights the need to explore search query parameters and determine their impact on personalisation. This is a first step in exploring the mechanisms of personal data collection and how personalised search uses personal data, which subsequently impacts the information privacy of users. It was found that location parameters have more impact on personalisation than the parameter 'pws' that switches personalisation on or off. Hence, it is important to undertake further research that investigates the impact of other types of search query parameters, their contribution towards search personalisation and their impact on user information privacy.",
        "E-commerce product search: personalization, diversification, and beyond. In this tutorial we discuss challenges, techniques and analytics in search ranking particularly applied to product search in e-commerce. Several challenges appear in this context, both from a research as well as an application standpoint. We present various approaches adopted in the industry, review well-known research techniques developed over the last decade, draw parallels to traditional web search highlighting the new challenges in this setting, and dig deep into some of the algorithmic and technical approaches developed. A specific approach that advances theoretical techniques and illustrates practical impact considered here is of identifying most suited results quickly from a large database. Settings span cold start users and advanced users for whom personalization is possible. In this context, top-k and skylines are discussed as they form a key approach that spans the web, data mining, and database communities. These present powerful tools for search across multidimensional items with clear preferences within each attribute, like product search as opposed to regular web search. Categories and Subject Descriptors KeywordsSearch; e-commerce; top-k; skylines; algorithms MAIN TOPICSProblems in search and ranking ranges from traditional relational databases to document systems to web search and, more recently, vertical applications like product search in e-commerce or vertical search applications in insurance, finance, or health. This tutorial addresses the intersection of web, data mining, and the database research and also touches upon user interface issues and interaction experiences. A specific focus in e-commerce involves returning a small set of results to users, as well as requiring minimal intervention or input from them. These two constraints are main concerns with skylines and top-k, respectively, and therefore have drawn aCopyright is held by the author/owner(s). WWW'14 Companion, April 7-11, 2014, Seoul, Korea. ACM 978-1-4503-2745-9/14/04. http://dx.doi.org/10.1145/2567948.2577272.great deal of attention in the recent years with several interesting ideas being proposed in the research community. Our work describes several of these approaches. We talk about the research and application of some of these techniques to e-commerce websites like eBay.We have developed a series of techniques that introduce new approaches in this realm and have shown formal theoretical guarantees as well as direct applications to real world settings. The majority of the tutorial is based on the following papers.\u2022 Identifying representative subset of the skyline set: We focus on our work on efficient skyline algorithms [1] and regret-minimizing representative databases . For the former, we consider a streaming model and present theoretically provable bounds. The latter involves modeling users with unknown utility functions. Given a list of k tuples, we say that a user is x% satisfied with the list if the utility she obtains from the best tuple in this list is at least x% of the utility she obtains from the best tuple in the whole database. We show an algorithm exists that outputs a small set of k tuples that makes every user at least x% satisfied.\u2022 Interaction based approaches: Our work in this technique builds on [2] and presents an interactive regret minimization framework . In this paper, we adopt the notion of maximum regret ratio when users have linear utility functions proposed in [2]. Here we study how interactions through help further improve the guarantees on user happiness.\u2022 Web Search Applications: Web search ranking and relevance, and the importance of Top-k has been well studied. User attention and response to search results decay non-linearly by rank . Measures like DCG and NDCG are well studied in the IR community and have been applied in web search systems, advertising modeling systems . This tutorial delves in to some of these techniques originally designed for generic web search and contrasts them with approaches specifically designed for e-commerce product search.\u2022 e-commerce product search: More specifically in commerce search top-k plays an important role as product placement at the right rank has revenue implications. Relevance, diversity, and revenue all play a role in this. One of the primary works we focus on this topic is our industry application paper that goes beyond relevance in marketplace search . In this paper we study diversity and its relations to search relevance in the context of an online marketplace. We conduct a large-scale log-based study using click-stream data and introduce three main metrics: selection (diversity), trust, and value. In our analysis we also show how these interact with 189",
        "Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning Nowadays e-commerce search has become an integral part of many people's shopping routines. Two critical challenges stay in today's e-commerce search: how to retrieve items that are semantically relevant but not exact matching to query terms, and how to retrieve items that are more personalized to different users for the same search query. In this paper, we present a novel approach called DPSR, which stands for Deep Personalized and Semantic Retrieval, to tackle this problem. Explicitly, we share our design decisions on how to architect a retrieval system so as to serve industry-scale traffic efficiently and how to train a model so as to learn query and item semantics accurately. Based on offline evaluations and online A/B test with live traffics, we show that DPSR model outperforms existing models, and DPSR system can retrieve more personalized and semantically relevant items to significantly improve users' search experience by +1.29% conversion rate, especially for long tail queries by +10.03%. As a result, our DPSR system has been successfully deployed into JD.com's search production since 2019. CCS CONCEPTS \u2022 Computing methodologies \u2192 Neural networks; \u2022 Information systems \u2192 Information retrieval.",
        "A novel collaborative filtering-based framework for personalized services in m-commerce. With the rapid growth of wireless technologies and handheld devices, m-commerce is becoming a promising research area. Personalization is especially important to the success of mcommerce. This paper proposes a novel collaborative filteringbased framework for personalized services in m-commerce. The framework extends our previous work by using Online Analytical Processing (OLAP) to represent the relations among user, content and context information, and adopting a multi-dimensional collaborative filtering model to perform inference. It provides a powerful and well-founded mechanism to personalization for mcommerce. We implemented it in an existing m-commerce platform, and experimental results demonstrate its feasibility and correctness.",
        "A proactive personalised retrieval system. We present a personalised retrieval system that captures explicit relevance feedback to build an evolving user profile with multiple aspects. The user profile is used to proactively retrieve results between search sessions to support multi-session search tasks. This approach to supporting users with their multi-session search tasks is evaluated in a between-subjects multiple time-series study with ten subjects performing two simulated work situation tasks over five sessions. System interaction data shows that subjects using the personalised retrieval system issue fewer queries and interact with fewer results than subjects using a baseline system. The interaction data also shows a trend of subjects interacting with the proactively retrieved results in the personalised retrieval system.",
        "Information Discovery in E-commerce: Half-day SIGIR 2018 Tutorial EXTENDED. E-commerce (electronic commerce or EC) is the buying and selling of goods and services, or the transmitting of funds or data online. Ecommerce platforms come in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay, JD.com and platforms targeting specific markets such as Bol.com and Booking.com.Information retrieval has a natural role to play in e-commerce, especially in connecting people to goods and services. Information discovery in e-commerce concerns different types of search (exploratory search vs. lookup tasks), recommender systems, and natural language processing in e-commerce portals. Recently, the explosive popularity of e-commerce sites has made research on information discovery in e-commerce more important and more popular. There is increased attention for e-commerce information discovery methods in the community as witnessed by an increase in publications and dedicated workshops in this space. Methods for information discovery in e-commerce largely focus on improving the performance of e-commerce search and recommender systems, on enriching and using knowledge graphs to support e-commerce, and on developing innovative question-answering and bot-based solutions that help to connect people to goods and services.Below we describe why we believe that the time is right for an introductory tutorial on information discovery in e-commerce, the objectives of the proposed tutorial, its relevance, as well as more practical details, such as the format, schedule and support materials. MOTIVATIONIn recent years, the explosive popularity of e-commerce sites has reshaped users' shopping habits. An increasing number of customers Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA \u00a9 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210185 now prefer to spend more time shopping online. E-commerce corporations, e.g., Amazon, Alibaba, and JD.com, are amassing billions of user requests per day. As part of this process, large volumes of multi-modal data, including user search logs, clicks, orders, reviews, images, and chat logs, etc., are being generated. From an information retrieval point of view, discovering and employing appropriate information from the sheer volume of e-commerce data to enhance performances of e-commerce products presents interesting challenges for both academic and industrial researchers.Information discovery in e-commerce can be divided into five main directions:\u2022 e-commerce user behavior modeling and profiling \u2022 content analysis of e-commerce text, \u2022 e-commerce search and ranking, \u2022 e-commerce recommender systems, and \u2022 e-commerce conversational interaction systems.Each of these areas comes with its own set of research challenges. For example, in e-commerce search there may be no hypertext links between products; there is a click stream, but there is also an order stream. E-commerce information discovery problems are wide in scope and range from user interaction modalities. There is a growing body of established methods for information discovery in e-commerce (see the schedule below for a broad range of examples). Most of them are aimed at developing algorithms about product search in e-commerce [1, 10], candidate retrieval in e-commerce , user behavior analysis [52, 59], recommender systems [14, 21, 35, 37, 46], content analysis, and conversational interactions . These areas, and the methods developed, form the core around which most ongoing research efforts concerning information discovery for e-commerce is organized.The time is right to organize and present this material to a broad audience of interested information retrieval researchers, whether junior or senior, whether academic or industrial. One of the key aims of the proposed tutorial is to bring together, and offer a unified perspective on, the large number of methods for e-commerce information discovery that are available today. To achieve this, we describe the basic architecture about information discovery in e-commerce, algorithms for e-commerce information discovery, and evaluation principles. We supplement this with an account of available datasets and packages based on these. We also present e-commerce applications accompanied by examples.We expect the tutorial to be useful for both academic and industrial researchers who either want to develop e-commerce information discovery methods, use them in their own research, or",
        "Learning a Hierarchical Embedding Model for Personalized Product Search. Product search is an important part of online shopping. In contrast to many search tasks, the objectives of product search are not con ned to retrieving relevant products. Instead, it focuses on nding items that satisfy the needs of individuals and lead to a user purchase. e unique characteristics of product search make search personalization essential for both customers and e-shopping companies. Purchase behavior is highly personal in online shopping and users o en provide rich feedback about their decisions (e.g. product reviews). However, the severe mismatch found in the language of queries, products and users make traditional retrieval models based on bag-of-words assumptions less suitable for personalization in product search. In this paper, we propose a hierarchical embedding model to learn semantic representations for entities (i.e. words, products, users and queries) from di erent levels with their associated language data. Our contributions are three-fold: (1) our work is one of the initial studies on personalized product search; (2) our hierarchical embedding model is the rst latent space model that jointly learns distributed representations for queries, products and users with a deep neural network; (3) each component of our network is designed as a generative model so that the whole structure is explainable and extendable. Following the methodology of previous studies, we constructed personalized product search benchmarks with Amazon product data. Experiments show that our hierarchical embedding model signi cantly outperforms existing product search baselines on multiple benchmark datasets.",
        "A Transformer-based Embedding Model for Personalized Product Search Product search is an important way for people to browse and purchase items on E-commerce platforms. While customers tend to make choices based on their personal tastes and preferences, analysis of commercial product search logs has shown that personalization does not always improve product search quality. Most existing product search techniques, however, conduct undifferentiated personalization across search sessions. They either use a fixed coefficient to control the influence of personalization or let personalization take effect all the time with an attention mechanism. The only notable exception is the recently proposed zero-attention model (ZAM) that can adaptively adjust the effect of personalization by allowing the query to attend to a zero vector. Nonetheless, in ZAM, personalization can act at most as equally important as the query and the representations of items are static across the collection regardless of the items co-occurring in the user's historical purchases. Aware of these limitations, we propose a transformerbased embedding model (TEM) for personalized product search, which could dynamically control the influence of personalization by encoding the sequence of query and user's purchase history with a transformer architecture. Personalization could have a dominant impact when necessary and interactions between items can be taken into consideration when computing attention weights. Experimental results show that TEM outperforms state-of-the-art personalization product retrieval models significantly.",
        "Taxonomy discovery for personalized recommendation. Personalized recommender systems based on latent factor models are widely used to increase sales in e-commerce. Such systems use the past behavior of users to recommend new items that are likely to be of interest to them. However, latent factor model suffer from sparse user-item interaction in online shopping data: for a large portion of items that do not have sufficient purchase records, their latent factors cannot be estimated accurately.In this paper, we propose a novel approach that automatically discovers the taxonomies from online shopping data and jointly learns a taxonomy-based recommendation system. Out model is non-parametric and can learn the taxonomy structure automatically from the data. Since the taxonomy allows purchase data to be shared between items, it effectively improves the accuracy of recommending tail items by sharing strength with the more frequent items. Experiments on a large-scale online shopping dataset confirm that our proposed model improves significantly over state-ofthe-art latent factor models. Moreover, our model generates high-quality and human readable taxonomies. Finally, using the algorithm-generated taxonomy, our model even outperforms latent factor models based on the human-induced taxonomy, thus alleviating the need for costly manual taxonomy generation.",
        "Personalized Ranking in eCommerce Search We address the problem of personalization in the context of eCommerce search. Specifically, we develop personalization ranking features that use in-session context to augment a generic ranker optimized for conversion and relevance. We use a combination of latent features learned from item co-clicks in historic sessions and content based features that use item title and price. Personalization in search has been discussed extensively in the existing literature. The novelty of our work is combining and comparing content based and content agnostic features and showing that they complement each other to result in a significant improvement of the ranker. We experimentally show that our technique significantly outperforms a generic ranker in terms of Mean Reciprocal Rank (MRR). We also provide anecdotal evidence for the semantic similarity captured by the item embeddings on the eBay search engine.",
        "Local collaborative ranking. Personalized recommendation systems are used in a wide variety of applications such as electronic commerce, social networks, web search, and more. Collaborative filtering approaches to recommendation systems typically assume that the rating matrix (e.g., movie ratings by viewers) is lowrank. In this paper, we examine an alternative approach in which the rating matrix is locally low-rank. Concretely, we assume that the rating matrix is low-rank within certain neighborhoods of the metric space defined by (user, item) pairs. We combine a recent approach for local low-rank approximation based on the Frobenius norm with a general empirical risk minimization for ranking losses. Our experiments indicate that the combination of a mixture of local low-rank matrices each of which was trained to minimize a ranking loss outperforms many of the currently used stateof-the-art recommendation systems. Moreover, our method is easy to parallelize, making it a viable approach for large scale real-world rank-based recommendation systems.",
        "Persona-ization: Searching on Behalf of Others Many information retrieval tasks involve searching on behalf of others. Example scenarios include searching for a present to give a friend, trying to find \"cool\" clothes for a teenage child, looking for medical supplies for an elderly relative [1], or planning a group activity that many friends will enjoy. In this paper, we use demographically annotated web search logs to present a large-scale study of such \"on behalf of\" searches. We develop an exploratory technique for recognizing such searches, and present information to describe and understand the phenomenon, including the demographics of who is searching, who they are searching for and on what topics.",
        "Personalized interactive faceted search. Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. A faceted search system presents users with keyvalue metadata that is used for query refinement. While popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. Nor are there repeatable benchmarks for evaluating a faceted search engine. This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user's behavior. This paper also proposes a utility based framework to evaluate the faceted interface. In order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology.",
        "Challenges in commerce search. Commerce search engines allow users to discover products, learn about them, and, importantly, make purchases. Commerce search is a challenging problem -one that is very different to conventional text and web search. In this talk, we discuss what makes commerce search hard, how eBay has solved some of these problems, and what challenges eBay faces in the next generation of its search technologies. We also discuss the recent release of eBay's Cassini engine, share facts and figures about its scale, and outline the progress eBay has made in ranking and relevance for commerce search.",
        "Personalized click prediction in sponsored search. Sponsored search is a multi-billion dollar business that generates most of the revenue for search engines. Predicting the probability that users click on ads is crucial to sponsored search because the prediction is used to influence ranking, filtering, placement, and pricing of ads. Ad ranking, filtering and placement have a direct impact on the user experience, as users expect the most useful ads to rank high and be placed in a prominent position on the page. Pricing impacts the advertisers' return on their investment and revenue for the search engine. The objective of this paper is to present a framework for the personalization of click models in sponsored search. We develop user-specific and demographic-based features that reflect the click behavior of individuals and groups. The features are based on observations of search and click behaviors of a large number of users of a commercial search engine. We add these features to a baseline non-personalized click model and perform experiments on offline test sets derived from user logs as well as on live traffic. Our results demonstrate that the personalized models significantly improve the accuracy of click prediction.",
        "Search from Personal to Social Context: Progress and Challenges User and behavioral modeling plays a critical role in a variety of online services such as web search, advertising, ecommerce, and news recommendation. For example, our ability to accurately interpret the intent of a web search can be informed by knowledge of the web pages a searcher was viewing when initiating the search or recent actions of the searcher such as queries issued, results clicked, and pages viewed. In this talk, I will describe a recent framework for personalized search which improves the quality of search results by enabling a representation of a broad variety of context including the searcher's long-term interests, recent activity, current focus, and other user characteristics. Then, I will review a variety of related work that extends these approaches from signals focused on the individual to social signals such as likes, cohorts, and affiliation networks. Finally, I'll speculate on how social signals and networks can provide directions for relatively unexplored directions in social personalized retrieval.",
        "Attentive Long Short-Term Preference Modeling for Personalized Product Search E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users' inherent purchasing bias and evolves slowly. By contrast, the latter reflects users' purchasing inclination in a relatively short period. They both affect users' current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long-and shortterm user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users' current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long-and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Nowadays, e-commerce has become very popular with the flourishing of the Internet. Its convenient access to an enormous variety of goods enables people to shop almost all products at home. When users on e-commerce websites like Amazon 1 intend to purchase a product, they usually pick their desired ones among millions of items by searching. The standard scenario for the online product search is that a user submits a query, and then the search engine returns a ranked list of products relevant to the given query. Typically, queries submitted by users comprise only a few keywords (e.g., white T-shirt for men), which are usually too short or ambiguous to express the users' needs precisely, resulting in unsatisfactory search results. Besides, users' preferences on products could be very diverse (due to different backgrounds, i.e., age, gender, income) or strongly affected by the current contexts (e.g., season, location). Thereby it is not appropriate to return the same search results to different users for the same query. In the light of this, considering the user's personal intentions under the current contexts and aiming to return relevant products to the given query, the so-called personalized product search, plays a pivotal role in meeting the user's current shopping needs.The prerequisite for a good personalized product search engine is to accurately model the user preferences and effectively integrate them with the current query. It is well recognized that there are two types of user preferences : long-term ones and short-term ones. The former refers to the user's inherent and relatively stable (evolving slowly) purchasing bias, such as favorite colors, preferred fashion designs, fitting sizes and consumption level, which is imperceptibly influenced by the user's personal backgrounds, like age, upbringing, marriage, education and income. By contrast, the short-term user preference conveys user's purchasing intention in a relatively short period. It is affected by incidentally transient events, such as new product release, season change and special personal occasions like birthday , which can be inferred from the user's recently purchased products. Compared to the long-term user preference, the short-term one changes more frequently and drastically.Traditional approaches to product search ] often employ simple matching between the queries and products without harnessing the user's specific attributes. They hardly characterize the user specificity, let alone the heterogeneity. It thus often leads to sub-optimal performance due to the user's diverse expectations. In view of this, personalization becomes quite necessary for the product search. In fact, personalized search has been widely studied in literature over the past few years. Thereinto, the study of personalized web search incorporating the user preference is the most related sub-direction to our work, and it can be roughly divided into two categories: short-term session-based 2 and long-term profile-based web search. Approaches in the first category capture the short-term user preference from search sessions. Nevertheless, different from the web search whereby a session often contains plenty of queries and rich click results, the session data in the product search 3 are usually too sparse to train a good personalized"
    ],
    "sentiment analysis": [
        "Self-training from labeled features for sentiment analysis a b s t r a c tSentiment analysis concerns about automatically identifying sentiment or opinion expressed in a given piece of text. Most prior work either use prior lexical knowledge defined as sentiment polarity of words or view the task as a text classification problem and rely on labeled corpora to train a sentiment classifier. While lexicon-based approaches do not adapt well to different domains, corpus-based approaches require expensive manual annotation effort.In this paper, we propose a novel framework where an initial classifier is learned by incorporating prior information extracted from an existing sentiment lexicon with preferences on expectations of sentiment labels of those lexicon words being expressed using generalized expectation criteria. Documents classified with high confidence are then used as pseudo-labeled examples for automatical domain-specific feature acquisition. The word-class distributions of such self-learned features are estimated from the pseudolabeled examples and are used to train another classifier by constraining the model's predictions on unlabeled instances. Experiments on both the movie-review data and the multi-domain sentiment dataset show that our approach attains comparable or better performance than existing weakly-supervised sentiment classification methods despite using no labeled documents.",
        "A Meta-Framework for Modeling the Human Reading Process in Sentiment Analysis This article introduces a sentiment analysis approach that adopts the way humans read, interpret, and extract sentiment from text. Our motivation builds on the assumption that human interpretation should lead to the most accurate assessment of sentiment in text. We call this automated process Human Reading for Sentiment (HRS). Previous research in sentiment analysis has produced many frameworks that can fit one or more of the HRS aspects; however, none of these methods has addressed them all in one approach. HRS provides a meta-framework for developing new sentiment analysis methods or improving existing ones. The proposed framework provides a theoretical lens for zooming in and evaluating aspects of any sentiment analysis method to identify gaps for improvements towards matching the human reading process. Key steps in HRS include the automation of humans low-level and high-level cognitive text processing. This methodology paves the way towards the integration of psychology with computational linguistics and machine learning to employ models of pragmatics and discourse analysis for sentiment analysis. HRS is tested with two state-of-the-art methods; one is based on feature engineering, and the other is based on deep learning. HRS highlighted the gaps in both methods and showed improvements for both. CCS Concepts: r Information systems \u2192 Retrieval tasks and goals;Additional Key Words and Phrases: Sentiment analysis, human reading, psychology, supervised learning and notions ACM Reference Format:Ramy Baly, Roula Hobeica, Hazem Hajj, Wassim El-Hajj, Khaled Bashir Shaban, and Ahmad Al-Sallab. 2016. A meta-framework for modeling the human reading process in sentiment analysis.",
        "Active learning for sentiment analysis on data streams: Methodology and workflow implementation in the ClowdFlows platform a b s t r a c tSentiment analysis from data streams is aimed at detecting authors' attitude, emotions and opinions from texts in real-time. To reduce the labeling effort needed in the data collection phase, active learning is often applied in streaming scenarios, where a learning algorithm is allowed to select new examples to be manually labeled in order to improve the learner's performance. Even though there are many on-line platforms which perform sentiment analysis, there is no publicly available interactive on-line platform for dynamic adaptive sentiment analysis, which would be able to handle changes in data streams and adapt its behavior over time. This paper describes ClowdFlows, a cloud-based scientific workflow platform, and its extensions enabling the analysis of data streams and active learning. Moreover, by utilizing the data and workflow sharing in ClowdFlows, the labeling of examples can be distributed through crowdsourcing. The advanced features of ClowdFlows are demonstrated on a sentiment analysis use case, using active learning with a linear Support Vector Machine for learning sentiment classification models to be applied to microblogging data streams.",
        "Appraisal navigator. Much interesting text on the web consists largely of opinionated or evaluative text, as opposed to directly informative text. The new field of 'sentiment analysis' seeks to characterize such aspects of natural language text, as opposed to just the bare facts. We suggest that 'appraisal expression extraction' should be viewed as a fundamental task for sentiment analysis. We define an 'appraisal expression' to be a piece of text expressing some evaluative stance towards a particular object. The task is to find these elements and characterize the type and orientation (positive or negative) of the evaluative stance, as well as its target and possibly its source. Potential applications of these methods include new approaches to the now-traditional tasks of sentiment classification and opinion mining, as well as possibly for adversarial textual analysis and intention detection for intelligence applications.",
        "Joint sentiment/topic model for sentiment analysis. Sentiment analysis or opinion mining aims to use automated tools to detect subjective information such as opinions, attitudes, and feelings expressed in text. This paper proposes a novel probabilistic modeling framework based on Latent Dirichlet Allocation (LDA), called joint sentiment/topic model (JST), which detects sentiment and topic simultaneously from text. Unlike other machine learning approaches to sentiment classification which often require labeled corpora for classifier training, the proposed JST model is fully unsupervised. The model has been evaluated on the movie review dataset to classify the review sentiment polarity and minimum prior information have also been explored to further improve the sentiment classification accuracy. Preliminary experiments have shown promising results achieved by JST.",
        "Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification. Current approaches for contextual sentiment lexicon construction in phrase-level sentiment analysis assume that the numerical star rating of a review represents the overall sentiment orientation of the review text. Although widely adopted, we find through user rating analysis that this is not necessarily true. In this paper, we attempt to bridge the gap between phrase-level and review/document-level sentiment analysis by leveraging the results given by review-level sentiment classification to boost phrase-level sentiment polarity labeling in contextual sentiment lexicon construction tasks, using a novel constrained convex optimization framework. Experimental results on both English and Chinese reviews show that our framework improves the precision of sentiment polarity labeling by up to 5.6%, which is a significant improvement from current approaches.",
        "Sentiment-Specific Representation Learning for Document-Level Sentiment Analysis. In this paper, we propose a representation learning research framework for document-level sentiment analysis. Given a document as the input, document-level sentiment analysis aims to automatically classify its sentiment/opinion (such as thumbs up or thumbs down) based on the textural information. Despite the success of feature engineering in many previous studies, the hand-coded features do not well capture the semantics of texts. In this research, we argue that learning sentiment-specific semantic representations of documents is crucial for document-level sentiment analysis. We decompose the document semantics into four cascaded constitutes: (1) word representation, (2) sentence structure, (3) sentence composition and (4) document composition. Specifically, we learn sentiment-specific word representations, which simultaneously encode the contexts of words and the sentiment supervisions of texts into the continuous representation space. According to the principle of compositionality, we learn sentiment-specific sentence structures and sentence-level composition functions to produce the representation of each sentence based on the representations of the words it contains. The semantic representations of documents are obtained through document composition, which leverages the sentiment-sensitive discourse relations and sentence representations.",
        "Residual-Duet Network with Tree Dependency Representation for Chinese Question-Answering Sentiment Analysis Question-answering sentiment analysis (QASA) is a novel but meaningful sentiment analysis task based on question-answering online reviews. Existing neural network-based models that conduct sentiment analysis of online reviews have already achieved great success. However, the syntax and implicitly semantic connection in the dependency tree have not been made full use of, especially for Chinese which has specific syntax. In this work, we propose a Residual-Duet Network leveraging textual and tree dependency information for Chinese question-answering sentiment analysis. In particular, we explore the synergies of graph embedding with structural dependency links to learn syntactic information. The transverse and longitudinal compression encoders are developed to capture sentiment evidence with disparate types of compression and different residual connections. We evaluate our model on three Chinese QASA datasets in different domains. Experimental results demonstrate the superiority of our proposed model in Chinese question-answering sentiment analysis.",
        "Approval network: a novel approach for sentiment analysis in social networks. The data-centric impetus and the development of online social networks has led to a significant amount of research that is nowadays more flexible in demonstrating several sociological hypotheses, such as the sentiment influence and transfer among users. Most of the works regarding sentiment classification usually consider text as unique source of information, do not taking into account that social networks are actually networked environments. To overcome this limitation, two main sociological theories should be accounted for addressing any sentiment analysis tasks: homophily and constructuralism. In this paper, we propose Approval Network as a novel graph representation to jointly model homophily and constructuralism, which is intended to better represent the contagion on social networks. To show the potentiality of the proposed representation, two novel sentiment analysis models have been proposed. The first one, related to user-level polarity classification, is approached by presenting a semi-supervised framework grounded on a Markov-based probabilistic model. The second task, aimed at simultaneously extracting aspects and sentiment at message level, is addressed by proposing a novel fully unsupervised generative model. The experimental results show that the proposes sentiment analysis models grounded on Approval Networks are able to outperform not only the traditional models where the relationships are disregarded, but also those computational approaches based on traditional friendship connections.",
        "A Cross-Lingual Joint Aspect/Sentiment Model for Sentiment Analysis. Sentiment analysis in various languages has been a research hotspot with many applications. However, sentiment resources (e.g., labeled corpora, sentiment lexicons) of different languages are unbalanced in terms of quality and quantity, which arouses interests in cross-lingual sentiment analysis aiming at using the resources in a source language to improve sentiment analysis in a target language. Nevertheless, many existing cross-lingual related works rely on a certain machine translation system to directly adapt the labeled data from the source language to the target language, which usually suffers from inaccurate results generated by the machine translation system. On the other hand, most sentiment analysis studies focus on document-level sentiment classification that cannot solve the aspect dependency problem of sentiment words. For instance, in the reviews on a cell phone, long is positive for the lifespan of its battery, but negative for the response time of its operating system. To solve these problems, this paper develops a novel Cross-Lingual Joint Aspect/Sentiment (CLJAS) model to carry out aspect-specific sentiment analysis in a target language using the knowledge learned from a source language. Specifically, the CLJAS model jointly detects aspects and sentiments of two languages simultaneously by incorporating sentiments into a cross-lingual topic model framework. Extensive experiments on different domains and different languages demonstrate that the proposed model can significantly improve the accuracy of sentiment classification in the target language.",
        "SAAN: A Sentiment-Aware Attention Network for Sentiment Analysis. Analyzing public opinions towards products, services and social events is an important but challenging task. Despite the remarkable successes of deep neural networks in sentiment analysis, these approaches do not make full use of the prior sentiment knowledge (e.g., sentiment lexicon, negation words, intensity words). In this paper, we propose a SentimentAware Attention Network (SAAN) to boost the performance of sentiment analysis, which adopts a three-step strategy to learn the sentiment-specific sentence representation. First, we employ a word-level mutual attention mechanism to model word-level correlation. Next, a phrase-level convolutional attention is designed to obtain phrase-level correlation. Finally, a sentence-level multi-head attention mechanism is proposed to capture various sentimental information from different subspaces. The experiments on Movie Review (MR) and Stanford Sentiment Treebank (SST) show that our model consistently outperform the previous methods for sentiment analysis.",
        "A knowledge-based approach for summarising opinions. The popularity of on-line resources, which allow users to review products or services, is motivating new interest in the area of Sentiment Analysis . An important task is the classification of documents according to the overall sentiment, i.e. whether positive or negative. A common behaviour among reviewers is to summarise the overall sentiment of the review in a single sentence, or in a short passage. On the other hand, the rest of the review can express a feeling which is different from the overall judgement. This can be explained by the presence of several aspects or features that the reviewers want to comment on. Traditional summarisation techniques are not always suitable for building opinion-based summaries, and more sentiment-oriented approaches are needed. In this research, we investigate the use of summarisation techniques applied to reviews, and we propose a knowledge-based approach to summarisation, in the context of sentiment analysis. The proposed research is focused on three different aspects. Firstly, we investigate the use of summarisation techniques for sentiment classification. Capturing the key passage of a review can be beneficial for both a sentiment classifier, and for a user who could understand the polarity of a review without reading the full text. Secondly, we investigate how to combine knowledge extracted from the reviews or integrated from external sources, with the purpose of producing opinion-oriented summaries. A knowledgebased approach enables to build aspect-based summaries, i.e. summaries which are focused on a specific aspect of the item under analysis. Thirdly, we analyse the possibility of generating personalised (user-oriented or query-biased) opinion-based summaries. Knowledge extraction tools can be used in order to generate classifications and relationships. Given a representation of a document consisting of the terms, classifications, and relationships extracted from the genuine document, the question is how to generate a summary from the knowledge-based representation. We view term, classification and relationships as propositions. In traditional knowledge-based representations, only classifications and relationships are considered. A proposition-context-based representation that combines terms, classifications, and relationships is proposed in . Sentiment adds an extra dimension to this representation. For example, in the sentence \"Maximus is a brave general\", the term \"brave\" is expressing a positive polarity on the class, while in the sentence \"Peter is a good friend of Mary\" the sentiment is related to the relationship. In a knowledge-oriented approach the sentiment dimension would be inferred to the correct class, leading to the understanding of the sentiment carried by the review. One Copyright is held by the author/owner(s). SIGIR '12, August 12-16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472 of the advantages of such a knowledge representation consists in the possibility of augmenting the knowledge through its integration with external knowledge bases. Resources like WordNet 1 , DBpedia 2 or other domain-specific ontologies can be exploited for this purpose. In feature-based sentiment analysis, a knowledge-based approach is particularly suitable, as the description of complex domains can benefit from the knowledge representation. Complex objects made of different components and attributes can be described in the knowledge base, and linked to opinion terms and phrases. We view knowledge-based summarisation as a key technology, since it enables reasoning over retrieval results. Entity summarisation is a further application of this technology, as the knowledge about a specific entity can be used to create entity profiles. A knowledge-based approach to summarisation can be exploited to produce personalised summaries. In the multi-document context, a user can be interested in a specific aspect of topic. For example, given a set of reviews of a movie, the user might want to read all the comments about a specific actor. This can be also extended for multiple topics. The generation of personalised summaries can be triggered explicitly, i.e. through a specific query, or it can take advantage of a user profile. A user model can be based on explicit keywords selected by the user, or on implicit feedback. A user-oriented study is needed to test the efficacy of a personalised summarisation system.",
        "Tracking Sentiment by Time Series Analysis. In recent years social media have emerged as popular platforms for people to share their thoughts and opinions on all kind of topics. Tracking opinion over time is a powerful tool that can be used for sentiment prediction or to detect the possible reasons of a sentiment change. Understanding topic and sentiment evolution allows enterprises or government to capture negative sentiment and act promptly. In this study, we explore conventional time series analysis methods and their applicability on topic and sentiment trend analysis. We use data collected from Twitter that span over nine months. Finally, we study the usability of outliers detection and different measures such as sentiment velocity and acceleration on the task of sentiment tracking.",
        "A Multimodal Annotation Schema for Non-Verbal Affective Analysis in the Health-Care Domain",
        "Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia. Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using additional images and videos to express their opinions and share their experiences. Sentiment analysis of such large-scale textual and visual content can help better extract user sentiments toward events or topics. Motivated by the needs to leverage large-scale social multimedia content for sentiment analysis, we propose a cross-modality consistent regression (CCR) model, which is able to utilize both the state-of-the-art visual and textual sentiment analysis techniques. We first fine-tune a convolutional neural network (CNN) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis. On top of them, we train our multi-modality regression model. We use sentimental queries to obtain half a million training samples from Getty Images. We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets. The results show that the proposed model can achieve better performance than the state-of-theart textual and visual sentiment analysis algorithms alone.",
        "A hybrid ensemble pruning approach based on consensus clustering and multi-objective evolutionary algorithm for sentiment classification a b s t r a c tSentiment analysis is a critical task of extracting subjective information from online text documents. Ensemble learning can be employed to obtain more robust classification schemes. However, most approaches in the field incorporated feature engineering to build efficient sentiment classifiers.The purpose of our research is to establish an effective sentiment classification scheme by pursuing the paradigm of ensemble pruning. Ensemble pruning is a crucial method to build classifier ensembles with high predictive accuracy and efficiency. Previous studies employed exponential search, randomized search, sequential search, ranking based pruning and clustering based pruning. However, there are tradeoffs in selecting the ensemble pruning methods. In this regard, hybrid ensemble pruning schemes can be more promising.In this study, we propose a hybrid ensemble pruning scheme based on clustering and randomized search for text sentiment classification. Furthermore, a consensus clustering scheme is presented to deal with the instability of clustering results. The classifiers of the ensemble are initially clustered into groups according to their predictive characteristics. Then, two classifiers from each cluster are selected as candidate classifiers based on their pairwise diversity. The search space of candidate classifiers is explored by the elitist Pareto-based multi-objective evolutionary algorithm.For the evaluation task, the proposed scheme is tested on twelve balanced and unbalanced benchmark text classification tasks. In addition, the proposed approach is experimentally compared with three ensemble methods (AdaBoost, Bagging and Random Subspace) and three ensemble pruning algorithms (ensemble selection from libraries of models, Bagging ensemble selection and LibD3C algorithm). Results demonstrate that the consensus clustering and the elitist pareto-based multi-objective evolutionary algorithm can be effectively used in ensemble pruning. The experimental analysis with conventional ensemble methods and pruning algorithms indicates the validity and effectiveness of the proposed scheme.",
        "Twitter sentiment analysis using hybrid cuckoo search method a b s t r a c tSentiment analysis is one of the prominent fields of data mining that deals with the identification and analysis of sentimental contents generally available at social media. Twitter is one of such social medias used by many users about some topics in the form of tweets. These tweets can be analyzed to find the viewpoints and sentiments of the users by using clustering-based methods. However, due to the subjective nature of the Twitter datasets, metaheuristic-based clustering methods outperforms the traditional methods for sentiment analysis. Therefore, this paper proposes a novel metaheuristic method (CSK) which is based on K-means and cuckoo search. The proposed method has been used to find the optimum cluster-heads from the sentimental contents of Twitter dataset. The efficacy of proposed method has been tested on different Twitter datasets and compared with particle swarm optimization, differential evolution, cuckoo search, improved cuckoo search, gauss-based cuckoo search, and two n-grams methods. Experimental results and statistical analysis validate that the proposed method outperforms the existing methods. The proposed method has theoretical implications for the future research to analyze the data generated through social networks/medias. This method has also very generalized practical implications for designing a system that can provide conclusive reviews on any social issues.",
        "In the mood for affective search with web stereotypes. Models of sentiment analysis in text require an understanding of what kinds of sentiment-bearing language are generally used to describe specific topics. Thus, fine-grained sentiment analysis requires both a topic lexicon and a sentiment lexicon, and an affective mapping between both. For instance, when one speaks disparagingly about a city (like London, say), what aspects of city does one generally focus on, and what words are used to disparage those aspects? As when we talk about the weather, our language obeys certain familiar patterns -what we might call clich\u00e9s and stereotypes -when we talk about familiar topics. In this paper we describe the construction of an affective stereotype lexicon, that is, a lexicon of stereotypes and their most salient affective qualities. We show, via a demonstration system called MOODfinger, how this lexicon can be used to underpin the processes of affective query expansion and summarization in a system for retrieving and organizing news content from the Web. Though we adopt a simple bipolar +/-view of sentiment, we show how this stereotype lexicon allows users to coin their own nuanced moods on demand.",
        "A Co-Memory Network for Multimodal Sentiment Analysis. With the rapid increase of diversity and modality of data in usergenerated contents, sentiment analysis as a core area of social media analytics has gone beyond traditional text-based analysis. Multimodal sentiment analysis has become an important research topic in recent years. Most of the existing work on multimodal sentiment analysis extracts features from image and text separately, and directly combine them to train a classifier. As visual and textual information in multimodal data can mutually reinforce and complement each other in analyzing the sentiment of people, previous research all ignores this mutual influence between image and text. To fill this gap, in this paper, we consider the interrelation of visual and textual information, and propose a novel co-memory network to iteratively model the interactions between visual contents and textual words for multimodal sentiment analysis. Experimental results on two public multimodal sentiment datasets demonstrate the effectiveness of our proposed model compared to the state-ofthe-art methods.",
        "Joint Aspect-Sentiment Analysis with Minimal User Guidance Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models. CCS CONCEPTS \u2022 Information systems \u2192 Sentiment analysis.",
        "MultiSentiNet: A Deep Semantic Network for Multimodal Sentiment Analysis. With the prevalence of more diverse and multiform user-generated content in social networking sites, multimodal sentiment analysis has become an increasingly important research topic in recent years. Previous work on multimodal sentiment analysis directly extracts feature representation of each modality and fuse these features for classification. Consequently, some detailed semantic information for sentiment analysis and the correlation between image and text have been ignored. In this paper, we propose a deep semantic network, namely MultiSentiNet, for multimodal sentiment analysis. We first identify object and scene as salient detectors to extract deep semantic features of images. We then propose a visual feature guided attention LSTM model to extract words that are important to understand the sentiment of whole tweet and aggregate the representation of those informative words with visual semantic features, object and scene. The experiments on two public available sentiment datasets verify the effectiveness of our MultiSentiNet model and show that our extracted semantic features demonstrate high correlations with human sentiments.",
        "Customer Sentiment in Web-Based Service Interactions: Automated Analyses and New Insights. We adjust sentiment analysis techniques to automatically detect customer emotion in on-line service interactions of multiple business domains. Then we use the adjusted sentiment analysis tool to report insights into the dynamics of emotion in on-line service chats, using a large dataset of telecommunications customer service interactions. Our analyses show customer emotions start out negative and evolve into positive feelings, as the interaction unfolds. Also, we identify a close relationship between customer emotion dynamics during the service interaction and the concepts of service failure and recovery. This connection manifests itself in customer service quality evaluations after the interaction ends. Our study highlights the connection between customer emotion and service quality as service interactions unfold, and suggests the use of sentiment analysis tools for real-time monitoring and control of web-based service quality.",
        "Aspect-Based Financial Sentiment Analysis using Deep Learning. Aspect based sentiment analysis aims to detect an aspect (i.e. features) in a given text and then perform sentiment analysis of the text with respect to that aspect. This paper aims to give a solution for the FiQA 2018 challenge subtask 1 1 . We perform aspect-based sentiment analysis on the microblogs and headlines of financial domain. We use a multi-channel convolutional neural network for sentiment analysis and a recurrent neural network with bidirectional long short-term memory units to extract aspect from a given headline or microblog. Our proposed model produces a weighted average F1 score of 0.69 for the aspect extraction task and predicts sentiment intensity scores with a mean squared error of 0.112 on 10-fold cross validation. We believe that the developed system has direct applications in the financial domain."
    ],
    "informational retrieval using neural networks": [
        "INLI@FIRE-2018: A Native Language Identification System using Convolutional Neural Networks Native Language Identification is the problem of identifying the first language of speakers based on his/her writings in another language. The proposed approach is a deep learning based methodology using convolutional neural networks. Convolutional neural networks are a class of neural networks that have proven very effective in areas such as pattern recognition and classification. They are able to capture the local texture within the text and can be used to find the representative patterns in a text document. The proposed system consists of a language identification model, which is trained by a corpus of 1233 documents. The experiments were conducted using the dataset provided for INLI@FIRE2018. The results indicate that the system is capable of giving performance comparable to the methods employing more sophisticated approaches.",
        "Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval KeywordsNeural networks; deep learning; information retrieval Website: http://research.microsoft.com/neuir2016 MOTIVATIONIn recent years, deep neural networks have yielded significant performance improvements on speech recognition and computer vision tasks , as well as led to exciting breakthroughs in novel application areas such as automatic voice translation , image captioning , and conversational agents . Despite demonstrating good performance on natural language processing (NLP) tasks (e.g., language modelling [6] and machine translation [1]), the performance of deep neural networks on information retrieval (IR) tasks has had relatively less scrutiny. Recent work in this area has mainly focused on word embeddings [3, 9, 14] and neural models for short text similarity .The lack of many positive results in this area of information retrieval is partially due to the fact that IR tasks such as ranking are fundamentally different from NLP tasks, but also because the IR and neural network communities are only beginning to focus on the application of these techniques to core information retrieval problems. Given that deep learning has made such a big impact, first on speech processing and computer vision and now, increasingly, also on computational linguistics, it seems clear that deep learning will have a major impact on information retrieval and that this is an ideal time for a workshop in this area.Neu-IR (pronounced \"new IR\") will be a forum for new research relating to deep learning and other neural network based approaches to IR. The purpose is to provide an opportunity for people to present new work and early results, compare notes on neural network toolkits, share best practices, and discuss the main challenges facing this line of research.",
        "Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits We study the utility of the lexical translation model (IBM Model 1) for English text retrieval, in particular, its neural variants that are trained end-to-end. We use the neural Model1 as an aggregator layer applied to context-free or contextualized query/document embeddings. This new approach to design a neural ranking system has benefits for effectiveness, efficiency, and interpretability. Specifically, we show that adding an interpretable neural Model 1 layer on top of BERT-based contextualized embeddings (1) does not decrease accuracy and/or efficiency; and (2) may overcome the limitation on the maximum sequence length of existing BERT models. The context-free neural Model 1 is less effective than a BERT-based ranking model, but it can run efficiently on a CPU (without expensive index-time precomputation or query-time operations on large tensors). Using Model 1 we produced best neural and non-neural runs on the MS MARCO document ranking leaderboard in late 2020. Research Question 1. In the past, Model 1 was trained on question-document pairs of similar lengths which simplifies the task of finding useful associations",
        "End-to-End Contextualized Document Indexing and Retrieval with Neural Networks Relevance in ad-hoc retrieval is a fundamental problem of text understanding. Developing neural network methods for this foundational task of Information Retrieval (IR) has the potential to impact many search domains. Recently, a new generation of Transformerbased [6] neural re-ranking models initiated a new era, by providing substantial effectiveness increases in ad-hoc search tasks [1, 4, 5]. They operate on the full text of a query and a list of candidate documents from an initial retrieval. Using self-attention, they contextualize term occurrences conditioned on the sequence they are contained in. However, self-attention, because it is applied to a whole sequence and commonly uses many layers, is inefficient and re-ranking models still depended on the bottleneck of the initial retrieval of candidate documents. In our thesis we plan to address these shortcomings. First, we plan to make contextualization efficient enough to to be usable in resource constrained environments, and second we plan to use the efficient contextualization components to create a novel approach for learning to index and retrieve in a unified neural model. Hence, our main research questions are: RQ1 How can we balance the trade-off between efficiency and effectiveness in contextualized neural re-ranking? Solving efficiency problems with massively parallelized hardware is neither an economically nor environmentally friendly solution. We wish to tackle the problem of efficient contextualization in two common scenarios of ad-hoc ranking: passage retrieval and document retrieval. As part of this PhD we already proposed a novel efficient Transformer based passage re-ranking model: The TK (Transformer-Kernel) model [4] utilizes shallow Transformerlayers to contextualize query and passages separately, and the kernel-pooling technique [7] to score individual term interactions between a query and a passage. Additionally, we explored localattention as an effective approach for document-length ranking with an extension to the TK for long text (TKL) [3]. RQ2 What is needed to learn generalized contextualized representations for indexing and retrieval? Currently, IR systems are a patchwork of traditional and neural systems. Different parts of the pipeline are optimized in isolation and missing integration during training. To overcome bottlenecks and sub-optimal integration of pipeline components, we plan to develop a unified neural index and ranking model. A unified indexing and ranking model needs to learn how to efficiently store document Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).",
        "Computing Interdisciplinarity of Scholarly Objects using an Author-Citation-Text Model There has been a growing need to determine if research proposals and results are truly interdisciplinary or to analyze research trends by analyzing research papers, reports, proposals and even researchers. In this paper, we tackle the problem and propose a method for measuring interdisciplinarity of scholarly objects. The newly proposed model takes into account authors, citations, and text content of scholarly objects together by building author networks, citation networks and text models. The three types of information are mixed by building network embeddings and sentence embeddings, which rely on the network topology and context-driven word semantics, respectively, through neural network learning. In addition, we propose a new measure that considers not only evenness of disciplines but also distributions of the magnitudes of disciplines so that saliency of disciplines is well represented.",
        "Improving Similar Question Retrieval using a Novel Tripartite Neural Network based Approach. Collective intelligence of the crowds is distilled together in various Community estion Answering (CQA) Services such as ora, Yahoo Answers, Stack Over ow forums, wherein users share their knowledge, providing both informational and experiential support to other users. As users o en search for similar information, probabilities are high that for a new incoming question, there is a related question-answer pair existing in the CQA dataset. erefore, an e cient technique for similar question identi cation is need of the hour. While data is not a bo leneck in this scenario, addressing the vocabulary diversity generated by a variety pool of users certainly is.is paper proposes a novel tripartite neural network based approach towards the similar question retrieval problem. e network takes inputs in the form of question-answer and new question triplet and learns internal representations from similarities among them. Our approach achieves classi cation performances upto 77% on a real world CQA dataset.We have also compared our method with two other baselines and found that it performs significantly be er in handling the problem of vocabulary diversity and 'zero-lexical overlap' among questions.",
        "Adapting deep RankNet for personalized search. RankNet is one of the widely adopted ranking models for web search tasks. However, adapting a generic RankNet for personalized search is little studied. In this paper, we first continue-trained a variety of RankNets with different number of hidden layers and network structures over a previously trained global RankNet model, and observed that a deep neural network with five hidden layers gives the best performance. To further improve the performance of adaptation, we propose a set of novel methods categorized into two groups. In the first group, three methods are proposed to properly assess the usefulness of each adaptation instance and only leverage the most informative instances to adapt a user-specific RankNet model. These assessments are based on KL-divergence, click entropy or a heuristic to ignore top clicks in adaptation queries. In the second group, two methods are proposed to regularize the training of the neural network in RankNet: one of these methods regularize the error back-propagation via a truncated gradient approach, while the other method limits the depth of the back propagation when adapting the neural network. We empirically evaluate our approaches using a large-scale real-world data set. Experimental results exhibit that our methods all give significant improvements over a strong baseline ranking system, and the truncated gradient approach gives the best performance, significantly better than all others.",
        "Diagnosis Ranking with Knowledge Graph Convolutional Networks The automatic diagnosis of a medical condition provided the symptoms exhibited by a patient is at the basis of systems for clinical decision support, as well as for applications such as symptom checkers. Existing methods have not fully exploited medical knowledge: this likely hinders their effectiveness. In this work, we propose a knowledgeaware diagnosis ranking framework based on medical knowledge graph (KG) and graph convolutional neural network (GCN). The medical KG is used to model hierarchy and causality relationships between diseases and symptoms. We have evaluated our proposed method using realistic patient cases. The empirical results show that our knowledge-aware diagnosis ranking framework can improve the effectiveness of medical diagnosis.",
        "Ranking Multiple Choice Question Distractors using Semantically Informed Neural Networks",
        "A deep network model for paraphrase detection in short text messages A B S T R A C TThis paper is concerned with paraphrase detection, i.e., identifying sentences that are semantically identical. The ability to detect similar sentences written in natural language is crucial for several applications, such as text mining, text summarization, plagiarism detection, authorship authentication and question answering. Recognizing this importance, we study in particular how to address the challenges with detecting paraphrases in user generated short texts, such as Twitter, which often contain language irregularity and noise, and do not necessarily contain as much semantic information as longer clean texts. We propose a novel deep neural network-based approach that relies on coarse-grained sentence modelling using a convolutional neural network (CNN) and a recurrent neural network (RNN) model, combined with a specific fine-grained wordlevel similarity matching model. More specifically, we develop a new architecture, called DeepParaphrase, which enables to create an informative semantic representation of each sentence by (1) using CNN to extract the local region information in form of important n-grams from the sentence, and (2) applying RNN to capture the long-term dependency information. In addition, we perform a comparative study on state-of-the-art approaches within paraphrase detection. An important insight from this study is that existing paraphrase approaches perform well when applied on clean texts, but they do not necessarily deliver good performance against noisy texts, and vice versa. In contrast, our evaluation has shown that the proposed DeepParaphrase-based approach achieves good results in both types of texts, thus making it more robust and generic than the existing approaches.",
        "Entire Information Attentive GRU for Text Representation. Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been widely utilized in sequence representation. However, RNNs neglect variational information and long-term dependency. In this paper, we propose a new neural network structure for extracting a comprehension sequence embedding by handling the entire representation of the sequence. Unlike previous works that put attention mechanism after all steps of GRU, we add the entire representation to the input of the GRU which means the GRU model takes the entire information of the sequence into consideration in every step. We provide three various strategies to adding the entire information which are the Convolutional Neural Network (CNN) based attentive GRU (CBAG), the GRU inner attentive GRU (GIAG) and the pre-trained GRU inner attentive GRU (Pre-GIAG). To evaluate our proposed methods, we conduct extensive experiments on a benchmark sentiment classification dataset. Our experimental results show that our models outperform state-of-the-art baselines significantly.",
        "A novel image retrieval algorithm based on transfer learning and fusion features. With proliferation of social media, image has become ubiquitous giving rise to the demand and importance of image semantic analysis and retrieval to access information quickly on social media. However, even with humongous information available, there are certain categories of images which are important for certain applications but are very scarce. Convolutional neural network is an effective method to extract high-level semantic features for image database retrieval. To overcome the problem of over-fitting when the number of training samples in dataset is limited, this paper proposes an image database retrieval algorithm based on the framework of transfer learning and feature fusion. Based on the fine-tuning of the pre-trained Convolutional Neural Network (CNN), the proposed algorithm first extracts the semantic features of the images. Principal Component Analysis (PCA) is then applied for dimension reduction and to reduce the computational complexity. Last, the semantic feature extracted from the CNN is fused with traditional low-level visual feature to improve the retrieval accuracy further. Experimental results demonstrated the effectiveness of the proposed method for image database retrieval.",
        "Learning to Match using Local and Distributed Representations of Text for Web Search. Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favourable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or 'duet' performs significantly better than either neural network individually on a Web page ranking task, and significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
        "On the Theory of Weak Supervision for Information Retrieval. Neural network approaches have recently shown to be effective in several information retrieval (IR) tasks. However, neural approaches often require large volumes of training data to perform effectively, which is not always available. To mitigate the shortage of labeled data, training neural IR models with weak supervision has been recently proposed and received considerable attention in the literature. In weak supervision, an existing model automatically generates labels for a large set of unlabeled data, and a machine learning model is further trained on the generated \"weak\" data. Surprisingly, it has been shown in prior art that the trained neural model can outperform the weak labeler by a significant margin. Although these obtained improvements have been intuitively justified in previous work, the literature still lacks theoretical justification for the observed empirical findings. In this paper, we provide a theoretical insight into weak supervision for information retrieval, focusing on learning to rank. We model the weak supervision signal as a noisy channel that introduces noise to the correct ranking. Based on the risk minimization framework, we prove that given some sufficient constraints on the loss function, weak supervision is equivalent to supervised learning under uniform noise. We also find an upper bound for the empirical risk of weak supervision in case of non-uniform noise. Following the recent work on using multiple weak supervision signals to learn more accurate models, we find an information theoretic lower bound on the number of weak supervision signals required to guarantee an upper bound for the pairwise error probability. We empirically verify a set of presented theoretical findings, using synthetic and real weak supervision data.",
        "Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answeringquestion-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.",
        "Neural Query Performance Prediction using Weak Supervision from Multiple Signals. Predicting the performance of a search engine for a given query is a fundamental and challenging task in information retrieval. Accurate performance predictors can be used in various ways, such as triggering an action, choosing the most effective ranking function per query, or selecting the best variant from multiple query formulations. In this paper, we propose a general end-to-end query performance prediction framework based on neural networks, called NeuralQPP. Our framework consists of multiple components, each learning a representation suitable for performance prediction. These representations are then aggregated and fed into a prediction subnetwork. We train our models with multiple weak supervision signals, which is an unsupervised learning approach that uses the existing unsupervised performance predictors using weak labels. We also propose a simple yet effective component dropout technique to regularize our model. Our experiments on four newswire and web collections demonstrate that NeuralQPP significantly outperforms state-of-the-art baselines, in nearly every case. Furthermore, we thoroughly analyze the effectiveness of each component, each weak supervision signal, and all resulting combinations in our experiments.",
        "Using the Cosine Measure in a Neural Network for Document. The task of document retrieval systems is to match one natural language query against a large number of natural language documents.Neural networks are known to be good pattern matchers. This paper reports our investigations in implementing a document retrieval system based on a neural network model. It shows that many of the standard strategies of information retrieval are applicable in a neural network model.",
        "Neural Networks for Information Retrieval Machine learning plays a role in many aspects of modern IR systems, and deep learning is applied in all of them. The fast pace of modernday research has given rise to many di erent approaches for many di erent IR problems. The amount of information available can be overwhelming both for junior students and for experienced researchers looking for new research topics and directions. Additionally, it is interesting to see what key insights into IR problems the new technologies are able to give us. The aim of this full-day tutorial is to give a clear overview of current tried-and-trusted neural methods in IR and how they bene t IR research. It covers key architectures, as well as the most promising future directions. MOTIVATIONPrompted by the advances of deep learning in computer vision research, neural networks have resurfaced as a popular machine learning paradigm in many other directions of research as well, including information retrieval. Recent years have seen neural networks being applied to all key parts of the typical modern IR pipeline, such core ranking algorithms , click models , knowledge graphs , text similarity , entity retrieval , language modeling [5], question answering , and dialogue systems .A key advantage that sets neural networks apart from many learning strategies employed earlier, is their ability to work from raw input data. E.g., when given enough training data, well-designed networks can become feature extractors themselves, e.g., incorporating basic input characteristics such as term frequency (tf) and term saliency (idf)-that used to be pre-calculated o ine-in their * Corresponding author.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7--11, 2017, Shinjuku, Tokyo, Japan initial layers. Where designing features used to be a crucial aspect and contribution of newly proposed IR approaches, the focus has shifted to designing network architectures instead. As a consequence, many di erent architectures and paradigms have been proposed, such as auto-encoders, recursive networks, recurrent networks, convolutional networks, various embedding methods, deep reinforcement and deep q-learning, and, more recently, generative adversarial networks, of which most have been applied in IR settings. The aim of the neural networks for IR (NN4IR) tutorial is to provide a clear overview of the main network architectures currently applied in IR and to show explicitly how they relate to previous work. The tutorial covers methods applied in industry and academia, with in-depth insights into the underlying theory, core IR tasks, applicability, key assets and handicaps, scalability concerns and practical tips and tricks.We expect the tutorial to be useful both for academic and industrial researchers and practitioners who either want to develop new neural models, use them in their own research in other areas or apply the models described here to improve actual IR systems. OBJECTIVESThe material in the tutorial covers a broad range of IR applications. It is structured as follows:Preliminaries (60 minutes). The recent surge of interest in deep learning has given rise to a myriad of architectures. Di erent though the inner structures of neural networks can be, there are many concepts common to all of them. This rst session covers the preliminaries; we brie y recapitulate the basic concepts involved in neural systems, such as back propagation Semantic matching I: supervised learning (60 minutes). The problem of matching items based on their textual descriptions arises in many IR systems. The traditional approach involves counting query term occurrences in the description text (e.g., BM25 ). However, to bridge the lexical gap caused by vocabulary-related and linguistic di erences many latent semantic models have been proposed , and more recently neural embedding methods . In this session we will focus on semantic matching settings where a supervised signal is available. The signal can be explicit,",
        "Understanding the Representational Power of Neural Retrieval Models Using NLP Tasks. The ease of constructing effective neural networks has resulted in a large number of varying architectures iteratively improving performance on a task. Due to the nature of these models being black boxes, standard weight inspection is difficult. We propose a probe based methodology to evaluate what information is important or extraneous at each level of a network. We input natural language processing datasets into a trained answer passage neural network. Each layer of the neural network is used as input into a unique classifier, or probe, to correctly label that input with respect to a natural language processing task, probing the internal representations for information.Using this approach, we analyze the information relevant to retrieving answer passages from the perspective of information needed for part of speech tagging, named entity retrieval, sentiment classification, and textual entailment. We show a significant information need difference between two seemingly similar question answering collections, and demonstrate that passage retrieval and textual entailment share a common information space, while POS and NER information is used only at a compositional level in the lower layers of an information retrieval model. Lastly, we demonstrate that incorporating this information into a multitask environment is correlated to the information retained by these models during the probe inspection phase.",
        "Experiments with Convolutional Neural Network Models for Answer Selection. In recent years, neural networks have been applied to many text processing problems. One example is learning a similarity function between pairs of text, which has applications to paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. Within the information retrieval community, the convolutional neural network model proposed by Severyn and Moschi i in a SIGIR 2015 paper has gained prominence. is paper focuses on the problem of answer selection for question answering: we a empt to replicate the results of Severyn and Moschi i using their open-source code as well as to reproduce their results via a de novo (i.e., from scratch) implementation using a completely di erent deep learning toolkit. Our de novo implementation is instructive in ascertaining whether reported results generalize across toolkits, each of which have their idiosyncrasies. We were able to successfully replicate and reproduce the reported results of Severyn and Moschi i, albeit with minor di erences in e ectiveness, but a rming the overall design of their model. Additional ablation experiments break down the components of the model to show their contributions to overall e ectiveness. Interestingly, we nd that removing one component actually increases e ectiveness and that a simpli ed model with only four word overlap features performs surprisingly well, even be er than convolution feature maps alone.",
        "Deep Learning for Information Retrieval. Recent years have observed a significant progress in information retrieval and natural language processing with deep learning technologies being successfully applied into almost all of their major tasks. The key to the success of deep learning is its capability of accurately learning distributed representations (vector representations or structured arrangement of them) of natural language expressions such as sentences, and effectively utilizing the representations in the tasks. This tutorial aims at summarizing and introducing the results of recent research on deep learning for information retrieval, in order to stimulate and foster more significant research and development work on the topic in the future.The tutorial mainly consists of three parts. In the first part, we introduce the fundamental techniques of deep learning for natural language processing and information retrieval, such as word embedding, recurrent neural networks, and convolutional neural networks. In the second part, we explain how deep learning, particularly representation learning techniques, can be utilized in fundamental NLP and IR problems, including matching, translation, classification, and structured prediction. In the third part, we describe how deep learning can be used in specific application tasks in details. The tasks are search, question answering (from either documents, database, or knowledge base), and image retrieval."
    ],
    "Query log analysis": [
        "Translating related words to videos and back through latent topics. Documents containing video and text are becoming more and more widespread and yet content analysis of those documents depends primarily on the text. Although automated discovery of semantically related words from text improves free text query understanding, translating videos into text summaries facilitates better video search particularly in the absence of accompanying text. In this paper, we propose a multimedia topic modeling framework suitable for providing a basis for automatically discovering and translating semantically related words obtained from textual metadata of multimedia documents to semantically related videos or frames from videos. The framework jointly models video and text and is flexible enough to handle different types of document features in their constituent domains such as discrete and real valued features from videos representing actions, objects, colors and scenes as well as discrete features from text. Our proposed models show much better fit to the multimedia data in terms of held-out data log likelihoods. For a given query video, our models translate low level vision features into bag of keyword summaries which can be further translated using simple natural language generation techniques into human readable paragraphs. We quantitatively compare the results of video to bag of words translation against a state-of-the-art baseline object recognition model from computer vision. We show that text translations from multimodal topic models vastly outperform the baseline on a multimedia dataset downloaded from the Internet.",
        "Improving IP Geolocation using Query Logs. IP geolocation databases map IP addresses to their geographical locations. These databases are important for several applications such as local search engine relevance, credit card fraud protection, geotargetted advertising, and online content delivery. While they are the most popular method of geolocation, they can have low accuracy at the city level. In this paper we evaluate and improve IP geolocation databases using data collected from search engine logs. We generate a large ground-truth dataset using real time global positioning data extracted from search engine logs. We show that incorrect geolocation information can have a negative impact on implicit user metrics. Using the dataset we measure the accuracy of three state-of-the-art commercial IP geolocation databases. We then introduce a technique to improve existing geolocation databases by mining explicit locations from query logs. We show significant accuracy gains in 44 to 49 out of the top 50 countries, depending on the IP geolocation database. Finally, we validate the approach with a large scale A/B experiment that shows improvements in several user metrics.",
        "Analysis of an expert search query log. Expert search has made rapid progress in modeling, algorithms and evaluations in the recent years. However, there is very few work on analyzing how users interact with expert search systems. In this paper, we conduct analysis of an expert search query log. The aim is to understand the special characteristics of expert search usage. To the best of our knowledge, this is one of the earliest work on expert search query log analysis. We find that expert search users generally issue shorter queries, more common queries, and use more advanced search features, with fewer queries in a session, than general Web search users do. This study explores a new research direction in expert search by analyzing and exploiting query logs.",
        "The Characteristics of Voice Search: Comparing Spoken with Typed-in Mobile Web Search Queries The growing popularity of mobile search and the advancement in voice recognition technologies have opened the door for web search users to speak their queries rather than type them. While this kind of voice search is still in its infancy, it is gradually becoming more widespread. In this article, we report a comprehensive voice search query log analysis of a commercial web search engine's mobile application. We compare voice and text search by various aspects, with special focus on the semantic and syntactic characteristics of the queries. Our analysis suggests that voice queries focus more on audio-visual content and question answering and less on social networking and adult domains. In addition, voice queries are more commonly submitted on the go. We also conduct an empirical evaluation showing that the language of voice queries is closer to natural language than the language of text queries. Our analysis points out further differences between voice and text search. We discuss the implications of these differences for the design of future voice-enabled web search tools. The popularity of search from mobile devices (mobile search) has rapidly increased in recent years . In fact, the number of mobile queries has already exceeded the number of those submitted from desktop devices in the United States and other countries . The nature of mobile search has also evolved, with growth in the number of unique queries and shifts in searched topics, from entertainment and adult content to business and commerce, similarly to the shifts of web search topics in its early days .A prominent characteristic of the advancement in mobile search is the emergence of voice search, allowing users to input queries in a spoken language and then retrieve the relevant entries based on system-generated transcriptions of the voice queries . Recent developments This manuscript is an extended version of Guy (2016). Author's address: I. Guy, P.O. Box 653 Beer-Sheva 8410501, Israel; email: idoguy@acm.org. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 30:2 I. Guy in speech recognition, backed by high bandwidth coverage and high-quality speech signal acquisition, are enabling higher quality voice search (Chelba and Schalkwyk 2013). Already in 2010, Google presented a case study stating that their goal is to make voice search ubiquitously available and that a level of performance was achieved such that usage is growing, and many users become repeat users . Since then, further enhancements to automatic speech recognition (ASR) for web search have been reported , taking advantage of the large data that started to accumulate on voice search logs (Chelba and Schalkwyk 2013), and applying advanced learning methods . The use of voice has also been promoted by the increasing popularity of voice-activated intelligent assistants, such as Google Assistant, Amazon's Alexa, Apple's Siri, and Microsoft's Cortana. These assistants provide context-based query-less personalized advice for mobile users, but also enable web search . A recent survey of 1400 U.S. smartphone users, executed by Northstar Research and commissioned by Google, found that many teenagers use voice search every day . It is therefore becoming important for information retrieval researchers and practitioners to understand this new medium of search and its differences from traditional text search.Using voice as a means to search holds various potential advantages. Although typing usability has improved in recent years, querying by voice is still likely to be substantially easier and faster for the vast majority of mobile users. Voice is also the natural way people communicate with one another and express themselves. For users with visual or manual impairment, or with limited literacy skills, voice search may break down the entry barrier into web search. In addition, as searching by voice does not require visual attention or the use of hands, it can be performed in situations such as driving, cooking, or exercising, where typed search might be especially cumbersome, error-prone, and even dangerous. In the aforementioned survey, 78% of the teens who used voice search pointed out its usefulness for multitasking as a key motivating factor. 1 In spite of its growing popularity, the area of voice search has not received much attention in the information retrieval literature. Early work compared voice and text queries in a laboratory study, however, these did not represent typical web search queries, but rather complex long questions In this work, we perform a query log analysis of half a million voice queries, issued to the mobile application of the Yahoo commercial web search engine, over a period of 6 months. The log includes English-only queries, from the United States, transcribed from voice to text using high-quality ASR. We compare the voice queries with a similar-size sample of mobile text queries, typed on the same mobile application. Our comparison inspects characteristics of context, clicks, sessions, and, primarily, the query text itself. We examine both semantic and syntactic features and compare them for voice versus text queries. In the final part of our analysis, we directly compare the similarity of the voice and text query language to natural language corpora, which include traditional news articles and the titles of questions in a large community question answering (CQA) website. -To the best of our knowledge, we present the most comprehensive analysis of a web search engine voice query log. -We combine a semantic analysis using novel methods, such as analyzing a broad set of triggered cards, with an in-depth syntactic analysis, to shed more light on the commonalities and differences between voice and text queries. -We provide empirical evidence, based on language modeling, that voice queries are closer to natural language than text queries, yet are still distant from natural question language.Our findings suggest different ways for search systems to enhance their support and take advantage of the unique characteristics of voice queries. We conclude the article by summarizing the key findings and discussing their implications and future research directions. RELATED WORKStudies of mobile query log analysis have been published throughout the past decade, ever since mobile devices became ubiquitous. One of the early studies (Kamvar and Baluja 2006) compared search patterns (queries, clicks, time spent on each search phase) on 12-key keypad cellphones, PDAs (with a QWERTY keyboard or a stylus input), and desktop (PC) computers. It found that the diversity of queries on mobile was substantially lower than on desktop and that the most popular query in each of the three device types was different. Baeza-Yates et al. compared mobile and desktop search queries on Yahoo Japan and found that mobile queries included fewer characters, more queries in the Business category, and fewer in Art. performed a large-scale query log analysis of the Yahoo OneSearch mobile service and found that mobile query patterns were dynamic, as users were exploring how to use the devices. Pattern use also varied among different geographies and application types. With the evolution of mobile devices into smartphones, mobile search has also been shown to change. examined search behavior on iPhones and found it was more similar to desktop search than to search on basic mobile phones. performed a broad 3-month log analysis of Bing search on desktop, iPad, and iPhone. They found that both mobile and tablet users issued significantly fewer navigational queries than desktop users, due to the wide availability of mobile apps on these two platforms. Due to the significant differences between user search patterns on the three platforms, they proposed a ranking system that considered platformspecific features. focused on the transition of users across device types during the search process by analyzing a search log from desktop computers, smartphones, tablets, and game consoles. In this work, we focus on mobile devices for the comparison between voice and text queries.With the advancement of speech recognition technologies, studies of mobile search using voice started to emerge. Many of the studies focused on voice recognition challenges. defined voice search as \"the technology underlying many spoken dialog systems that provide users with the information they request with a spoken query\" and reviewed key challenges, such as environmental noise, pronunciation variance, and linguistic issues. described the architecture of the speech recognition interface of Microsoft's \"Live Search for Mobile\". The key challenge they pointed out was the loss of signal-to-noise ratio caused by the fact that users often speak at arms length while looking at the screen or use the application in inherently noisy environments, such as in cars or on the street. discussed the interleaving of ASR with information retrieval (IR) systems and suggested to combine acoustic and semantic models to enhance performance.",
        "Selecting related terms in query-logs using two-stage SimRank. It is commonly believed that query logs from Web search are a gold mine for search business, because they reflect users' preference over Web pages presented by search engines, so a lot of studies based on query logs have been carried out in the last few years. In this study, we assume that two queries are relevant to each other when they have same clicked page in their result lists, and we also consider the queries' topics of user's need. Thus, we propose a TwoStage SimRank (called TSS in this paper) algorithm based on SimRank and some clustering algorithms to compute the similarity among queries, and then use it to discover relevant terms for query expansion, considering the information of topics and the global relationships of queries concurrently, with a query log collected by a practical search engine.Experimental results on two TREC test collections show that our approach can discover qualified terms effectively and improve retrieval performance.",
        "The query-flow graph: model and applications. Query logs record the queries and the actions of the users of search engines, and as such they contain valuable information about the interests, the preferences, and the behavior of the users, as well as their implicit feedback to searchengine results. Mining the wealth of information available in the query logs has many important applications including query-log analysis, user profiling and personalization, advertising, query recommendation, and more.In this paper we introduce the query-flow graph, a graph representation of the interesting knowledge about latent querying behavior. Intuitively, in the query-flow graph a directed edge from query qi to query qj means that the two queries are likely to be part of the same \"search mission\". Any path over the query-flow graph may be seen as a searching behavior, whose likelihood is given by the strength of the edges along the path.The query-flow graph is an outcome of query-log mining and, at the same time, a useful tool for it. We propose a methodology that builds such a graph by mining time and textual information as well as aggregating queries from different users. Using this approach we build a real-world query-flow graph from a large-scale query log and we demonstrate its utility in concrete applications, namely, finding logical sessions, and query recommendation. We believe, however, that the usefulness of the query-flow graph goes beyond these two applications.",
        "Query log analysis: social and technological challenges. Analysis of search engine query logs is an important tool for developers and researchers.However, the potentially personal content of query logs raises a number of questions about the use of that data. Privacy advocates are concerned about potential misuse of personal data; search engine providers are interested in protecting their users while maintaining a competitive edge; and academic researchers are frustrated by barriers to shared learning though shared data analysis. This paper reports on a workshop held at the WWW 2007 Conference to foster dialogue on the social and technical challenges that are posed by the content of query logs and the analysis of that content.",
        "Multidimensional mining of large-scale search logs: a topic-concept cube approach. In addition to search queries and the corresponding clickthrough information, search engine logs record multidimensional information about user search activities, such as search time, location, vertical, and search device. Multidimensional mining of search logs can provide novel insights and useful knowledge for both search engine users and developers. In this paper, we describe our topic-concept cube project, which addresses the business need of supporting multidimensional mining of search logs effectively and efficiently. We answer two challenges. First, search queries and click-through data are well recognized sparse, and thus have to be aggregated properly for effective analysis. Second, there is often a gap between the topic hierarchies in multidimensional aggregate analysis and queries in search logs. To address those challenges, we develop a novel topicconcept model that learns a hierarchy of concepts and topics automatically from search logs. Enabled by the topicconcept model, we construct a topic-concept cube that supports online multidimensional mining of search log data. A distinct feature of our approach is that, in addition to the standard dimensions such as time and location, our topicconcept cube has a dimension of topics and concepts, which substantially facilitates the analysis of log data. To handle a huge amount of log data, we develop distributed algorithms for learning model parameters efficiently. We also devise approaches to computing a topic-concept cube. We report an empirical study verifying the effectiveness and efficiency of our approach on a real data set of 1.96 billion queries and 2.73 billion clicks.",
        "A Search Engine based on Query Logs and Search Log Analysis at the University of Sunderland This work describes a variation on the traditional Information Retrieval paradigm, where instead of text documents being indexed according to their content, they are indexed according to the search terms previous users have used in finding them. We determine the effectiveness of this approach by indexing a sample of query logs from the European Library, and describe its usefulness for multilingual searching. In our analysis of the search logs, we determine the language of the past queries automatically, and annotate the search logs accordingly. From this information, we derive matrices to show that a) users tend to persist with the same query language throughout a query session, and b) submit queries in the same language as the interface they have selected, except in a large number of cases where the English interface is used to submit Latin queries.",
        "Beyond actions: Exploring the discovery of tactics from user logs a b s t r a c tSearch log analysis has become a common practice to gain insights into user search behaviour: it helps gain an understanding of user needs and preferences, as well as an insight into how well a system supports such needs. Currently, log analysis is typically focused on low-level user actions, i.e. logged events such as issued queries and clicked results, and often only a selection of such events are logged and analysed. However, types of logged events may differ widely from interface to interface, making comparison between systems difficult. Further, the interpretation of the meaning of and subsequent analysis of a selection of events may lead to conclusions out of context-e.g. the statistics of observed query reformulations may be influenced by the existence of a relevance feedback component. Alternatively, in lab studies user activities can be analysed at a higher level, such as search tactics and strategies, abstracted away from detailed interface implementation. Unfortunately, until now the required manual codings that map logged events to higherlevel interpretations have prevented large-scale use of this type of analysis. In this paper, we propose a new method for analysing search logs by (semi-)automatically identifying user search tactics from logged events, allowing large-scale analysis that is comparable across search systems. In addition, as the resulting analysis is at a tactical level we reduce potential issues surrounding the need for interpretation of low-level user actions for log analysis. We validate the efficiency and effectiveness of the proposed tactic identification method using logs of two reference search systems of different natures: a product search system and a video search system. With the identified tactics, we perform a series of novel log analyses in terms of entropy rate of user search tactic sequences, demonstrating how this type of analysis allows comparisons of user search behaviours across systems of different nature and design. This analysis provides insights not achievable with traditional log analysis.",
        "Hourly analysis of a very large topically categorized web query log. We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a generalpurpose commercial web search service. Previously, query logs have been studied from a single, cumulative view. In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms.",
        "Search in audiovisual broadcast archives Documentary makers, journalists, news editors, and other media professionals routinely require previously recorded audiovisual material for new productions. For example, a news editor might wish to reuse footage shot by overseas services for the evening news, or a documentary maker might require shots of Christmas trees recorded over the decades. Important sources for reusable broadcasts are audiovisual broadcast archives, which preserve and manage audiovisual material. With digitization, media professional can be given online access to video. This increases ease of access, but increases the need for search capabilities tailored for the media professional. Search in audiovisual broadcast archives, then, is the subject of this thesis.We begin by investigating the search behavior of media professionals in current daily practice. To this end we perform a large-scale log analysis of their search actions at a national audiovisual broadcast archive. Our analysis characterizes not only the searches of media professionals, but also their purchasing behavior. In order to model the observed behavior we follow our log analysis with a simulation experiment. Here we investigate simulation methods for recreating the searches and purchases recorded in the archive to create evaluation testbeds.In the second half of the thesis we turn to investigate the use of state-of-art methods for retrieval with automatically generated content metadata from video, Specifically we focus on their application for improving audiovisual fragment search in the audiovisual broadcast archive. We use logged searches and purchases to define new test collections for retrieval evaluation. These are used as the basis for experiments aimed at solving specific problems that are faced when searching with automatically generated descriptions of video content. Finally, we combine state-of-the-art methods with the current daily practice of the archive, and investigate their potential combined impact on search in audiovisual broadcast archives.The contributions of this thesis include the characterization of searching and purchasing behaviour of media professionals at a large audiovisual broadcast archive, and a framework for simulating their logged queries and purchases. Contributions in the second half of the thesis include an in-depth user study of how text queries should be mapped to visual concepts, a retrieval model that accounts for the temporal mismatch between the speech and visual tracks in audiovisual material, and a set of experiments demonstrating the effectiveness of automatically generated content metadata for improving retrieval in the audiovisual broadcast archive.The thesis can be accessed at http://dare.uva.nl/record/358972 DOCTORAL. Search in Audiovisual Broadcast ArchivesBouke Huurnink ISLA, University of Amsterdam bhuurnink@uva.nl Documentary makers, journalists, news editors, and other media professionals routinely require previously recorded audiovisual material for new productions. For example, a news editor might wish to reuse footage shot by overseas services for the evening news, or a documentary maker might require shots of Christmas trees recorded over the decades. Important sources for reusable broadcasts are audiovisual broadcast archives, which preserve and manage audiovisual material. With digitization, media professional can be given online access to video. This increases ease of access, but increases the need for search capabilities tailored for the media professional. Search in audiovisual broadcast archives, then, is the subject of this thesis.We begin by investigating the search behavior of media professionals in current daily practice. To this end we perform a large-scale log analysis of their search actions at a national audiovisual broadcast archive. Our analysis characterizes not only the searches of media professionals, but also their purchasing behavior. In order to model the observed behavior we follow our log analysis with a simulation experiment. Here we investigate simulation methods for recreating the searches and purchases recorded in the archive to create evaluation testbeds.In the second half of the thesis we turn to investigate the use of state-of-art methods for retrieval with automatically generated content metadata from video, Specifically we focus on their application for improving audiovisual fragment search in the audiovisual broadcast archive. We use logged searches and purchases to define new test collections for retrieval evaluation. These are used as the basis for experiments aimed at solving specific problems that are faced when searching with automatically generated descriptions of video content. Finally, we combine state-of-the-art methods with the current daily practice of the archive, and investigate their potential combined impact on search in audiovisual broadcast archives.The contributions of this thesis include the characterization of searching and purchasing behaviour of media professionals at a large audiovisual broadcast archive, and a framework for simulating their logged queries and purchases. Contributions in the second half of the thesis include an in-depth user study of how text queries should be mapped to visual concepts, a retrieval model that accounts for the temporal mismatch between the speech and visual tracks in audiovisual material, and a set of experiments demonstrating the effectiveness of automatically generated content metadata for improving retrieval in the audiovisual broadcast archive.The Documentary makers, journalists, news editors, and other media professionals routinely require previously recorded audiovisual material for new productions. For example, a news editor might wish to reuse footage shot by overseas services for the evening news, or a documentary maker might require shots of Christmas trees recorded over the decades. Important sources for reusable broadcasts are audiovisual broadcast archives, which preserve and manage audiovisual material. With digitization, media professional can be given online access to video. This increases ease of access, but increases the need for search capabilities tailored for the media professional. Search in audiovisual broadcast archives, then, is the subject of this thesis.We begin by investigating the search behavior of media professionals in current daily practice. To this end we perform a large-scale log analysis of their search actions at a national audiovisual broadcast archive. Our analysis characterizes not only the searches of media professionals, but also their purchasing behavior. In order to model the observed behavior we follow our log analysis with a simulation experiment. Here we investigate simulation methods for recreating the searches and purchases recorded in the archive to create evaluation testbeds.In the second half of the thesis we turn to investigate the use of state-of-art methods for retrieval with automatically generated content metadata from video, Specifically we focus on their application for improving audiovisual fragment search in the audiovisual broadcast archive. We use logged searches and purchases to define new test collections for retrieval evaluation. These are used as the basis for experiments aimed at solving specific problems that are faced when searching with automatically generated descriptions of video content. Finally, we combine state-of-the-art methods with the current daily practice of the archive, and investigate their potential combined impact on search in audiovisual broadcast archives.The contributions of this thesis include the characterization of searching and purchasing behaviour of media professionals at a large audiovisual broadcast archive, and a framework for simulating their logged queries and purchases. Contributions in the second half of the thesis include an in-depth user study of how text queries should be mapped to visual concepts, a retrieval model that accounts for the temporal mismatch between the speech and visual tracks in audiovisual material, and a set of experiments demonstrating the effectiveness of automatically generated content metadata for improving retrieval in the audiovisual broadcast archive.The thesis can be accessed at",
        "Query reformulation using anchor text. Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that logbased query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.",
        "Large scale query log analysis of re-finding. Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of refinding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of refinding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.",
        "Domain-independent entity extraction from web search query logs. Query logs of a Web search engine have been increasingly used as a vital source for data mining. This paper presents a study on largescale domain-independent entity extraction from search query logs. We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures. We compare against existing techniques that use Web documents as well as search logs, and show that we improve over the state of the art. We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods. Categories and Subject DescriptorsI.2.6 [Artificial Intelligence]: Learning-knowledge acquisition General TermsAlgorithms Keywords entity extraction, query logs, data mining QUERY LOGS ENTITY EXTRACTIONEntity extraction is an important part of many Web-based applications [3], defined as the task of extracting entities of pre-defined classes (e.g., 'Brad Pitt' for the class Actors). Typically, extraction tasks are run over \"well-formed\" documents such as news articles or web pages . Recently, Pasca proposed to extract entities from query-logs instead of a classical Web corpus, by using a semisupervised approach that inputs a pre-defined list of classes represented by a small set of hand-made seeds. Extracting entities from query logs instead of web corpora has several advantages, in view of query-log-based application . Along this direction, we present a completely unsupervised (i.e. we do not need seeds) and domainindependent (i.e., we do not need pre-defined classes) extraction method over query logs. Our study is inspired by the open domain information extraction (OIE) framework, that has been recently implemented to extract entities from Web-scale corpora, using simple unsupervised techniques . In the same spirit, our method is also unsupervised, by applying heuristics that specifically address OIE over query-logs. To our knowledge, ours is the first attempt to devise an algorithm specifically designed to achieve the following two goals at the same time: (a) extract entities from a query log; (b) extract entities in an open-domain fashion (OIE).Copyright is held by the author/owner(s). Starting with raw search query logs, our approach performs three steps: (1) identify candidate entities from search logs, (2) select reliable entities from the candidates using confidence scores, (3) execute a subsumption filter to eliminate noise. Generating candidate entities: User queries are short and lack syntactic structure, thus impairing traditional extraction approaches based on contextual evidence and syntactic interpretation. We therefore rely on a simple observation: oftentimes users construct their search query by copy-pasting phrases from existing texts in a web pages, thus carrying over surface-level properties such as capitalization. We generate candidate entities as contiguous capitalized words from a user query: given a query Q = q1 q2 \u00b7 \u00b7 \u00b7 qn, we define a candidate entity E = e1 e2 \u00b7 \u00b7 \u00b7 em as the maximal sequence of words (i.e., alpha-numeric characters) in the query such that each word ei in the entity begins with an uppercase character. This method can be far from perfect: for instance, a small fraction of user search queries are entered using only uppercase characters. We then discard spurious entities by employing text-based evidence described next. Deriving confidence scores: We assign two confidence scores to a candidate entity E. The representation score captures the intuition that the case-sensitive representation observed for E in Q, should be a likely representation for E, as observed on a Web corpus. For example given the query 'Galapagos Island vacations', we assign a high score to the candidate 'Galapagos Island' because we can observe it oftentimes on the Web. On the contrary, given the query 'DOor HANGING tips', we assign a low score to the candidate 'DOor HANGING', as it is seldom observed in that representation. More formally, the score is computed as:where |x| is the number of occurrences of a string x in the corpus, O(E) is the set of all occurrences of string E, \u03b3(i) is a casesensitive representation of the string i. The standalone score is based on the observation that a candidate E should often occur in a standalone form among the query logs, in order to get the status of proper entity:We retain candidate entities for which rw(E) \u2265 \u03c4r and sq(E) \u2265 \u03c4s (we experimentally set \u03c4r = 0.1 and \u03c4s = 0.2). Applying the Subsumption Filter: As a final step, we deal with boundary detection. Often, the score-based filters miss erroneous candidates that have substantial overlap with good candidates. For example 'Barack Obama' and 'Barack Obama Biography' are both often used in capitalized form and in standalone queries, but the",
        "Mining search engine query logs for query recommendation. This paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial-strength search engine. In order to get a more comprehensive solution, we combine two methods together. On the one hand, we study and model search engine users' sequential search behavior, and interpret this consecutive search behavior as client-side query refinement, that should form the basis for the search engine's own query refinement process. On the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. To evaluate our method, we use one hundred day worth query logs from SINA' search engine to do off-line mining. Then we analyze three independent editors evaluations on a query test set. Based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. In addition to the subjective editors' rating, we also perform tests based on actual anonymous user search sessions.",
        "Analysis of a Very Large Web Search Engine Query Log. In this paper we present an analysis of an AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents almost 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. We also present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques may not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such.",
        "Cross-Lingual Topic Discovery From Multilingual Search Engine Query Log Today, major commercial search engines are operating in a multinational fashion to provide web search services for millions of users who compose search queries by different languages. Hence, the search engine query log, which serves as the backbone of many search engine applications, records millions of users' search history in a wide spectrum of human languages and demonstrates a strong multilingual phenomenon. However, with its salience, the multilingual nature of a search engine query log is usually ignored by existing works, which usually consider query log entries of different languages as being orthogonal and independent. This kind of oversimplified assumption heavily distorts the underlying structure of web search data. In this article, we pioneer in recognition of the multilingual nature of a query log and make the first attempt to cross the language barrier in query logs. We propose a novel model named Cross-Lingual Query Log Topic Model (CL-QLTM) to analyze query logs from a cross-lingual perspective and derive the latent topics of web search data. The CL-QLTM comprehensively integrates web search data in different languages by collectively utilizing cross-lingual dictionaries, as well as the co-occurrence relations in the query log. In order to relieve the efficiency bottleneck of applying the CL-QLTM on voluminous query logs, we propose an efficient parameter inference algorithm based on the MapReduce computing paradigm. Both qualitative and quantitative experimental results show that the CL-QLTM is able to effectively derive cross-lingual topics from multilingual query logs and spawn a wide spectrum of new search engine applications.",
        "Real Life Information Retrieval: A Study of User Queries on the Web. We analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of Excite, a major Internet search service. We provide data on: (i) queries -the number of search terms, and the use of logic and modifiers, (ii)sessions -changes in queries during a session, number of pages viewed, and use of relevance feedback, and (iii) terms -their rank/frequency distribution and the most highly used search terms. Common mistakes are also observed. Implications are discussed.",
        "Beyond Query Logs: Recommendation and Evaluation. Query recommendation in Web search is typically manifested in algorithms that 1) recommend previously issued queries from a query log or 2) make incremental changes to queries in a user's current session. While such approaches have been effective in improving retrieval, they either are limited to suggesting queries in a query log or fail to make appropriate leaps that are necessary for query recommendation. More crucially, these approaches only recommend queries that are a coarse approximation of the information a user needs to complete their goal. They do not directly attempt to model the need and generate recommendations from it. This work will propose a framework for generating novel yet focused queries for query recommendation. CCS Concepts\u2022Information systems \u2192 Query suggestion; Personalization; Document topic models; Retrieval effectiveness; KeywordsSearch session analysis; Diversity; User simulations; Query recommendation RESEARCH PROPOSALQuery recommendation can be a very powerful tool in enhancing the search experience. In general, there are two different types of query recommenders. A \"generative\" recommender takes user context and modifies queries the user previously issued in the session. A \"discriminative\" approach recommends previously issued queries from a query log, relying on a similarity measure or transition probabilities between queries to make recommendations. Yet generative models do not take the exploratory leaps often required of recommendations . Also, the discriminative ones do not scale, since the number of queries can be exponential in the number of words. While diverse exploration is desirable, Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). perhaps a more compact recommender is required. The purpose of this proposed work is to answer the following research questions: (RQ1) To what extent, in terms of effectiveness and efficiency, can current discriminative and generative approaches to query recommendation be outperformed by an approach that generates queries directly from an information need? (RQ2) How can such an approach be evaluated in the absence of relevance judgments for documents?Previous work has suggested the ineffectiveness of user queries for query recommendation , calling for something beyond the discriminative recommender that recommends user queries. A first step towards the model in RQ2 used topic modeling and skipgrams to determine 1) which topic to search next and 2) how to generate queries from that topic.[3] It recommended queries that did not exist in the log; pages therefore did not have relevance judgments. In the proposed work, new evaluation methods will be defined, either with an evaluation metric in the absence of relevance judgments or a systematic simulation method for simulating user interactions, borrowing from [1]. The topic modeling will also be refined, such as with a structured approach to generating queries, a natural language processing analysis of text segments, or an extension to task-topic modeling in .",
        "User modeling in search logs via a nonparametric bayesian approach. Searchers' information needs are diverse and cover a broad range of topics; hence, it is important for search engines to accurately understand each individual user's search intents in order to provide optimal search results. Search log data, which records users' search behaviors when interacting with search engines, provides a valuable source of information about users' search intents. Therefore, properly characterizing the heterogeneity among the users' observed search behaviors is the key to accurately understanding their search intents and to further predicting their behaviors.In this work, we study the problem of user modeling in the search log data and propose a generative model, dpRank, within a non-parametric Bayesian framework. By postulating generative assumptions about a user's search behaviors, dpRank identifies each individual user's latent search interests and his/her distinct result preferences in a joint manner. Experimental results on a large-scale news search log data set validate the effectiveness of the proposed approach, which not only provides in-depth understanding of a user's search intents but also benefits a variety of personalized applications.",
        "University of Essex at LogCLEF 2011: Studying Query Refinement This paper describes the analysis we performed for the query success task of LogCLEF 2011. In particular, we address the issue of query refinement. The motivating assumption of our work is that query success can be improved by a system that can make good query refinement suggestions. We investigate how log files as provided in LogCLEF can assist in learning good suggestions. We used the distributed search logs of Deutscher Bildungsserver (DBS) and the logs of The European Library (TEL). We first processed the logs to extract the actually submitted search queries together with user session information and the browsing information following the submission of a query. Our initial analysis shows that a large proportion of the sessions on DBS in particular resulted in reformulations of the original query. The focus of our work is to demonstrate that the given log files can be used to acquire structured knowledge that can assist users in searching the collections (and thus shortening the number of steps needed when compared to a system that does not employ query modification suggestions). We use the paradigm of ant colony optimisation to derive query suggestions and evaluate the results by applying the fully automated AutoEval methodology that relies entirely on the search logs.",
        "WSCD2013: workshop on web search click data 2013. WSCD 20131 is the third workshop on Web Search Click Data, following WSCD 2009 and WSCD 2012. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This workshop comes with a new dataset based on logged user search behaviour and an accompanying challenge to predict switches between search engines within a given search session.",
        "On the Interplay Between Search Behavior and Collections in Digital Libraries and Archives. Log analysis is an unobtrusive technique used to better understand search behavior and evaluate search systems. However, in contrast with open web search, in a vertical search system such as a digital library or media archive the collection is known and central to its purpose. This drives different, more collection-oriented questions when studying the logs. For example, whether users need different support in different parts of the collection.In a digital library, the collection is categorized using professionally curated metadata. We conjecture that using this metadata can improve and extend the methods and techniques for log analysis. We investigate how to identify different types of search behavior using the metadata explicitly, how to explain and predict user interactions for the different types of behavior found, and finally how to communicate our research results to domain experts. CCS CONCEPTS MOTIVATIONSearch log analysis is an unobtrusive technique used to better understand user behavior in search systems . It can be used to evaluate search algorithms or user interfaces, or to (re-)design systems.Traditional log analysis focuses on queries and clicks [1, 3, 8, 10-13, 16, 19]. This focus poses some disadvantages. First, queries are Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ambiguous, as they form an uncontrolled vocabulary and have little context to interpret the information need. Second, most queries are in the long tail; they occur infrequently making it hard to find recurring patterns. Third, queries may contain privacy-sensitive information such as names and personal information , and thus are seldom shared among researchers.In a closed, vertical search system such as a digital library or archive other data is available in addition to the logs: the documents in the collection and their categorizations using professionally curated metadata. This metadata is often reflected in the search interface in facets, acting as a filter over the search results. This extra information is normally not an integral part of a log analysis.We conjecture that using the metadata of the collection explicitly can improve and extend analytical methods for logs collected in such search systems, in order to be able to examine detailed types of search behavior in relation to specific subsets in the collection. The collection inspires some of the questions relevant here: What parts of the collection have a high user interest? Do users search differently in different parts of the collection? Do they need different support for different parts of the collection? Are there potential gaps in the collection or are some parts of the collection harder to find? A focus on the metadata of the collection, both with respect to the facets used in search and the metadata of clicked documents, makes it possible to answer these type of questions. For example, we can identify gaps in the collection where people search for certain categories of documents but have difficulties finding them. Or we may discover that search behavior within specific subsets of the collection is different, suggesting the need for a different kind of support from the search system. Additionally, with this shift away from the query to the metadata of facet use and clicked documents, we alleviate the disadvantages previously mentioned. First, facet and document metadata is not ambiguous, as it forms a controlled vocabulary. Second, we can group infrequent queries based on shared metadata. Third, the metadata is less privacy-sensitive.In our research we investigate how to identify different types of search behavior based on metadata of search and clicked documents, how to explain and predict user interactions for these types of behavior, and how to communicate our research results to the domain experts.",
        "Measuring website similarity using an entity-aware click graph. Query logs record the actual usage of search systems and their analysis has proven critical to improving search engine functionality. Yet, despite the deluge of information, query log analysis often suffers from the sparsity of the query space. Based on the observation that most queries pivot around a single entity that represents the main focus of the user's need, we propose a new model for query log data called the entity-aware click graph. In this representation, we decompose queries into entities and modifiers, and measure their association with clicked pages. We demonstrate the benefits of this approach on the crucial task of understanding which websites fulfill similar user needs, showing that using this representation we can achieve a higher precision than other query log-based approaches.",
        "Analysis of long queries in a large scale search log. We propose to use the search log to study long queries, in order to understand the types of information needs that are behind them, and to design techniques to improve search effectiveness when they are used. Long queries arise in many different applications, such as CQA (community-based question answering) and literature search, and they have been studied to some extent using TREC data. They are also, however, quite common in web search, as can be seen by looking at the distribution of query lengths in a large scale search log.In this paper we analyze the long queries in the search log with the aim of identifying the characteristics of the most commonly occurring types of queries, and the issues involved with using them effectively in a search engine. In addition, we propose a simple yet effective method for evaluating the performance of the queries in the search log using a combination of the click data in the search log with the existing TREC corpora.",
        "LogCLEF 2009: the CLEF 2009 Multilingual Logfile Analysis Track Overview Log data constitute a relevant aspect in the evaluation process of the quality of a search engine and the quality of a multilingual search service; log data can be used to study the usage of a search engine, and to better adapt it to the objectives the users were expecting to reach. The interest in multilingual log analysis was promoted by the Cross Language Evaluation Forum (CLEF) for the first time with a track named LogCLEF. LogCLEF is an evaluation initiative for the analysis of queries and other logged activities as expression of user behavior. The goal is the analysis and classification of queries in order to understand search behavior in multilingual contexts and ultimately to improve search systems. Two tasks were defined: Log Analysis and Geographic Query Identification (LAGI) and Log Analysis for Digital Societies (LADS). Five groups using a variety of approaches submitted experiments. The data for the track, the evaluation methodology and some results are presented.",
        "Evaluation of Digital Library Services Using Complementary Logs In recent years, the importance of log analysis has grown, log data constitute a relevant aspect in the evaluation process of the quality of a digital library system. In this paper, we address the problem of log analysis for complex systems such as digital library systems, and how the analysis of search query logs or Web logs is not sufficient to study users and interpret their preferences. In fact the combination of implicitly and explicitly collected data improves understanding of behavior with respect to the understanding that can be gained by analyzing the sets of data separately.",
        "How do Greeks search the web?: a query log analysis study. This study investigates how Greeks search the web through the analysis of search logs from the AltaVista search engine. The analysis looks at the data as a whole and also examines the use of the Greek language in web searching. Statistics include frequencies of queries, term distributions, languages used including mixed language queries, search features used in query formulation and query length patterns. Categories and Subject Descriptors General TermsMeasurement, Experimentation, Human Factors. KeywordsGreek web search, query log analysis, user behavior, non-English web search."
    ],
    "entity recognition": [
        "AMRITA_CEN@FIRE-2014: Named Entity Recognition for Indian Languages using Rich Features. This paper aims at implementing Named Entity Recognition (NER) for four languages such as English, Tamil, Hindi and Malayalam. The results obtained from this work are submitted to a research evaluation workshop Forum for Information Retrieval and Evaluation (FIRE 2014). This system detects three levels of named entity tags which are referred as nested named entities. It is a multi-label problem solved using chain classifier method. In this work, Conditional Random Field (CRF) and Support Vector Machine (SVM) are used for implementing NER system. In FIRE 2014, we developed a English NER system using CRF and other NER system for Tamil, Hindi and Malayalam are based on SVM. The FIRE estimated the average precision for all the four languages as 41.93 for outermost level and 33.25 for inner level.In order to improve the performance of Indian languages, we implemented CRF based NER system for the same corpus in Tamil, Hindi and Malayalam. The average precision measure for these mentioned languages are 42.87 for outer level and 36.31 for inner level. The overall performance of the NER system improved by 2.24% for outer level and 9.20% for inner level.",
        "CLEF eHealth Evaluation Lab 2015 Task 1b: Clinical Named Entity Recognition This paper reports on Task 1b of the 2015 CLEF eHealth evaluation lab which extended the previous information extraction tasks of ShARe/CLEF eHealth evaluation labs by considering ten types of entities including disorders, that were to be extracted from biomedical text in French. The task consisted of two phases: entity recognition (phase 1), in which participants could supply plain or normalized entities, and entity normalization (phase 2). The entities to be extracted were defined according to Semantic Groups in the Unified Medical Language System R (UMLS R), which was also used for normalizing the entities. Participant systems were evaluated against a blind reference standard of 832 titles of scientific articles indexed in MEDLINE and 3 full text drug monographs published by the European Medicines Agency (EMEA) using Precision, Recall and F-measure. In total, seven teams participated in phase 1, and three teams in phase 2. The highest performance was obtained on the EMEA corpus, with an overall F-measure of 0.756 for plain entity recognition, 0.711 for normalized entity recognition and 0.872 for entity normalization.",
        "Fine-grained named entity recognition and relation extraction for question answering",
        "Name Entity Recognition with Policy-Value Networks",
        "Named Entity Recognition in Chemical Patents using Ensemble of Contextual Language Models Chemical patent documents describe a broad range of applications holding key reaction and compound information, such as chemical structure, reaction formulas, and molecular properties. These informational entities should be first identified in text passages to be utilized in downstream tasks. Text mining provides means to extract relevant information from chemical patents through information extraction techniques. As part of the Information Extraction task of the Cheminformatics Elsevier Melbourne University challenge, in this work we study the effectiveness of contextualized language models to extract reaction information in chemical patents. We assess transformer architectures trained on a generic and specialised corpora to propose a new ensemble model. Our best model, based on a majority ensemble approach, achieves an exact F1-score of 92.30% and a relaxed F1-score of 96.24%. The results show that ensemble of contextualized language models can provide an effective method to extract information from chemical patents.",
        "Disease named entity recognition using semisupervised learning and conditional random fields",
        "Fine-Grained Named Entity Recognition Using Conditional Random Fields for Question Answering. In many QA systems, fine-grained named entities are extracted by coarse-grained named entity recognizer and fine-grained named entity dictionary. In this paper, we describe a fine-grained Named Entity Recognition using Conditional Random Fields (CRFs) for question answering. We used CRFs to detect boundary of named entities and Maximum Entropy (ME) to classify named entity classes. Using the proposed approach, we could achieve an 83.2% precision, a 74.5% recall, and a 78.6% F1 for 147 fined-grained named entity types. Moreover, we reduced the training time to 27% without loss of performance compared to a baseline model. In the question answering, The QA system with passage retrieval and AIU archived about 26% improvement over QA with passage retrieval. The result demonstrated that our approach is effective for QA.",
        "PRIS at TREC 2011 Entity Track: Related Entity Finding and Entity List Completion",
        "An optimization framework for entity recognition and disambiguation. We present a system for entity recognition and disambiguation (ERD) in short text, aiming at identifying all text fragments referring to an entity contained in Freebase. The task is organized in two steps. Given a short text the first step is discovering text fragments which possibly refer to an entity. Since multiple entities may share common mention, identifying which entity the mention is referring to in the given short text is necessary. Our system integrates three kinds of features: mention-entity similarity, entityentity similarity and context-mention entity similarity. By considering every possible combination of mention-entity pair, we select the one with highest confidence score. An implementation of our system is described, along with our evaluation results. Experiments show that the proposed features improve the performance to a certain extent.",
        "A Robust Named-Entity Recognition System Using Syllable Bigram Embedding with Eojeol Prefix Information. Korean named-entity recognition (NER) systems have been developed mainly on the morphological-level, and they are commonly based on a pipeline framework that identi es named-entities (NEs) following the morphological analysis. However, this framework can mean that the performance of NER systems is degraded, because errors from the morphological analysis propagate into NER systems.is paper proposes a novel syllable-level NER system, which does not require a morphological analysis and can achieve a similar or be er performance compared with the morphological-level NER systems. In addition, because the proposed system does not require a morphological analysis step, its processing speed is about 1.9 times faster than those of the previous morphological-level NER systems.",
        "VinAI at ChEMU 2020: An Accurate System for Named Entity Recognition in Chemical Reactions from Patents This paper describes our VinAI system for the ChEMU task 1 of named entity recognition (NER) in chemical reactions. Our system employs a BiLSTM-CNN-CRF architecture [6] with additional contextualized word embeddings. It achieves very high performance, officially ranking second with regards to both exact-and relaxed-match F1 scores at 94.33% and 96.84%, respectively. In a post-evaluation phase, fixing a mapping bug which converts the column-based format into the brat standoff format helps our system to obtain higher results. In particular, we obtain an exact-match F1 score at 95.21% and especially a relaxedmatch F1 score at 97.26%, thus achieving the highest relaxed-match F1 compared to all other participating systems. We believe our system can serve as a strong baseline for future research and downstream applications of chemical NER over chemical reactions from patents.",
        "A search based approach to entity recognition: magnetic and IISAS team at ERD challenge. ERD 2014 was a research challenge focused on the task of recognition and disambiguation of knowledge base entities in short and long texts. This write-up describes Magnetic-IISAS team's approach to the entity recognition in search queries with which we have participated in ERD 2014 challenge. Our approach combines techniques of information retrieval, gazetteer based annotation and entity link graph analysis to identify and disambiguate candidate entities. We built a search index with multiple structured fields extracted from Wikipedia, Freebase and DBPedia. When processing a query, we first retrieve top matching entities from the index. For all retrieved entities, we gather plausible verbalizations, surface forms, that retrieved entities may be referred to with. We match gathered entity surface forms against the original query to confirm the entity relevance to the query. Finally, we exploit Wikipedia link graph to asses the similarity of candidate entities for the purpose of disambiguation and further candidate filtering. In the paper we discuss successful as well as unsuccessful attempts to improve the quality of system results that we have tried during the course of the challenge.",
        "A Comparative Study of Named Entity Recognition for Telugu. In this paper, we apply three classification learning algorithms to Telugu Named Entity Recognition (NER) task and we present a comparative study between these three learning algorithms on Telugu dataset (NER for South and South-East Asian Languages (NERSSEAL) Competition). The empirical results show that Support Vector Machine achieves the best F-measure of 54.78% on the dataset.",
        "Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization Contextualized embeddings use unsupervised language model pretraining to compute word representations depending on their context. This is intuitively useful for generalization, especially in Named-Entity Recognition where it is crucial to detect mentions never seen during training. However, standard English benchmarks overestimate the importance of lexical over contextual features because of an unrealistic lexical overlap between train and test mentions. In this paper, we perform an empirical analysis of the generalization capabilities of stateof-the-art contextualized embeddings by separating mentions by novelty and with out-of-domain evaluation. We show that they are particularly beneficial for unseen mentions detection, especially out-of-domain. For models trained on CoNLL03, language model contextualization leads to a +1.2% maximal relative micro-F1 score increase in-domain against +13% out-of-domain on the WNUT dataset (The code is available at https://github.com/btaille/contener).",
        "Named entity recognition with multiple segment representations a b s t r a c tNamed entity recognition (NER) is mostly formalized as a sequence labeling problem in which segments of named entities are represented by label sequences. Although a considerable effort has been made to investigate sophisticated features that encode textual characteristics of named entities (e.g. PEOPLE, LOCATION, etc.), little attention has been paid to segment representations (SRs) for multi-token named entities (e.g. the IOB2 notation). In this paper, we investigate the effects of different SRs on NER tasks, and propose a feature generation method using multiple SRs. The proposed method allows a model to exploit not only highly discriminative features of complex SRs but also robust features of simple SRs against the data sparseness problem. Since it incorporates different SRs as feature functions of Conditional Random Fields (CRFs), we can use the well-established procedure for training. In addition, the tagging speed of a model integrating multiple SRs can be accelerated equivalent to that of a model using only the most complex SR of the integrated model. Experimental results demonstrate that incorporating multiple SRs into a single model improves the performance and the stability of NER. We also provide the detailed analysis of the results.",
        "Named entity recognition for tweets Two main challenges of Named Entity Recognition (NER) for tweets are the insufficient information in a tweet and the lack of training data. We propose a novel method consisting of three core elements: (1) normalization of tweets; (2) combination of a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model; and (3) semisupervised learning framework. The tweet normalization preprocessing corrects common ill-formed words using a global linear model. The KNN-based classifier conducts prelabeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semisupervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of normalization, KNN, and semisupervised learning.",
        "Biomedical Named Entity Recognition Based on the Combination of Regional and Global Text Features. The biomedical information extraction, especially Named Entity Recognition (NER), is a primary task in biomedical text-mining due to the rapid growth of large-scale literature. Extracting biomedical entities aims at identifying specific entities (words or phrases) from those unstructured text data. In this work, we introduce a novel biomedical NER system utilizing a combination of regional and global text features: linguistic, lexical, contextual, and syntactic features. Our system adopts Conditional Random Fields (CRFs) [1] as a machine learning algorithm and consists of two major pipelines (see . We especially focus on constructing the first pipeline for text processing in a modularized manner and discovering rich feature sets regarding comprehensive linguistics and contexts. To implement the CRF framework in the second pipeline, our system uses a modified version of Mallet to take advantage of feature induction. As a result of 10-fold cross-validation, our system achieves from 0.99% up to 18.47% of F-measure improvement as well as the highest precision compared to existing open-source biomedical NER systems on GENETAG corpus . We figure out that several components such as abundant key features, external resources, and feature induction contribute to the performance of the proposed system. ACKNOWLEDGMENTS",
        "Neural Chinese Named Entity Recognition via CNN-LSTM-CRF and Joint Training with Word Segmentation",
        "Clinical Entity Recognition Using Cost-Sensitive Structured Perceptron for NTCIR-10 MedNLP This paper reports on our approach to the NTCIR-10 MedNLP task, which aims at identifying personal and medical information in Japanese clinical texts. We applied a machine learning (ML) algorithm for sequential labeling, specifically, structured perceptron, and defined a cost function for lowering misclassification cost. On the test set provided by the organizers, our approach achieved an F-score of 77.00 for the de-identification task and 79.14 for the complaint and diagnosis task.",
        "The Effect of Entity Recognition on Answer Validation",
        "Named-Entity Recognition in Bengali. This paper 1 describes two systems for Named Entity Recognition (NER) and performance of two systems has been compared. The first system is a rule-based one whereas the second one is statistical (based on CRF) in nature. The systems vary in some other aspects too, for example, the first system works on untagged data (not even POS tag is done) to identify NER whereas the second system makes use of a POS tagger and a chunker. The rules used by the first system are mined from the training data. The CRF-based classification does not require any explicit linguistic rules but it uses a gazetteer built from Wiki and other sources.",
        "Named entity recognition using a modified Pegasos algorithm. In this paper, we describe a named entity recognition using a modified Pegasos algorithm for structural SVMs. We show the modified Pegasos algorithm significantly outperformed CRFs and the training time for the modified Pegasos algorithm is reduced 17~26 times compared to CRFs.",
        "Entity Recognition in Parallel Multi-lingual Biomedical Corpora: The CLEF-ER Laboratory Overview",
        "Named Entity Recognition in Crime Using Machine Learning Approach. Most of the crimes committed today are reported on the Internet by news articles, blogs and social networking sites. With the increasing volume of crime information available on the web, a means to retrieve and exploit them and provide insight into the criminal behavior and networks must be determined to fight crime more efficiently and effectively. We believe that an electronic system must be designed for crime named entity recognition from the newspaper articles. Thus, this study designs and develops a crime named entity recognition based on machine learning approaches that extract nationalities, weapons, and crime locations in online crime documents. This study also collected a new corpus of crime and manually labeled them. A machine learning classification framework is proposed based on Na\u00efve Bayes and SVM model in extracting nationalities, weapons, and crime location from online crime documents. To evaluate our model, a manually annotated data set was used, which was then validated by experiments. The results of the experiments showed that the developed techniques are promising.",
        "CUSAT_TEAM@IECSIL-FIRE-2018: A Named Entity Recognition System for Indian Languages Named Entity Recognition is the process of classifying the elementary units in a text document into meaningful categories such as person, location, organization, etc. It is a significant preprocessing step in the semantic analysis of natural language text. There is an enormous growth of Indian language content on various media types such as websites, blogs, email, chats, etc. over the past decade. Automatic processing of this huge unstructured data is a challenging task especially when the companies are interested to ascertain public view on their products and processes. NER is one of the subtasks of Information Extraction. Extracting structured information from the natural language text is the ultimate goal of IE systems. Different methods are proposed and experimented for NER. In this paper, we propose a Named Entity Recognition system for Indian languages using Conditional Random Fields. Training and testing are conducted using the shared corpus provided by 'ARNEKT-IECSIL 2018' competition organizers. The evaluation results show that the proposed system is able to outperform most of the reported methods in the competition.",
        "NERA: Named Entity Recognition for Arabic",
        "Named entity recognition in query. This paper addresses the problem of Named Entity Recognition in Query (NERQ), which involves detection of the named entity in a given query and classification of the named entity into predefined classes. NERQ is potentially useful in many applications in web search. The paper proposes taking a probabilistic approach to the task using query log data and Latent Dirichlet Allocation. We consider contexts of a named entity (i.e., the remainders of queries after the named entity is removed) as words of a document, and classes of the named entity as topics. The topic model is constructed by a novel and general learning method referred to as WS-LDA (Weakly Supervised Latent Dirichlet Allocation), which employs weakly supervised learning (rather than unsupervised learning) using partially labeled seed entities. Experimental results show that the proposed method based on WS-LDA can accurately perform NERQ, and outperform the baseline methods.",
        "Web-scale named entity recognition. Automatic recognition of named entities such as people, places, organizations, books, and movies across the entire web presents a number of challenges, both of scale and scope. Data for training general named entity recognizers is difficult to come by, and efficient machine learning methods are required once we have found hundreds of millions of labeled observations. We present an implemented system that addresses these issues, including a method for automatically generating training data, and a multi-class online classification training method that learns to recognize not only high level categories such as place and person, but also more finegrained categories such as soccer players, birds, and universities. The resulting system gives precision and recall performance comparable to that obtained for more limited entity types in much more structured domains such as company recognition in newswire, even though web documents often lack consistent capitalization and grammatical sentence construction.",
        "Entity Recognition and Language Identification with FELTS",
        "Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants We focus on improving the effectiveness of a Virtual Assistant (VA) in recognizing emerging entities in spoken queries. We introduce a method that uses historical user interactions to forecast which entities will gain in popularity and become trending, and it subsequently integrates the predictions within the Automated Speech Recognition (ASR) component of the VA. Experiments show that our proposed approach results in a 20% relative reduction in errors on emerging entity name utterances without degrading the overall recognition quality of the system.",
        "Effective named entity recognition for idiosyncratic web collections. Named Entity Recognition (NER) plays an important role in a variety of online information management tasks including text categorization, document clustering, and faceted search. While recent NER systems can achieve near-human performance on certain documents like news articles, they still remain highly domain-specific and thus cannot effectively identify entities such as original technical concepts in scientific documents. In this work, we propose novel approaches for NER on distinctive document collections (such as scientific articles) based on n-grams inspection and classification. We design and evaluate several entity recognition features-ranging from well-known part-of-speech tags to n-gram co-location statistics and decision trees-to classify candidates. In addition, we show how the use of external knowledge bases (either specific like DBLP or generic like DBPedia) can be leveraged to improve the effectiveness of NER for idiosyncratic collections. We evaluate our system on two test collections created from a set of Computer Science and Physics papers and compare it against state-ofthe-art supervised methods. Experimental results show that a careful combination of the features we propose yield up to 85% NER accuracy over scientific collections and substantially outperforms state-of-the-art approaches such as those based on maximum entropy.",
        "Building and Evaluating Universal Named-Entity Recognition English corpus",
        "Collective Named Entity Recognition in User Comments via Parameterized Label Propagation",
        "Incorporating Social Context and Domain Knowledge for Entity Recognition. Recognizing entity instances in documents according to a knowledge base is a fundamental problem in many data mining applications. The problem is extremely challenging for short documents in complex domains such as social media and biomedical domains. Large concept spaces and instance ambiguity are key issues that need to be addressed.Most of the documents are created in a social context by common authors via social interactions, such as reply and citations. Such social contexts are largely ignored in the instance-recognition literature. How can users' interactions help entity instance recognition? How can the social context be modeled so as to resolve the ambiguity of different instances?In this paper, we propose the SOCINST model to formalize the problem into a probabilistic model. Given a set of short documents (e.g., tweets or paper abstracts) posted by users who may connect with each other, SOCINST can automatically construct a context of subtopics for each instance, with each subtopic representing one possible meaning of the instance. The model is also able to incorporate social relationships between users to help build social context. We further incorporate domain knowledge into the model using a Dirichlet tree distribution.We evaluate the proposed model on three different genres of datasets: ICDM'12 Contest, Weibo, and I2B2. In ICDM'12 Contest, the proposed model clearly outperforms (+21.4%; p 1e \u2212 5 with t-test) all the top contestants. In Weibo and I2B2, our results also show that the recognition accuracy of SOCINST is up to 5.3-26.6% better than those of several alternative methods.",
        "Overview of ChEMU 2020: Named Entity Recognition and Event Extraction of Chemical Reactions from Patents",
        "Named Entity Recognition for Telugu News Articles using Na\u00efve Bayes Classifier The Named Entity Recognition (NER) is identifying name of Person, Location, Organization etc. in a given sentence or a document. In this paper, we have attempted to classify textual content from on-line Telugu newspapers using well known generative model. We have used generic features like contextual words and their part-of-speech (POS) to build the learning model. By understanding the syntax and grammar of Telugu language, we propose morphological pre-processing of the data and this step yields us better accuracy. We propose some interesting language dependent features like post-position feature, clue word feature and gazetteer feature to improve the performance of the model. The model achieved an overall average F1-Score of 88.87% for Person, 87.32% for Location and 72.69% for Organization.",
        "Towards Entity Correctness, Completeness and Emergence for Entity Recognition. Linking words or phrases in unstructured text to entities in knowledge bases is the problem of entity recognition and disambiguation. In this paper, we focus on the task of entity recognition in Web text to address the challenges of entity correctness, completeness and emergence that existing approaches mainly suffer from. Experimental results show that our approach significantly outperforms the state-of-the-art approaches in terms of precision, F-measure, micro-accuracy and macro-accuracy, while still preserving high recall.",
        "Clinical entity recognition using structural support vector machines with rich features. Named entity recognition (NER) is an important task for natural language processing (NLP) of clinical text. Conditional Random Fields (CRFs), a sequential labeling algorithm, and Support Vector Machines (SVMs), which is based on large margin theory, are two typical machine learning algorithms that have been widely applied to NER tasks, including clinical entity recognition. However, Structural Support Vector Machines (SSVMs), an algorithm that combines the advantages of both CRFs and SVMs, has not been investigated for clinical text processing. In this study, we applied the SSVMs algorithm to the Concept Extraction task of the 2010 i2b2 clinical NLP challenge, which was to recognize entities of medical problems, treatments, and tests from hospital discharge summaries. Using the same training (N = 27,837) and test (N = 45,009) sets in the challenge, our evaluation showed that the SSVMs-based NER system required less training time, while achieved better performance than the CRFs-based system for clinical entity recognition, when same features were used. Our study also demonstrated that rich features such as unsupervised word representations improved the performance of clinical entity recognition. When rich features were integrated with SSVMs, our system achieved a highest F-measure of 85.74% on the test set of 2010 i2b2 NLP challenge, which outperformed the best system reported in the challenge by 0.5%.",
        "The Effect of Entity Recognition in Answer Validation The Answer Validation Exercise (AVE) 2006 is aimed at developing systems able to decide whether the answer of a Question Answering (QA) system is correct or not using textual entailment. The most answers to be validated are given from questions that need entities as responses. The paper presents a system that has only used entities to participate in the AVE 2006. The results of the propose system are better than the ones of a baseline system that always accepts all answers, therefore the use of entities can improve the results of an answer validation system."
    ],
    "relevance assessments": [
        "Using crowdsourcing for TREC relevance assessment a b s t r a c tCrowdsourcing has recently gained a lot of attention as a tool for conducting different kinds of relevance evaluations. At a very high level, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee. This crowdsourcing approach makes possible to conduct information retrieval experiments extremely fast, with good results at a low cost.This paper reports on the first attempts to combine crowdsourcing and TREC: our aim is to validate the use of crowdsourcing for relevance assessment. To this aim, we use the Amazon Mechanical Turk crowdsourcing platform to run experiments on TREC data, evaluate the outcomes, and discuss the results. We make emphasis on the experiment design, execution, and quality control to gather useful results, with particular attention to the issue of agreement among assessors. Our position, supported by the experimental results, is that crowdsourcing is a cheap, quick, and reliable alternative for relevance assessment.",
        "SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement. One immediate challenge in searching the deep web databases is source selection-i.e. selecting the most relevant web databases for answering a given query. The existing database selection methods (both text and relational) assess the source quality based on the query-similarity-based relevance assessment. When applied to the deep web these methods have two deficiencies. First is that the methods are agnostic to the correctness (trustworthiness) of the sources. Secondly, the query based relevance does not consider the importance of the results. These two considerations are essential for the open collections like the deep web. Since a number of sources provide answers to any query, we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources. We compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source that we call SourceRank, is calculated as the stationary visit probability of a random walk. We evaluate SourceRank in multiple domains, including sources in Google Base, with sizes up to 675 sources. We demonstrate that the SourceRank tracks source corruption. Further, our relevance evaluations show that SourceRank improves precision by 22-60% over the Google Base and other baseline methods. SourceRank has been implemented in a system called Factal.",
        "Quality through flow and immersion: gamifying crowdsourced relevance assessments. Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequentlyrequested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent TREC data, we investigate the performance of traditional HIT designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.",
        "Interactive Evaluation of the Ostensive Model Using a New Test Collection of Images with Multiple Relevance Assessments",
        "Why Assessing Relevance in Medical IR is Demanding This study investigates if and why assessing relevance of clinical records for a clinical retrieval task is cognitively demanding. Previous research has highlighted the challenges and issues information retrieval systems are faced with when determining the relevance of documents in this domain, e.g., the vocabulary mismatch problem. Determining if this assessment imposes cognitive load on human assessors, and why this is the case, may shed lights on what are the (cognitive) processes that assessors use for determining document relevance (in this domain). High cognitive load may impair the ability of the user to make accurate relevance judgements and hence the design of IR mechanisms may need to take this into account in order to reduce the load.",
        "Visualization for Relevance Assessments",
        "Design and Implementation of Relevance Assessments Using Crowdsourcing. In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.",
        "Relevance assessments and retrieval system evaluation",
        "Relations Between Relevance Assessments, Bibliometrics and Altmetrics",
        "Experiments with dictionary-based CLIR using graded relevance assessments: Improving effectiveness by pseudo-relevance feedback",
        "Sound and complete relevance assessment for XML retrieval In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In XML retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results.A major difference between XML retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented XML retrieval as carried out at INEX, the evaluation campaign for XML retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents-even though the objective is to collect assessments at element level. The different methods of gathering assessments at INEX are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods.",
        "Relevance Estimation with Multiple Information Sources on Search Engine Result Pages. Relevance estimation is among the most important tasks in the ranking of search results. Current relevance estimation methodologies mainly concentrate on text matching between the query and Web documents, link analysis and user behavior models. However, users judge the relevance of search results directly from Search Engine Result Pages (SERPs), which provide valuable signals for reranking. Morden search engines aggregate heterogeneous information items (such as images, news, and hyperlinks) to a single ranking list on SERPs. The aggregated search results have different visual patterns, textual semantics and presentation structures, and a better strategy should rely on all these information sources to improve ranking performance. In this paper, we propose a novel framework named Joint Relevance Estimation model (JRE), which learns the visual patterns from screenshots of search results, explores the presentation structures from HTML source codes and also adopts the semantic information of textual contents. To evaluate the performance of the proposed model, we construct a large scale practical Search Result Relevance (SRR) dataset which consists of multiple information sources and 4-grade relevance scores of over 60, 000 search results. Experimental results show that the proposed JRE model achieves better performance than state-of-the-art ranking solutions as well as the original ranking of commercial search engines.",
        "Relevance Evaluation of Search Engines' Query Results",
        "User Evaluation of Multidimensional Relevance Assessment",
        "Relevance thresholds: a multi-stage predictive model of how users evaluate information",
        "SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement. We consider the problem of deep web source selection and argue that existing source selection methods are inadequate as they are based on local similarity assessment. Specifically, they fail to account for the fact that sources can vary in trustworthiness and individual results can vary in importance. In response, we formulate a global measure to calculate relevance and trustworthiness of a source based on agreement between the answers provided by different sources. Agreement is modeled as a graph with sources at the vertices. On this agreement graph, source quality scores-namely SourceRank -are calculated as the stationary visit probability of a weighted random walk. Our experiments on online databases and 675 book sources from Google Base show that SourceRank improves relevance of the results by 25-40% over existing methods and Google Base ranking. SourceRank also reduces linearly with the corruption levels of the sources.",
        "Assessing relevance and trust of the deep web sources and results based on inter-source agreement Deep web search engines face the formidable challenge of retrieving high-quality results from the vast collection of searchable databases. Deep web search is a two-step process of selecting the high-quality sources and ranking the results from the selected sources. Though there are existing methods for both the steps, they assess the relevance of the sources and the results using the query-result similarity. When applied to the deep web these methods have two deficiencies. First is that they are agnostic to the correctness (trustworthiness) of the results. Second, the query-based relevance does not consider the importance of the results and sources. These two considerations are essential for the deep web and open collections in general. Since a number of deep web sources provide answers to any query, we conjuncture that the agreements between these answers are helpful in assessing the importance and the trustworthiness of the sources and the results. For assessing source quality, we compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for the possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source, that we call SourceRank, is calculated as the stationary visit probability of a random walk. For ranking results, we analyze the second-order agreement between the results. Further extending SourceRank to multidomain search, we propose a source ranking sensitive to the query domains. Multiple domain-specific rankings of a source are computed, and these ranks are combined for the final ranking. We perform extensive evaluations on online and hundreds of Google Base sources spanning across domains. The proposed result and source rankings are implemented in the deep web search engine Factal. We demonstrate that the agreement analysis tracks source corruption. Further, our relevance evaluations show that our methods improve precision significantly over Google Base and the other baseline methods. The result ranking and the domain-specific source ranking are evaluated separately.",
        "Providing consistent and exhaustive relevance assessments for XML retrieval evaluation. Comparing retrieval approaches requires test collections, which consist of documents, queries and relevance assessments. Obtaining consistent and exhaustive relevance assessments is crucial for the appropriate comparison of retrieval approaches. Whereas the evaluation methodology for flat text retrieval approaches is well established, the evaluation of XML retrieval approaches is a research issue. This is because XML documents are composed of nested components that cannot be considered independent in terms of relevance. This paper describes the methodology adopted in INEX (the INitiative for the Evaluation of XML Retrieval) to ensure consistent and exhaustive relevance assessments.",
        "Assessing efficiency-effectiveness tradeoffs in multi-stage retrieval systems without using relevance judgments",
        "Variations in Relevance Assessments and the Measurement of Retrieval Effectiveness",
        "A Study of the Assessment of Relevance for the INEX'02 Test Collection",
        "Relevance assessment: are judges exchangeable and does it matter. We investigate to what extent people making relevance judgements for a reusable IR test collection are exchangeable. We consider three classes of judge: \"gold standard\" judges, who are topic originators and are experts in a particular information seeking task; \"silver standard\" judges, who are task experts but did not create topics; and \"bronze standard\" judges, who are those who did not define topics and are not experts in the task.Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold standard judges are preferred.",
        "Better than Their Reputation? On the Reliability of Relevance Assessments with Students",
        "Relevance Ranking Based on Query-Aware Context Analysis Word mismatch between queries and documents is a longstanding challenge in information retrieval. Recent advances in distributed word representations address the word mismatch problem by enabling semantic matching. However, most existing models rank documents based on semantic matching between query and document terms without an explicit understanding of the relationship of the match to relevance. To consider semantic matching between query and document, we propose an unsupervised semantic matching model by simulating a user who makes relevance decisions. The primary goal of the proposed model is to combine the exact and semantic matching between query and document terms, which has been shown to produce effective performance in information retrieval. As semantic matching between queries and entire documents is computationally expensive, we propose to use local contexts of query terms in documents for semantic matching. Matching with smaller query-related contexts of documents stems from the relevance judgment process recorded by human observers. The most relevant part of a document is then recognized and used to rank documents with respect to the query. Experimental results on several representative retrieval models and standard datasets show that our proposed semantic matching model significantly outperforms competitive baselines in all measures.",
        "Relevation!: an open source system for information retrieval relevance assessment}. Relevation! is a system for performing relevance judgements for information retrieval evaluation. Relevation! is web-based, fully configurable and expandable; it allows researchers to effectively collect assessments and additional qualitative data. The system is easily deployed allowing assessors to smoothly perform their relevance judging tasks, even remotely. Relevation! is available as an open source project at: http://ielab.github.io/relevation.",
        "Experiments with transitive dictionary translation and pseudo-relevance feedback using graded relevance assessments",
        "Relevance Judgments: Preferences, Scores and Ties",
        "Is relevance hard work?: evaluating the effort of making relevant assessments. The judging of relevance has been a subject of study in information retrieval for a long time, especially in the creation of relevance judgments for test collections. While the criteria by which assessors' judge relevance has been intensively studied, little work has investigated the process individual assessors go through to judge the relevance of a document. In this paper, we focus on the process by which relevance is judged, and in particular, the degree of effort a user must expend to judge relevance. By better understanding this effort in isolation, we may provide data which can be used to create better models of search. We present the results of an empirical evaluation of the effort users must exert to judge the relevance of document, investigating the effect of relevance level and document size. Results suggest that \"relevant\" documents require more effort to judge when compared to highly relevant and not relevant documents, and that effort increases as document size increases.",
        "Relevance Judgments for Assessing Recall",
        "How Relevant is the Long Tail? - A Relevance Assessment Study on Million Short. Users of web search engines are known to mostly focus on the top ranked results of the search engine result page. While many studies support this well known information seeking pattern only few studies concentrate on the question what users are missing by neglecting lower ranked results. To learn more about the relevance distributions in the so-called long tail we conducted a relevance assessment study with the Million Short long-tail web search engine. While we see a clear difference in the content between the head and the tail of the search engine result list we see no statistical significant differences in the binary relevance judgments and weak significant differences when using graded relevance. The tail contains different but still valuable results. We argue that the long tail can be a rich source for the diversification of web search engine result lists but it needs more evaluation to clearly describe the differences.",
        "Evaluating the effort involved in relevance assessments for images. How assessors and end users judge the relevance of images has been studied in information science and information retrieval for a considerable time. The criteria by which assessors' judge relevance has been intensively studied, and there has been a large amount of work which has investigated how relevance judgments for test collections can be more cheaply generated, such as through crowd sourcing. Relatively little work has investigated the process individual assessors go through to judge the relevance of an image. In this paper, we focus on the process by which relevance is judged for images, and in particular, the degree of effort a user must expend to judge relevance for different topics. Results suggest that topic difficulty and how semantic/visual a topic is impact user performance and perceived effort.",
        "Possibility and necessity measures for relevance assessment. The major question raised in information retrieval on semistructured documents relates to the manner of effectively handling the structure and the contents of the document for better answering the user's needs. These needs can be formulated by queries composed of only key words or key words and structural constraints. In this paper, we are interested in Information Retrieval in semi-structured document like XML. For these purposes, we present a model for the semi-structured information retrieval, based on the possibilistic networks. The documentelements and elements -terms relations are modelled by measures of possibility and necessity. In this model, the user's query starts a process of propagation to recover documents or portions of documents necessarily or at least possibly relevant. An example of such a research is proposed in order to illustrate the presented approach.",
        "Using graded relevance assessments in IR evaluation",
        "On the impact of domain expertise on query formulation, relevance assessment and retrieval performance in clinical settings a b s t r a c tThe large volumes of medical information available on the web may provide answers for a wide range of users attempting to solve health-related problems. While experts generally utilize reliable resources for diagnosis search and professional development, novices utilize different (social) web resources to obtain information that helps them manage their health or the health of people who they care for. A diverse number of related search topics address clinical diagnosis, advice searching, information sharing, connecting with experts, etc. This paper focuses on the extent to which expertise can impact clinical query formulation, document relevance assessment and retrieval performance in the context of tailoring retrieval models and systems to experts vs. non-experts. The results show that medical domain expertise 1) plays an important role in the lexical representations of information needs; 2) significantly influences the perception of relevance even among users with similar levels of expertise and 3) reinforces the idea that a single ground truth does not exist, thereby leading to the variability of system rankings with respect to the level of user's expertise. The findings of this study presents opportunities for the design of personalized health-related IR systems, but also for providing insights about the evaluation of such systems."
    ],
    "Deep Neural Networks": [
        "Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval KeywordsNeural networks; deep learning; information retrieval Website: http://research.microsoft.com/neuir2016 MOTIVATIONIn recent years, deep neural networks have yielded significant performance improvements on speech recognition and computer vision tasks , as well as led to exciting breakthroughs in novel application areas such as automatic voice translation , image captioning , and conversational agents . Despite demonstrating good performance on natural language processing (NLP) tasks (e.g., language modelling [6] and machine translation [1]), the performance of deep neural networks on information retrieval (IR) tasks has had relatively less scrutiny. Recent work in this area has mainly focused on word embeddings [3, 9, 14] and neural models for short text similarity .The lack of many positive results in this area of information retrieval is partially due to the fact that IR tasks such as ranking are fundamentally different from NLP tasks, but also because the IR and neural network communities are only beginning to focus on the application of these techniques to core information retrieval problems. Given that deep learning has made such a big impact, first on speech processing and computer vision and now, increasingly, also on computational linguistics, it seems clear that deep learning will have a major impact on information retrieval and that this is an ideal time for a workshop in this area.Neu-IR (pronounced \"new IR\") will be a forum for new research relating to deep learning and other neural network based approaches to IR. The purpose is to provide an opportunity for people to present new work and early results, compare notes on neural network toolkits, share best practices, and discuss the main challenges facing this line of research.",
        "Unified deep neural network for segmentation and labeling of multipanel biomedical figures",
        "PhonoNet: Multi-Stage Deep Neural Networks for Raga Identification in Hindustani Classical Music",
        "Can Deep Learning Only Be Neural Networks? The word \"deep learning\" is generally regarded as a synonym of \"deep neural networks (DNNs)\". In this talk, we will discuss on essentials in deep learning and claim that deep learning is not necessarily to be realized by neural networks and differentiable modules. We will then present an exploration to non-NN style deep learning, where the building blocks are non-differentiable modules and the training process does not rely on backpropagation or gradient-based adjustment. We will also talk about some recent advances and challenges in this direction of research.",
        "DKN: Deep Knowledge-Aware Network for News Recommendation. Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. To solve the above problem, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
        "Neural Networks for Information Retrieval. Machine learning plays a role in many aspects of modern IR systems, and deep learning is applied in all of them. The fast pace of modern-day research has given rise to many approaches to many IR problems. The amount of information available can be overwhelming both for junior students and for experienced researchers looking for new research topics and directions. The aim of this fullday tutorial is to give a clear overview of current tried-and-trusted neural methods in IR and how they benefit IR.",
        "Deep Ensemble Learning for Legal Query Understanding Legal query understanding is a complex problem that involves two natural language processing (NLP) tasks that needs to be solved together: (i) identifying intent of the user and (ii) recognizing entities within the queries. The problem equates to decomposing a legal query into its individual components and deciphering the underlying differences that can occur due to pragmatics. Identifying the desired intent and recognizing correct entities helps us return back relevant results to the user. Deep Neural Networks (DNNs) have recently achieved great success surpassing traditional statistical approaches. In this work, we experiment with several DNN architectures towards legal query intent classification and entity recognition. Deep Neural architectures like Recurrent Neural Networks (RNNs), Long Short Term Memory (LSTM), Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRU) were applied and compared against one another both individually and as combinations. The models were also compared against machine learning (ML) and rule-based approaches. In this paper, we describe a methodology that integrates posterior probabilities produced by the best DNN models and create a stacked framework for combining the different predictors to improve prediction accuracy and F-measure for legal intent classification and entity recognition.",
        "Multi-scale deep context convolutional neural networks for semantic segmentation. Recent years have witnessed the great progress for semantic segmentation using deep convolutional neural networks (DCNNs). This paper presents a novel fully convolutional network for semantic segmentation using multi-scale contextual convolutional features. Since objects in natural images tend to be with various scales and aspect ratios, capturing the rich contextual information is very critical for dense pixel prediction. On the other hand, when going deeper in convolutional layers, the convolutional feature maps of traditional DCNNs gradually become coarser, which may be harmful for semantic segmentation. According to these observations, we attempt to design a multi-scale deep context convolutional network (MDCCNet), which combines the feature maps from different levels of network in a holistic manner for semantic segmentation. The segmentation outputs of MDCCNets are further enhanced using dense connected conditional random fields (CRF). The proposed network allows us to fully exploit local and global contextual information, ranging from an entire scene to every single pixel, to perform pixel-wise label estimation. The experimental results demonstrate that our method outperforms or is comparable to state-of-the-art methods on PASCAL VOC 2012 and SIFTFlow semantic segmentation datasets.",
        "Multimodal Deep Convolutional Neural Network for Audio-Visual Emotion Recognition",
        "Event-Related Query Classification with Deep Neural Networks",
        "Random-Forest-Inspired Neural Networks Neural networks have become very popular in recent years, because of the astonishing success of deep learning in various domains such as image and speech recognition. In many of these domains, specific architectures of neural networks, such as convolutional networks, seem to fit the particular structure of the problem domain very well and can therefore perform in an astonishingly effective way. However, the success of neural networks is not universal across all domains. Indeed, for learning problems without any special structure, or in cases where the data are somewhat limited, neural networks are known not to perform well with respect to traditional machine-learning methods such as random forests. In this article, we show that a carefully designed neural network with random forest structure can have better generalization ability. In fact, this architecture is more powerful than random forests, because the back-propagation algorithm reduces to a more powerful and generalized way of constructing a decision tree. Furthermore, the approach is efficient to train and requires a small constant factor of the number of training examples. This efficiency allows the training of multiple neural networks to improve the generalization accuracy. Experimental results on real-world benchmark datasets demonstrate the effectiveness of the proposed enhancements for classification and regression.",
        "Digital watermarking for deep neural networks",
        "A deep neural network model for speakers coreference resolution in legal texts",
        "Deep Neural Networks for News Recommendations. A fundamental role of news websites is to recommend articles that are interesting to read. The key challenge of news recommendation is to recommend newly published articles. Unlike other domains, outdated items are considered to be irrelevant in the news recommendation task. Another challenge is that the recommendation candidates are not seen in the training phase. In this paper, we introduce deep neural network models to overcome these challenges. we propose a modified session-based Recurrent Neural Network (RNN) model tailored to news recommendation as well as a historybased RNN model that spans the whole user's past histories. Finally, we propose a Convolutional Neural Network (CNN) model to capture user preferences and to personalize recommendation results. Experimental results on real-world news dataset shows that our model outperforms competitive baselines.",
        "Aspect-based Financial Sentiment Analysis with Deep Neural Networks. Aspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold crossvalidation, and we compare our model with a variety of related deep neural network models.",
        "Deep Neural Networks with Voice Entry Estimation Heuristics for Voice Separation in Symbolic Music Representations",
        "A review of semantic segmentation using deep neural networks",
        "Embedding Watermarks into Deep Neural Networks",
        "Fighting Against Deepfake: Patch&Pair Convolutional Neural Networks (PPCNN)",
        "Automatic Tagging Using Deep Convolutional Neural Networks",
        "Spectrum-based Deep Neural Networks for Fraud Detection. In this paper, we focus on fraud detection on a signed graph with only a small set of labeled training data. We propose a novel framework that combines deep neural networks and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as the input of deep neural networks. Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training deep neural networks becomes feasible. We develop and evaluate two neural networks, deep autoencoder and convolutional neural network, in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based deep neural networks are e ective in fraud detection.",
        "Neural Collaborative Filtering. In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -collaborative filtering -on the basis of implicit feedback.Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items.By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
        "Decision Boundary of Deep Neural Networks: Challenges and Opportunities One crucial aspect that yet remains fairly unknown while can inform us about the behavior of deep neural networks is their decision boundaries. Trust can be improved once we understand how and why deep models carve out a particular form of decision boundary and thus make particular decisions. Robustness against adversarial examples is directly related to the decision boundary as adversarial examples are basically 'missed out' by the decision boundary between two classes. Investigating the decision boundary of deep neural networks, nevertheless, faces tremendous challenges. First, how we can generate instances near the decision boundary that are similar to real samples? Second, how we can leverage near decision boundary instances to characterize the behaviour of deep neural networks? Motivated to solve these challenges, we focus on investigating the decision boundary of deep neural network classifiers. In particular, we propose a novel approach to generate instances near decision boundary of pre-trained DNNs and then leverage these instances to characterize the behaviour of deep models. CCS CONCEPTS \u2022 Computing methodologies \u2192 Neural networks; Causal reasoning and diagnostics; Computer vision; Supervised learning.",
        "Deep Learning for the Web. Deep learning is a machine learning technology that automatically extracts higher-level representations from raw data by stacking multiple layers of neuron-like units. The stacking allows for extracting representations of increasingly-complex features without time-consuming, offline feature engineering. This tutorial provides the basics of deep learning as well as its key applications. We give the motivation and underlying ideas of deep learning and describe the architectures and learning algorithms for various deep learning models. We also cover applications of deep learning for image and video processing, natural language and text data analysis, social data analytics, and wearable IoT sensor data with an emphasis in the domain of Web systems. We will deliver the key insight and understanding of these techniques, using graphical illustrations and examples that could be important in analyzing a large amount of Web data. The tutorial is prepared to attract general audience at the WWW Conference, who are interested in machine learning and big data analysis for Web data.The tutorial consists of five parts. The first part presents the basics of neural networks, and their structures. Then we explain the training algorithm via backpropagation, which is a common method of training artificial neural networks including deep neural networks. We will emphasize how each of these concepts can be used in various Web data analysis. In the second part of the tutorial, we describe the learning algorithms for deep neural networks and related ideas, such as contrastive divergence, wake-sleep algorithms, and Monte Carlo simulation. We then describe various kinds of deep architectures, including stacked autoencoders, deep belief networks , convolutional neural networks , and deep hypernetworks .In the third part, we present more details of the recursive neural networks, which can learn structured tree outputs as well as vector representations for phrases and sentences. We first show how training the recursive neural network can be achieved by a modified version of the backpropagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Then we will present its applications to sentence analysis including POS tagging, and sentiment analysis. The fourth part discusses the neural networks used to generate word embeddings, such as Word2Vec [10], DSSM for deep semantic similarity , and object detection in images , such as GoogLeNet, and AlexNet. We will explain in detail the applications of these deep learning techniques in the analysis of various social network data. By this point, the audience should have a clear understanding of how to build a deep learning system for word, sentence and document level tasks. The fifth part of the tutorial will cover other application examples of deep learning. These include object segmentation and action recognition from videos , web data analytics, and wearable/IoT sensor data modeling for smart services.",
        "Quote Recommendation in Dialogue using Deep Neural Network. Quotes, or quotations, are well known phrases or sentences that we use for various purposes such as emphasis, elaboration, and humor. In this paper, we introduce a task of recommending quotes which are suitable for given dialogue context and we present a deep learning recommender system which combines recurrent neural network and convolutional neural network in order to learn semantic representation of each utterance and construct a sequence model for the dialog thread. We collected a large set of twitter dialogues with quote occurrences in order to evaluate proposed recommender system. Experimental results show that our approach outperforms not only the other state-of-the-art algorithms in quote recommendation task, but also other neural network based methods built for similar tasks.",
        "Locally Connected Deep Learning Framework for Industrial-scale Recommender Systems. In this work, we propose a locally connected deep learning framework for recommender systems, which reduces the complexity of deep neural network (DNN) by two to three orders of magnitude. We further extend the framework using the idea of recently proposed Wide&Deep model. Experiments on industrial-scale datasets show that our methods could achieve good results with much shorter runtime.",
        "Image-based Plant Species Identification with Deep Convolutional Neural Networks This paper presents deep learning techniques for image-based plant identification at very large scale. State-of-the-art Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 10,000 species. To improve identification performance several models trained on different datasets with multiple image dimensions and aspect ratios are ensembled. Various data augmentation techniques have been applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the LifeCLEF 2017 campaign. It provides the best system among all participating teams by achieving a mean reciprocal rank (MRR) of 92 % and a top-5 accuracy of 96 % on the official PlantCLEF test set.",
        "Neural Networks for Information Retrieval Machine learning plays a role in many aspects of modern IR systems, and deep learning is applied in all of them. The fast pace of modernday research has given rise to many di erent approaches for many di erent IR problems. The amount of information available can be overwhelming both for junior students and for experienced researchers looking for new research topics and directions. Additionally, it is interesting to see what key insights into IR problems the new technologies are able to give us. The aim of this full-day tutorial is to give a clear overview of current tried-and-trusted neural methods in IR and how they bene t IR research. It covers key architectures, as well as the most promising future directions. MOTIVATIONPrompted by the advances of deep learning in computer vision research, neural networks have resurfaced as a popular machine learning paradigm in many other directions of research as well, including information retrieval. Recent years have seen neural networks being applied to all key parts of the typical modern IR pipeline, such core ranking algorithms , click models , knowledge graphs , text similarity , entity retrieval , language modeling [5], question answering , and dialogue systems .A key advantage that sets neural networks apart from many learning strategies employed earlier, is their ability to work from raw input data. E.g., when given enough training data, well-designed networks can become feature extractors themselves, e.g., incorporating basic input characteristics such as term frequency (tf) and term saliency (idf)-that used to be pre-calculated o ine-in their * Corresponding author.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7--11, 2017, Shinjuku, Tokyo, Japan initial layers. Where designing features used to be a crucial aspect and contribution of newly proposed IR approaches, the focus has shifted to designing network architectures instead. As a consequence, many di erent architectures and paradigms have been proposed, such as auto-encoders, recursive networks, recurrent networks, convolutional networks, various embedding methods, deep reinforcement and deep q-learning, and, more recently, generative adversarial networks, of which most have been applied in IR settings. The aim of the neural networks for IR (NN4IR) tutorial is to provide a clear overview of the main network architectures currently applied in IR and to show explicitly how they relate to previous work. The tutorial covers methods applied in industry and academia, with in-depth insights into the underlying theory, core IR tasks, applicability, key assets and handicaps, scalability concerns and practical tips and tricks.We expect the tutorial to be useful both for academic and industrial researchers and practitioners who either want to develop new neural models, use them in their own research in other areas or apply the models described here to improve actual IR systems. OBJECTIVESThe material in the tutorial covers a broad range of IR applications. It is structured as follows:Preliminaries (60 minutes). The recent surge of interest in deep learning has given rise to a myriad of architectures. Di erent though the inner structures of neural networks can be, there are many concepts common to all of them. This rst session covers the preliminaries; we brie y recapitulate the basic concepts involved in neural systems, such as back propagation Semantic matching I: supervised learning (60 minutes). The problem of matching items based on their textual descriptions arises in many IR systems. The traditional approach involves counting query term occurrences in the description text (e.g., BM25 ). However, to bridge the lexical gap caused by vocabulary-related and linguistic di erences many latent semantic models have been proposed , and more recently neural embedding methods . In this session we will focus on semantic matching settings where a supervised signal is available. The signal can be explicit,",
        "BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network. Friend recommendation is a critical task in social networks. In this paper, we propose a Bayesian Personalized Ranking Deep Neural Network (BayDNN) model for friend recommendation in social networks. BayDNN rst extracts latent structural pa erns from the input network data and then use the Bayesian ranking to make friend recommendations. With BayDNN we achieve signi cant performance improvement on two public datasets: Epinions and Slashdot. For example, on Epinions dataset, BayDNN signi cantly outperforms the state-of-the-art algorithms, with a 5% improvement on NDCG over the best baseline. e advantages of the proposed BayDNN mainly come from a novel Bayesian personalized ranking (BPR) idea, which precisely captures the users' personal bias based on the extracted deep features, and its underlying convolutional neural network (CNN), which o ers a mechanism to extract latent deep structural feature representations of the complicated network data. To get good parameter estimation for the neural network, we present a netuned pre-training strategy for the proposed BayDNN model based on Poisson and Bernoulli probabilistic models.",
        "PatternNet: Visual Pattern Mining with Deep Neural Network",
        "Visualizing and Understanding Deep Neural Networks in CTR Prediction Although deep learning techniques have been successfully applied to many tasks, interpreting deep neural network models is still a big challenge to us. Recently, many works have been done on visualizing and analyzing the mechanism of deep neural networks in the areas of image processing and natural language processing. In this paper, we present our approaches to visualize and understand deep neural networks for a very important commercial task-CTR (Click-through rate) prediction. We conduct experiments on the productive data from our online advertising system with daily varying distribution. To understand the mechanism and the performance of the model, we inspect the model's inner status at neuron level. Also, a probe approach is implemented to measure the layer-wise performance of the model. Moreover, to measure the influence from the input features, we calculate saliency scores based on the backpropagated gradients. Practical applications are also discussed, for example, in understanding, monitoring, diagnosing and refining models and algorithms.",
        "A Deep Neural Network for Modeling Music",
        "Deep Neural Architecture for News Recommendation Deep neural networks have yielded immense success in speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks for recommender systems has received a relatively little introspection. Also, different recommendation scenarios have their own issues which creates the need for different approaches for recommendation. Specifically in news recommendation a major problem is that of varying user interests. In this work, we use deep neural networks with attention to tackle the problem of news recommendation. The key factor in user-item based collaborative filtering is to identify the interaction between user and item features. Matrix factorization is one of the most common approaches for identifying this interaction. It maps both the users and the items into a joint latent factor space such that user-item interactions in that space can be modeled as inner products in that space. Some recent work has used deep neural networks with the motive to learn an arbitrary function instead of the inner product that is used for capturing the user-item interaction. However, directly adapting it for the news domain does not seem to be very suitable. This is because of the dynamic nature of news readership where the interests of the users keep changing with time. Hence, it becomes challenging for recommendation systems to model both user preferences as well as account for the interests which keep changing over time. We present a deep neural model, where a non-linear mapping of users and item features are learnt first. For learning a non-linear mapping for the users we use an attention-based recurrent layer in combination with fully connected layers. For learning the mappings for the items we use only fully connected layers. We then use a ranking based objective function to learn the parameters of the network. We also use the content of the news articles as features for our model. Extensive experiments on a real-world dataset show a significant improvement of our proposed model over the state-of-the-art by 4.7% (Hit Ratio@10). Along with this, we also show the effectiveness of our model to handle the user cold-start and item cold-start problems.",
        "Temporal Relational Ranking for Stock Prediction Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction.However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered.In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively. Authors' addresses: F. Feng, National University of Singapore, 13 Computing Drive, 117417, Singapore; email: fulifeng93@ gmail.com; X. He (corresponding author), University of Science and Technology of China, 443 Huangshan Road, Hefei, 230031, China; email: xiangnanhe@gmail.com; X. Wang, National University of Singapore, 13 Computing Drive, 117417, Singapore; email: xiangwang@u.nus.edu; C. Luo, Tsinghua University, 30 Shuangqing Rd, Haidian, Beijing, China; email: chengluo@tsinghua.edu.cn; Y. Liu, Tsinghua University, 30 Shuangqing Rd, Haidian, Beijing, China; email: yiqunliu@ tsinghua.edu.cn; T.-S. Chua, National University of Singapore, 13 Computing Drive, 117417, Singapore; email: dcscts@ nus.edu.sg. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. According to the statistics reported by the World Bank in 2017, the overall capitalization of stock markets worldwide has exceeded US$64 trillion. 1 With the continual increase in stock market capitalization, trading stocks has become an attractive investment instrument for many investors. However, whether an investor could earn or lose money depends heavily on whether he/she can make the right stock selection. Stock prediction, which aims to predict the future trend and price of stocks, is one of the most popular techniques to make a profitable stock investment , although there are still debates about whether the stock market is predictable (a.k.a. the Efficient Markets Hypothesis) among financial economists . Some recent evidences indicate the predictability of stock markets, which motivates further exploration of stock prediction techniques .Traditional solutions for stock prediction are based on time-series analysis models, such as Kalman Filters , Autoregressive Models and their extensions . Given an indicator of a stock (e.g., stock price), this kind of model represents it as a stochastic process and takes the historical data of the indicator to fit the process. We argue that such mainstream solutions for stock prediction have three main drawbacks: (1) The models heavily rely on the selection of indicators, which is usually done manually and is hard to optimize without special knowledge of finance. The hypothesized stochastic processes are not always compatible with the volatile stock in the real world. (3) These models can only consider a few indicators since their inference complexity typically increases exponentially with the number of indicators. As such, they lack the capability to comprehensively describe a stock that could be influenced by a plethora of factors. Toward these drawbacks, advanced techniques like deep neural networks, especially the recurrent neural networks (RNNs), have become a promising solution to substitute the traditional time-series models to predict the future trend or exact price of a stock .A state-of-the-art neural network-based solution is the State Frequency Memory (SFM) network , which models the historical data in a recurrent fashion and captures temporal patterns in different frequencies. This method achieves promising performance of predicting the daily opening price of 50 U.S. stocks one day ahead with a mean square error (MSE) of less than US$6. However, we argue that such prediction methods are suboptimal to guide stock selection, since their optimization target is not at selecting the top stocks with the highest expected revenue. To be specific, they typically address stock prediction as either a classification (on price movement direction) or a regression (on price value) task, which would cause a large discrepancy on the investment revenue. gives an intuitive example, where a method with better prediction performance (measured by regression MSE) suggests a less profitable stock. This implies the possible discrepancy between the actual target of stock selection and the optimized target of regression (classification), such that an optimal method of regression (classification) does not necessarily select the optimal stock to trade.Another limitation of existing neural network-based solutions is that they typically treat stocks as independent of each other and ignore the relations between stocks. However, the rich relations between stocks and the corresponding companies may contain valuable clues for stock prediction."
    ],
    "Information Retrieval": [
        "On the Allocation of Documents in Multiprocessor Information Retrieval Systems. Information retrieval is the selection of documents that are potentially relevant to a user's information need. Given the vast volume of data stored in modern information retrieval systems, searching the document database requires vast computational resources. To meet these computational demands, various researchers have developed parallel information retrieval systems. As",
        "Information representation and retrieval in the digital age",
        "Information Retrieval as Statistical Translation",
        "Relating retrievability, performance and length. Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the document collection leads to good performance (though not the best performance possible). We also show that past a certain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune systems.",
        "Information retrieval in context: IRiX",
        "Information retrieval and its sister disciplines",
        "Information retrieval on the semantic web",
        "An Information Retrieval Ontology for Information Retrieval Nanopublications",
        "Comment on Some Recent Comments on Information Retrieval",
        "Information Retrieval: Searching in the 21st Century; Human Information Retrieval",
        "Intelligent Information Retrieval: An Introduction",
        "A risk minimization framework for information retrieval. This paper presents a probabilistic information retrieval framework in which the retrieval problem is formally treated as a statistical decision problem. In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem. We discuss how this framework can unify existing retrieval models and accommodate systematic development of new retrieval models. As an example of using the framework to model non-traditional retrieval problems, we derive retrieval models for subtopic retrieval, which is concerned with retrieving documents to cover many different subtopics of a general query topic. These new models differ from traditional retrieval models in that they relax the traditional assumption of independent relevance of documents.",
        "Linguistic and semantic passage retrieval strategies for question answering. Question Answering (QA) is the task of searching a large text collection for specific answers to questions posed in natural language. Though they often have access to rich linguistic and semantic analyses of their input questions, QA systems often rely on off-theshelf bag-of-words Information Retrieval (IR) solutions to retrieve passages matching a set of terms extracted from the question.There is a fundamental disconnect between the capabilities of the bag-of-words retrieval model and the retrieval needs of the QA system. Bag-of-words IR retrieves documents matching a query, but the QA system really needs documents that contain answers. Through question analysis, the QA system has compiled a sophisticated information need representation for what constitutes an answer to the question. This representation is composed of a set of linguistic and semantic constraints satisfied by answer-bearing passages. Unfortunately, off-the-shelf IR libraries commonly used in QA systems can not, in general, check these types of constraints at query-time. Poor quality retrieval can cause a QA system to fail if no answer-bearing text is retrieved, if it is not ranked highly enough, or if it is outranked or overwhelmed by false positives, text that matches the query well, yet supports a wrong answer.This thesis proposes two linguistic and semantic passage retrieval methods for QA, one based on structured retrieval and the other on rank-learning techniques. In addition, a methodology is proposed for mapping annotated text consisting of labeled spans and typed relations between them into an annotation graph representation. The annotation graph supports query-time linguistic and semantic constraint-checking, and serves as a unifying formalism for the QA system's information need and for retrieved passages. The proposed methods rely only on the relatively weak assumption that the QA system's information need can be represented as an annotation graph. The two approaches are shown to retrieve more answer-bearing text, more highly ranked, compared to a bag-of-words baseline for two different QA tasks. Linguistic and semantic passage retrieval methods are also shown to improve end-to-end QA system accuracy and answer MRR.. Question Answering (QA) is the task of searching a large text collection for specific answers to questions posed in natural language. Though they often have access to rich linguistic and semantic analyses of their input questions, QA systems often rely on off-theshelf bag-of-words Information Retrieval (IR) solutions to retrieve passages matching a set of terms extracted from the question.There is a fundamental disconnect between the capabilities of the bag-of-words retrieval model and the retrieval needs of the QA system. Bag-of-words IR retrieves documents matching a query, but the QA system really needs documents that contain answers. Through question analysis, the QA system has compiled a sophisticated information need representation for what constitutes an answer to the question. This representation is composed of a set of linguistic and semantic constraints satisfied by answer-bearing passages. Unfortunately, off-the-shelf IR libraries commonly used in QA systems can not, in general, check these types of constraints at query-time. Poor quality retrieval can cause a QA system to fail if no answer-bearing text is retrieved, if it is not ranked highly enough, or if it is outranked or overwhelmed by false positives, text that matches the query well, yet supports a wrong answer.This thesis proposes two linguistic and semantic passage retrieval methods for QA, one based on structured retrieval and the other on rank-learning techniques. In addition, a methodology is proposed for mapping annotated text consisting of labeled spans and typed relations between them into an annotation graph representation. The annotation graph supports query-time linguistic and semantic constraint-checking, and serves as a unifying formalism for the QA system's information need and for retrieved passages. The proposed methods rely only on the relatively weak assumption that the QA system's information need can be represented as an annotation graph. The two approaches are shown to retrieve more answer-bearing text, more highly ranked, compared to a bag-of-words baseline for two different QA tasks. Linguistic and semantic passage retrieval methods are also shown to improve end-to-end QA system accuracy and answer MRR.",
        "Information Retrieval and the Virtual Document",
        "Clustering and OCCC Approaches in Document Re-ranking In this paper, we describe our approach for information retrieval for question answering (IR4QA) of NTCIR-8 tasks. For improving information retrieval performance, we focus mostly on the document re-ranking technique, which locates between the first retrieval documents and query expansion. In this paper, we employ two approaches in document re-ranking. One is based on entropy clustering, a kind of unsupervised learning technology. Relevant documents from top initial retrieval result can be automatically clustered same class according to information entropy values. That is a continuation of our previous work. The other is One Class Co-Clustering (OCCC) approach. it aims to detect topical terms, and compute document's topicality score. The method is simple and performs well. The experiment result shows using the two approaches in Document Reranking, Clustering and OCCC, can improve information retrieval performance.",
        "Information Retrieval. s",
        "What We Talk About When We Talk About Information Retrieval",
        "Knowledge-based information retrieval",
        "Information Retrieval Using PU Learning Based Re-ranking In this paper, we describe our approach for information retrieval for question answering (IR4QA) on simple Chinese language of NTCIR-7 tasks. Firstly, we use both bi-grams and single Chinese characters as index units and use OKAPI BM25 as retrieval model. Secondly, we re-rank all documents' orders for the first retrieval documents. We focus mostly on the document re-ranking technique. We address probabilistically labeling relevant degree between the first retrieval documents and query topics. In other words, we want to know the probability of a document belongs to relevance/irrelevance class. We employ PU positive and unlabeled learning to solve this problem, and use Bayesian classifier and EM algorithm in process of computing the probability. Consequently, those relevant documents with high probability are updated rank. Lastly, we use re-ranked retrieved documents to do query expansion. Evaluation at NTCIR-7 shows that our group achieves 0.3862 and 0.3806 MAP based on pseudo-qrels and real qrels respectively.",
        "Diagnostic Evaluation of Information Retrieval Models Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.",
        "Proof of concept: concept-based biomedical information retrieval. In this thesis we investigate the possibility to integrate domain-specific knowledge into biomedical information retrieval (IR). Recent decades have shown a fast growing interest in biomedical research, reflected by an exponential growth in scientific literature. An important problem for biomedical IR is dealing with the complex and inconsistent terminology encountered in biomedical publications. Dealing with the terminology problem requires domain knowledge stored in terminological resources: controlled indexing vocabularies and thesauri. The integration of this knowledge is, however, far from trivial.The first research theme investigates heuristics for obtaining word-based representations from biomedical text for robust retrieval. We investigated the effect of choices in document preprocessing heuristics on retrieval effectiveness. Document preprocessing heuristics such as stop word removal, stemming, and breakpoint identification and normalization were shown to strongly affect retrieval performance. An effective combination of heuristics was identified to obtain a word-based representation from text for the remainder of this thesis.The second research theme deals with concept-based retrieval. We compared a word-based to a concept-based representation and determined to what extent a manual concept-based representation can be automatically obtained from text. Retrieval based on only concepts was demonstrated to be significantly less effective than word-based retrieval. This deteriorated performance could be explained by errors in the classification process, limitations of the concept vocabularies and limited exhaustiveness of the concept-based document representations. Retrieval based on a combination of word-based and automatically obtained concept-based query representations did significantly improve word-only retrieval.In the third and last research theme we propose a cross-lingual framework for monolingual biomedical IR. In this framework, the integration of a concept-based representation is viewed as a cross-lingual matching problem involving a word-based and concept-based representation language. This framework gives us the opportunity to adopt a large set of established crosslingual information retrieval methods and techniques for this domain. Experiments with basic term-to-term translation models demonstrate that this approach can significantly improve word-based retrieval.Directions for future work are using these concepts for communication between user and retrieval system, extending upon the translation models and extending CLIR-enhanced concept-based retrieval outside the biomedical domain.. In this thesis we investigate the possibility to integrate domain-specific knowledge into biomedical information retrieval (IR). Recent decades have shown a fast growing interest in biomedical research, reflected by an exponential growth in scientific literature. An important problem for biomedical IR is dealing with the complex and inconsistent terminology encountered in biomedical publications. Dealing with the terminology problem requires domain knowledge stored in terminological resources: controlled indexing vocabularies and thesauri. The integration of this knowledge is, however, far from trivial.The first research theme investigates heuristics for obtaining word-based representations from biomedical text for robust retrieval. We investigated the effect of choices in document preprocessing heuristics on retrieval effectiveness. Document preprocessing heuristics such as stop word removal, stemming, and breakpoint identification and normalization were shown to strongly affect retrieval performance. An effective combination of heuristics was identified to obtain a word-based representation from text for the remainder of this thesis.The second research theme deals with concept-based retrieval. We compared a word-based to a concept-based representation and determined to what extent a manual concept-based representation can be automatically obtained from text. Retrieval based on only concepts was demonstrated to be significantly less effective than word-based retrieval. This deteriorated performance could be explained by errors in the classification process, limitations of the concept vocabularies and limited exhaustiveness of the concept-based document representations. Retrieval based on a combination of word-based and automatically obtained concept-based query representations did significantly improve word-only retrieval.In the third and last research theme we propose a cross-lingual framework for monolingual biomedical IR. In this framework, the integration of a concept-based representation is viewed as a cross-lingual matching problem involving a word-based and concept-based representation language. This framework gives us the opportunity to adopt a large set of established crosslingual information retrieval methods and techniques for this domain. Experiments with basic term-to-term translation models demonstrate that this approach can significantly improve word-based retrieval.Directions for future work are using these concepts for communication between user and retrieval system, extending upon the translation models and extending CLIR-enhanced concept-based retrieval outside the biomedical domain.Available online from",
        "Information Retrieval: Algorithms and Heuristics",
        "Online reference and information retrieval",
        "Information Retrieval: New Systems and Current Research. Proceedings of the 15th Research Colloquium of the British Computer Society Information Retrieval Specialist Group, edited by Ruben Leon",
        "Information Retrieval at Cornell",
        "Information Retrieval in the Commentsphere This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as commenttargeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with commentexploiting retrieval. Throughout the article, connections to information retrieval research are pointed out.",
        "XML retrieval: what to retrieve?. The fundamental difference between standard information retrieval and XML retrieval is the unit of retrieval. In traditional IR, the unit of retrieval is fixed: it is the complete document. In XML retrieval, every XML element in a document is a retrievable unit. This makes XML retrieval more difficult: besides being relevant, a retrieved unit should be neither too large nor too small. The research presented here, a comparative analysis of two approaches to XML retrieval, aims to shed light on which XML elements should be retrieved. The experimental evaluation uses data from the Initiative for the Evaluation of XML retrieval (INEX 2002).",
        "The retrievability of documents. Retrievability is an important and interesting indicator that can be used in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, retrievability examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve a document or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of retrievability along with a number of retrievability measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how retrievability relates to effectiveness -describing some of the insights that researchers have discovered so far. We shall also show how retrievability relates to efficiency, and how the theory of retrievability can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of retrievability such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session of the day will look at example problems and ways to analyse and apply retrievability to other problems and domains. written numerous papers on the topic and given dozens of talks on the subject throughout the world. His research focuses on building formal models for Information Retrieval -usually drawing upon different disciplines for inspiration, such as Quantum Mechanics, Operations Research, Microeconomics, Transportation Planning and Gamification. Central to his research is the theoretical development of statistical and formal models for Information Retrieval, where his research interests include: (i) Models for the retrieval of documents, sentences, experts and other information objects ; (ii) Probabilistic models of user interaction and the simulation of users for evaluation ; (iii) Microeconomic models of information interaction, specifically how cost and effort affect interaction and performance with search systems [2]; (iv) Methods which assess the impact of search technology on society in application areas such as, search engine bias and the accessibility of e-Government information [6], and; (v) Searching for fun (i.e. the seven SINS of search) .",
        "On the Equilibrium of Query Reformulation and Document Retrieval. In this paper, we study jointly query reformulation and document relevance estimation, the two essential aspects of information retrieval (IR). Their interactions are modelled as a two-player strategic game: one player, a query formulator, taking actions to produce the optimal query, is expected to maximize its own utility with respect to the relevance estimation of documents produced by the other player, a retrieval modeler; simultaneously, the retrieval modeler, taking actions to produce the document relevance scores, needs to optimize its likelihood from the training data with respect to the refined query produced by the query formulator. Their equilibrium or equilibria will be reached when both are the best responses to each other. We derive our equilibrium theory of IR using normal-form representations: when a standard relevance feedback algorithm is coupled with a retrieval model, they would share the same objective function and thus form a partnership game; by contrast, pseudo relevance feedback pursues a rather different objective than that of retrieval models, therefore the interaction between them would lead to a general-sum game (though implicitly collaborative). Our game-theoretical analyses not only yield useful insights into the two major aspects of IR, but also offer new practical algorithms for achieving the equilibrium state of retrieval which have been shown to bring consistent performance improvements in both text retrieval and item recommendation.",
        "A Retrievability Analysis: Exploring the Relationship Between Retrieval Bias and Retrieval Performance. Retrievability provides an alternative way to assess an Information Retrieval (IR) system by measuring how easily documents can be retrieved. Retrievability can also be used to determine the level of retrieval bias a system exerts upon a collection of documents. It has been hypothesised that reducing the retrieval bias will lead to improved performance. To date, it has been shown that this hypothesis does not appear to hold on standard retrieval performance measures (MAP and P@10) when exploring the parameter space of a given retrieval model. However, the evidence is limited and confined to only a few models, collections and measures. In this paper, we perform a comprehensive empirical evaluation analysing the relationship between retrieval bias and retrieval performance using several well known retrieval models, five large TREC test collections and ten performance measures (including the recently proposed PRES, Time Biased Gain (TBG) and U-Measure). For traditional relevance based measures (MAP, P@10, MRR, Recall, etc) the correlation between retrieval bias and performance is moderate. However, for TBG and U-Measure, we find that there is strong and significant negative correlations between retrieval bias and performance (i.e as bias drops, performance increases). These findings suggest that for these more sophisticated, user oriented measures the retrievability bias hypothesis tends to hold. The implication is that for these measures, systems can then be tuned using retrieval bias, without recourse to relevance judgements.",
        "Information Retrieval with Language Knowledge",
        "Implicit information retrieval",
        "Evaluating retrieval models using retrievability measurement. Evaluation is the main driving force in research, development and applications related to information retrieval (IR). In the traditional IR evaluation paradigm a list of query topics along with their relevance judgments are given. The main limitation of this kind of evaluation paradigm is that it focuses almost exclusively on a small set of judged documents and does not consider what influence the given retrieval models have on accessing all the relevant information in the collection. This is particularly important for recall oriented retrieval applications where we want to ensure that that everything relevant has been found.In this thesis we analyze the effectiveness of retrieval models from the documents retrievability point of view. We focus particularly on the retrieval bias of different retrieval models, and try to examine to what extent this bias restricts the users in retrieving relevant information. We explore this research with the help of three factors. First, we analyze the relationship between different characteristics of queries and retrievability. This is important from the query generation point of view, since in case of exhaustive queries, it is practically infeasible to complete retrievability approximation in reasonable time. The strong correlation between retrievability and query characteristics allows us to approximate the retrievability score accurately with the help of a query subset without processing an exhaustive number of queries. After this, we examine to what extent the retrievability and other IR effectiveness measures are related to each other. This specifically helps us to understand to what extent it is possible to automatically rank the effectiveness of retrieval models on the basis of their retrieval bias. This also offers a basis for optimizing retrieval systems for specific collections without the need to provide manually annotated ground truth. This is particularly useful for those retrieval domains where it is difficult to obtain a sufficient amount of relevance judgments. At the end we investigate and devise different retrieval strategies for mitigating the effect of low retrievability of documents. These include collection partitioning and query expansion on the basis of improved pseudo relevance feedback selection.The work present in this thesis provides an a novel approach for the evaluation and optimization of retrieval models particularly for recall oriented retrieval domains, where the focus is on retrieving all relevant information but not just retrieving a subset of relevant information. Available online at: http://www.ifs.tuwien.ac.at/~bashir/shariqbashir_ phd_thesis.pdf",
        "Introduction to Information Retrieval",
        "Theory of Retrieval: The Retrievability of Information. Retrievability is an important and interesting indicator that can be used in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, retrievability examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve it or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of retrievability along with a number of retrievability measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how retrievability relates to effectivenessdescribing some of the insights that researchers have discovered thus far. We shall also show how retrievability relates to efficiency, and how the theory of retrievability can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of retrievability such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session will look at example problems and ways to analyse and apply retrievability to other problems and domains. Participants are invited to bring their own problems to be discussed after the tutorial. This half-day tutorial is ideal for: (i) researchers curious about retrievability and wanting to see how it can impact their research, (ii) researchers who would like to expand their set of analysis techniques, and/or (iii) researchers who would like to use retrievability to perform their own analysis.",
        "Retrievability: An Independent Evaluation Measure. Information Retrieval systems have traditionally been evaluated in terms of efficiency and performance. These aspects of retrieval systems, whilst very important, do not cover a crucial aspect of the system, the access it provides to the documents of the collection. Retrievability, a document centric evaluation measure, introduced by Azzopardi and Vinay, provides an alternative approach to evaluation . Retrievability is the ease with which a document can be retrieved using a retrieval system. The more queries which retrieve the document, and the higher up the document is returned, the more retrievable it is. It can thus be used to describe how difficult it is to find documents in the collection given a particular configuration of a retrieval system. Unlike typical performance evaluations, performing a retrievability analysis can be done without recourse to relevancy judgements meaning there is no reliance on a test collection. This has major advantages when tuning a retrieval systems parameters as the tuning can be performed on the live collection.Several applications of retrievability have been explored, however, many areas remain untouched or partially studied. Thus far, work in this PhD has explored the relationship between retrievability bias and performance in the context of ad hoc retrieval . A benefit that would allow researchers in academia and industry to tune their system on their own collection and would save the resources of creating a test collection. Other aspects of retrievability also covered in this PhD have investigated the impact of query length on the estimation of Gini , investigating methods of computing retrievability more efficiently [4] and the application of various inequality metrics instead of the Gini Coefficient when estimating the overall inequality of the system. The remainder of this PhD aims to continue investigating the relationship between retrievability bias and performance, specifically to examine whether or not the relationship found thus far is generalisable to other domains and performance metrics. Applications of the theory of retrievability are also to be examined to see if there are alternative uses of rePermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). trievability to aid information retrieval outside of evaluation. Finally, due to the high cost of computing retrievability, another vein of research which is vital to moving retrievability more towards mainstream evaluation is to develop more efficient methods of estimating retrievability. Work in each of these areas will increase both the usefulness of retrievability by allowing demonstrating it can be applied to various domains and its attractiveness in both evaluation and general use for information retrieval.",
        "Information retrieval by metabrowsing",
        "Information Storage and Retrieval, A Survey and Functional Description",
        "Term Distillation for Cross-DB Retrieval In cross-DB retrieval, the domain of queries differs from the retrieval target in the distribution of that of term occurrences. This causes incorrect term weighting in the retrieval system which assigns to each term a retrieval weight based on the distribution of term occurrences. To resolve the problem, we propose \\term distillation\" which is a framework for query term selection in cross-DB retrieval. The experiments using the NTCIR patent retrieval test collection demonstrate that term distillation is e ective for cross-DB retrieval.",
        "ICTNET at TREC2017 Complex Answer Retrieval Track Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers) [1]. The most common and popular information retrieval application is web search engine such as Google [2] , Baidu [3] , Bing [4] and Sogou [5]. These application will return top-N best retrieval result to users. Information Retrieval systems for the Web, i.e., web search engines, are mainly devoted to finding relevant web documents in response to a user's query [6]. Current retrieval systems performance well in phrase-level retrieval tasks which provide simple fact and entitycentric needs. Complex Answer Retrieval Track is a new track in 2017, which requests a more complex and longer retrieval result to answer a query. It focuses on developing systems that are capable of answering complex information needs by collating relevant information from an entire corpus. Given an article stub Q, retrieval for each of its sections Hi, a ranking of relevant entity-passage tuples (E, P). Tow tasks are offered: passage ranking and entity ranking. This paper introduces an algorithm and a system for passage ranking. The retrieval queries are outlines which consist of titles and section titles of articles. The retrieval collection consists of paragraphs which are come from Wikipedia articles. We use the BM25 algorithm and develop a system to retrieval the top-100 most relevant paragraphs."
    ],
    "Analysis for Android apps": [
        "An automatically vetting mechanism for SSL error-handling vulnerability in android hybrid Web apps. A large set of diverse hybrid mobile apps, which use both native Android app UIs and Web UIs, are widely available in today's smartphones. These hybrid apps usually use SSL or TLS to secure HTTP based communication. However, researchers show that incorrect implementation of SSL or TLS may lead to serious security problems, such as Man-In-The-Middle (MITM) attacks and phishing attacks. This paper investigates a particular SSL vulnerability that results from error-handling code in the hybrid mobile Web apps. Usually such error-handling code is used to terminate an ongoing communication, but the vulnerability of interest is able to make the communication proceed regardless of SSL certificate verification failures, eventually lead to MITM attacks. To identify those vulnerable apps, we develop a hybrid approach, which combines both static analysis and dynamic analysis to Web UIs to trigger the error-handling code; (2) accurately select the correct paths from the app entry-point to the targeted code, meanwhile avoiding the crash of apps, and populate messaging objects for the communication between components. Specifically, we construct inter-component call graphs to model the connections, and design algorithms to select the paths from the established graph and determine the parameters by backtracing. To evaluate our approach, we have implemented and tested it with 13,820 real world mobile Web apps from Google Play. The experimental results demonstrate that 1,360 apps are detected as potentially vulnerable ones solely using the static analysis. The dynamic analysis process further confirms that 711 apps are truly vulnerable among the potentially vulnerable set.",
        "User Comment Analysis for Android apps and CSPI Detection with Comment Expansion Along with the exponential growth on markets of mobile apps, comes the serious public concern about the security and privacy issues. User comments serves as a valuable source of information for evaluating a mobile app, for both new users and developers. However, for the purpose of evaluation on the security/privacy aspects of an app, user comments are not always directly useful. Most of the comments are about issues like functionality, missing feature or just pure emotional expression. Therefore, further efforts are required in order to identify those Comments with Security/Privacy Issues (CSPI) for future evaluation. In this paper, a dataset of comments is collected from Google Play, and a two dimensional label system is proposed to describe those CSPI within it. A supervised multi-label learning method utilizing comment expansion is adopted to detect different types of CSPI described by this label system. Experiments on the collected dataset shows that the proposed method outperforms the method without the comment expansion.",
        "Reconciling mobile app privacy and usability on smartphones: could user privacy profiles help?. As they compete for developers, mobile app ecosystems have been exposing a growing number of APIs through their software development kits. Many of these APIs involve accessing sensitive functionality and/or user data and require approval by users. Android for instance allows developers to select from over 130 possible permissions. Expecting users to review and possibly adjust settings related to these permissions has proven unrealistic.In this paper, we report on the results of a study analyzing people's privacy preferences when it comes to granting permissions to different mobile apps. Our results suggest that, while people's mobile app privacy preferences are diverse, a relatively small number of profiles can be identified that offer the promise of significantly simplifying the decisions mobile users have to make. Specifically, our results are based on the analysis of settings of 4.8 million smartphone users of a mobile security and privacy platform. The platform relies on a rooted version of Android where users are allowed to choose between \"granting\", \"denying\" or \"requesting to be dynamically prompted\" when it comes to granting 12 different Android permissions to mobile apps they have downloaded.",
        "Understanding the Purpose of Permission Use in Mobile Apps Mobile apps frequently request access to sensitive data, such as location and contacts. Understanding the purpose of why sensitive data is accessed could help improve privacy as well as enable new kinds of access control. In this article, we propose a text mining based method to infer the purpose of sensitive data access by Android apps. The key idea we propose is to extract multiple features from app code and then use those features to train a machine learning classifier for purpose inference. We present the design, implementation, and evaluation of two complementary approaches to infer the purpose of permission use, first using purely static analysis, and then using primarily dynamic analysis. We also discuss the pros and cons of both approaches and the trade-offs involved.",
        "Is this app safe for children?: a comparison study of maturity ratings on Android and iOS applications. There is a rising concern among parents who have experienced unreliable content maturity ratings for mobile applications (apps) that result in inappropriate risk exposure for their children and adolescents. In reality, there is no consistent maturity rating policy for mobile applications. The maturity ratings of Android apps are provided purely by developers' self-disclosure and are rarely verified. While Apple's iOS app ratings are considered to be more accurate, they can also be inconsistent with Apple's published policies. To address these issues, this research aims to systematically uncover the extent and severity of unreliable maturity ratings for mobile apps. Specifically, we develop mechanisms to verify the maturity ratings of mobile apps and investigate possible reasons behind the incorrect ratings. We believe that our findings have important implications for platform providers (e.g., Google or Apple) as well as for regulatory bodies and application developers.",
        "AppHolmes: Detecting and Characterizing App Collusion among Third-Party Android Markets. Background activities on smartphones are essential to today's \"always-on\" mobile device experience. Yet, there lacks a clear understanding of the cooperative behaviors among background activities as well as a quantification of the consequences. In this paper, we present the first in-depth study of app collusion, in which one app surreptitiously launches others in the background without user's awareness. To enable the study, we develop AppHolmes, a static analysis tool for detecting app collusion by examining the app binaries. By analyzing 10,000 apps from top third-party app markets, we found that i) covert, cooperative behaviors in background app launch are surprisingly pervasive, ii) most collusion is caused by shared services, libraries, or common interest among apps, and iii) collusion has serious impact on performance, efficiency, and security. Overall, our work presents a strong implication on future mobile system design.",
        "MadDroid: Characterizing and Detecting Devious Ad Contents for Android Apps",
        "A Descriptive Analysis of a Large-Scale Collection of App Management Activities. Smartphone users adopt an increasing number of mobile applications (a.k.a., apps) in the recent years. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behaviors and preferences of mobile users. Existing literature provides very limited understanding about app management activities, due to the lack of user behavioral data at scale. This paper analyzes a very large collection of app management log of the users of a leading Android app marketplace in China. The data set covers one month of detailed activities of how users download, update, and uninstall the apps on their smart devices, involving 8,306,181 anonymized users and 394,661 apps. We characterize how these users manage the apps on their devices and identify behavioral patterns that correlate with users' online ratings of the apps.",
        "Deriving User Preferences of Mobile Apps from Their Management Activities App marketplaces host millions of mobile apps that are downloaded billions of times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile device users, infer the quality of apps, and improve user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of app usage data at scale. This article takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The dataset covers 5 months of detailed downloading, updating, and uninstallation activities, which involve 17 million anonymized users and 1 million apps. We present a surprising finding that the metrics commonly used to rank apps in app stores do not truly reflect the users' real attitudes. We then identify behavioral patterns from the app management activities that more accurately indicate user preferences of an app even when no explicit rating is available. A systematic statistical analysis is designed to evaluate machine learning models that are trained to predict user preferences using these behavioral patterns, which features an inverse probability weighting method to correct the selection biases in the training process.",
        "Roaming Through the Castle Tunnels: An Empirical Analysis of Inter-app Navigation of Android Apps",
        "Voting with Their Feet: Inferring User Preferences from App Management Activities. Smartphone users have adopted an explosive number of mobile applications (a.k.a., apps) in the recent years. App marketplaces for iOS, Android and Windows Phone platforms host millions of apps which have been downloaded for more than 100 billion times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile users, to infer the quality of apps, and to improve the user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of user behavioral data at scale. This paper takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The data set covers five months of detailed downloading, updating, and uninstallation activities, involving 17 million anonymized users and one million apps. We present a surprising finding that the metrics commonly used by app stores to rank apps do not truly reflect the users' real attitudes towards the apps. We then identify useful patterns from the app management activities that much more accurately predict the user preferences of an app even when no user rating is available.",
        "An Explorative Study of the Mobile App Ecosystem from App Developers' Perspective. With the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users."
    ],
    "The University of Amsterdam": [
        "The University of Amsterdam's Concept Detection System at ImageCLEF 2010 Our group within the University of Amsterdam participated in the large-scale visual concept detection task of ImageCLEF 2010. The submissions from our visual concept detection system have resulted in the best visual-only run in the per-concept evaluation. In the per-image evaluation, it achieves the highest score in terms of example-based F-measure across all types of runs.",
        "The University of Amsterdam at the TREC 2003 Question Answering Track We describe our participation in the TREC 2003 Question Answering track. We explain the ideas underlying our approaches to the task, report on our results, provide an error analysis, and give a summary of our findings so far.",
        "Filtering Documents over Time on Evolving Topics - The University of Amsterdam at TREC 2013 KBA CCR",
        "The University of Amsterdam at QA@CLEF 2005 We describe the official runs of our team for the CLEF 2005 question answering track. We took part in the monolingual Dutch task, and focused most of our efforts on refining our existing multi-stream architecture, and porting parts of it to an XMLbased platform.",
        "The University of Amsterdam at TREC 2010: Session, Entity and Relevance Feedback",
        "Semanticizing search engine queries: the University of Amsterdam at the ERD 2014 challenge. This paper describes the University of Amsterdam's participation in the short track of the Entity Recognition & Disambiguation Challenge 2014 (ERD 2014). We describe how we adapt the Semanticizer-an open-source entity linking framework developed primarily at the University of Amsterdam-to the task of the ERD challenge: linking named entities in search engine queries. We steer the Semanticizer's linking towards named entities by adapting an existing training corpus, and extend the Semanticizer's set of features with contextual features that aim to leverage the limited context provided by search queries. With an F1 score of 0.6062 our final system run achieves median performance, and better than mean performance (0.5329).",
        "The University of Amsterdam's Concept Detection System at ImageCLEF 2009",
        "The University of Amsterdam at TREC 2002 We describe our participation in the TREC 2002 Novelty, Question answering, and Web tracks. We provide a detailed account of the ideas underlying our approaches to these tasks. All our runs used the FlexIR information retrieval system.",
        "The University of Amsterdam at the CLEF Cross Language Speech Retrieval Track 2007 In this paper we present the contents of the University of Amsterdam submission in the CLEF Cross Language Speech Retrieval 2007 English task. We describe the effects of using character n-grams and field combinations on both monolingual English retrieval, and crosslingual Dutch to English retrieval.",
        "The University of Amsterdam at QA@CLEF 2004 This paper describes the official runs of our team for the CLEF 2004 question answering tasks. We took part in the monolingual Dutch task and in the bilingual English to Dutch task.",
        "The University of Amsterdam at WiQA 2006 This paper describes our participation in the WiQA 2006 pilot on question answering using Wikipedia. We present an analysis of the results of our system for both monolingual and bilingual runs. The system currently works for Dutch and English.",
        "The University of Amsterdam at QA@CLEF2003 This paper describes the official runs of our team for QA@CLEF 2003. We took part in the monolingual Dutch Question Answering task.",
        "The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track We describe the participation of the University of Amsterdams ILPS group in the Total Recall track at TREC 2015. Based on the provided Baseline Model Implemention (\"BMI\") we set out to provide two more baselines we can compare to in future work. The two methods are bootstrapped by a synthetic document based on the query, use TF/IDF features, and sample with dynamic batch sizes which depend on the percentage of predicted relevant documents. We sample at least 1 percent of the corpus and stop sampling if a batch contains no relevant documents. The methods differ in the classifier used, i.e. Logistic Regression and Random Forest."
    ],
    "Neural Ranking for eCommerce Product Search": [
        "Interest Diffusion in Heterogeneous Information Network for Personalized Item Ranking. Personalized item ranking for recommending top-N items of interest to a user is an interesting and challenging problem in ecommerce. Researchers and practitioner are continuously trying to devise new methodologies to improve the accuracy of recommendations. Recommendation problem becomes more challenging for sparse binary implicit feedback, due to the absence of explicit signals of interest and sparseness of data. In this paper, we deal with the problem of the sparseness of data and accuracy of recommendations. To address the issue, we propose an interest diffusion methodology in heterogeneous information network for items to be recommended using the meta-information related to items. In this heterogeneous information network, graph regularized interest diffusion is performed to generate personalized recommendations of top-N items. For interest diffusion, personalized weight learning is performed for different meta-information object types in the network. The experimental evaluation and comparison of the proposed methodology with the state-of-the-art techniques using the real-world datasets show the effectiveness of the proposed approach1 KEYWORDSInterest diffusion; heterogeneous information network; metainformation; implicit feedback",
        "Amazon Search: The Joy of Ranking Products. Amazon is one of the world's largest e-commerce sites and Amazon Search powers the majority of Amazon's sales. As a consequence, even small improvements in relevance ranking both positively influence the shopping experience of millions of customers and significantly impact revenue. In the past, Amazon's product search engine consisted of several handtuned ranking functions using a handful of input features. A lot has changed since then. In this talk we are going to cover a number of relevance algorithms used in Amazon Search today. We will describe a general machine learning framework used for ranking within categories, blending separate rankings in All Product Search, NLP techniques used for matching queries and products, and algorithms targeted at unique tasks of specific categories -books and fashion. RANKING MODELSRanking models are responsible for a function that, given a customer's query, returns a sorted list of products in a match set. A single ranking model usually covers a combination of a category and a marketplace, e.g., Books in Japan.For training the ranking models we use labels based on customers actions, such as purchases, add-to-basket, or clicks.We use the search engine to collect our training sets. Several times per day we compute the unique set of keywords issued for each context of interest. The context can be a combination of marketplace, category, and some user features. Then we re-issue these queries in their context requesting feature values for all items in the match set. This feature collection runs regularly, so that the feature vector collected is as close as possible to the one observed when the query was originally issued by customers.To train ranking models, we construct training, validation, and test sets by collecting data from several days of customer traffic. Test sets are constructed from dates after the training set dates. We choose impressions that resulted in either click or purchase as positive examples. There are two Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).SIGIR '16 July 17-21, 2016, Pisa, Italy types of negative example impressions: seen, corresponding to items which were displayed to a customer, and unseen, corresponding to items which matched the query terms but were never shown due to pagination. To manage the size of the training set, we sample unseen examples.Gradient boosted trees[1] are our method of choice in ranking because they can discover complex feature interactions, can handle categorical and real-valued features, are robust in the presence of missing values, and work well even without significant tuning. For ranking problems, we use models trained with pairwise objectives and nDCG as the default objective function.We do feature selection in two stages. First, with fixed values of tree depth and learning rate, we train a \"kitchen sink\" model allowing all features available. We then discard the features which end up being ranked lower than random features and use the remaining features to grow the final feature set in a forward selection process. Once the feature set is chosen, we perform a grid search over the pairs of values for tree depth and learning rate, choosing the model which has the highest offline score on the validation set.Finally, the model is evaluated through an A/B test. We look at a large variety of metrics, including the number of converting customers, the number of products they purchase and the overall revenue. BEHAVIORAL FEATURESWhen training ranking models we use many features. Some of them measure intrinsic properties of products (e.g., sales, reviews). Others reflect properties of the queries or the context in which the query is issued (e.g., query specificity, customer status). Other features provide different measures of textual similarity. However in product search, hundreds of products might share very similar descriptions and seem to be equally relevant to a particular query. But some of those products are more popular than others and should be ranked higher. That's why behavioral features drive the rankings in Amazon Search to a much larger extent than they do in Web Search. Typically, they account for most of the variance reduction in gradient-boosted trees.It is well-known that users tend to click more often on results on the top of a search result page. To correct for that, we have tried to use classical versions of click-overexpected-clicks features . However, since more relevant documents tend to appear higher in the ranking, the observed click-through rate at a given position captures not only the position bias, but also the typical relevance at this position. This problem is more pronounced for a product 459",
        "Position Bias Estimation for Unbiased Learning-to-Rank in eCommerce Search",
        "Neural IR Meets Graph Embedding: A Ranking Model for Product Search",
        "End-to-End Neural Ranking for eCommerce Product Search: an Application of Task Models and Textual Embeddings Yan. 2018. End-to-End Neural Ranking for eCommerce Product Search: An application of task models and textual embeddings. In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR 2018 eCom). ACM,",
        "Ranking Sentences from Product Description & Bullets for Better Search Products in an ecommerce catalog contain information-rich fields like description and bullets that can be useful to extract entities (attributes) using NER based systems. However, these fields are often verbose and contain lot of information that is not relevant from a search perspective. Treating each sentence within these fields equally can lead to poor full text match and introduce problems in extracting attributes to develop ontologies, semantic search etc. To address this issue, we describe two methods based on extractive summarization with reinforcement learning by leveraging information in product titles and search click through logs to rank sentences from bullets, description, etc. Finally, we compare the precision of these two models.",
        "Turning Clicks into Purchases: Revenue Optimization for Product Search in E-Commerce. In recent years, product search engines have emerged as a key factor for online businesses. According to a recent survey, over 55% of online customers begin their online shopping journey by searching on an E-Commerce (EC) website like Amazon as opposed to a generic web search engine like Google 1 . Information retrieval research to date has been focused on optimizing search ranking algorithms for web documents while little attention has been paid to product search. There are several intrinsic diferences between web search and product search that make the direct application of traditional search ranking algorithms to EC search platforms diicult. First, the success of web and product search is measured diferently; one seeks to optimize for relevance while the other must optimize for both relevance and revenue. Second, when using real-world EC transaction data, there is no access to manually annotated labels. In this paper, we address these diferences with a novel learning framework for EC product search called LETORIF (LEarning TO Rank with Implicit Feedback). In this framework, we utilize implicit user feedback signals (such as user clicks and purchases) and jointly model the diferent stages of the shopping journey to optimize for EC sales revenue. We conduct experiments on real-world EC transaction data and introduce a a new evaluation metric to estimate expected revenue after re-ranking. Experimental results show that LETORIF outperforms top competitors in improving purchase rates and total revenue earned.",
        "Session-aware Information Embedding for E-commerce Product Recommendation. Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art.",
        "Weakly Supervised Co-Training of Query Rewriting andSemantic Matching for e-Commerce. Relevance is the core problem of a search engine, and one of the main challenges is the vocabulary gap between user queries and documents. This problem is more serious in ecommerce, because language in product titles is more professional. Query rewriting and semantic matching are two key techniques to bridge the semantic gap between them to improve relevance. Recently, deep neural networks have been successfully applied to the two tasks and enhanced the relevance performance. However, such approaches suffer from the sparseness of training data in e-commerce scenario. In this study, we investigate the instinctive connection between query rewriting and semantic matching tasks, and propose a co-training framework to address the data sparseness problem when training deep neural networks. We first build a huge unlabeled dataset from search logs, on which the two tasks can be considered as two different views of the relevance problem. Then we iteratively co-train them via labeled data generated from this unlabeled set to boost their performance simultaneously.We conduct a series of offline and online experiments on a real-world e-commerce search engine, and the results demonstrate that the proposed method improves relevance significantly."
    ],
    "web pages evolution": [
        "The volume and evolution of web page templates. Web pages contain a combination of unique content and template material, which is present across multiple pages and used primarily for formatting, navigation, and branding. We study the nature, evolution, and prevalence of these templates on the web. As part of this work, we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality. Our results show that 40-50% of the content on the web is template content. Over the last eight years, the fraction of template content has doubled, and the growth shows no sign of abating. Text, links, and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8% per year. We discuss the deleterious implications of this growth for information retrieval and ranking, classification, and link analysis.",
        "Microscale evolution of web pages. We track a large set of \"rapidly\" changing web pages and examine the assumption that the arrival of content changes follows a Poisson process on a microscale. We demonstrate that there are significant differences in the behavior of pages that can be exploited to maintain freshness in a web corpus.",
        "String Processing and Information Retrieval - 17th International Symposium, SPIRE 2010, Los Cabos, Mexico, October 11-13, 2010. Proceedings The annual SPIRE conference provides researchers within fields related to string processing and/or information retrieval a possibility to present their original contributions and to meet and talk with other researchers with similar interests. The call for papers invited submissions related to string processing (dictionary algorithms; text searching; pattern matching; text and sequence compression; automata-based string processing), information retrieval (information retrieval models; indexing; ranking and filtering; querying and interface design), natural language processing (text analysis; text mining; machine learning; information extraction; language models; knowledge representation), search applications and usage (cross-lingual information access systems; multimedia information access; digital libraries; collaborative retrieval and Web-related applications; semi-structured data retrieval; evaluation), and interaction of biology and computation (DNA sequencing and applications in molecular biology; evolution and phylogenetics; recognition of genes and regulatory elements; sequence driven protein structure prediction).The papers presented at the symposium were selected from 109 submissions written by authors from 30 different countries. Each submission was reviewed by at least three reviewers, with a maximum of five reviews for particularly challenging papers. The Program Committee accepted 39 papers (corresponding to \u224835% acceptance rate): 26 long papers and 13 short papers. In addition to these presentations, SPIRE 2010 also featured invited talks by Gonzalo Navarro (Universidad de Chile) and Mark Najork (Microsoft Research, USA).We are especially thankful to the members of the Program Committee, who provided us with thorough and timely reviews. Every PC member completed all their reviews on a very tight schedule. We wish to thank the SPIRE Steering Committee, via their coordinator Ricardo Baeza-Yates and the editorial office staff at Springer. The student volunteer staff led by Francisco Claude was especially helpful with attending local matters for the conference. The students of the Information Retrieval course of CICESE helped in gathering all the kits for attendees. Yahoo! Research had generously provided sponsorship for partial support for accommodation costs for student volunteers. We finally thank Universidad Michoacana and University of California for donating the time of the organizers. September 2010Edgar 1993, 1995, 1996 and 1997. WSP was transformed into SPIRE in 1998, when the scope of the conference was broadened to include also the area of information retrieval.. This paper focuses on using hyperlinks in the ranking of web search results. We give a brief overview of the vast body of work in the area; we provide a quantitative comparison of the different features; we sketch how link-based ranking features can be implemented in large-scale search engines; and we identify promising avenues for future research. Keywords:Web graph, link analysis, ranking, PageRank, HITS, SALSA. The Ranking ProblemOne of the fundamental problems of information retrieval is the ranking problem: given a corpus of documents and a query reflecting a user's information need, and having drawn from the corpus all the \"result\" documents that satisfy the query, order the results by decreasing relevance with respect to the user's information need. The aim of ranking algorithms is to maximize the utility of the result list to the user; or (more subjectively) to maximize the user's satisfaction.Ranking algorithms deployed in commercial search engines typically draw on a multitude of individual features, where each feature manifests itself as a numerical score, and combine these features in some way, e.g. by weighted linear combination. Features can be classified across many possible dimensions. shows one possible taxonomy incorporating two dimensions: when a feature is computed, and what it is based on. Query-dependent or \"dynamic\" features take the query into account and thus can only be computed at query time, whereas query-independent or \"static\" features do not rely on the query and thus can be computed ahead of time. Loosely speaking, dynamic features estimate the relevance of a document with respect to a query, while static features estimate its general quality.Much of the classic research in information retrieval focused on small-to medium-sized curated corpora of high-quality documents, and assumed that queries are issued by trained library scientists. Consequently, early ranking algorithms leveraged textual features, based on the words or \"terms\" contained in documents and queries. Examples of query-independent textual features are readability measures such as Flesch-Kincaid [15]; examples of query-dependent textual features include BM25 [30]. As information retrieval broadened to web search, some of these assumptions became less tenable (the web is not curated; pages are more variable in length and quality; and users of web search engines are less experienced and tend to frame fairly short queries), but the fact that documents are interconnected by hyperlinks made new features available. Examples of query-independent link-based features include PageRank [28]; examples of query-dependent link-based features include HITS [16] and its many descendants. Finally, the explosive growth in the popularity of the web itself and of commercial web search engines generates an enormous amount of data on user activity both on search engines and the web at large, which can be harnessed into usage-based features. An example of a query-independent usage-based feature is Alexa Traffic Rank [2]; an example of a query-dependent usage-based feature is result click-through rate [31].In the remainder of this paper, we will focus on link-based features. However, it is worth pointing out that all features -whether text-, link-or usage-based -are generated by human activity: by authors creating documents and endorsing other authors' documents through hyperlinks, and by users surfing the web, framing queries, and clicking on results. So, the ranking problem is about identifying features generated by observable human activities that are well-correlated with human satisfaction with the ordering (or more generally the selection and presentation) of the results of a query. In short, at its very core ranking is neither a mathematical nor an algorithmic problem, but a social one, concerned with the behaviors and interactions of people. Given that search is ultimately a social activity and that so far we are not not able to model human behavior well enough to accurately predict the effectiveness of any new ranking algorithm, the importance of experimental studies cannot be overemphasized. Using Hyperlinks to Rank Web Search ResultsMarchiori was the first to propose leveraging hyperlinks for ranking web search results. Specifically, he suggested computing a conventional test-based score for the result pages of a query, and then to propagate these scores (appropriately attenuated) to neighboring pages in the web graph [20]. This work inspired other researchers to consider hyperlinks as ranking features in their own right. Querying the Web Graph 3Hyperlinks can be viewed as endorsements made by web page authors of other web pages, and they can be classified along several dimensions. First, we can distinguish between \"egotistic\" and \"altruistic\" hyperlinks 1 -links that endorse pages in which the author has a vested interest vs. those that endorse other pages. It is very hard to identify altruistic links with high confidence, but there are some easy-to-determine indicators of egotistic links: links that point to pages on the same web server, on a web server in the same domain, on a server with the same IP address or in the same IP subnet, or registered to the same entity. Second, we can distinguish between \"topical\" and \"templatic\" links 2 -links that point to pages that are on the same topic as the linking page vs. those that point to topically dissimilar pages and often are part of a design template applied to the entire web site (for example, links to the site's privacy policy). In this world view, altruistic topical links provide the most meaningful endorsements. PageRankPage (together with Brin, Motwani and Winograd) proposed a purely link-based ranking function that went beyond the straightforward counting of endorsing hyperlinks in two respects: First, he suggested that the reputation of a page should be dependent on the reputation of its endorsing pages, and second, he suggested that the endorsement ability of a page should be divided among the pages it endorses. Furthermore, to prevent degenerate behavior in the presence of cycles and sink pages, he suggested affording each page a guaranteed minimal score. The resulting ranking function is the now-famous PageRank [28]. In order to define it formally, we will need some notation.Web pages and hyperlinks induce a graph with vertex (page) set V and edge (link) set E \u2286 V \u00d7V . We write I(v, E) to denote the set of pages {u : (u, v) \u2208 E} that link to v, and O(u, E) to denote the set of pages {v : (u, v) \u2208 E} that u links to. The teleportation vector t : V \u2192 [0, 1] controls the guaranteed minimum score of each page, the link weight matrix W : V \u00d7 V \u2192 [0, 1] is typically informed by the graph's adjacency matrix and controls the endorsement power of each hyperlink, and the damping factor \u03bb balances the influence of either component. Using this notation, the PageRank p(v) of a page v is defined in its most general form as the fixed point of the following recurrence relation:or, using linear algebra notation, as: p, t and the rows of W are typically constrained to add up to 1, such that they can be viewed as probability distributions. That raises the question of how to deal with a \"terminal\" or \"dangling\" web page u that does not contain any hyperlinks, i.e. v\u2208V W (u, v) = 0. Page et al. proposed pruning such terminal vertices from the web graph (which may require multiple iterations, since pruning such terminal vertices and their incoming links may leave other vertices without outgoing links), computing PageRank on the core graph, and finally adding the pruned vertices and edges back in and propagating scores to them [28]. As it turns out, we can ignore the problem. To understand why, let us assume that we add a \"phantom vertex\" \u03c6 to the graph, plus a reflexive edge (\u03c6, \u03c6) and a phantom edge from each terminal to \u03c6. The modified graph is free of terminal vertices. We set t(\u03c6) = 0, W (u, \u03c6) = 1 iff O(u, E) = \u2205 and 0 otherwise, W (\u03c6, \u03c6) = 1, and W (\u03c6, v) = 0 for all v \u2208 V . If we compute p on this graph using power iteration, p 1 ( the 1 -norm of p) will not change over iterations. Significantly, the p(v) score of any non-phantom vertex is the same as it would be had we computed p using power iteration on the original graph while resigning ourselves that zerosum rows in W will cause p 1 to shrink. The score mass that simply disappears when computing PageRank on the original graph can be found in the phantom node of the augmented graph. In other words, adding the phantom node and phantom edges to the graph is a useful intellectual device, but not actually needed for computing PageRank.M .N a j o r k \u03bbt(v) + (1 \u2212 \u03bb)W (u, v), assuming that p adds up to 1. In the classical definitionOne very popular interpretation of the PageRank formula is the \"random surfer model\". In this model, a \"surfer\" traverses the web graph, visiting a new vertex at each transition. The surfer either \"steps\" with probability 1 \u2212 \u03bb or \"jumps\" with probability \u03bb. The destination vertex of a step is conditioned on the departure vertex, while that of a jump is not. Assuming that W is based on the web graph's adjacency matrix, taking a step means following a link. The surfer transitions from vertex u to vertex v with probability u, v). In this interpretation, p(v) is the probability that the surfer is at vertex v at any given point in time. The random surfer model is very intuitive, and it relates PageRank to Markov random walks. Unfortunately, it also has led to a fair amount of confusion in the community: the random surfer model suggests that PageRank is modeling the web surfing behavior of users, i.e. of consumers of web content; the endorsement model described above suggests that PageRank is modeling the cross-reference behavior of authors, i.e. of producers of web content. We subscribe to the latter interpretation. Incidentally, commercial search engines have fairly direct ways of observing user behavior (e.g. through browser toolbars) and thus little need to model it.Since PageRank is query-independent, it can be computed off-line. The PageRank vector p is typically computed using power iteration. Major commercial search engines maintain web graphs with tens of billions of vertices, and thus Querying the Web Graph 5 compute p in a distributed setting. There are two standard approaches of doing this. The first approach uses data-parallel frameworks such as MapReduce [10], Hadoop [13] or DryadLINQ [32]. In this setting, the link weight matrix W , the teleportation vector t, and the old and new score vectors p reside on disk and are processed in a streaming fashion. Multiplying W and p corresponds to a join operation in relational algebra followed by a group-by, and the data streams to be joined must be sorted on the join key. W can be sorted ahead of time, while p has to be re-sorted every iteration. The second, ad-hoc approach partitions p, t and the rows of W such that all entries corresponding to pages on the same web server fall into the same partition. The new score vector is kept in memory (partitioned over many machines). W , t and the old score vector, which all reside on disk, are read in a streaming fashion, and fractions of the old scores are used to increment entries of the new score vector. These updates have a random-access pattern, but most will affect the local partition of the score array due to link locality -typically, most hyperlinks in a web page refer to other pages on the same web server. After every iteration, the new score vector is written to disk and becomes the old vector in the next iteration. The advantage of the ad-hoc approach over the MapReduce-approach is that it does not require any sorting; the two disadvantages are that it requires more engineering (e.g. to provide for fault-tolerance) and that computing scores for a larger graph requires more memory (typically by provisioning additional machines).PageRank scores are computed off-line, and used at query-time to score results. As stated above, major search engines maintain corpora of tens of billions of documents. At the same time, they typically aim to answer queries within a fraction of a second [19]. In order to answer queries over such a large corpus and with such tight latency constraints, engines maintain all or at least the \"hot\" part of the index in main memory, partitioned across many machines, with each partition replicated multiple times across machines to achieve fault tolerance and increase throughput. The web corpus is commonly partitioned by document, i.e. all the terms of a given document are stored in the same sub-index. When a query arrives at the search engine, it is distributed to a set of index-serving machines that together hold the full index. Each machine finds the set of results in its sub-index that satisfy the query, scores these results using locally available features, and sends the top-k results to a result aggregation machine, which integrates them into the overall result set, possibly performing a more in-depth scoring. Since PageRank is a query-independent document score, the PageRank vector can be partitioned in the same way as the web corpus, and index-serving nodes can determine the PageRank scores of results using local table lookups. In other words, the query-time portion of PageRank is both extremely efficient and extremely scalable.PageRank's elegance and intuitiveness, combined with the fact that it was credited for much of Google's extraordinary success, led to a plethora of research. Broadly speaking, this research falls into four classes: PageRank's mathematical properties (e.g. Alas, there is a paucity in experimental validations of its effectiveness as a ranking function. Part of this is due to the fact that it is hard to assemble a large web corpus, and expensive to compile human judgments required in Cranfield-style evaluations of ranking effectiveness. Early studies tried to overcome these obstacles by leveraging existing search engines to obtain linkage information [4]; more recent work by our group performed substantial web crawls and used a test set compiled by a commercial search engine [23]. To our surprise, we found that PageRank in its classic form (with uniform teleportation and link bias, and \u03bb = 0.15) is even less effective than simply counting altruistic (inter-domain) hyperlinks. summarizes the results of these studies, which were all based on the same data sets.At first glance, it is surprising that PageRank does not outperform interdomain link-counting -after all, link-counting ignores the reputation of the linking page, and considers only the immediate neighborhood of each web page. We believe that there are two reasons why classic PageRank fares so poorly: First, it treats all links the same; it ignores whether they are egotistic or altruistic, topical or templatic. Inter-domain link-counting on the other hand will discard links that are obviously egotistic. Second, being credited as an important factor in Google's ranking algorithm, PageRank is under attack by legions of search engine optimizers. In its classic formulation, each page receives a guaranteed minimum score, so the obvious attack is to publish millions of pages that all endorse a single landing page, which will receive the (slightly dampened) sum of the scores of the endorsing pages. The key enabler for spammers is that web pages can be automatically generated on the fly, so publishing even a very large collection of pages is virtually free. This technique is known as link spam or link bombs, and there are several studies on the most effective shape of such link bombs [1,12].Given that the key enabler of link spam is the low cost of publishing, the appropriate countermeasure is to correlate the teleportation vector with a feature that has actual economic cost. Examples of features that have non-zero cost are domain names (since it requires payment to a registrar), IP addresses (IPv4 addresses in particular are becoming quite scarce), and of course actual traffic on a page [22]. For example, using visits(u) to denote the number of visits to page u in a given amount of time (optionally weighted by the dwell-time), we can define the teleportation vector to discount unpopular pages:As a second example, using domain(u) to denote the domain of web page u, we can define the teleportation vector to discount domains with many web pages:Using the random-surfer analogy, in the probability-\u03bb event of a jump, the surfer does not choose a page uniformly at random, but instead first chooses a domain and then a page within that domain, thereby giving equal minimum-endorsement ability to each (nonzero-cost) domain instead of each (zero-cost) page. As a third example, using addresses(u) to denote the IP address(es) of the web server(s) serving page u, we can define the teleportation vector to discount servers with many web pages while giving credit to multi-hosted pages:Again using the random-surfer analogy, in the event of a jump the surfer first chooses a web server IP address and then chooses a page served by that machine.In addition to adjusting the teleportation vector to dilute the ability of link farms to endorse target pages, we could adjust the link weight matrix to prefer altruistic or topical links. As we said above, any altruistic link classifier suffers from a non-negligible false-positive rate: it is impossible to rule out the possibility that two web publishers are colluding. On the other hand, the topicality of a link from u to v can be captured by straightforward means, e.g. by quantifying the textual similarity between u and v, using, say, the cosine similarity between their tf.IDF-weighted term vectors [29]. Given a similarity measure \u03c3 : V \u00d7 V \u2192 R where higher values indicate higher similarity, we can define W as follows:There are many other possible techniques for distinguishing topical from templatic links, for example segmenting the page into \"blocks\" and discounting links contained in navigational or advertising blocks [6]. Hopefully the above examples will convince you that there is room for improving PageRank, by finding ways to make it a more effective ranking function and more resilient to link-spam. Any such research should be data-driven: given the availability of large web corpora (e.g. the billion-page ClueWeb09 crawl [8]) and associated test sets (e.g. the TREC 2009 web track judgments [7]), it is possible to evaluate ranking functions in a repeatable fashion. As a corollary, studies of PageRank's mathematical properties and efforts to speed up its computation have more impact if they don't assume t and W to be uniform in any way. HITS and Its VariantsAt about the same time as Page et al. proposed PageRank, Jon Kleinberg suggested a different link-based ranking algorithm called \"Hyperlink-Induced Topic Search\" [16]. HITS takes the result set of a query as its input, expands the result set to include immediately neighboring pages in the web graph, projects this expanded vertex set onto the full web graph to obtain a neighborhood graph, and computes scores for each vertex in that neighborhood graph. Since it takes a query's result set as input, HITS is query-dependent, and the latency introduced by computing it at query-time is a major concern to commercial search engines, given the high correlation between response time and audience engagement [19].Fundamentally, HITS consists of two distinct steps: First, given a result set, compute a neighborhood graph; and second, compute scores for each vertex in the neighborhood graph. Kleinberg suggested that the neighborhood graph should be computed by extending the result set to include all vertices that are within distance 1 in the link graph, ignoring \"intrinsic\" (egotistic by our terminology) links. In order to keep high-in-degree result vertices from inflating the neighborhood vertex set too much, he suggested including just (say) 50 randomly chosen endorsing vertices of each such high-indegree result. The neighborhood graph consists of the neighborhood vertices and those edges of the full web graph that connect neighborhood vertices and are not intrinsic. To formalize this, we write R n (A) to denote a uniform random sample of n elements from set A (R n (A) = A iff |A| \u2264 n), and we assume that all intrinsic edges have been removed from the web graph (V, E). Using this notation and given a result set R \u2286 V to query q, Kleinberg's neighborhood graph consists of a neighborhood vertex set V R and a neighborhood edge set E R :Kleinberg furthermore suggested computing two scores for each v \u2208 V R : an authority score a(v) and a hub score h(v), the former indicating whether v is a good authority (i.e. how relevant v is with respect to q), and the latter indicating whether v is a good hub (i.e. if v links to pages that are good authorities). Kleinberg defined a and h in a mutually recursive fashion, as a(v) = u\u2208I (v,ER) h(u) and h(u) = v\u2208O(u,ER) a(v), and suggested computing the fixed point of this recurrence by performing power iteration and normalizing a and h to unit length after each step. Using linear algebra notation leads to a more concise definition. If we define the neighborhood graph's adjacency matrix as A(u, v) = 1 iff (u, v) \u2208 E R and 0 otherwise, then the authority score vector is the principal eigenvector of the matrix A T A, and the hub score vector is the principal eigenvector of the matrix AA T . Based on experimental studies [23], authority scores are useful ranking features, while hub scores carry virtually no signal.Lempel and Moran suggested a variant of HITS called the Stochastic Approach to Link Sensitivity Analysis, or SALSA for short [18]. SALSA combines Querying the Web Graph 9 ideas from HITS and PageRank: The SALSA authority score vector is the stationary probability distribution of a random walk over the neighborhood graph, where each transition consists of choosing an incoming link and traversing it backwards, and then choosing an outgoing link and traversing it forwards. Using linear algebra notation, the authority score vector is the principal eigenvector of the matrix In the course of performing the aforementioned experimental studies [23,24] into the effectiveness of HITS and SALSA as ranking features, we found that the choice of neighborhood graph has a significant impact on effectiveness. We discovered four modifications to the neighborhood selection algorithm that each increase effectiveness: first, using consistent instead of random sampling; second, sampling both the endorsed as well as the endorsing vertices of each result; third, omitting edges that don't touch results; and fourth, sampling the eligible edges [26]. Using C n (A) to denote an unbiased consistent sample [5] of n elements from set A, we select the neighborhood graph of result set R as follows:The free variables a, b, c, and d in the above formulas determine how many adjacent vertices and edges are sampled for each result vertex. In our experiments, effectiveness was maximal for a, b in the mid-single digits and c, d around 1000. The SALSA-SETR row of provides effectiveness numbers.Experimenting with HITS-like ranking algorithms requires fast access to vertices and edges in the web graph. While it would be possible to use a standard relational database to store the graph, extracting the neighborhood graph of a given result set exhibits a very random access pattern, such that disk latency would become a serious bottleneck. For this reason, we developed the Scalable Hyperlink Store [27], a bespoke system that maintains a web graph in main memory, distributed over multiple machines and employing compression techniques that leverage structural properties of web graphs, e.g. link locality. This infrastructure enables us to get one-minute turnarounds when scoring 100 TREC queries; however, the time required for scoring an individual query is pushing the boundary of what is acceptable for a commercial search engine, where the aim is to keep overall query latencies within fractions of a second.In order to overcome this problem, we experimented with techniques for performing as much of the SALSA computation off-line as possible. We explored two approaches. The first and more radical approach is to assume that each page in the web graph is a singleton result set (to some unspecified query), and to off-line perform a separate SALSA computation for each page. More specifically, for each page u, we extract the neighborhood graph (V {u} , E {u} ) around u, compute SALSA authority scores for all vertices in V {u} , and store a mapping from u to the k highest-scoring v \u2208 V {u} together with their scores. At query time, 10 M. Najork having determined the result set R for the given query, we retrieve the entries for each v \u2208 R from the mapping (yielding a multi-set S of k|R| vertex-score pairs), and for each v \u2208 R we find all occurrences of v in S and sum up their scores to produce an overall score for v. The parameter k controls a trade-off between ranking effectiveness and required space. The SS-SALSA-3 row in shows the effectiveness for k = 10 (i.e. using 120 bytes per page in the web corpus). The effectiveness is below that of Lempel and Moran's \"classic\" SALSA, but the query-time portion of the computation is much cheaper, simply requiring a table lookup for each result. The table can be distributed across multiple machines in the same way the index is in a modern search engine; the score multi-set of each result can be looked up by the same index-serving machine that held the result, but the final scoring has to be performed by the result aggregator.The second, less radical approach is to move the neighborhood graph extraction off-line while keeping the score computation on-line [26]. The basic idea is to compute and store a short summary for each node in the web graph, consisting of two small samples of endorsing and endorsed vertices, and two Bloom filters containing larger samples of endorsing and endorsed vertices. At query time, we look up the summary of each vertex in the result set, we use these summaries to construct an approximation of the neighborhood graph, and we compute SALSA authority scores on this approximate graph. The size of a summary is governed by how many neighbors are sampled and how many hash functions are used by the Bloom filter, and (as one might expect) there is a trade-off between size and ranking effectiveness. Choosing parameters that lead to 500 byte summaries makes this algorithm as effective as the (purely online) SETR variant of SALSA described above. The summary table can be distributed across the index-serving machines and summaries can be passed along with results to the result aggregator; however, the construction of the approximate neighborhood graph and the subsequent scoring requires all summaries, and therefore has to be performed on the machine responsible for query distribution and result aggregation.Much of the design space in HITS-like ranking algorithms remains to be explored. For example, HITS considers only the distance-one neighborhood of the result set. In the Companion algorithm [11], Dean and Henzinger suggested including more-distant neighbors; how does this impact ranking effectiveness? As another example, HITS and SALSA both discard egotistic hyperlinks, but make no attempts to incorporate link topicality. Could effectiveness be improved by weighing each edge according to the textual similarity of the linked pages, in a fashion similar to what we suggested above for PageRank? And as a final example, how affected are HITS and SALSA by link spam, and could we improve their resiliency by using ideas similar to what was described above, e.g. by sampling endorsing and endorsed vertices in a traffic-biased fashion? ConclusionThis paper gives a high-level overview of the two main approaches to leveraging hyperlinks as ranking features: query-independent features as exemplified Querying the Web Graph 11 by the PageRank algorithm, and query-dependent features as exemplified by the HITS algorithm. The main research challenge in this space is to identify features that (in combination with many other features) improve ranking effectiveness, but that at the same are very efficient to determine at query time. For PageRank-style features, query-time efficiency is not a significant issue, but ranking effectiveness is, due to the increase in link spam as well as the decreasing fraction of on-topic links. Effectiveness can be improved by biasing PageRank towards on-topic links and against \"spammy\" web pages. On the other hand, various descendants of HITS produce highly effective ranking features, but have a high query-time cost. Two approaches to containing this expense are to implement highly-optimized infrastructure for executing HITS-like computations, or to devise ways to to move as much of the computation as possible off-line.The ranking problem is about identifying and leveraging observable human activities (production by authors and consumption by readers) that correlate well with human satisfaction with the performance of an IR system. In order to truly solve this problem, we need a model of these authors and readers. But at present, we do not possess any model with predictive abilities -i.e. a model that would allow us to a priori predict the performance of a new ranking feature and that would stand up to subsequent experimental validation. In the absence of such a model, experimental validation is of paramount importance!. Query recommender systems give users hints on possible interesting queries relative to their information needs. Most query recommenders are based on static knowledge models built on the basis of past user behaviors recorded in query logs. These models should be periodically updated, or rebuilt from scratch, to keep up with the possible variations in the interests of users. We study query recommender algorithms that generate suggestions on the basis of models that are updated continuously, each time a new query is submitted. We extend two stateof-the-art query recommendation algorithms and evaluate the effects of continuous model updates on their effectiveness and efficiency. Tests conducted on an actual query log show that contrasting model aging by continuously updating the recommendation model is a viable and effective solution. References",
        "Visualising Scientific Topic Evolution",
        "Visualizing historical content of web pages. Recently, along with the rapid growth of the Web, the preservation efforts have also increased. As a consequence, large amounts of past Web data are stored in Web archives. This historical data can be used for better understanding of long-term page topics and characteristics. In this paper, we propose an interactive visualization system called Page History Explorer for exploring page histories. It allows for roughly portraying evolution of pages and summarizing their content over time. We use a temporal term cloud as a structure for visualizing prevailing and active terms appearing on pages in the past.",
        "Web page rank prediction with markov models. In this paper we propose a method for predicting the ranking position of a Web page. Assuming a set of successive past top-k rankings, we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training, which are in turn used to predict future rankings. The predictions are highly accurate for all experimental setups and similarity measures.",
        "What's new on the web?: the evolution of the web from a search engine perspective. We seek to gain improved insight into how Web search engines should cope with the evolving Web, in an attempt to provide users with the most up-to-date results possible. For this purpose we collected weekly snapshots of some 150 Web sites over the course of one year, and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate of creation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e., high rates of birth and death, coupled with an even higher rate of turnover in the hyperlinks that connect them. For pages that persist over time we found that, perhaps surprisingly, the degree of content shift as measured using TF.IDF cosine distance does not appear to be consistently correlated with the frequency of content updating. Despite this apparent noncorrelation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications of our results for the design of effective Web search engines.",
        "Impact of search engines on page popularity. Recent studies show that a majority of Web page accesses are referred by search engines. In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web. In particular, we study how much impact search engines have on the popularity evolution of Web pages. For example, given that search engines return currently \"popular\" pages at the top of search results, are we somehow penalizing newly created pages that are not very well known yet? Are popular pages getting even more popular and new pages completely ignored? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data. We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results. Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages.",
        "WebRelievo: A System for Browsing and Analyzing the Evolution of Related Web Pages WebRelievo is a system for browsing and analyzing the evolution of the web graph structure based on link analysis. This system enables us to answer historical questions, and to detect changes in topics on the Web. WebRelievo extracts web pages related to a focused page using link analysis, and visualizes the evolution of their relationships with a time series of graphs. This visualization enables us to understand when related pages appeared, and how their relationships have evolved over time. The user can interactively browse those related pages by changing the focused page and by changing layouts of graphs. WebRelievo is implemented on six Japanese web archives crawled from 1999 to 2003.",
        "Detecting phrase-level duplication on the world wide web. Two years ago, we conducted a study on the evolution of web pages over time. In the course of that study, we discovered a large number of machine-generated \"spam\" web pages emanating from a handful of web servers in Germany. These spam web pages were dynamically assembled by stitching together grammatically wellformed German sentences drawn from a large collection of sentences. This discovery motivated us to develop techniques for finding other instances of such \"slice and dice\" generation of web pages, where pages are automatically generated by stitching together phrases drawn from a limited corpus. We applied these techniques to two data sets, a set of 151 million web pages collected in December 2002 and a set of 96 million web pages collected in June 2004. We found a number of other instances of large-scale phrase-level replication within the two data sets. This paper describes the algorithms we used to discover this type of replication, and highlights the results of our data mining.",
        "A large-scale study of the evolution of web pages. How fast does the web change? Does most of the content remain unchanged once it has been authored, or are the documents continuously updated? Do pages change a little or a lot? Is the extent of change correlated to any other property of the page? All of these questions are of interest to those who mine the web, including all the popular search engines, but few studies have been performed to date to answer them.One notable exception is a study by Cho and Garcia-Molina, who crawled a set of 720,000 pages on a daily basis over four months, and counted pages as having changed if their MD5 checksum changed. They found that 40% of all web pages in their set changed within a week, and 23% of those pages that fell into the .com domain changed daily.This paper expands on Cho and Garcia-Molina's study, both in terms of coverage and in terms of sensitivity to change. We crawled a set of 150,836,209 HTML pages once every week, over a span of 11 weeks. For each page, we recorded a checksum of the page, and a feature vector of the words on the page, plus various other data such as the page length, the HTTP status code, etc. Moreover, we pseudo-randomly selected 0.1% of all of our URLs, and saved the full text of each download of the corresponding pages.After completion of the crawl, we analyzed the degree of change of each page, and investigated which factors are correlated with change intensity. We found that the average degree of change varies widely across top-level domains, and that larger pages change more often and more severely than smaller ones. This paper describes the crawl and the data transformations we performed on the logs, and presents some statistical observations on the degree of change of different classes of pages.",
        "Effects of web document evolution on genre classification. The World Wide Web is a massive corpus that constantly evolves. Classification experiments usually grab a snapshot (temporally and spatially) of the Web for a corpus. In this paper, we examine the effects of page evolution on genre classification of Web pages. Web genre refers to the type of the page characterized by features such as style, form or presentation layout, and meta-content; Web genre can be used to tune spider crawling re-visits and inform relevance judgments for search engines. We found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version. We show that an old corpus can be used for training when testing on new Web pages, with only a marginal drop in accuracy rates on genre classification. We also show that features found to be useful in one corpus do not transfer well to other corpora with different genres.",
        "Web Structure and Evolution Web Structure, Age, and Page Quality This paper is aimed at the study of quantitative measures of the relation between Web structure, age, and quality o f W eb pages. Quality is studied from di erent link-based metrics and their relationship with the structure of the Web and the last modi cation time of a page. We show that, as expected, Pagerank is biased against new pages. As a subproduct we propose a P agerank variant that includes age into account a n d w e obtain information on how the rate of change is related with Web structure."
    ],
    " exhaustivity of index ": [
        "Investigating the exhaustivity dimension in content-oriented XML element retrieval evaluation. INEX, the evaluation initiative for content-oriented XML retrieval, has since its establishment defined the relevance of an element according to two graded dimensions, exhaustivity and specificity. The former measures how exhaustively an XML element discusses the topic of request, whereas specificity measures how focused the element is on the topic of request. However, obtaining relevance assessments is a costly task. In XML retrieval this problem is exacerbated as the elements of the document must also be assessed with respect to the exhaustivity and specificity dimensions. A continuous discussion in INEX has been whether such a sophisticated definition of relevance, and in particular the exhaustivity dimension, was needed. This paper attempts to answer this question through extensive statistical tests to compare the conclusions about system performance that could be made under different assessment scenarios.",
        "Selective Search: Efficient and Effective Search of Large Textual Collections The traditional search solution for large collections divides the collection into subsets (shards), and processes the query against all shards in parallel (exhaustive search). The search cost and the computational requirements of this approach are often prohibitively high for organizations with few computational resources. This article investigates and extends an alternative: selective search, an approach that partitions the dataset based on document similarity to obtain topic-based shards, and searches only a few shards that are estimated to contain relevant documents for the query. We propose shard creation techniques that are scalable, efficient, self-reliant, and create topic-based shards with low variance in size, and high density of relevant documents.The experimental results demonstrate that the effectiveness of selective search is on par with that of exhaustive search, and the corresponding search costs are substantially lower with the former. Also, the majority of the queries perform as well or better with selective search. An oracle experiment that uses optimal shard ranking for a query indicates that selective search can outperform the effectiveness of exhaustive search. Comparison with a query optimization technique shows higher improvements in efficiency with selective search. The overall best efficiency is achieved when the two techniques are combined in an optimized selective search approach.",
        "Revisiting Exhaustivity and Specificity Using Propositional Logic and Lattice Theory. Exhaustivity and Specificity in logical Information Retrieval framework were introduced by Nie . However, even with some attempts, they are still theoretical notions without a clear idea of how to be implemented. In this study, we present a new approach to deal with them. We use propositional logic and lattice theory in order to redefine the two implications and their uncertainty P (d \u2192 q) and P (q \u2192 d).We also show how to integrate the two notions into a concrete IR model for building a new effective model. Our proposal is validated against six corpora, and using two types of terms (words and concepts). The experimental results showed the validity of our viewpoint, which state: the explicit integration of Exhaustivity and Specificity into IR models will improve the retrieval performance of these models. Moreover, there should be a type of balance between the two notions.",
        "Theoretical benchmarks of XML retrieval. This poster investigates the use of theoretical benchmarks to describe the matching functions of XML retrieval systems and the properties of specificity and exhaustivity in XML retrieval. Theoretical benchmarks concern the formal representation of qualitative properties of IR models. To this end, a Situation Theory framework for the meta-evaluation of XML retrieval is presented.",
        "Simulation of User Judgments in Bibliographic Retrieval Systems Abs t ractThe general model and simulation algorithms for bibliographic retrieval systems presented in an earlier paper I are expanded.The new model integrates the physical as well as the logical and semantic elements of these systems. A modified algorithm is developed for the simulation of user relevance judgments, and is validated, by means of recall-precision curves and a Kolmogorov-Smirnov test of recall, for two test collections.Other approaches to goodness-of-fit testing are suggested.Zeigler 2 defines a real system as a part of the world which is a source of behavioral data, a model as a set of instructions for generating such behavioral data, and a computer simulati'on as the computational process which, by means of a suitable encoding of the model instructions, can actually generate the data.The real systems described by the model and simulation algorithms in this paper are bibliographic retrieval systems, i.e., systems which provide data in the form of references or document descriptions relating to an informational query.These systems presently exist in a variety of commercial, experimental, automated, and non-automated forms.The purpose in modeling and simulating such systems is to attempt to optimize certain aspects of their operation, notably the effectiveness of the document and query representations and the efficiency of the accessing algorithms and associated data structures.Simulation permits a controlled variation of such parameters as indexing exhaustivity, vocabulary size, document set size, query exhaustivity, and search expression structure,"
    ],
    "query optimization": [
        "Rule-Based Query Optimization, Revisited. We present the architecture and a performance assessment of an extensible query optimizer written in Venus. Venus is a generalpurpose active-database rule language embedded in C++. Following the developments in extensible database query optimizers, first in rule-based form, followed by optimizers written as object-oriented programs, the Venus-based optimizer avails to the advantages of both. Venus' modular structure allows us to go a step further and provide extensibility in search by defining parameterized search components in a declarative form that has the additional effect of integrating heuristic and cost-based optimization. We compare optimizers developed with Volcano, OPI++ and Venus. Venus' optimizing compiler yields code whose performance is comparable with Volcano and OPT++ on smaller queries. The ability to introduce additional pruning heuristics yields better scalability on larger queries. Evaluation of the system using quantitative software metrics supports a claim that the Venusbased optimizer is more easily maintained and extended than are its predecessors.",
        "Obsolescent Materialized Views in Query Processing of Enterprise Information Systems. '.In recent years, query processing has become more complex as data sources are frequently replicated and data are periodically processed and embedded within several data sources simultaneously.These trends have necessitated the optimization of techniques for query processing in order to exploit these new alternatives. Accordingly, this paper introduces an improved query optimization technique. which is capable of assessing query plans that use both current and obsolescent data. In particular, we provide a cost model by which the tradeoffs of using obsolescent materialized views can be evaluated and we also discuss the method's applicability to contemporary query optimization techniques.",
        "Efficient query processing in distributed search engines. Web search engines have to deal with a rapidly increasing amount of information, high query loads and tight performance constraints. The success of a search engine depends on the speed with which it answers queries (efficiency) and the quality of its answers (effectiveness). These two metrics have a large impact on the operational costs of the search engine and the overall user satisfaction, which determine the revenue of the search engine. In this context, any improvement in query processing efficiency can reduce the operational costs and improve user satisfaction, hence improve the overall benefit.In this thesis, we elaborate on query processing efficiency, address several problems within partitioned query processing, pruning and caching and propose several novel techniques:First, we look at term-wise partitioned indexes and address the main limitations of the state-of-the-art query processing methods. Our first approach combines the advantage of pipelined and traditional (non-pipelined) query processing. This approach assumes one disk access per posting list and traditional term-at-a-time processing. For the second approach, we follow an alternative direction and look at document-at-a-time processing of sub-queries and skipping. Subsequently, we present several skipping extensions to pipelined query processing, which as we show can improve the query processing performance and/or the quality of results. Then, we extend one of these methods with intra-query parallelism, which as we show can improve the performance at low query loads.Second, we look at skipping and pruning optimizations designed for a monolithic index. We present an efficient self-skipping inverted index designed for modern index compression methods and several query processing optimizations. We show that these optimizations can provide a significant speed-up compared to a full (non-pruned) evaluation and reduce the performance gap between disjunctive (OR) and conjunctive (AND) queries. We also propose a linear programming optimization that can further improve the I/O, decompression and computation efficiency of Max-Score.Third, we elaborate on caching in Web search engines in two independent contributions. First, we present an analytical model that finds the optimal split in a static memory-based two-level cache. Second, we present several strategies for selecting, ordering and scheduling prefetch queries and demonstrate that these can improve the efficiency and effectiveness of Web search engines.We carefully evaluate our ideas either using a real implementation or by simulation using real-world text collections and query logs. Most of the proposed techniques are found to",
        "Optimizing top-k document retrieval strategies for block-max indexes. Large web search engines use significant hardware and energy resources to process hundreds of millions of queries each day, and a lot of research has focused on how to improve query processing efficiency. One general class of optimizations called early termination techniques is used in all major engines, and essentially involves computing top results without an exhaustive traversal and scoring of all potentially relevant index entries. Recent work in proposed several early termination algorithms for disjunctive top-k query processing, based on a new augmented index structure called Block-Max Index that enables aggressive skipping in the index.In this paper, we build on this work by studying new algorithms and optimizations for Block-Max indexes that achieve significant performance gains over the work in . We start by implementing and comparing Block-Max oriented algorithms based on the well-known Maxscore and WAND approaches. Then we study how to build better Block-Max index structures and design better index-traversal strategies, resulting in new algorithms that achieve a factor of 2 speed-up over the best results in [9] with acceptable space overheads. We also describe and evaluate a hierarchical algorithm for a new recursive Block-Max index structure.",
        "Configuring bitmap materialized views for optimizing XML queries. In recent years the inverted lists evaluation model along with holistic stack-based algorithms have been established as the most prominent techniques for evaluating XML queries on large persistent XML data. In this framework, we are using materialized views for optimizing XML queries. We consider a novel approach which instead of materializing the answer of a view materializes exactly the inverted sublists that are necessary for computing the answer of the view. This originality allows storing view materializations as compressed bitmaps, a solution that minimizes the materialization space and empowers performing optimization operations as CPU-efficient bitwise operations. To realize the potential of bitmap materialized views in optimizing query performance, we define and address the following problem (view configuration problem): given an XML tree and its schema find a template of tree-pattern views (view configuration) such that: (a) the views of this configuration can answer all the queries that can be issued against the schema, (b) their materialization fits in the space provided, and (c) evaluating the queries using these views minimizes the overall query evaluation cost. We consider an instance of this problem for tree pattern queries. Our intension is to find view configurations whose materializations are small enough to be stored in main memory. We find two candidate solution configurations and we identify cases where views can be excluded from materialization in a configuration without affecting query performance. In order to compare our approach with an approach which also can support the optimization of every query on the schema, we implemented an improvement of a state-of-the-art approach which is based on structural indexes. Our experimental results show that our approach is stable, greatly improves evaluating queries without materialized views, outperforms the structural index approach on all test cases and is very close to the optimal. These results characterize our approach as the best candidate for supporting the optimization of queries in the framework of the inverted lists model.",
        "Query-level loss functions for information retrieval. Many machine learning technologies such as support vector machines, boosting, and neural networks have been applied to the ranking problem in information retrieval. However, since originally the methods were not developed for this task, their loss functions do not directly link to the criteria used in the evaluation of ranking. Specifically, the loss functions are defined on the level of documents or document pairs, in contrast to the fact that the evaluation criteria are defined on the level of queries. Therefore, minimizing the loss functions does not necessarily imply enhancing ranking performances. To solve this problem, we propose using query-level loss functions in learning of ranking functions. We discuss the basic properties that a query-level loss function should have and propose a query-level loss function based on the cosine similarity between a ranking list and the corresponding ground truth. We further design a coordinate descent algorithm, referred to as RankCosine, which utilizes the proposed loss function to create a generalized additive ranking model. We also discuss whether the loss functions of existing ranking algorithms can be extended to query-level. Experimental results on the datasets of TREC web track, OHSUMED, and a commercial web search engine show that with the use of the proposed querylevel loss function we can significantly improve ranking accuracies. Furthermore, we found that it is difficult to extend the document-level loss functions to query-level loss functions.",
        "Quick and Incomplete Responses: The Semantic Approach. Conventional query optimization usually minimizes the total time for complete evaluation of a query. We argue that this criterion may not be the only or even necessary requirement. In some interactive environments, a complete answer set may not be required and quick responses to queries can be very desirable. Minimizing the processing time for obtaining a single answer, a few answers, or most answers are identified as other potential optimization objectives. We attempt these optimization problems by adopting a strategy that tist looks at places where answers are most concentrated We are implementing a system (QAus) which provides quick answers to the user. The design of QAns is a loosely couple of a query transformer to a conventional query optimizer. The idea is based on semantic que~opti-mization.However, we introduce an additional logical equivalence in the query transformation process. We target the transformations towards reducing the cost of the set of more expensive operations in the original query plan. Inexact, as well as exaet rules are allowed in the transformation. The proliferation of inexact rules also leads to an issue of the \"goodness\" of such rules. We propose an informationtheoretic critetion for incorporating inexact rules in the system.",
        "A Hybrid Framework for Online Execution of Linked Data Queries. Linked Data has been widely adopted over the last few years, with the size of the Linked Data cloud almost doubling every year. However, there is still no well-defined, efficient mechanism for querying such a Web of Data. We propose a framework that incorporates a set of optimizations to tackle various limitations in the state-of-the-art. The framework aims at combining the centralized query optimization capabilities of the data warehouse-based approaches with the result freshness and explorative data source discovery capabilities of link-traversal approaches. This is achieved by augmenting base-line link-traversal query execution with a set of optimization techniques. The proposed optimizations fall under two categories: metadata-based optimizations and semantics-based optimizations.",
        "Type-based Semantic Optimization for Scalable RDF Graph Pattern Matching. Scalable query processing relies on early and aggressive determination and pruning of query-irrelevant data. Besides the traditional space-pruning techniques such as indexing, type-based optimizations that exploit integrity constraints defined on the types can be used to rewrite queries into more efficient ones. However, such optimizations are only applicable in strongly-typed data and query models which make it a challenge for semi-structured models such as RDF. Consequently, developing techniques for enabling type-based query optimizations will contribute new insight to improving the scalability of RDF processing systems.In this paper, we address the challenge of type-based query optimization for RDF graph pattern queries. The approach comprises of (i) a novel type system for RDF data induced from data and ontologies and (ii) a query optimization and evaluation framework for evaluating graph pattern queries using type-based optimizations. An implementation of this approach integrated into Apache Pig is presented and evaluated. Comprehensive experiments conducted on real-world and synthetic benchmark datasets show that our approach is up to 500X faster than existing approaches.",
        "The use of dynamic context to improve casual internet searching Research has shown that most users' online information searches are suboptimal. Query optimization based on a relevance feedback or genetic algorithm using dynamic query contexts can help casual users search the Internet. These algorithms can draw on implicit user feedback based on the surrounding links and text in a search engine result set to expand user queries with a variable number of keywords in two manners. Positive expansion adds terms to a user's keywords with a Boolean \"and,\" negative expansion adds terms to the user's keywords with a Boolean \"not.\" Each algorithm was examined for three user groups, high, middle, and low achievers, who were classified according to their overall performance. The interactions of users with different levels of expertise with different expansion types or algorithms were evaluated. The genetic algorithm with negative expansion tripled recall and doubled precision for low achievers, but high achievers displayed an opposed trend and seemed to be hindered in this condition. The effect of other conditions was less substantial.",
        "Cm-pmi: improved web-based association measure with contextual label matching. WebPMI is a popular web-based association measure to evaluate the semantic similarity between two queries (i.e. words or entities) by leveraging search results returned by search engines. This paper proposes a novel measure named CM-PMI to evaluate query similarity at a finer granularity than WebPMI, under the assumption that a query is usually associated with more than one aspect and two queries are deemed semantically related if their associated aspect sets are highly consistent with each other. CM-PMI first extracts contextual labels from search results to represent the aspects of a query, and then uses the optimal matching method to assess the consistency between the aspects of two queries. Experimental results on the benchmark Miller Charles' dataset demonstrate the good effectiveness of the proposed CM-PMI measure. Moreover, we further fuse WebPMI and CM-PMI to obtain improved results. Categories and Subject Descriptors: METHODThe study of measuring semantic similarity between words or entities has become very important for many web-related tasks, including word clustering, query reformulation and substitution, name disambiguation, community mining, etc. In recent years, web-based association measures have been well studied to evaluate the semantic similarity between two words or entities. In contrast with knowledge-based measures relying on existing knowledge databases or taxonomies (e.g. WordNet), web-based measures make use of the up-to-date web search results returned by web search engines and they can reflect the updated semantic similarity between two words or entities. Moreover, web-based measures can be successfully applied to compute the semantic similarity between new words or entities, which are usually not defined in any existing knowledge database. . WebPMI is the most popular one used today and it uses the number of hits returned by a web search engine for assessing the semantic similarity between two queries q 1 and q 2 as follows: where N is the number of web pages indexed by the search engine and c is a threshold. In this study we set N=10 10 and c=5 as in .hits() returns the hits page count for the query, estimated by the search engine.The above WebPMI measure directly computes the similarity between two queries. However, we observe that for each short query, the number of the returned search results is usually very large and the results usually contain diverse aspects about the query. For example, some results for the query \"bike\" are about bike photos, while other results are about bike components. Therefore, the search results for a query can be organized into a few subtopics about the query, each subtopic representing a specific aspect of the query. We believe that the subtopics in the search results for a query can reflect the query at a fine-grained level, while the single topic representation of the search results for a query in previous work is coarse-grained. The more the subtopic sets of two queries are consistent with each other, the more the queries are semantically similar with each other. In this study, we use a contextual label of a query to represent a subtopic for the query. A contextual label is actually a word which occurs frequently nearby the query in the search results. gives the contextual labels for synonyms \"bike\" and \"bicycle\", and the two words share many common aspects linked by the dash line. Figure 1. Contextual label matching exampleBased on the above assumption, we propose a novel association measure named CM-PMI to make full use of the aspect information of each query. The measure evaluates semantic similarity between words or entities in an indirect way. It first extracts the contextual labels in the search results for each query and then measures the consistency between the contextual label sets. The optimal matching method is employed to measure the consistency between the contextual label sets by formalizing the problem as the optimal matching problem in the graph theory. The normalized optimal matching weight is used as the semantic similarity between the queries.The CM-PMI measure consists of three steps: search results retrieval, contextual label extraction and contextual label matching.",
        "To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation. We propose a method of optimizing search result presentation for queries with diverse intents, by selectively presenting query suggestions for leading users to more relevant search results. The optimization is based on a probabilistic model of users who click on query suggestions in accordance with their intents, and modified versions of intent-aware evaluation metrics that take into account the co-occurrence between intents. Showing many query suggestions simply increases a chance to satisfy users with diverse intents in this model, while it in fact requires users to spend additional time for scanning and selecting suggestions, and may result in low satisfaction for some users. Therefore, we measured the loss of time caused by query suggestion presentation by conducting a user study in different settings, and included its negative effects in our optimization problem. Our experiments revealed that the optimization of search result presentation significantly improved that of a single ranked list, and was beneficial especially for patient users. Moreover, experimental results showed that our optimization was effective particularly when intents of a query often co-occur with a small subset of intents.",
        "A fast unified optimal route query evaluation algorithm. We investigate the problem of how to evaluate, fast and efficiently, classes of optimal route queries on a massive graph in a unified framework. To evaluate a route query effectively, a large network is partitioned into a collection of fragments, and distances of some optimal routes in the network are precomputed. Under such a setting, we find a unified algorithm that can evaluate classes of optimal route queries. The classes that can be processed efficiently are called constraint preserving (CP) which include, among others, shortest path, forbidden edges, forbidden nodes, \u03b1-autonomy and some of hypothetical weight changes optimal route query classes. We prove the correctness of the unified algorithm. We then turn our attention to the optimization of the proposed algorithm. Several pruning and optimization techniques are derived that minimize the search time and I/O accesses. We show empirically that these techniques are effective. The proposed optimal route query evaluation algorithm, with all these techniques incorporated, is compared with a mainmemory and a disk-based brute-force CP algorithms. We show experimentally that the proposed unified algorithm outperforms the brute-force algorithms, both in term of CPU time and I/O cost, by a wide margin.",
        "Joint Optimization of Cost and Coverage of Query Plans in Data Integration. Existing approaches for optimizing queries in data integration use decoupled strategies-attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.",
        "Coverage, relevance, and ranking: The impact of query operators on Web search engine results Research has reported that about 10% of Web searchers utilize advanced query operators, with the other 90% using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE (\" \"). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.",
        "Efficient and Effective Query Expansion for Web Search. Query Expansion (QE) techniques expand the user queries with additional terms, e.g., synonyms and acronyms, to enhance the system recall. State-of-the-art solutions employ machine learning methods to select the most suitable terms. However, most of them neglect the cost of processing the expanded queries, thus selecting effective, yet very expensive, terms. The goal of this paper is to enable QE in scenarios with tight time constraints proposing a QE framework based on structured queries and efficiency-aware term selection strategies. In particular, the proposed expansion selection strategies aim at capturing the efficiency and the effectiveness of the expansion candidates, as well as the dependencies among them. We evaluate our proposals by conducting an extensive experimental assessment on real-world search engine data and public TREC data. Results confirm that our approach leads to a remarkable efficiency improvement w.r.t. the state-of-the-art: a reduction of the retrieval time up to 30 times, with only a small loss of effectiveness.",
        "Framework for Web service query algebra and optimization We present a query algebra that supports optimized access of Web services through service-oriented queries. The service query algebra is defined based on a formal service model that provides a highlevel abstraction of Web services across an application domain. The algebra defines a set of algebraic operators. Algebraic service queries can be formulated using these operators. This allows users to query their desired services based on both functionality and quality. We provide the implementation of each algebraic operator. This enables the generation of Service Execution Plans (SEPs) that can be used by users to directly access services. We present an optimization algorithm by extending the Dynamic Programming (DP) approach to efficiently select the SEPs with the best user-desired quality. The experimental study validates the proposed algorithm by demonstrating significant performance improvement compared with the traditional DP approach.",
        "Towards selective user specific query expansion. Queries submitted to a search engine are usually short and ambiguous. Query expansion is an effective way to resolve the ambiguity of a query. We attempt to make a consolidated study of the effectiveness of query logs in this regard. We report empirical findings on the effect of personalization over an anonymous query log. We have designed a novel evaluation method to quantify improvements due to personalization. In the second part of our work, we present a novel approach to intelligently detect queries whose expansions potentially lead to improvements in retrieval. Empirical results obtained using this approach clearly show significant improvements over indiscriminate query expansion.",
        "Using genetic algorithms to evolve a population of topical queries. Systems for searching the Web based on thematic contexts can be built on top of a conventional search engine and benefit from the huge amount of content as well as from the functionality available through the search engine interface. The quality of the material collected by such systems is highly dependant on the vocabulary used to generate the search queries. In this scenario, selecting good query terms can be seen as an optimization problem where the objective function to be optimized is based on the effectiveness of a query to retrieve relevant material. Some characteristics of this optimization problem are: (1) the high-dimensionality of the search space, where candidate solutions are queries and each term corresponds to a different dimension, (2) the existence of acceptable suboptimal solutions, (3) the possibility of finding multiple solutions, and in many cases (4) the quest for novelty. This article describes optimization techniques based on Genetic Algorithms to evolve ''good query terms\" in the context of a given topic. The proposed techniques place emphasis on searching for novel material that is related to the search context. We discuss the use of a mutation pool to allow the generation of queries with new terms, study the effect of different mutation rates on the exploration of query-space, and discuss the use of a especially developed fitness function that favors the construction of queries containing novel but related terms.",
        "How Foreign Function Integration Conquers Heterogeneous Query Processing. With the emergence of application systems which encapsulate databases and related application components, pure data integration using, for example, a federated database system is not possible anymore. Instead, access via predefined functions is the only way to get data from an application system. As a result, retrieval of such heterogeneous and encapsulated data sources needs the combination of generic query as well as predefined function access. In this paper, we present a middleware approach supporting such a novel and extended kind of integration. Starting with the overall architecture, we explain the functionality and cooperation of its core components: a federated database system and a workflow management system connected via a wrapper. Afterwards, we concentrate on essential aspects of query processing across these heterogeneous components focusing on the impact of the functions included. We discuss the operations the wrapper should provide in order to extend the workflow system's native functionality. In addition to selection and projection, these operations could include aggregation and the support of subqueries. Moreover, we point out modifications to the traditional cost model needed to consider the cost estimates for the function calls as well. KeywordsFederated database system, workflow management system, function integration, wrapper, heterogeneous query processing, cost model. MOTIVATIONMost enterprises have to cope with heterogeneous system environments where different network and operating systems, database systems (DBSs), as well as applications are used to cover the whole life cycle of a product. Initial approaches primarily focusing on problems of data heterogeneity were federated database systems (FDBSs) and multidatabase systems. So there exist adequate solutions for database integration even if there are Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the Ml citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. But the database environment is changing now. While many enterprises had selected \"their\" DBS and designed their tailored DB schema in the past, they are now confronted with databases being delivered within packaged software. In such cases, the database system and the related application are integrated, and an application programming interface, the so-called API, is the only way to access the data. Thus, a (generic) database interface is not supported anymore. In the following, we call systems realizing such an encapsulation concept application systems. One of the most frequently used application systems is, for example, SAP R/3 [ 131, whose data can be accessed via predefined functions only. The same characteristics can be found in proprietary software solutions implemented by the enterprises.As a consequence, pure data integration is not possible, since \"traditional\" DBSs have to be accessed using a generic query language (SQL) whereas application systems only provide data access via predefined functions. Instead, a combined approach of data and function access has to be achieved. Such scenarios can be encountered in many practical and/or legacy applications.We consider an FDBS as an effective integration platform, since it provides a powerful declarative query language. Furthermore, it offers a large set of numerical processing functions as well as a broad range of scalability. Many applications are SQL-based to take full advantage of these properties. A query involving both databases and application systems includes SQL predicates as well as some kind of foreign function access. According to SQL99[7] such a reference may occur as a function or as a table. In our view, the most important case is the reference to a function as a table, strictly considered as an abstract table.To implement such an extended kind of integration, we have developed an integration architecture consisting of two key components: an FDBS and a workflow management system (W&IS). The FDBS is responsible for the integration of data whereas the WfMS is used to implement a kind of function integration. As a result, the WfMS provides so-called federated functions which are made available to the FDBS. Obviously, efficient query processing requires that these two components work together very closely. Such heterogeneous query processing reveals interesting aspects, since two completely different models a data model and a function model -must be able to communicate and to work together.In the remainder of this paper, we discuss basic questions about a smooth cooperation of these two components and we examine 215 various implementation aspects of heterogeneous processing. For this purpose, we introduce our integration architecture in Sect. 2 by depicting the structure and the participating systems. Since the connection of FDBS and WfMS based on a wrapper is a fundamental part of our architecture, we describe it in detail in Sect. 3. Moreover, we consider requirements on its functionality and outline its support for heterogeneous query processing in Sect. 4. The impact on the cost model is shown in Sect. 5. Finally, we briefly review related work and summarize our ideas. OVERALL ARCHITECTUREThe goal of our three-tier integration architecture is to enable the applications to transparently access heterogeneous data sources, no matter if they can be accessed by means of SQL or functions (see . Applications accessing the integrated data sources comprise the upper tier, and the heterogeneous sources represent the bottom tier. Due to space limitations, we focus on the middle tier, the so-called integration server, which consists of two key components: an FDBS achieving the data integration and a WfMS which realizes a kind of function integration by invoking and controlling the access to predefined functions. In our terms, function integration means to provide federated functions combining functionality of one or more local functions .In principle, specialized wrappers could be used to access each of the local functions typically supplied by different applications systems. These functions are frequently called together in a way where the output data of a function call is the input data of a subsequent function call. The execution of the single functions could be directly controlled by the FDBS. However, such an approach would require substantial extensions of the FDBS components in addition to the writing of the specialized wrappers. Furthermore, the FDBS had to cope with the different application systems and their local functions which could be distributed, heterogeneous, and autonomous. Our key idea is to use a workflow for the execution of a federated function where its activities embody the local function calls and where the WfMS controls the parameter transfer together with the precedence structure among the local function calls. Then, a unified wrapper can be used to isolate the FDBS from the intricacies of the federated function execution and to bridge to the WfMS thereby supplying missing functionality (glue) and making various query optimization options available.The mapping from federated to local functions is guarded by a precedence graph and it typically consists of a sequence of function calls observing the specific dependencies between the local functions. As a key concept of our approach, we use a WfIvlS as the engine processing such a graph-based mapping .The workflow to be executed is a production workflow representing a highly automated process [lo]. We decided to use a WfMS as a generic vehicle to obtain function access and, at the same time, to make such accesses transparent for the FDBS. Using such an approach, we can fall back on existing technology incorporated in commercially available products that supports complex mapping scenarios and transparent access to heterogeneous platforms. Moreover, it provides interfaces to the application systems to be integrated and can cope with different kinds of error handling, thereby facilitating distributed program execution across heterogeneous applications. Another important point is the fact that the solution is easier to adapt when the environment has to be changed.FDBS and WfMS are connected by an interface realized by means of a wrapper according to the draft of SQLhJED (Database Languages -SQL -Part 9: Management of External Data, [S]). Actually, this wrapper consists of two parts: one part is located at the FDBS side serving the SQL/MED interface at the FDBS. The other part should be placed at the WfMS side wrapping the WfMS interface. Thus, communication between FDBS and WfMS is basically realized between these two wrapper parts. As a result, the WfMS provides so-called federated functions used by the FDBS to process queries across multiple external data sources.The applications (users) can access the integration server via an object-relational interface connecting them to the FDBS. The FDBS's query processor evaluates the user queries and those parts requiring foreign function access are handed over to the wrapper which activates the WfMS. The workflow engine performs the function integration by calling the local functions of the referenced application systems as specified in the predefined workflow process. The wrapper returns the result back to the FDBS where it is mapped to an abstract table. The remaining parts of the user query are processed by the FDBS, i.e., the query is divided into the appropriate SQL subqueries for the SQL sources. Eventually, the subquery results are further processed by the query processor, if necessary, and merged to the final result.All issues of query processing in the FDBS are well explored [11]. Therefore, we will focus on the new aspects of our architecture. How can we smoothly integrate calls of predefined functions into the overall query processing of an FDBS? For this purpose, we have a deeper look into the wrapper-based connection between FDBS and WfMS and its realization in the next section. HOW FUNCTIONS STEP INTO THE RELATIONAL WORLDBefore discussing how to handle functions within heterogeneous query processing, we address the key problem of function evaluation and how to map functions to tables. Moreover, we point out the requirements on the wrapper's functionality. Mapping Functions to TablesWhen mapping functions to tables, we have to distinguish between structural and semantic aspects. The structural mapping defines the way how to represent a function's signature in the form of a table so that it can be referenced in SQL queries. Semantic mapping, on the other hand, considers the semantics of the functionality implemented by a specific function and its mapping to function references in the form of abstract tables 216 expressed by SQL queries. Unfortunately, this information cannot be derived automatically from a function's signature. Most functions to be integrated are provided by legacy systems which support a hierarchical view on their data. In most cases, the input parameter values are used as input for predicates processing an \"equals\" operation like \"Give me the numbers of all departments with a budget of X dollars\": SELECT deptno FROM departments WHERE budget = X. Such legacy system functionality is rather fixed and, therefore, inadequate for the support of SQL queries using, for example, range conditions like SELECT deptno FROM departments WHERE budget < X. Obviously, the semantics of \"<\" has to be reproduced by multiple compensating function calls.' Hence, future systems should provide more flexible APIs allowing, for instance, to pass in addition to input values the related (comparison) operator as parameter. In that case, the expressiveness of SQL queries could be more adequately simulated by function calls.Next, we will describe our way of structurally mapping functions to tables. Structural MappingRealizing a structural mapping by a wrapper, we have to bridge the conceptual differences between FDBS and WfMS. Proceeding this way, the mapping is absolutely transparent to (external) users or applications. Neither they have to pay attention to restrictions nor they have to learn a particular use of SQL when accessing abstract tables. Needed Wrapper FunctionalityFirst, we take a look at the tasks the wrapper must fulfil in order to provide for a relational FDBS access to a WfMS. In order to achieve this functionality, the wrapper must mediate between both systems and map the functional model to the data model, as well ' In this paper, we will not discuss synchronization issues to be performed by the application systems, if repeatable results are required.as provide support for abstract table queues returning the result to the FDBS.From the FDBS side, it must be able to accept a partial quet$ regarding data access to the abstract tables. This query may include operators such as projection and selection using comparison predicates.Based on its knowledge about the abstract table representations, the wrapper prepares a local access plan. In the first place, such a local access plan consists of calls to WfMS process instances. Additionally, it may include wrapper-internal operations for requested functionality not natively supported by the W&IS. As usual, the optimization target is to push down as many predicates as possible to the WfMS.During the execution of a local access plan, the wrapper invokes functions embodied by WfMS process instances. For each call, the wrapper derives input values from the query and receives result values from the WfMS. The input/output value pairs of each WfMS function can be processed by further operations. Finally, the query result set is returned to the FDBS query execution engine as an abstract table queue.In addition to these basic tasks, the wrapper may be extended supporting further operations. We group these functions in core, base, and extended wrapper services. As our rationale, we follow the idea of realizing as much optimization effect with as little implementation effort as possible. Afterwards, we explain why particular operations should not be addressed by the wrapper, but left to the FDBS instead.Core functionality consists of all functions needed to map between abstract tables and WfMS function calls. If no additional functionality is available, each reference to an abstract table in an SQL query has to be realized by materializing and delivering the complete foreign table. For this reason, wrappers to be integrated into DBMS or FDBS processing should allow for basic optimization options, that is, they should enable push-down of selection and projection operations in the first place.Hence, base functionality on abstract tables should include projection to a subset of columns and selection of a subset of rows using any boolean combination of comparison predicates of columns with constants or list of constants. Such comparison predicates contain an operator 0 E {=, f, <, 5, >, 2).Core and base functionality could be extended by more advanced query execution operations such as grouping and aggregation, subqueries regarding abstract tables, and set comparison. Furthermore, the join of two abstract tables might be implemented within the wrapper in order to minimize communication caused by wrapper calls of the FDBS. On the other hand, join operators have been implemented in FDBSs in very efficient ways. Therefore, it should be critically evaluated whether a reimplementation of join functionality is justified.Because wrappers receive only queries regarding abstract tables, there is no need to implement any functions beyond a subselect (such as union, except, intersect). Separate subselects are usually represented by independent queries to be combined by the FDBS.' In the following, we denote a partial query delegated to the wrapper as query for short. 217Subqueries regarding FDBS-managed tables should not be executed by the wrapper, since such functions can be handled better by the FDBS.Transfer of the result set is performed via an abstract table queue which may be achieved in a pipeline mode depending on the availability of result tuples or in a block mode at the end of the function call. Furthermore, a result set returned by a function invocation may have some useful characteristics important for the overall query optimization. For example, a result contains an interesting sort order. Such information should be passed as a hint to the FDBS query processor to avoid further sort operations. QUERY PROCESSING ACROSS FDBS AND WfMSSo far, we have described how FDBS and WfMS are connected.In the following, we focus on heterogeneous query processing performed by these systems. First, we introduce some terms and definitions describing the native functionality supported by the workflow engine. Afterwards, we focus on the realization of the operations defined as base and extended functionality within the wrapper. Since optimization aspects like pushing down operations and reducing the amount of data shipped between FDBS and WfMS have been sufficiently analyzed in the past [ 161, we will not elaborate on them. Terms and DefinitionsIn the following, we describe our forma1 representation of the query processing. It is based on the relational algebra extended by two additional operators. Hence, a function call is equivalent to a selection on relation R specified by predicates based on the values of the input parameters. In addition, there is a projection on the output parameters. Since this term exactly describes the functionality of our WfMS, we introduce a new operator cp representing the workflow functionality:In order to be able to map a SELECT * functionality, i.e. the projection on all attributes rc,,o, the input parameters have to be concatenated to the corresponding tuples of the result set returned by the WfMS. This operation is described by the operator K:Based on these definitions, we will now discuss the functionality which should be made available by the wrapper in order to extend the native query support provided by the WfMS. Base FunctionalityIn our view, the combined functionality of wrapper and WfMS should at least contain selection and projection in order to reduce data shipping between FDBS and WfMS. Therefore, the wrapper has to extend the WfMS core functionality by additional functions. In the following, we discuss the realization of such a functionality and its specific characteristics. I SelectionBefore describing the selection operation in detail, we present some prerequisites which are essential for our considerations. Afterwards, we differentiate between selections on input parameters and those on output parameters. PrerequisitesAs described above, the values for the input parameters have to be derived from the predicates in the WHERE clause of the SQL query to be evaluated. Only if the WHERE clause specifies a comparison predicate (containing an equals operator) for each single input parameter, i.e. q(R), the result set can be captured by a single function invocation. In any other case, additional function calls have to be initiated to compensate the missing input parameter values; in the extreme case, all possible values of a missing input parameter have to be supplied. In order to perform such substitutions, that is, to simulate the original query by additional function calls, some restrictions on the value count as well as the value set of an input parameter have to be applied:1. The value count V,(R,, i) of each input parameter of an abstract table R, has to be finite.2. It must be possible to enumerate the elements of its value set V,(R,, 0. Otherwise, the number of compensating function calls cannot be determined and, consequently, the query must be rejected. Selection Based on Input ParametersAssume the comparison operator specified in the predicate matches the operator implemented by the function and all required input values are given by constant predicates. Then, the selection is defined as follows causing exactly one function call: 01 CR)= KI b(R)) As described above, the operation o,(R) is processed by calling exactly one function. If one input parameter value Vi is missing, V,(R, i) function calls must be executed to go through all possible combinations of the given values and the missing input parameter value. This number of calls is multiplied with the value count of each unspecified input parameter value. Consequently, the operation o,,(R) with only a single input parameter value causes the following number of function calls:Thus, if I={i,, . . . . i,} and we want to process o+,,(R), we get the union of the result sets of the required function calls:In this equation, n describes the number of function calls needed to compute the entire result set and corresponds to the number derived above based on the value counts. 218Similar aspects have to be considered, if the comparison operator of a predicate does not match the operator implemented by the function. For instance, a function f(i, o) computes the predicate (i = constant) whereas the SQL statement contains the predicate fi I constant). This case also results in multiple function calls to derive the anticipated result. Again, the restrictions described above have to be applied to enable the derivation of the required function calls. In our example, a function with fi = constant) and further functions for every single element of the value set of i less than the specified constant value must be processed. Consequently, the union of the result sets of several selections has to be derived as shown in the following example (provided that V,(R, i) = { 1,2, 3, . . . . IO}): Selection Based on Output ParametersConsidering predicates including output parameters, we again identify two different cases. First, we suppose that all required input values are specified by predicates in the SQL statement. Then, the selection has to be applied on the result set returned by the function call:If, on the other hand, the predicates are based on output parameters only, all combinations of input parameter values have to be determined for the corresponding functions. Based on the union of the returned result sets, the selection on the output parameter can be processed. Obviously, this case can lead to an enormous number of function calls. ProjectionRegarding projection we have to consider three different cases: projection on a) input parameters only, b) output parameters only, and c) both types of parameters.When applied on input parameters, projection can be realized without calling any function in some cases. For that, the wrapper has to check if the specified input values are included in the corresponding value set. If not, the result set is 0 and no function call is needed. Moreover, if the function's signature contains only one input parameter, the result of n;(R) is i, if the value specified is part of V,(R, i), derived without any function call. In any other case, the wrapper must initiate the required functions and add the input values to the result set. If there is a projection on a subset of the input parameters, the wrapper has to cut off the parameter values not needed.Looking at projection on output values, i.e. no(R), it is obvious that it matches exactly the native functionality cp(R) of the WfMS. Consequently, the result set of the WfMS can be passed on to the FDBS. If there is a projection on the subset of the output parameters, the wrapper has to cut off the other ones by applying the operation ~~(cp(R)).Finally, input as well as output parameters are projected. The general case of projecting on all parameters (SELECT *) is described in Sect. 4.1. In any other case, the projection on subsets of input and output parameters is the concatenation of the results processed for each parameter type as described above. For instance, the SQL statement",
        "Fast Evaluation of Structured Queries for Information Retrieval. Information retrieval systems are being challenged to manage larger and larger document collections.In an effort to provide better retrieval performance on large collections, more sophisticated retrieval techniques have been developed that support rich, structured queries.Structured queries are not amenable to previously proposed optimization techniques. Optimizing execution, however, is even more important in the context of large document collections. We present a new structured query optimization technique which we have implemented in an inference network-based information retrieval system. Experimental results show that query evaluation time can be reduced by more than half with little impact on retrieval effectiveness.",
        "Query execution timing: taming real-time anytime queries on multicore processors. Answering real-time queries, especially over probabilistic data, is becoming increasingly important for service providers. We study anytime query processing algorithms, and extend the traditional query execution plan with a timing component. Our focus is how to determine this timing component, given the queries' deadline constraints. We consider the common multicore processors. Specifically, we propose two query optimization modes: offline periodic optimization and online optimization. We devise efficient algorithms for both offline and online cases followed by a competitive analysis to show the power of our online optimization. Finally, we perform a systematic experimental evaluation using realworld datasets to verify our approaches.",
        "Query optimization using restructured views. We study optimization of relational queries using materialized views, where views may be regular or restructured. In a restructured view, some data from the base table(s) are represented as metadata -that is, schema information, such as table and attribute names -or vice versa.Using restructured views in query optimization opens up a new spectrum of views that were not previously available, and can result in significant additional savings in queryevaluation costs. These savings can be obtained due to a significantly larger set of views to choose from, and may involve reduced table sizes, elimination of self-joins, clustering produced by restructuring, and horizontal partitioning.In this paper we propose a general query-optimization framework that treats regular and restructured views in a uniform manner and is applicable to SQL select-project-join queries and views with or without aggregation. Within the framework we provide (1) algorithms to determine when a view (regular or restructured) is usable in answering a query, and (2) algorithms to rewrite a query using usable views.Semantic information, such as knowledge of the key of a view, can be used to further optimize a rewritten query. Within our general query-optimization framework, we develop techniques for determining the key of a (regular or restructured) view, and show how this information can be used to further optimize a rewritten query. It is straightforward to integrate all our algorithms and techniques into standard query-optimization algorithms.",
        "H-DB: a hybrid quantitative-structural sql optimizer. Structural decomposition methods are query optimization methods specifically conceived in the database theory community to efficiently answer (near-)acyclic queries. We propose to demonstrate H-DB, an SQL query optimizer that combines classical quantitative optimization techniques with such structural decomposition methods, which so far have been just analyzed from the theoretical viewpoint. The system provides support to optimizing SQL queries with arbitrary output variables, aggregate operators, ORDER BY statements, and nested queries. H-DB can be put on top of any existing database management system supporting JDBC technology, by transparently interacting/replacing its standard query optimization module. However, to push at maximum its optimization capabilities, H-DB should be coupled with an ad-hoc physical semi-join operator, which (as a relevant example) we implemented and integrated within the PostgreSQL database management system.",
        "Pruning nested XQuery queries. We present in this paper an approach for XQuery optimization that exploits minimization opportunities raised in composition-style nesting of queries. More precisely, we consider the simplification of XQuery queries in which the intermediate result constructed by a subexpression is queried by another subexpression. Based on a large subset of XQuery, we describe a rule-based algorithm that recursively prunes query expressions, eliminating useless intermediate results. Our algorithm takes as input an XQuery expression that may have navigation within its subexpressions and outputs a simplified, equivalent XQuery expression, and is thus readily usable as an optimization module in any existing XQuery processor. We demonstrate by experiments the impact of our rewriting approach on query evaluation costs and we prove formally its correctness.",
        "Selectivity-based partitioning: a divide-and-union paradigm for effective query optimization. Modern query optimizers select an efficient join ordering for a physical execution plan based essentially on the average join selectivity factors among the referenced tables. In this paper, we argue that this \"monolithic\" approach can miss important opportunities for the effective optimization of relational queries. We propose selectivitybased partitioning, a novel optimization paradigm that takes into account the join correlations among relation fragments in order to essentially enable multiple (and more effective) join orders for the evaluation of a single query. In a nutshell, the basic idea is to carefully partition a relation according to the selectivities of the join operations, and subsequently rewrite the query as a union of constituent queries over the computed partitions. We provide a formal definition of the related optimization problem and derive properties that characterize the set of optimal solutions. Based on our analysis, we develop a heuristic algorithm for computing efficiently an effective partitioning of the input query. Results from a preliminary experimental study verify the effectiveness of the proposed approach and demonstrate its potential as an effective optimization technique.",
        "WassRank: Listwise Document Ranking Using Optimal Transport Theory. Learning to rank has been intensively studied and has shown great value in many fields, such as web search, question answering and recommender systems. This paper focuses on listwise document ranking, where all documents associated with the same query in the training data are used as the input. We propose a novel ranking method, referred to as WassRank, under which the problem of listwise document ranking boils down to the task of learning the optimal ranking function that achieves the minimum Wasserstein distance. Specifically, given the query level predictions and the ground truth labels, we first map them into two probability vectors. Analogous to the optimal transport problem, we view each probability vector as a pile of relevance mass with peaks indicating higher relevance. The listwise ranking loss is formulated as the minimum cost (the Wasserstein distance) of transporting (or reshaping) the pile of predicted relevance mass so that it matches the pile of ground-truth relevance mass. The smaller the Wasserstein distance is, the closer the prediction gets to the ground-truth. To better capture the inherent relevance-based order information among documents with different relevance labels and lower the variance of predictions for documents with the same relevance label, ranking-specific cost matrix is imposed. To validate the effectiveness of WassRank, we conduct a series of experiments on two benchmark collections. The experimental results demonstrate that: compared with four non-trivial listwise ranking methods (i.e., LambdaRank, ListNet, ListMLE and ApxNDCG), WassRank can achieve substantially improved performance in terms of nDCG and ERR across different rank positions. Specifically, the maximum improvements of WassRank over LambdaRank, ListNet, ListMLE and ApxNDCG in terms of nDCG@1 are 15%, 5%, 7%, 5%, respectively.",
        "Optimal rare query suggestion with implicit user feedback. Query suggestion has been an effective approach to help users narrow down to the information they need. However, most of existing studies focused on only popular/head queries. Since rare queries possess much less information (e.g., clicks) than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. In this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant. However, we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently. Hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. Our model specifically optimizes two parameters: (1) the restarting (jumping) rate of random walk, and (2) the combination ratio of click and skip information. Unlike the Rocchio algorithm, our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs. Consequently, our model is capable of scaling up to the need of commercial search engines. Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo-relevance feedback models."
    ],
    " Cosine similarity vector ": [
        "An Optimization Framework for Propagation of Query-Document Features by Query Similarity Functions. It is well known that a great number of query-document features which significantly improve the quality of ranking for popular queries, however, do not provide any benefit for new or rare queries since there is typically not enough data associated with those queries that is required to reliably compute the values of those features. It is a common practice to propagate the values of such features from popular to tail queries, if the queries are similar according to some predefined query similarity functions. In this paper, we propose new algorithms that facilitate and increase the effectiveness of this propagation. Given a query similarity function and a query-document relevance feature, we introduce two different approaches (linear weighting approach and tree-based approach) to learn a function of values of the similarity function and values of the feature for the similar queries w.r.t. the given document. The propagated value of the feature equals the value of the obtained function for the given query-document pair. For the purpose of finding the most effective method of propagating query-document features, we measure the effectiveness of different approaches to features propagation by performing experiments on a large dataset of labeled queries.",
        "Context modeling and discovery using vector space bases. In this paper, context is modeled by vector space bases and its evolution is modeled by linear transformations from one base to another. Each document or query can be associated to a distinct base, which corresponds to one context. Also, algorithms are proposed to discover contexts from document, query or groups or them. Linear algebra can thus by employed in a mathematical framework to process context, its evolution and application.",
        "A Tweets Classifier based on Cosine Similarity The 2017 Microblog Cultural Contextualization task consists in three challenges: (1) Content Analysis, (2) Microblog search, and (3) TimeLine illustration. This paper describes the use of cosine similarity, which is characterized by the comparison of similarity between two vectors of an inner product space. This research used two approaches: (1) word2vec and (2) Bag-of-Words (BoW) for extracting all relevant tweets to each event related to the four festivals: Charrues, Transmusicales, Avignon and Edinburgh.",
        "A Similarity-based Probability Model for Latent Semantic Indexing. A dual probability model is constructed for the Latent Semantic Indexing LSI using the cosine similarity measure. Both the document-document similarity matrix and the term-term similarity matrix naturally arise from the maximum likelihood estimation of the model parameters, and the optimal solutions are the latent semantic vectors of of LSI. Dimensionality reduction is justi ed by the statistical signi cance of latent semantic vectors as measured by the likelihood of the model. This leads to a statistical criterion for the optimal semantic dimensions, answering a critical open question in LSI with practical importance. Thus the model establishes a statistical framework for LSI. Ambiguities related to statistical modeling of LSI are clari ed.",
        "Patent Map Generation Using Concept-Based Vector Space Model This paper proposes a patent map generation system using concept-based vector space model and presents evaluation results from the NTCIR-4 patent feasibility study (FS) task. The concept-base is a knowledge base of words, which expresses each word as an associated vector. The word vectors are computed based on word co-occurrence in a target document set. Therefore, the word vectors reflect target documents' characteristics. Each document in the target document set is expressed as a vector that is composed of vectors associated with words included in the document. The word vectors and document vectors are positioned in an identical vector space and the relevant degree of similarity between any two words and/or documents can be computed as a cosine coefficient of the two vectors. Taking advantage of this model, problems sections and solutions sections of patent documents are expressed as vectors, then, they are clustered and the label word for each cluster is chosen from words which give high cosine coefficient to the center of gravity of the cluster. A trial of generating patent maps for NTCIR-4 patent FS task topics using the system has been done. Comparing with humangenerated patent maps, the system provides fairly good accuracy of clustering of target patents but poor accuracy of cluster labeling.",
        "Learning query and document similarities from click-through bipartite graph with metadata. We consider learning query and document similarities from a clickthrough bipartite graph with metadata on the nodes. The metadata contains multiple types of features of queries and documents. We aim to leverage both the click-through bipartite graph and the features to learn query-document, document-document, and queryquery similarities. The challenges include how to model and learn the similarity functions based on the graph data.We propose solving the problems in a principled way. Specifically, we use two different linear mappings to project the queries and documents in two different feature spaces into the same latent space, and take the dot product in the latent space as their similarity. Query-query and document-document similarities can also be naturally defined as dot products in the latent space. We formalize the learning of similarity functions as learning of the mappings that maximize the similarities of the observed query-document pairs on the enriched click-through bipartite graph. When queries and documents have multiple types of features, the similarity function is defined as a linear combination of multiple similarity functions, each based on one type of features. We further solve the learning problem by using a new technique called Multi-view Partial Least Squares (M-PLS). The advantages include the global optimum which can be obtained through Singular Value Decomposition (SVD) and the capability of finding high quality similar queries. We conducted large scale experiments on enterprise search data and web search data. The experimental results on relevance ranking and similar query finding demonstrate that the proposed method works significantly better than the baseline methods.",
        "Learning Latent Vector Spaces for Product Search",
        "A weighted tag similarity measure based on a collaborative weight model. The problem of measuring semantic relatedness between social tags remains largely open. Given the structure of social bookmarking systems, similarity measures need to be addressed from a social bookmarking systems perspective. We address the fundamental problem of weight model for tags over which every similarity measure is based. We propose a weight model for tagging systems that considers the user dimension unlike existing measures based on tag frequency. Visual analysis of tag clouds depicts that the proposed model provides intuitively better scores for weights than tag frequency. We also propose weighted similarity model that is conceptually different from the contemporary frequency based similarity measures. Based on the weighted similarity model, we present weighted variations of several existing measures like Dice and Cosine similarity measures. We evaluate the proposed similarity model using Spearman's correlation coefficient, with WordNet as the gold standard. Our method achieves 20% improvement over the traditional similarity measures like dice and cosine similarity and also over the most recent tag similarity measures like mutual information with distributional aggregation. Finally, we show the practical effectiveness of the proposed weighted similarity measures by performing search over tagged documents using Social SimRank over a large real world dataset.",
        "An Analysis of Vector Space Models Based on Computational Geometry. This paper analyzes the properties, structures and limitations of vector-based models for information retrieval from the computational geometry point of view. It is shown that both the pseudo-cosine and the standard vector space models can be viewed as special cases of a generalized linear model. More importantly, both the necessary and sufficient conditions have been identified, under which ranking functions such as the inner-product, cosine, pseudo-cosine, Dice, covariance and productmoment correlation measures can be used to rank the documents. The structure of the solution region for acceptable ranking is analyzed and an algorithm for vectors is suggested.",
        "Learning similarity measures in non-orthogonal space. Many machine learning and data mining algorithms crucially rely on the similarity metrics. The Cosine similarity, which calculates the inner product of two normalized feature vectors, is one of the most commonly used similarity measures. However, in many practical tasks such as text categorization and document clustering, the Cosine similarity is calculated under the assumption that the input space is an orthogonal space which usually could not be satisfied due to synonymy and polysemy. Various algorithms such as Latent Semantic Indexing (LSI) were used to solve this problem by projecting the original data into an orthogonal space. However LSI also suffered from the high computational cost and data sparseness. These shortcomings led to increases in computation time and storage requirements for large scale realistic data. In this paper, we propose a novel and effective similarity metric in the non-orthogonal input space. The basic idea of our proposed metric is that the similarity of features should affect the similarity of objects, and vice versa. A novel iterative algorithm for computing non-orthogonal space similarity measures is then proposed. Experimental results on a synthetic data set, a real MSN search click-thru logs, and 20NG dataset show that our algorithm outperforms the traditional Cosine similarity and is superior to LSI. *This work conducted at Microsoft Research Asia.",
        "A Measure Based on Optimal Matching in Graph Theory for Document Similarity. Measuring pairwise document similarity is critical to various text retrieval and mining tasks. The most popular measure for document similarity is the Cosine measure in Vector Space Model. In this paper, we propose a new similarity measure based on optimal matching in graph theory. The proposed measure takes into account the structural information of a document by considering the word distributions over different text segments. It first calculates the similarities for different pairs of text segments in the documents and then gets the total similarity between the documents optimally through optimal matching. We set up experiments of document similarity search to test the effectiveness of the proposed measure. The experimental results and user study demonstrate that the proposed measure outperforms the most popular Cosine measure."
    ],
    " reverse indexing": [
        "Optimal Space-time Tradeoffs for Inverted Indexes. Inverted indexes are usually represented by dividing posting lists into constant-sized blocks and representing them with an encoder for sequences of integers. Different encoders yield a different point in the space-time trade-off curve, with the fastest being several times larger than the most space-efficient. An important design decision for an index is thus the choice of the fastest encoding method such that the index fits in the available memory.However, a better usage of the space budget could be obtained by using faster encoders for frequently accessed blocks, and more space-efficient ones those that are rarely accessed. To perform this choice optimally, we introduce a linear time algorithm that, given a query distribution and a set of encoders, selects the best encoder for each index block to obtain the lowest expected query processing time respecting a given space constraint.To demonstrate the effectiveness of this approach we perform an extensive experimental analysis, which shows that our algorithm produces indexes which are significantly faster than single-encoder indexes under several query processing strategies, while respecting the same space constraints.",
        "Compressing Inverted Indexes with Recursive Graph Bisection: A Reproducibility Study. Document reordering is an important but often overlooked preprocessing stage in index construction. Reordering document identifiers in graphs and inverted indexes has been shown to reduce storage costs and improve processing efficiency in the resulting indexes. However, surprisingly few document reordering algorithms are publicly available despite their importance. A new reordering algorithm derived from recursive graph bisection was recently proposed by Dhulipala et al., and shown to be highly effective and efficient when compared against other state-ofthe-art reordering strategies. In this work, we present a reproducibility study of this new algorithm. We describe the implementation challenges encountered, and explore the performance characteristics of our cleanroom reimplementation. We show that we are able to successfully reproduce the core results of the original paper, and show that the algorithm generalizes to other collections and indexing frameworks. Furthermore, we make our implementation publicly available to help promote further research in this space.",
        "Heavy-tailed distributions and multi-keyword queries. Intersecting inverted indexes is a fundamental operation for many applications in information retrieval and databases. Efficient indexing for this operation is known to be a hard problem for arbitrary data distributions. However, text corpora used in Information Retrieval applications often have convenient power-law constraints (also known as Zipf's Law and long tails) that allow us to materialize carefully chosen combinations of multi-keyword indexes, which significantly improve worst-case performance without requiring excessive storage. These multi-keyword indexes limit the number of postings accessed when computing arbitrary index intersections. Our evaluation on an e-commerce collection of 20 million products shows that the indexes of up to four arbitrary keywords can be intersected while accessing less than 20% of the postings in the largest single-keyword index.",
        "The Feasibility of Brute Force Scans for Real-Time Tweet Search. The real-time search problem requires making ingested documents immediately searchable, which presents architectural challenges for systems built around inverted indexing. In this paper, we explore a radical proposition: What if we abandon document inversion and instead adopt an architecture based on brute force scans of document representations? In such a design, \"indexing\" simply involves appending the parsed representation of an ingested document to an existing buffer, which is simple and fast. Quite surprisingly, experiments with TREC Microblog test collections show that query evaluation with brute force scans is feasible and performance compares favorably to a traditional search architecture based on an inverted index, especially if we take advantage of vectorized SIMD instructions and multiple cores in modern processor architectures. We believe that such a novel design is worth further exploration by IR researchers and practitioners.",
        "Fast Dictionary-Based Compression for Inverted Indexes. Dictionary-based compression schemes provide fast decoding operation, typically at the expense of reduced compression effectiveness compared to statistical or probability-based approaches. In this work, we apply dictionary-based techniques to the compression of inverted lists, showing that the high degree of regularity that these integer sequences exhibit is a good match for certain types of dictionary methods, and that an important new trade-off balance between compression effectiveness and compression efficiency can be achieved. Our observations are supported by experiments using the document-level inverted index data for two large text collections, and a wide range of other index compression implementations as reference points. Those experiments demonstrate that the gap between efficiency and effectiveness can be substantially narrowed.",
        "Supporting sub-document updates and queries in an inverted index. Inverted indexes have become the standard indexing method for supporting search queries in a variety of content-based applications. Examples of such applications include enterprise document management, e-mail, web search, and social networks. One shortcoming in current inverted index designs is that they support only document-level updates, forcing a full document to be reindexed even if just part of it changes. This paper describes a new inverted index design that enables applications to break a document into semantically meaningful sub-documents or \"sections\". Each section of a document can be updated separately, but search queries can still work seamlessly across sections. Our index design is motivated by applications where there is metadata associated with each document that tends to be smaller and more frequently updated than the document's content, but at the same time, it is desireable to search the metadata and content with the same index structure. A novel self-optimizing query execution algorithm is described to efficiently join the sections of a document in the inverted index. Experimental results on TREC and patent data are provided, showing that sections can dramatically improve overall system throughput on a mixed workload of updates and queries.",
        "Permutation indexing: fast approximate retrieval from large corpora. Inverted indexing is a ubiquitous technique used in retrieval systems including web search. Despite its popularity, it has a drawback -query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique, permutation indexing, where retrieval cost is strictly bounded and has only logarithmic dependence on the corpus size. Our approach is based on two novel techniques: (a) partitioning of the term space into overlapping clusters of terms that frequently co-occur in queries, and (b) a data structure for compactly encoding results of all queries composed of terms in a cluster as continuous sequences of document ids. Then, query results are retrieved by fetching few small chunks of these sequences. There is a price though: our encoding is lossy and thus returns approximate result sets. The fraction of the true results returned, recall, is controlled by the level of redundancy. The more space is allocated for the permutation index the higher is the recall. We analyze permutation indexing both theoretically under simplified document and query models, and empirically on a realistic document and query collections. We show that although permutation indexing can not replace traditional retrieval methods, since high recall cannot be guaranteed on all queries, it covers up to 77% of tail queries and can be used to speed up retrieval for these queries.",
        "Q+Tree: An Efficient Quad Tree based Data Indexing for Parallelizing Dynamic and Reverse Skylines",
        "Monochromatic and bichromatic ranked reverse boolean spatial keyword nearest neighbors search. Recently, Reverse k Nearest Neighbors (RkNN) queries, returning every answer for which the query is one of its k nearest neighbors, have been extensively studied on the database research community. But the RkNN query cannot retrieve spatio-textual objects which are described by their spatial location and a set of keywords. Therefore, researchers proposed a RSTkNN query to find these objects, taking both spatial and textual similarity into consideration. However, the RSTkNN query cannot control the size of answer set and to be sorted according to the degree of influence on the query. In this paper, we propose a new problem Ranked Reverse Boolean Spatial Keyword Nearest Neighbors query called Ranked-RBSKNN query, which considers both spatial similarity and textual relevance, and returns t answers with most degree of influence. We propose a separate index and a hybrid index to process such queries efficiently. Experimental results on different real-world and synthetic datasets show that our approaches achieve better performance.",
        "Dynamic maintenance of web indexes using landmarks. Recent work on incremental crawling has enabled the indexed document collection of a search engine to be more synchronized with the changing World Wide Web. However, this synchronized collection is not immediately searchable, because the keyword index is rebuilt from scratch less frequently than the collection can be refreshed. An inverted index is usually used to index documents crawled from the web. Complete index rebuild at high frequency is expensive. Previous work on incremental inverted index updates have been restricted to adding and removing documents. Updating the inverted index for previously indexed documents that have changed has not been addressed.In this paper, we propose an efficient method to update the inverted index for previously indexed documents whose contents have changed. Our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated. Our experiments verify that our landmark-diff method results in significant savings in the number of update operations on the inverted index.",
        "Two Hierarchical Text Categorization Approaches for BioASQ Semantic Indexing Challenge This paper describes our participation in the BioASQ semantic indexing challenge with two hierarchical text categorization systems. Both systems originated from previous research in thesaurus topic assignment applied on small domains from the legal document management field. One of the described systems employs a classical top-down approach based on a collection of local classifiers. The other system builds a Bayesian network induced by the thesaurus structure and contents, taking into account descriptor labels and related terms. We describe the adaptations required to deal with a large thesaurus like MeSH and a huge document collection and discuss the results obtained in the BioASQ challenge and the limitations of both approaches.",
        "Fast on-line index construction by geometric partitioning. Inverted index structures are the mainstay of modern text retrieval systems. They can be constructed quickly using off-line mergebased methods, and provide efficient support for a variety of querying modes. In this paper we examine the task of on-line index construction -that is, how to build an inverted index when the underlying data must be continuously queryable, and the documents must be indexed and available for search as soon they are inserted. When straightforward approaches are used, document insertions become increasingly expensive as the size of the database grows. This paper describes a mechanism based on controlled partitioning that can be adapted to suit different balances of insertion and querying operations, and is faster and scales better than previous methods. Using experiments on 100 GB of web data we demonstrate the efficiency of our methods in practice, showing that they dramatically reduce the cost of on-line index construction.",
        "Multi-modal Indexing and Retrieval Using an LSA-Based Kernel. This article proposes a Latent Semantic Analysis (LSA) based kernel function which effectively integrates low-level visual features with higher semantic ones into a common latent space that correlates multimodal features in the same latent space. The method's potential was evaluated on two early fusion experiments in a realistic scenario of image retrieval as the one provided by the ImageCLEF medical task. The performance of the method depends on the distributions of the multimodal latent features and the experimental results show that it outperforms the latent indexing generated by singular value decomposition.",
        "Inverted indexes for phrases and strings. Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed tradeoffs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.",
        "Fast-Forward Index Methods for Pseudo-Relevance Feedback Retrieval The inverted index is the dominant indexing method in information retrieval systems. It enables fast return of the list of all documents containing a given query term. However, for retrieval schemes involving query expansion, as in pseudo-relevance feedback (PRF), the retrieval time based on an inverted index increases linearly with the number of expansion terms. In this regard, we have examined the use of a forward index, which consists of the mapping of each document to its constituent terms. We propose a novel forward indexbased reranking scheme to shorten the PRF retrieval time. In our method, a first retrieval of the original query is performed using an inverted index, and then a forward index is employed for the PRF part. We have studied several new forward indexes, including using a novel spstring data structure and the weighted variable bit-block compression (wvbc) signature. With modern hardware such as solid-state drives (SSDs) and sufficiently large main memory, forward index methods are particularly promising. We find that with the whole index stored in main memory, PRF retrieval using a spstring or wvbc forward index excels in time efficiency over an inverted index, being able to obtain the same levels of performance measures at shorter times. Categories and Subject",
        "Engineering basic algorithms of an in-memory text search engine Inverted index data structures are the key to fast text search engines. We first investigate one of the predominant operation on inverted indexes, which asks for intersecting two sorted lists of document IDs of different lengths. We explore compression and performance of different inverted list data structures. In particular, we present Lookup, a new data structure that allows intersection in expected time linear in the smaller list.Based on this result, we present the algorithmic core of a full text data base that allows fast Boolean queries, phrase queries, and document reporting using less space than the input text. The system uses a carefully choreographed combination of classical data compression techniques and inverted-index-based search data structures. Our experiments show that inverted indexes are preferable over purely suffix-array-based techniques for in-memory (English) text search engines.A similar system is now running in practice in each core of the distributed data base engine TREX of SAP.",
        "Reverse annotation based retrieval from large document image collections. A number of projects are dedicated to creating digital libraries from scanned books, such as Google Books, UDL, Digital Library of India (DLI), etc. The ability to search in the content of document images is essential for the usability and popularity of these DLs. In this work, we aim toward building a retrieval system over 120K document images coming from 1000 scanned books of Telugu literature. This is a challenge because: i) OCRs are not robust enough for Indian languages, especially the Telugu script, ii) the document images contain large number of degradations and artifacts, iii) scalability to large collections is hard. Moreover, users expect that the search system accept text queries and retrieve relevant results in interactive times.We propose a Reverse Annotation framework , that labels word-images by their equivalent text label in the offline phase. Reverse Annotation applies a retrieval based approach to recognition. Unlike traditional annotation/recognition that identifies keywords for data, Reverse Annotation identifies data that corresponds to a given keyword. It first selects a set of keywords which are considered useful for labeling and retrieval, such as those that repeat often, and ignoring stopwords and rare-words. Exemplars are obtained for each word from a crude OCR or human annotations. The labels are then propagated across the rest of the collection by matching words in the image-feature space. Since such a matching is computationally expensive, scalability is achieved using a fast approximate nearest neighbor technique based on Hierarchical K-Means. Once text labels are assigned, each document image is considered a bag-of-words over the labeled keywords. A standard search engine is used to build a search index for quick online retrieval. An example query and the retrieved results are shown in . We are unaware of any conventional OCRs which can retrieve such images for the given query.There are three major contributions of our work: i) recognizing the entire document collection together, instead of one-at-a-time; this means that the repetition of words in the test set is effectively used for improving accuracy, ii) speeding up recognition by clustering multiple instances of a given word, iii) recognising at the word-level, avoiding the pitfalls of character segmentation and recognition. Other OCR techniques that use word-level context still rely on inaccurate component-level classification. Using the techniques developed from this work, we were able to successfully build a retrieval system over our challenging dataset. To the best of our knowledge, this is the largest collection of document images that has been made searchable for any Indian language. Our algorithm is easily scalable to larger collections, and directly applicable to documents from other language scripts.The first issue to discuss, is the fraction of word-images that remain unrecognized at the end of the Reverse Annotation phase. Rare-words, nouns etc. are not labeled in the test set. It is important to estimate the cost of not being able to answer such queries. If this cost is indeed high, we need to explore methods to label such infrequently occurring words in the collection. Needless to say, such methods should be computationally efficient without compromising on accuracy.The other major issue to discuss is the evaluation of retrieval results. The true recall of the retrieval system cannot be computed, since it is impossible to identify every occurrence of the given query in such large data. Questions to be considered include: whether precision alone is a sufficient indicator of retrieval performance; whether there is some better document-level effectiveness assessment possible; and how best to estimate the relative satisfaction of the user's information need.",
        "The NLM Medical Text Indexer System for Indexing Biomedical Literature In the face of a growing workload and dwindling resources, the US National Library of Medicine (NLM) created the Indexing Initiative project in the mid-1990s. This cross-library team's mission is to explore indexing methodologies that can help ensure that MEDLINE and other NLM document collections maintain their quality and currency and thereby contribute to NLM's mission of maintaining quality access to the biomedical literature. The NLM Medical Text Indexer (MTI) is the main product of this project and has been providing indexing recommendations based on the Medical Subject Headings (MeSH) vocabulary since 2002. In 2011, NLM expanded MTI's role by designating it as the first-line indexer (MTIFL) for a few journals; today the MTIFL workflow includes about 100 journals and continues to increase. Due to a close collaboration with the Index Section at NLM, MTI continues to grow and expand its ability to provide assistance to the indexers. This paper provides an overview of MTI's functionality, performance, and its evolution over the years.",
        "Partitioned multi-indexing: bringing order to social search. To answer search queries on a social network rich with usergenerated content, it is desirable to give a higher ranking to content that is closer to the individual issuing the query. Queries occur at nodes in the network, documents are also created by nodes in the same network, and the goal is to find the document that matches the query and is closest in network distance to the node issuing the query. In this paper, we present the \"Partitioned Multi-Indexing\" scheme, which provides an approximate solution to this problem. With m links in the network, after an offline\u00d5(m) pre-processing time, our scheme allows for social index operations (i.e., social search queries, as well as insertion and deletion of words into and from a document at any node), all in time\u00d5(1). Further, our scheme can be implemented on open source distributed streaming systems such as Yahoo! S4 or Twitter's Storm so that every social index operation takes\u00d5(1) processing time and network queries in the worst case, and just two network queries in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node.Building on Das Sarma et al.'s approximate distance oracle, the worst-case approximation ratio of our scheme is O(1) for undirected networks. Our simulations on the social network Twitter as well as synthetic networks show that in practice, the approximation ratio is actually close to 1 for both directed and undirected networks. We believe that this work is the first demonstration of the feasibility of social search with real-time text updates at large scales.",
        "Efficient Search Engine Measurements We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities are biased due to inaccurate approximation of the so called \"document degrees\". In addition, the estimators in are quite costly, due to their reliance on rejection sampling.We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in . Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy."
    ],
    "index compression techniques": [
        "Leveraging Context-Free Grammar for Efficient Inverted Index Compression. Large-scale search engines need to answer thousands of queries per second over billions of documents, which is typically done by querying a large inverted index. Many highly optimized integer encoding techniques are applied to compress the inverted index and reduce the query processing time. In this paper, we propose a new grammar-based inverted index compression scheme, which can improve the performance of both index compression and query processing.Our approach identifies patterns (common subsequences of docIDs) among different posting lists and generates a context-free grammar to succinctly represent the inverted index. To further optimize the compression performance, we carefully redesign the index structure. Experiments show a reduction up to 8.8% in space usage while decompression is up to 14% faster.We also design an efficient list intersection algorithm which utilizes the proposed grammar-based inverted index. We show that our scheme can be combined with common docID reassignment methods and encoding techniques, and yields about 14% to 27% higher throughput for AND queries by utilizing multiple threads.",
        "Optimizing positional index structures for versioned document collections. Versioned document collections are collections that contain multiple versions of each document. Important examples are Web archives, Wikipedia and other wikis, or source code and documents maintained in revision control systems. Versioned document collections can become very large, due to the need to retain past versions, but there is also a lot of redundancy between versions that can be exploited. Thus, versioned document collections are usually stored using special differential (delta) compression techniques, and a number of researchers have recently studied how to exploit this redundancy to obtain more succinct full-text index structures.In this paper, we study index organization and compression techniques for such versioned full-text index structures. In particular, we focus on the case of positional index structures, while most previous work has focused on the non-positional case. Building on earlier work in [32], we propose a framework for indexing and querying in versioned document collections that integrates non-positional and positional indexes to enable fast top-k query processing. Within this framework, we define and study the problem of minimizing positional index size through optimal substring partitioning. Experiments on Wikipedia and web archive data show that our techniques achieve significant reductions in index size over previous work while supporting very fast query processing.",
        "Inverted index compression via online document routing. Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages.",
        "Access Time Tradeoffs in Archive Compression. Web archives, query and proxy logs, and so on, can all be very large and highly repetitive; and are accessed only sporadically and partially, rather than continually and holistically. This type of data is ideal for compression-based archiving, provided that random-access to small fragments of the original data can be achieved without needing to decompress everything. The recent RLZ (relative Lempel Ziv) compression approach uses a semi-static model extracted from the text to be compressed, together with a greedy factorization of the whole text encoded using static integer codes. Here we demonstrate more precisely than before the scenarios in which RLZ excels. We contrast RLZ with alternatives based on block-based adaptive methods, including approaches that \"prime\" the encoding for each block, and measure a range of implementation options using both hard-disk (HDD) and solid-state disk (SSD) drives. For HDD, the dominant factor affecting access speed is the compression rate achieved, even when this involves larger dictionaries and larger blocks. When the data is on SSD the same effects are present, but not as markedly, and more complex trade-offs apply.",
        "Divide, Compress and Conquer: Querying XML via Partitioned Path-Based Compressed Data Blocks. We propose a novel partition path-based (PPB) grouping strategy to store compressed XML data in a stream of blocks. In addition, we employ a minimal indexing scheme called block statistic signature (BSS) on the compressed data, which is a simple but effective technique to support evaluation of selection and aggregate XPath queries of the compressed data. We present a formal analysis and empirical study of these techniques. The BSS indexing is first extended into effective cluster statistic signature (CSS) and multiple-cluster statistic signature (MSS) indexing by establishing more layers of indexes. We analyze how the response time is affected by various parameters involved in our compression strategy such as the data stream block size, the number of cluster layers, and the query selectivity. We also gain further insight about the compression and querying performance by studying the optimal block size in a stream, which leads to the minimum processing cost for queries. The cost model analysis provides a solid foundation for predicting the querying performance. Finally, we demonstrate that our PPB grouping and indexing strategies are not only efficient enough to support path-based selection and aggregate queries of the compressed XML data, but they also require relatively low computation time and storage space when compared with other state-of-the-art compression strategies.",
        "Performance of compressed inverted list caching in search engines. Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination.We focus on two techniques, inverted index compression and index caching, which play a crucial rule in web search engines as well as other high-performance information retrieval systems. We perform a comparison and evaluation of several inverted list compression algorithms, including new variants of existing algorithms that have not been studied before. We then evaluate different inverted list caching policies on large query traces, and finally study the possible performance benefits of combining compression and caching. The overall goal of this paper is to provide an updated discussion and evaluation of these two techniques, and to show how to select the best set of approaches and settings depending on parameter such as disk speed and main memory cache size.",
        "On Inverted Index Compression for Search Engine Efficiency. Efficient access to the inverted index data structure is a key aspect for a search engine to achieve fast response times to users' queries. While the performance of an information retrieval (IR) system can be enhanced through the compression of its posting lists, there is little recent work in the literature that thoroughly compares and analyses the performance of modern integer compression schemes across different types of posting information (document ids, frequencies, positions). In this paper, we experiment with different modern integer compression algorithms, integrating these into a modern IR system. Through comprehensive experiments conducted on two large, widely used document corpora and large query sets, our results show the benefit of compression for different types of posting information to the space-and time-efficiency of the search engine. Overall, we find that the simple Frame of Reference compression scheme results in the best query response times for all types of posting information. Moreover, we observe that the frequency and position posting information in Web corpora that have large volumes of anchor text are more challenging to compress, yet compression is beneficial in reducing average query response times.",
        "Improving Web search efficiency via a locality based static pruning method. The unarguably fast, and continuous, growth of the volume of indexed (and indexable) documents on the Web poses a great challenge for search engines. This is true regarding not only search effectiveness but also time and space efficiency. In this paper we present an index pruning technique targeted for search engines that addresses the latter issue without disconsidering the former. To this effect, we adopt a new pruning strategy capable of greatly reducing the size of search engine indices. Experiments using a real search engine show that our technique can reduce the indices' storage costs by up to 60% over traditional lossless compression methods, while keeping the loss in retrieval precision to a minimum. When compared to the indices size with no compression at all, the compression rate is higher than 88%, i.e., less than one eighth of the original size. More importantly, our results indicate that, due to the reduction in storage overhead, query processing time can be reduced to nearly 65% of the original time, with no loss in average precision. The new method yields significative improvements when compared against the best known static pruning method for search engine indices. In addition, since our technique is orthogonal to the underlying search algorithms, it can be adopted by virtually any search engine.",
        "Fast Dictionary-Based Compression for Inverted Indexes. Dictionary-based compression schemes provide fast decoding operation, typically at the expense of reduced compression effectiveness compared to statistical or probability-based approaches. In this work, we apply dictionary-based techniques to the compression of inverted lists, showing that the high degree of regularity that these integer sequences exhibit is a good match for certain types of dictionary methods, and that an important new trade-off balance between compression effectiveness and compression efficiency can be achieved. Our observations are supported by experiments using the document-level inverted index data for two large text collections, and a wide range of other index compression implementations as reference points. Those experiments demonstrate that the gap between efficiency and effectiveness can be substantially narrowed.",
        "Large Scale Sentiment Analysis with Locality Sensitive BitHash. As social media data rapidly grows, sentiment analysis plays an increasingly more important role in classifying users' opinions, attitudes and feelings expressed in text. However, most studies have been focused on the effectiveness of sentiment analysis, while ignoring the storage efficiency when processing large-scale high-dimensional text data. In this paper, we incorporate the machine learning based sentiment analysis with our proposed Locality Sensitive One-Bit Min-Hash (BitHash) method. BitHash compresses each data sample into a compact binary hash code while preserving the pairwise similarity of the original data. The binary code can be used as a compressed and informative representation in replacement of the original data for subsequent processing, for example, it can be naturally integrated with a classifier like SVM. By using the compact hash code, the storage space is significantly reduced. Experiment on the popular open benchmark dataset shows that, as the hash code length increases, the classification accuracy of our proposed method could approach the state-of-the-art method, while our method only requires a significantly smaller storage space.",
        "Graph-based large scale RDF data compression. We propose a two-stage lossless compression approach on large scale RDF data. Our approach exploits both Representation Compression and Component Compression techniques to support query and dynamic operations directly on the compressed data. Categories and Subject Descriptors PROJECT OVERVIEWThe movement of Linked Open Data aims at providing Web data in a standard format (i.e., Resource Description Framework) which can be accessed, manipulated and understood automatically by machines. With increase of data providers taking actions on publishing their data in RDF format, the volume of RDF data is booming. As a result, efficiently managing scalable RDF datasets becomes a critical challenge. Reducing the size of RDF datasets is one approach to achieve scalability. Most current RDF storage systems (i.e., triple-stores) are developed based on relational database (e.g., ). Recently, graph-based RDF systems are proposed (e.g., ). However, few efforts have been devoted to compressing RDF datasets. Current RDF compresssion approaches can be classified into two groups: i) Representation Compression (e.g., [1]), which focuses on reducing the size of RDF datasets by modifying their representations; and ii) Component Compression (e.g., [2]), which concentrates on eliminating redundant triples.We propose a two-stage lossless compression approach that exploits both compression techniques. Our method supports querying and dynamic operations directly on the compressed Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6-11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2610377.datasets. To the best of our knowledge, there is no existing solution that achieves both compression and dynamic operations on very large RDF datasets.In the first stage, we compress the representations of RDF triples. Inspired by [1], we leverage Dictionary Encoding principles and decompose an RDF dataset into two components: Dictionary and Triples. We propose to use Dynamic FM-Index [5] to represent the dictionary and choose the graph-based representation of the RDF dataset for the triple part. Two-level adjacency lists will be used to represent triples. Specifically, subject representation is omitted by ordering its corresponding predicates list and objects list sequentially. A succinct data structure, wavelet tree [3], can be used to represent the predicates and objects sequences, as this data structure can achieve nH0(S) + O(n) bits storage and O(1 + lg|\u03a3|/lglgn) query time, where H0(S) is the zero-order empirical entropy of sequence S, n and \u03a3 are the length and alphabet of S respectively. The output of the first stage is a collection of numerical values, which serves as the input of the component compression stage. We propose to use rule-based inference methods to find the redundant triples. Specifically, we will leverage the frequent itemset mining technique: FP-Growth algorithm to find frequent triple items. Thus the association rules are generated and redundant triples can be eliminated according to the rules. Decompression process simply uses rules and remaining triples to recover the eliminated ones. By leveraging the characteristics of self-index and succinct data structure, querying can be efficiently performed directly on the compressed representation of RDF dataset without decompressing it first.",
        "ANS-Based Index Compression. Techniques for e ectively representing the postings lists associated with inverted indexes have been studied for many years. Here we combine the recently developed \"asymmetric numeral systems\" (ANS) approach to entropy coding and a range of previous index compression methods, including VByte, Simple, and Packed. e ANS mechanism allows each of them to provide markedly improved compression e ectiveness, at the cost of slower decoding rates. Using the 426 GiB Gov2 collection, we show that the combination of blocking and ANS-based entropy-coding against a set of 16 magnitudebased probability models yields compression e ectiveness superior to most previous mechanisms, while still providing reasonable decoding speed.",
        "Automatic Text Summarization Using Two-Step Sentence Extraction. Automatic text summarization sets the goal at reducing the size of a document while preserving its content. Our summarization system is based on Two-step Sentence Extraction. As it combines statistical methods and reduces noise data through two steps efficiently, it can achieve high performance. In our experiments for 30% compression and 10% compression, our method is compared with Title, Location, Aggregation Similarity, and DOCUSUM methods. As a result, our method showed higher performance than other methods.",
        "A scalable pattern mining approach to web graph compression with communities. A link server is a system designed to support efficient implementations of graph computations on the web graph. In this work, we present a compression scheme for the web graph specifically designed to accommodate community queries and other random access algorithms on link servers. We use a frequent pattern mining approach to extract meaningful connectivity formations. Our Virtual Node Miner achieves graph compression without sacrificing random access by generating virtual nodes from frequent itemsets in vertex adjacency lists. The mining phase guarantees scalability by bounding the pattern mining complexity to O(E log E). We facilitate global mining, relaxing the requirement for the graph to be sorted by URL, enabling discovery for both inter-domain as well as intra-domain patterns. As a consequence, the approach allows incremental graph updates. Further, it not only facilitates but can also expedite graph computations such as PageRank and local random walks by implementing them directly on the compressed graph. We demonstrate the effectiveness of the proposed approach on several publicly available large web graph data sets. Experimental results indicate that the proposed algorithm achieves a 10-to 15-fold compression on most real word web graph data sets.",
        "Index Compression Using Byte-Aligned ANS Coding and Two-Dimensional Contexts. We examine approaches used for block-based inverted index compression, such as the OptPFOR mechanism, in which fixed-length blocks of postings data are compressed independently of each other. Building on previous work in which asymmetric numeral systems (ANS) entropy coding is used to represent each block, we explore a number of enhancements: (i) the use of two-dimensional conditioning contexts, with two aggregate parameters used in each block to categorize the distribution of symbol values that underlies the ANS approach, rather than just one; (ii) the use of a byte-friendly strategic mapping from symbols to ANS codeword buckets; and (iii) the use of a context merging process to combine similar probability distributions. Collectively, these improvements yield superior compression for index data, outperforming the reference point set by the Interp mechanism, and hence representing a significant step forward. We describe experiments using the 426 GiB gov2 collection and a new large collection of publicly-available news articles to demonstrate that claim, and provide query evaluation throughput rates compared to other block-based mechanisms.",
        "Reorganizing compressed text. Recent research has demonstrated beyond doubts the benefits of compressing natural language texts using word-based statistical semistatic compression. Not only it achieves extremely competitive compression rates, but also direct search on the compressed text can be carried out faster than on the original text; indexing based on inverted lists benefits from compression as well.Such compression methods assign a variable-length codeword to each different text word. Some coding methods (Plain Huffman and Restricted Prefix Byte Codes) do not clearly mark codeword boundaries, and hence cannot be accessed at random positions nor searched with the fastest text search algorithms. Other coding methods (Tagged Huffman, End-Tagged Dense Code, or (s, c)-Dense Code) do mark codeword boundaries, achieving a self-synchronization property that enables fast search and random access, in exchange for some loss in compression effectiveness.In this paper, we show that by just performing a simple reordering of the target symbols in the compressed text (more precisely, reorganizing the bytes into a wavelet-treelike shape) and using little additional space, searching capabilities are greatly improved without a drastic impact in compression and decompression times. With this approach, all the codes achieve synchronism and can be searched fast and accessed at arbitrary points. Moreover, the reordered compressed text becomes an implicitly indexed representation of the text, which can be searched for words in time independent of the text length. That is, we achieve not only fast sequential search time, but indexed search time, for almost no extra space cost.We experiment with three well-known word-based compression techniques with different characteristics Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Codes), and show the searching capabilities achieved by reordering the compressed representation on several corpora. We show that the reordered versions are not only much more efficient than their classical counterparts, but also more efficient than explicit inverted indexes built on the collection, when using the same amount of space.",
        "An Experimental Study of Index Compression and DAAT Query Processing Methods. In the last two decades, the IR community has seen numerous advances in top-k query processing and inverted index compression techniques. While newly proposed methods are typically compared against several baselines, these evaluations are often very limited, and we feel that there is no clear overall picture on the best choices of algorithms and compression methods. In this paper, we attempt to address this issue by evaluating a number of state-of-the-art index compression methods and safe disjunctive DAAT query processing algorithms. Our goal is to understand how much index compression performance impacts overall query processing speed, how the choice of query processing algorithm depends on the compression method used, and how performance is impacted by document reordering techniques and the number of results returned, keeping in mind that current search engines typically use sets of hundreds or thousands of candidates for further reranking.",
        "Fast and flexible word searching on compressed text We present a fast compression and decompression technique for natural language texts. The novelties are that (1) decompression of arbitrary portions of the text can be done very efficiently, (2) exact search for words and phrases can be done on the compressed text directly, using any known sequential pattern-matching algorithm, and (3) word-based approximate and extended search can also be done efficiently without any decoding. The compression scheme uses a semistatic word-based model and a Huffman code where the coding alphabet is byte-oriented rather than bit-oriented. We compress typical English texts to about 30% of their original size, against 40% and 35% for Compress and Gzip, respectively. Compression time is close to that of Compress and approximately half the time of Gzip, and decompression time is lower than that of Gzip and one third of that of Compress. We present three algorithms to search the compressed text. They allow a large number of variations over the basic word and phrase search capability, such as sets of characters, arbitrary regular expressions, and approximate matching. Separators and stopwords can be discarded at search time without significantly increasing the cost. When searching for simple words, the experiments show that running our algorithms on a compressed text is twice as fast as running the best existing software on the uncompressed version of the same text. Permission to make digital / hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and / or a fee. mate patterns, our algorithms are up to 8 times faster than the search on uncompressed text. We also discuss the impact of our technique in inverted files pointing to logical blocks and argue for the possibility of keeping the text compressed all the time, decompressing only for displaying purposes. In this article we present an efficient compression technique for natural language texts that allows fast and flexible searching of words and phrases. To search for simple words and phrases, the patterns are compressed, and the search proceeds without any decoding of the compressed text. Searching words and phrases that match complex expressions and/or allowing errors can be done on the compressed text at almost the same cost of simple searches. The reduced size of the compressed text makes the overall searching time much smaller than on plain uncompressed text. The compression and decompression speeds and the amount of compression achieved are very good when compared to well-known algorithms in the literature .The compression scheme presented in this article is a variant of the word-based Huffman code . The Huffman codeword assigned to each text word is a sequence of whole bytes, and the Huffman tree has degree either 128 (which we call \"tagged Huffman code\") or 256 (which we call \"plain Huffman code\"), instead of 2. In tagged Huffman coding each byte uses seven bits for the Huffman code and one bit to signal the beginning of a codeword. As we show later, using bytes instead of bits does not significantly degrade the amount of compression. In practice, byte processing is much faster than bit processing because bit shifts and masking operations are not necessary at compression, decompression, and search times. The decompression can start at any point in the compressed file. In particular, the compression scheme allows fast decompression of fragments that contain the search results, which is an important feature in information retrieval systems.Notice that our compression scheme is designed for large natural language texts containing at least 1MB to achieve an attractive amount of compression. Also, the search algorithms are word-oriented, as the pattern is a sequence of elements to be matched to a sequence of text words. Each pattern element can be a simple word or a complex expression, and the search can be exact or allowing errors in the match. In this context, we present three search algorithms.",
        "Scalable techniques for document identifier assignment ininverted indexes. Web search engines use the full-text inverted index data structure. Because query processing performance is impacted by the size of the inverted index, a plethora of research has focused on fast and effective techniques for compressing this structure. Recently, researchers have proposed techniques for improving index compression by optimizing the assignment of document identifiers in the collection, leading to significant reduction in overall index size.In this paper, we propose improved techniques for document identifier assignment. Previous work includes simple and fast heuristics such as sorting by URL, as well as more involved approaches based on the Traveling Salesman Problem or on graph partitioning. These techniques achieve good compression but do not scale to larger document collections. We propose a new framework based on performing a Traveling Salesman computation on a reduced sparse graph obtained through Locality Sensitive Hashing. This technique achieves improved compression while scaling to tens of millions of documents. Based on this framework, we describe new algorithms, and perform a detailed evaluation on three large data sets showing improvements in index size.",
        "Inverted indexes for phrases and strings. Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed tradeoffs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.",
        "Indexes for highly repetitive document collections. We introduce new compressed inverted indexes for highly repetitive document collections. They are based on runlength, Lempel-Ziv, or grammar-based compression of the differential inverted lists, instead of gap-encoding them as is the usual practice. We show that our compression methods significantly reduce the space achieved by classical compression, at the price of moderate slowdowns. Moreover, many of our methods are universal, that is, they do not need to know the versioning structure of the collection.We also introduce compressed self-indexes in the comparison. We show that techniques can compress much further, using a small fraction of the space required by our new inverted indexes, yet they are orders of magnitude slower.",
        "Improved index compression techniques for versioned document collections. Current Information Retrieval systems use inverted index structures for efficient query processing. Due to the extremely large size of many data sets, these index structures are usually kept in compressed form, and many techniques for optimizing compressed size and query processing speed have been proposed. In this paper, we focus on versioned document collections, that is, collections where each document is modified over time, resulting in multiple versions of the document. Consecutive versions of the same document are often similar, and several researchers have explored ideas for exploiting this similarity to decrease index size.We propose new index compression techniques for versioned document collections that achieve reductions in index size over previous methods. In particular, we first propose several bitwise compression techniques that achieve a compact index structure but that are too slow for most applications. Based on the lessons learned, we then propose additional techniques that come close to the sizes of the bitwise technique while also improving on the speed of the best previous methods.",
        "Inverted index compression and query processing with optimized document ordering. Web search engines use highly optimized compression schemes to decrease inverted index size and improve query throughput, and many index compression techniques have been studied in the literature. One approach taken by several recent studies first performs a renumbering of the document IDs in the collection that groups similar documents together, and then applies standard compression techniques. It is known that this can significantly improve index compression compared to a random document ordering.We study index compression and query processing techniques for such reordered indexes. Previous work has focused on determining the best possible ordering of documents. In contrast, we assume that such an ordering is already given, and focus on how to optimize compression methods and query processing for this case. We perform an extensive study of compression techniques for document IDs and present new optimizations of existing techniques which can achieve significant improvement in both compression and decompression performances. We also propose and evaluate techniques for compressing frequency values for this case. Finally, we study the effect of this approach on query processing performance. Our experiments show very significant improvements in index size and query processing speed on the TREC GOV2 collection of 25.2 million web pages."
    ],
    "search engine optimization with query logs": [
        "Selecting related terms in query-logs using two-stage SimRank. It is commonly believed that query logs from Web search are a gold mine for search business, because they reflect users' preference over Web pages presented by search engines, so a lot of studies based on query logs have been carried out in the last few years. In this study, we assume that two queries are relevant to each other when they have same clicked page in their result lists, and we also consider the queries' topics of user's need. Thus, we propose a TwoStage SimRank (called TSS in this paper) algorithm based on SimRank and some clustering algorithms to compute the similarity among queries, and then use it to discover relevant terms for query expansion, considering the information of topics and the global relationships of queries concurrently, with a query log collected by a practical search engine.Experimental results on two TREC test collections show that our approach can discover qualified terms effectively and improve retrieval performance.",
        "The Characteristics of Voice Search: Comparing Spoken with Typed-in Mobile Web Search Queries The growing popularity of mobile search and the advancement in voice recognition technologies have opened the door for web search users to speak their queries rather than type them. While this kind of voice search is still in its infancy, it is gradually becoming more widespread. In this article, we report a comprehensive voice search query log analysis of a commercial web search engine's mobile application. We compare voice and text search by various aspects, with special focus on the semantic and syntactic characteristics of the queries. Our analysis suggests that voice queries focus more on audio-visual content and question answering and less on social networking and adult domains. In addition, voice queries are more commonly submitted on the go. We also conduct an empirical evaluation showing that the language of voice queries is closer to natural language than the language of text queries. Our analysis points out further differences between voice and text search. We discuss the implications of these differences for the design of future voice-enabled web search tools. The popularity of search from mobile devices (mobile search) has rapidly increased in recent years . In fact, the number of mobile queries has already exceeded the number of those submitted from desktop devices in the United States and other countries . The nature of mobile search has also evolved, with growth in the number of unique queries and shifts in searched topics, from entertainment and adult content to business and commerce, similarly to the shifts of web search topics in its early days .A prominent characteristic of the advancement in mobile search is the emergence of voice search, allowing users to input queries in a spoken language and then retrieve the relevant entries based on system-generated transcriptions of the voice queries . Recent developments This manuscript is an extended version of Guy (2016). Author's address: I. Guy, P.O. Box 653 Beer-Sheva 8410501, Israel; email: idoguy@acm.org. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 30:2 I. Guy in speech recognition, backed by high bandwidth coverage and high-quality speech signal acquisition, are enabling higher quality voice search (Chelba and Schalkwyk 2013). Already in 2010, Google presented a case study stating that their goal is to make voice search ubiquitously available and that a level of performance was achieved such that usage is growing, and many users become repeat users . Since then, further enhancements to automatic speech recognition (ASR) for web search have been reported , taking advantage of the large data that started to accumulate on voice search logs (Chelba and Schalkwyk 2013), and applying advanced learning methods . The use of voice has also been promoted by the increasing popularity of voice-activated intelligent assistants, such as Google Assistant, Amazon's Alexa, Apple's Siri, and Microsoft's Cortana. These assistants provide context-based query-less personalized advice for mobile users, but also enable web search . A recent survey of 1400 U.S. smartphone users, executed by Northstar Research and commissioned by Google, found that many teenagers use voice search every day . It is therefore becoming important for information retrieval researchers and practitioners to understand this new medium of search and its differences from traditional text search.Using voice as a means to search holds various potential advantages. Although typing usability has improved in recent years, querying by voice is still likely to be substantially easier and faster for the vast majority of mobile users. Voice is also the natural way people communicate with one another and express themselves. For users with visual or manual impairment, or with limited literacy skills, voice search may break down the entry barrier into web search. In addition, as searching by voice does not require visual attention or the use of hands, it can be performed in situations such as driving, cooking, or exercising, where typed search might be especially cumbersome, error-prone, and even dangerous. In the aforementioned survey, 78% of the teens who used voice search pointed out its usefulness for multitasking as a key motivating factor. 1 In spite of its growing popularity, the area of voice search has not received much attention in the information retrieval literature. Early work compared voice and text queries in a laboratory study, however, these did not represent typical web search queries, but rather complex long questions In this work, we perform a query log analysis of half a million voice queries, issued to the mobile application of the Yahoo commercial web search engine, over a period of 6 months. The log includes English-only queries, from the United States, transcribed from voice to text using high-quality ASR. We compare the voice queries with a similar-size sample of mobile text queries, typed on the same mobile application. Our comparison inspects characteristics of context, clicks, sessions, and, primarily, the query text itself. We examine both semantic and syntactic features and compare them for voice versus text queries. In the final part of our analysis, we directly compare the similarity of the voice and text query language to natural language corpora, which include traditional news articles and the titles of questions in a large community question answering (CQA) website. -To the best of our knowledge, we present the most comprehensive analysis of a web search engine voice query log. -We combine a semantic analysis using novel methods, such as analyzing a broad set of triggered cards, with an in-depth syntactic analysis, to shed more light on the commonalities and differences between voice and text queries. -We provide empirical evidence, based on language modeling, that voice queries are closer to natural language than text queries, yet are still distant from natural question language.Our findings suggest different ways for search systems to enhance their support and take advantage of the unique characteristics of voice queries. We conclude the article by summarizing the key findings and discussing their implications and future research directions. RELATED WORKStudies of mobile query log analysis have been published throughout the past decade, ever since mobile devices became ubiquitous. One of the early studies (Kamvar and Baluja 2006) compared search patterns (queries, clicks, time spent on each search phase) on 12-key keypad cellphones, PDAs (with a QWERTY keyboard or a stylus input), and desktop (PC) computers. It found that the diversity of queries on mobile was substantially lower than on desktop and that the most popular query in each of the three device types was different. Baeza-Yates et al. compared mobile and desktop search queries on Yahoo Japan and found that mobile queries included fewer characters, more queries in the Business category, and fewer in Art. performed a large-scale query log analysis of the Yahoo OneSearch mobile service and found that mobile query patterns were dynamic, as users were exploring how to use the devices. Pattern use also varied among different geographies and application types. With the evolution of mobile devices into smartphones, mobile search has also been shown to change. examined search behavior on iPhones and found it was more similar to desktop search than to search on basic mobile phones. performed a broad 3-month log analysis of Bing search on desktop, iPad, and iPhone. They found that both mobile and tablet users issued significantly fewer navigational queries than desktop users, due to the wide availability of mobile apps on these two platforms. Due to the significant differences between user search patterns on the three platforms, they proposed a ranking system that considered platformspecific features. focused on the transition of users across device types during the search process by analyzing a search log from desktop computers, smartphones, tablets, and game consoles. In this work, we focus on mobile devices for the comparison between voice and text queries.With the advancement of speech recognition technologies, studies of mobile search using voice started to emerge. Many of the studies focused on voice recognition challenges. defined voice search as \"the technology underlying many spoken dialog systems that provide users with the information they request with a spoken query\" and reviewed key challenges, such as environmental noise, pronunciation variance, and linguistic issues. described the architecture of the speech recognition interface of Microsoft's \"Live Search for Mobile\". The key challenge they pointed out was the loss of signal-to-noise ratio caused by the fact that users often speak at arms length while looking at the screen or use the application in inherently noisy environments, such as in cars or on the street. discussed the interleaving of ASR with information retrieval (IR) systems and suggested to combine acoustic and semantic models to enhance performance.",
        "(Big) usage data in web search EXTENDED. Web Search, which takes its root in the mature field of information retrieval, evolved tremendously over the last 15 years. The field encountered its first revolution when it started to deal with huge amounts of Web pages. Then, a major step was accomplished when engines started to consider the structure of the Web graph and leveraged link analysis in both crawling and ranking. Finally, a more discrete, but no less critical step, was made when search engines started to monitor and exploit the numerous (mostly implicit) signals provided by users while interacting with the search engine. In this tutorial we focus on this \"revolution\" of large scale usage data.In the first part of this tutorial, we focus on usage data, which typically refers to any type of information provided by the user while interacting with the search engine. It comes first under its raw form as a set of individual signals, but is typically mined after multiple signals have been aggregated and linked to the same interaction event. The two major types of such data are (1) query streams, which include the query string that the user issued, together with the time-stamp of the query, a user identifier, possibly the IP of the machine on which the browser runs, and (2) click data, which include the reference to the element the user clicked on the page together with the timestamp, user identifier, possibly IP, the rank of the link if it is a result, etc.Exploiting usage data under its multiple forms brought an unprecedented wealth of implicit information to Web Search. We discuss in the second part of this tutorial some of the key Web search applications that it made possible. One such example is the query spelling correction feature embodied now in all search engines. In fact, after years of very sophisticated spell checking research, simply counting similar queries at a small edit distance would in most cases surface the most popular spelling as the correct one, a beautiful and simple demonstration of the wisdom of crowds principle.Copyright is held by the author/owner(s). SIGIR '12, August 12-16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472 In practice, such a feature clearly uses more sophisticated techniques than direct counting, but this simple example illustrates how even trivial \"data crunching\" over big real data brings more value than sophisticated techniques on small datasets. Query logs have also revolutionized query assistance tools such as related queries (recommended queries after the search was issued) or query completions (suggested queries as the query is being typed). While, in the past, query assistance tools would analyze the document corpus in order to identify phrases that could serve as alternate queries, the tremendous growth in Web search engine traffic allowed these tools to mostly rely on real user-issued queries. Using query logs as the source corpus significantly improved the quality of suggestions, at least as perceived by the user. In practice, query assistance tools are now deployed on all major Web search engines, and their quality improves as query logs grow. An additional revolutionary benefit of usage data is to consider clicks as a measure of satisfaction or at least of interest from the user, as if the user, by clicking, had actually voted in favor of the clicked element. One major reason of the quick pace of innovation in Web search can be credited to these \"pseudo-votes\". Instead of testing a new feature, or any sort of change, on a small sample of beta-users in a controlled environment, search engines now use real users on a much larger scale. Various metrics are used to verify whether users react positively or not to the change, thus helping search engines to decide whether to deploy the change to all users. After deployment, user behavior is constantly monitored, not only at pre-launch time, and features for which clicks or other metrics are decreasing might be discontinued or retired as it often happens.In spite of its multiple benefits, adequately leveraging usage data is not always straightforward. We discuss, in the third part of this tutorial, the effects of three major factors that often pull in opposite directions: \uf0b7Size of the data: Large-scale or big usage data is a key prerequisite to inferring new insights. As a consequence, the needs of the crowd dominate long-tail needs, which are expressed by definition by very few individuals. Averaging on the more popular needs can dominate so much, in some cases, that it conflicts with some specific needs that personalization should address. More data also brings algorithmic challenges as we need faster and faster algorithms. Hence, we can have a tradeoff between the quality of the output and the performance to obtain it. \uf0b7 Personalization:In order for search engines to personalize their services for a specific user, threy obviously need enough data to know more about the persona behind the individual, at the risk of exposing some private aspects. Hence,",
        "The query-flow graph: model and applications. Query logs record the queries and the actions of the users of search engines, and as such they contain valuable information about the interests, the preferences, and the behavior of the users, as well as their implicit feedback to searchengine results. Mining the wealth of information available in the query logs has many important applications including query-log analysis, user profiling and personalization, advertising, query recommendation, and more.In this paper we introduce the query-flow graph, a graph representation of the interesting knowledge about latent querying behavior. Intuitively, in the query-flow graph a directed edge from query qi to query qj means that the two queries are likely to be part of the same \"search mission\". Any path over the query-flow graph may be seen as a searching behavior, whose likelihood is given by the strength of the edges along the path.The query-flow graph is an outcome of query-log mining and, at the same time, a useful tool for it. We propose a methodology that builds such a graph by mining time and textual information as well as aggregating queries from different users. Using this approach we build a real-world query-flow graph from a large-scale query log and we demonstrate its utility in concrete applications, namely, finding logical sessions, and query recommendation. We believe, however, that the usefulness of the query-flow graph goes beyond these two applications.",
        "Catching the User - Logging the Information Retrieval Dialogue This position paper supports the idea of the information dialog between IR systems and users during an information search task. In order to satisfy the communication and interaction needs of humans, IR systems should explicitly support the cognitive abilities of the users. An information dialogue which does not only support an individual query but also the complete search process is necessary. Only in this way it is possible to satisfy an information need.",
        "Optimal distance bounds for fast search on compressed time-series query logs Consider a database of time-series, where each datapoint in the series records the total number of users who asked for a specific query at an internet search engine. Storage and analysis of such logs can be very beneficial for a search company from multiple perspectives. First, from a data organization perspective, because query Weblogs capture important trends and statistics, they can help enhance and optimize the search experience (keyword recommendation, discovery of news events). Second, Weblog data can provide an important polling mechanism for the microeconomic aspects of a search engine, since they can facilitate and promote the advertising facet of the search engine (understand what users request and when they request it).Due to the sheer amount of time-series Weblogs, manipulation of the logs in a compressed form is an impeding necessity for fast data processing and compact storage requirements. Here, we explicate how to compute the lower and upper distance bounds on the time-series logs when working directly on their compressed form. Optimal distance estimation means tighter bounds, leading to better candidate selection/elimination and ultimately faster search performance. Our derivation of the optimal distance bounds is based on the careful analysis of the problem using optimization principles. The experimental evaluation suggests a clear performance advantage of the proposed method, compared to previous compression/search techniques. The presented method results in a 10-30% improvement on distance estimations, which in turn leads to 25-80% improvement on the search performance. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. ACM Reference Format:Vlachos, M., Kozat, S. S., and Yu, P. S. 2010. Optimal distance bounds for fast search on compressed time-series query logs.",
        "Mining search engine query logs for query recommendation. This paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial-strength search engine. In order to get a more comprehensive solution, we combine two methods together. On the one hand, we study and model search engine users' sequential search behavior, and interpret this consecutive search behavior as client-side query refinement, that should form the basis for the search engine's own query refinement process. On the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. To evaluate our method, we use one hundred day worth query logs from SINA' search engine to do off-line mining. Then we analyze three independent editors evaluations on a query test set. Based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. In addition to the subjective editors' rating, we also perform tests based on actual anonymous user search sessions.",
        "(big) usage data in web search EXTENDED. Web Search, which takes its root in the mature field of information retrieval, evolved tremendously over the last 15 years. The field encountered its first revolution when it started to deal with huge amounts of Web pages. Then, a major step was accomplished when engines started to consider the structure of the Web graph and leveraged link analysis in both crawling and ranking. Finally, a more discrete, but no less critical step, was made when search engines started to monitor and exploit the numerous (mostly implicit) signals provided by users while interacting with the search engine. In this tutorial we focus on this \"revolution\" of large scale usage data.In the first part of this tutorial, we focus on usage data, which typically refers to any type of information provided by the user while interacting with the search engine. It comes first under its raw form as a set of individual signals, but is typically mined after multiple signals have been aggregated and linked to the same interaction event. The two major types of such data are (1) query streams, which include the query string that the user issued, together with the time-stamp of the query, a user identifier, possibly the IP of the machine on which the browser runs, and (2) click data, which include the reference to the element the user clicked on the page together with the timestamp, user identifier, possibly IP, the rank of the link if it is a result, etc.Exploiting usage data under its multiple forms brought an unprecedented wealth of implicit information to Web Search. We discuss in the second part of this tutorial some of the key Web search applications that it made possible. One such example is the query spelling correction feature embodied now in all search engines. In fact, after years of very sophisticated spell checking research, simply counting similar queries at a small edit distance would in most cases surface the most popular spelling as the correct one, a beautiful and simple demonstration of the wisdom of crowds principle.Copyright is held by the author/owner(s). WSDM '13, February 4-8, 2013, Rome, Italy. ACM 978-1-4503-187=69-3/13/02.In practice, such a feature clearly uses more sophisticated techniques than direct counting, but this simple example illustrates how even trivial \"data crunching\" over big real data brings more value than sophisticated techniques on small datasets. Query logs have also revolutionized query assistance tools such as related queries (recommended queries after the search was issued) or query completions (suggested queries as the query is being typed). While, in the past, query assistance tools would analyze the document corpus in order to identify phrases that could serve as alternate queries, the tremendous growth in Web search engine traffic allowed these tools to mostly rely on real user-issued queries. Using query logs as the source corpus significantly improved the quality of suggestions, at least as perceived by the user. In practice, query assistance tools are now deployed on all major Web search engines, and their quality improves as query logs grow. An additional revolutionary benefit of usage data is to consider clicks as a measure of satisfaction or at least of interest from the user, as if the user, by clicking, had actually voted in favor of the clicked element. One major reason of the quick pace of innovation in Web search can be credited to these \"pseudo-votes\". Instead of testing a new feature, or any sort of change, on a small sample of beta-users in a controlled environment, search engines now use real users on a much larger scale. Various metrics are used to verify whether users react positively or not to the change, thus helping search engines to decide whether to deploy the change to all users. After deployment, user behavior is constantly monitored, not only at pre-launch time, and features for which clicks or other metrics are decreasing might be discontinued or retired as it often happens.In spite of its multiple benefits, adequately leveraging usage data is not always straightforward. We discuss, in the third part of this tutorial, the effects of three major factors that often pull in opposite directions:\u2022 Size of the data: Large-scale or big usage data is a key prerequisite to inferring new insights. As a consequence, the needs of the crowd dominate long-tail needs, which are expressed by definition by very few individuals. Averaging on the more popular needs can dominate so much, in some cases, that it conflicts with some specific needs that personalization should address. More data also brings algorithmic challenges as we need faster and faster algorithms. Hence, we can have a trade-off between the quality of the output and the performance to obtain it.\u2022 Personalization: In order for search engines to personalize their services for a specific user, threy obviously need enough data to know more about the persona behind the individual, at the risk of exposing some private aspects. Hence, personalization in many cases might conflict with the user's privacy. 783",
        "Mining term association patterns from search logs for effective query reformulation. Search engine logs are an emerging new type of data that offers interesting opportunities for data mining. Existing work on mining such data has mostly attempted to discover knowledge at the level of queries (e.g., query clusters). In this paper, we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query. We define two novel term association patterns (i.e., context-sensitive term substitutions and term additions) and propose new methods for mining such patterns from search engine logs. These two patterns can be used to address the mis-specification and under-specification problems of ineffective queries. Experiment results on real search engine logs show that the mined context-sensitive term substitutions can be used to effectively reword queries and improve their accuracy, while the mined context-sensitive term addition patterns can be used to support query refinement in a more effective way.",
        "Towards selective user specific query expansion. Queries submitted to a search engine are usually short and ambiguous. Query expansion is an effective way to resolve the ambiguity of a query. We attempt to make a consolidated study of the effectiveness of query logs in this regard. We report empirical findings on the effect of personalization over an anonymous query log. We have designed a novel evaluation method to quantify improvements due to personalization. In the second part of our work, we present a novel approach to intelligently detect queries whose expansions potentially lead to improvements in retrieval. Empirical results obtained using this approach clearly show significant improvements over indiscriminate query expansion.",
        "Mining for insights in the search engine query stream. Search engines record a large amount of metadata each time a user issues a query. While efficiently mining this data can be challenging, the results can be useful in multiple ways, including monitoring search engine performance, improving search relevance, prioritizing research, and optimizing day-to-day operations. In this poster, we describe an approach for mining query log data for actionable insights -specific query segments (sets of queries) that require attention, and actions that need to be taken to improve the segments. Starting with a set of important metrics, we identify query segments that are \"interesting\" with respect to these metrics using a distributed frequent itemset mining algorithm.",
        "Query word deletion prediction. Web search query logs contain traces of users' search modifications. One strategy users employ is deleting terms, presumably to obtain greater coverage. It is useful to model and automate term deletion when arbitrary searches are conjunctively matched against a small hand-constructed collection, such as a hand-built hierarchy, or collection of high-quality pages matched with key phrases. Queries with no matches can have words deleted till a match is obtained. We provide algorithms which perform substantially better than the baseline in predicting which word should be deleted from a reformulated query, for increasing query coverage in the context of web search on small high-quality collections. Categories and Subject Descriptors General Terms Experimentation KeywordsWeb search, query reformulation, query modeling Spink et al. [4] found that reformulations of queries constituted 40% to 52% of queries. Other research using web search query logs has sought to characterize the individual queries , or to use the clickthroughs to optimize retrieval [1] performance. Work on query expansion looks at queries in isolation, and attempts to increase recall by expanding the query . Lau and Horvitz [2] learn to predict general web search behaviors from query logs annotated with search strategies and goals, ignoring the words in the query. This work differs in that experimental results are compared to the query reformulations made by real users, with term deletion examined as a specific reformulation strategy. RELATED WORK QUERY LOG DATAWe use complete logs of user web accesses from April through December 2002, recorded on client-side software provided by an ISP. In our logs there are on average 693,000 users accessing web pages per day (\u03c3 = 218). An average user makes 79 web accesses per day (\u03c3 = 116), with an average of 3.6 web searches per day (\u03c3 = 11). A candidate reformulation or query pair is a pair of successive queries to any search engines issued by a single user on a single day. We collapse repeated searches for the same terms, as well as query pairs repeated by the same user on the same day. This results in 600,000 query pairs daily. To examine the frequency and type of query reformulation, we chose 1236 query pairs uniformly at random and labeled them by hand. The breakdown by labels is summarized in . DELETION PREDICTIONIn our data there are 5,514,971 instances of query pairs in which a single word is deleted. The average length of these queries is 3.07 (\u03c3 = 1.17) words, so we would expect deleting a random word to match the actual word deleted 33% of the time. A model which predicts which word to delete from a query can help a user to refine an overly specific query to one which has more coverage. Most query reformulation research focuses on query expansion (eg. ). Our research on query term deletion prediction is novel, and applicable to at least 5% of all web search query reformulations. We use as training data the single-word deletion data from April to December 2002. The test set is the first 2000 instances of 435",
        "Optimizing web search using web click-through data. The performance of web search engines may often deteriorate due to the diversity and noisy information contained within web pages. User click-through data can be used to introduce more accurate description (metadata) for web pages, and to improve the search performance. However, noise and incompleteness, sparseness, and the volatility of web pages and queries are three major challenges for research work on user click-through log mining. In this paper, we propose a novel iterative reinforced algorithm to utilize the user click-through data to improve search performance. The algorithm fully explores the interrelations between queries and web pages, and effectively finds \"virtual queries\" for web pages and overcomes the challenges discussed above. Experiment results on a large set of MSN click-through log data show a significant improvement on search performance over the naive query log mining algorithm as well as the baseline search engine."
    ],
    "BM25": [
        "On the Effectiveness of Bayesian Network-based Models for Document Ranking. Theoretical soundness and technical feasibility of treating the problem of document ranking in IR as an inference problem in Bayesian Networks, was studied recently. A pilot framework was also proposed there. In this paper, we provide two implementations of the framework: BNBM25, the one based on BM25, and BNMATF, which is based on MATF, a recently proposed innovative ranking function. We empirically verify the effectiveness of these two implementations on several standard test collections. Positive, significant results are obtained. Potentials of this BN-based framework in addition to its verified effectiveness are also discussed. As a result of the study, we believe that the technique is promising, worthy of further analysis and application.",
        "A New Term Frequency Normalization Model for Probabilistic Information Retrieval. In probabilistic BM25, term frequency normalization is one of the key components. It is often controlled by parameters k 1 and b, which need to be optimized for each given data set. In this paper, we assume and show empirically that term frequency normalization should be specific with query length in order to optimize retrieval performance. Following this intuition, we first propose a new term frequency normalization with query length for probabilistic information retrieval, namely BM25QL. Then BM25QL is incorporated into the state-of-the-art models CRTER2 and LDA-BM25, denoted as CRTER2 QL and LDA-BM25QL respectively. A series of experiments show that our proposed approaches BM25QL, CRTER2QL and LDA-BM25QL are comparable to BM25, CRTER2 and LDA-BM25 with the optimal b setting in terms of MAP on all the data sets.",
        "Ad hoc IR: not much room for improvement. Ranking function performance reached a plateau in 1994. The reason for this is investigated. First the performance of BM25 is measured as the proportion of queries satisfied on the first page of 10 results -it performs well. The performance is then compared to human performance. They perform comparably. The conclusion is there isn't much room for ranking function improvement.",
        "StatBM25: An Aggregative and Statistical Approach for Document Ranking. In Information Retrieval and Web Search, BM25 is one of the most influential probabilistic retrieval formulas for document weighting and ranking. BM25 involves three parameters k 1 , k 3 and b, which provide scalar approximation and scaling of important document features such as term frequency, document frequency, and document length. We investigate in this paper aggregative and statistical document features for document ranking. Shortly speaking, a statistically adjusted BM25 is used to score in an aggregative way on virtual documents, which are generated by randomly combining documents from the original collection. The problem size, in the number of virtual documents to be ranked, is an expansion to the problem size of the original problem. As a result, ranking is actually realized through performing statistical sampling. Rejection Sampling, a simple Monte Carlo sampling method is used at present. This new framework is called StatBM25, in emphasizing first the fact that the original problem domain space is K-expanded (a concept to be further explained in the paper); Further, statistical sampling is employed in the model. Empirical studies are performed on several standard test collections, where StatBM25 demonstrates convincingly high degree of both uniqueness and effectiveness compared to BM25. This means, in our belief, that StatBM25 as a statistically smoothed and normalized variant to BM25, might eventually lead to discoveries of useful new statistic measures for document ranking.",
        "Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.'s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \"matter\"? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene's often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.",
        "DCU@TRECMed 2012: Using adhoc Baselines for Domain-Specific Retrieval This paper describes the first participation of DCU in the TREC Medical Records Track (TRECMed) 2012. We performed initial experiments on the the 2011 TRECMed data based on the BM25 retrieval model. Surprisingly, we found that the standard BM25 model with default parameters performs comparable to the best automatic runs submitted to TRECMed 2011 and our experiments would have ranked among the top four out of 29 participating groups. We expected that some form of domain adaptation would increase performance. However, results on the 2011 data proved otherwise: query expansion decreased performance, and filtering and reranking by term proximity also decreased performance slightly. We submitted four runs based on the BM25 retrieval model to TRECMed 2012 using standard BM25, standard query expansion, result filtering, and concept-based query expansion. Official results for 2012 confirm that domain-specific knowledge, as applied by us, does not increase performance compared to the BM25 baseline.",
        "FIRE2019@AILA: Legal Information Retrieval Using Improved BM25 This paper details the approaches of implementing the tasks of identifying relevant precedents and identifying relevant statues in the evaluation of Artificial Intelligence for Legal Assistance proposed by Forum of Information Retrieval Evaluation in 2019(AILA@Fire2019). We formalize the two tasks as the issue of information retrieval, and present the improved BM25 models to retrieve the prior cases and identify the relevant statues. For the task of identifying relevant precedents, the proposed improved BM25 model integrates the relevance scores of the original current case and the filtered current case. For the task of identifying relevant statues, the proposed improved BM25 models exploit the search results as the reference documents of the current case and integrate the ranking information of search results into the BM25 model. Comparisons to the other submissions for the same tasks, our improved BM25 model achieves the top performers for the task of identifying relevant precedents on all evaluation measures. For the task of identifying relevant statues, the improved BM25 model wins the second place on 1/rank of first relevant document and the third place on BPREF.",
        "When documents are very long, BM25 fails!. We reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which \"shifts\" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General TermsAlgorithms Keywords BM25, BM25L, term frequency, very long documents MOTIVATIONThe Okapi BM25 retrieval function has been the state-of-the-art for nearly two decades. BM25 scores a document D with respect to query Q as follows:where c(q, Q) is the count of q in Q, N is the total number of documents, df (q) is the document frequency of q, and k3 is a parameter. Following [1], we use a modified IDF formula in BM25 to avoid its problem of possibly negative IDF values. A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula:where |D| represents document length, avdl stands for average document length, c(q, D) is the raw TF of q in D, and b and k1 are two parameters. c (q, D) is the normalized TF by document length using pivoted length normalization .Copyright is held by the author/owner(s). SIGIR'11, July 24-28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07. BOOSTING VERY LONG DOCUMENTSIn order to avoid overly-penalizing very long documents, we need to add a constraint in TF normalization to make sure that the \"score gap\" of f One heuristic way to achieve this goal is to define f (q, D) as follows:",
        "Choosing document structure weights. Existing ranking schemes assume all term occurrences in a given document are of equal influence. Intuitively, terms occurring in some places should have a greater influence than those elsewhere. An occurrence in an abstract may be more important than an occurrence in the body text. Although this observation is not new, there remains the issue of finding good weights for each structure.Vector space, probability, and Okapi BM25 ranking are extended to include structure weighting. Weights are then selected for the TREC WSJ collection using a genetic algorithm. The learned weights are then tested on an evaluation set of queries. Structure weighted vector space inner product and structure weighted probabilistic retrieval show an about 5% improvement in mean average precision over their unstructured counterparts. Structure weighted BM25 shows nearly no improvement. Analysis suggests BM25 cannot be improved using structure weighting.",
        "Enhancing ad-hoc relevance weighting using probability density estimation. Classical probabilistic information retrieval (IR) models, e.g. BM25, deal with document length based on a trade-off between the Verbosity hypothesis, which assumes the independence of a document's relevance of its length, and the Scope hypothesis, which assumes the opposite. Despite the effectiveness of the classical probabilistic models, the potential relationship between document length and relevance is not fully explored to improve retrieval performance. In this paper, we conduct an in-depth study of this relationship based on the Scope hypothesis that document length does have its impact on relevance. We study a list of probability density functions and examine which of the density functions fits the best to the actual distribution of the document length. Based on the studied probability density functions, we propose a length-based BM25 relevance weighting model, called BM25L, which incorporates document length as a substantial weighting factor. Extensive experiments conducted on standard TREC collections show that our proposed BM25L markedly outperforms the original BM25 model, even if the latter is optimized.",
        "Keeping keywords fresh: a BM25 variation for personalized keyword extraction. Keyword extraction from web pages is essential to various text mining tasks including contextual advertising, recommendation selection, user profiling and personalization. For example, extracted keywords in contextual advertising are used to match advertisements with the web page currently browsed by a user. Most of the keyword extraction methods mainly rely on the content of a single web page, ignoring the browsing history of a user, and hence, potentially leading to the same advertisements or recommendations.In this work we propose a new feature scoring algorithm for web page terms extraction that, assuming a recent browsing history per user, takes into account the freshness of keywords in the current page as means of shifting users interests. We propose BM25H, a variant of BM25 scoring function, implemented on the client-side, that takes into account the user browsing history and suggests keywords relevant to the currently browsed page, but also fresh with respect to the user's recent browsing history. In this way, for each web page we obtain a set of keywords, representing the time shifting interests of the user. BM25H avoids repetitions of keywords which may be simply domain specific stop-words, or may result in matching the same ads or similar recommendations. Our experimental results show that BM25H achieves more than 70% in precision at 20 extracted keywords (based on human blind evaluation) and outperforms our baselines (TF and BM25 scoring functions), while it succeeds in keeping extracted keywords fresh compared to recent user history."
    ],
    " What makes Natural Language Processing natural? ": [
        "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking. Many fundamental problems in natural language processing rely on determining what entities appear in a given text. Commonly referenced as entity linking, this step is a fundamental component of many NLP tasks such as text understanding, automatic summarization, semantic search or machine translation. Name ambiguity, word polysemy, context dependencies and a heavy-tailed distribution of entities contribute to the complexity of this problem.We here propose a probabilistic approach that makes use of an effective graphical model to perform collective entity disambiguation. Input mentions (i.e., linkable token spans) are disambiguated jointly across an entire document by combining a document-level prior of entity co-occurrences with local information captured from mentions and their surrounding context. The model is based on simple sufficient statistics extracted from data, thus relying on few parameters to be learned.Our method does not require extensive feature engineering, nor an expensive training procedure. We use loopy belief propagation to perform approximate inference. The low complexity of our model makes this step sufficiently fast for real-time usage. We demonstrate the accuracy of our approach on a wide range of benchmark datasets, showing that it matches, and in many cases outperforms, existing stateof-the-art methods.",
        "Exploiting long-term temporal dynamics for video captioning. Automatically describing videos with natural language is a fundamental challenge for computer vision and natural language processing. Recently, progress in this problem has been achieved through two steps: 1) employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) (e.g. VGG, ResNet or C3D) to extract spatial and/or temporal features to encode video contents; and 2) applying Recurrent Neural Networks (RNNs) to generate sentences to describe events in videos. Temporal attention-based model has gained much progress by considering the importance of each video frame. However, for a long video, especially for a video which consists of a set of sub-events, we should discover and leverage the importance of each sub-shot instead of each frame. In this paper, we propose a novel approach, namely temporal and spatial LSTM (TS-LSTM), which systematically exploits spatial and temporal dynamics within video sequences. In TS-LSTM, a temporal pooling LSTM (TP-LSTM) is designed to incorporate both spatial and temporal information to extract long-term temporal dynamics within video sub-shots; and a stacked LSTM is introduced to generate a list of words to describe the video. Experimental results obtained in two public video captioning benchmarks indicate that our TS-LSTM outperforms the state-of-the-art methods.",
        "SPARQL2NL: verbalizing sparql queries. Linked Data technologies are now being employed by a large number of applications. While experts can query the backend of these applications using the standard query language SPARQL, most lay users lack the expertise necessary to proficiently interact with these applications. Consequently, non-expert users usually have to rely on forms, query builders, question answering or keyword search tools to access RDF data. Yet, these tools are usually unable to make the meaning of the queries they generate plain to lay users, making it difficult for these users to i) assess the correctness of the query generated out of their input, and ii) to adapt their queries or iii) to choose in an informed manner between possible interpretations of their input.We present SPARQL2NL, a generic approach that allows verbalizing SPARQL queries, i.e., converting them into natural language. In addition to generating verbalizations, our approach can also explain the output of queries by providing a natural-language description of the reasons that led to each element of the result set being selected. Our evaluation of SPARQL2NL within a large-scale user survey shows that SPARQL2NL generates complete and easily understandable natural language descriptions. In addition, our results suggest that even SPARQL experts can process the natural language representation of SPARQL queries computed by our approach more efficiently than the corresponding SPARQL queries. Moreover, non-experts are enabled to reliably understand the content of SPARQL queries. Within the demo, we present the results generated by our approach on arbitrary questions to the DBpedia and MusicBrainz datasets. Moreover, we present how our framework can be used to explain results of SPARQL queries in natural language.",
        "HITS@FIRE task 2015: Twitter based Named Entity Recognizer for Indian Languages Natural Language processing (NLP) in its pure sense, is a platform that provides the ability for transforming natural language text to useful information. Named Entity Recognition (NER) is a key task in NLP for classification of named entities in natural languages. Though, there are several algorithms for named entity classification, identifying named entities in twitter data is a demanding task. Loads of information are being shared by people in twitter on a daily basis. This information is unstructured and often contains important information about organizations, politics, disasters, promotional advertisements etc. In this paper, we provide a NER that can effectively classify named entities in twitter data for Indian Languages such as English, Hindi and Tamil. POS, Chunk, Suffix, Prefix information has been used for training in Conditional Random Fields (CRF) based NER Model. CRF is a popular model for labeling and classification in text mining. Performance analysis was done using n-fold validation and F-measure. A maximum precision of 93.82 for English, 92.28 for Hindi and 86.94 for Tamil twitter data was achieved through N fold validation. Results provided by ESM-IL share task in terms of precision for English is 50.48, for Hindi is 81.49 and for Tamil 70.42. The proposed algorithm has a higher classification accuracy and it is achieved through n-fold validation. CCS Concepts \u2022 Human centered computing\u279d Human machine interaction\u279d Collaborative and social computing \u2022 Applied Computing \u279dDocument managing and text computing methodologies\u279d Artificial Intelligence and Machine Learning.",
        "Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach.",
        "Word classification and hierarchy using co-occurrence word information. By the development of the computer in recent years, calculating a complex advanced processing at high speed has become possible. Moreover, a lot of linguistic knowledge is used in the natural language processing (NLP) system for improving the system. Therefore, the necessity of co-occurrence word information in the natural language processing system increases further and various researches using co-occurrence word information are done. Moreover, in the natural language processing, dictionary is necessary and indispensable because the ability of the entire system is controlled by the amount and the quality of the dictionary. In this paper, the importance of co-occurrence word information in the natural language processing system was described. The classification technique of the co-occurrence word (receiving word) and the co-occurrence frequency was described and the classified group was expressed hierarchically. Moreover, this paper proposes a technique for an automatic construction system and a complete thesaurus. Experimental test operation of this system and effectiveness of the proposal technique is verified.",
        "Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification Phrase/Sentence representation is one of the most important problems in natural language processing. Many neural network models such as Convolutional Neural Network (CNN), Recursive Neural Network (RNN), and Long Short-Term Memory (LSTM) have been proposed to learn representations of phrase/sentence, however, rich syntactic knowledge has not been fully explored when composing a longer text from its shorter constituent words. In most traditional models, only word embeddings are utilized to compose phrase/sentence representations, while the syntactic information of words is yet to be explored. In this article, we discover that encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation. Specifically, we propose to learn tag-specific composition functions and tag embeddings in recursive neural networks, and propose to utilize POS tags to control the gates of tree-structured LSTM networks. We evaluate these models on two benchmark datasets for sentiment classification, and demonstrate that improvements can be obtained with such syntactic knowledge encoded.",
        "Lab Report Special Section: Natural Language Processing and Information Retrieval Group Information Access and User Interfaces Division National Institute of Standards and Technology OverviewThe Natural Language Processing and Information Retrieval Group was formed in 1994 at the National Institute of Standards and Technology (NIST) in recognition of the importance of managing the ever-increasing amount of electronically available text. The formal objective of the group is \"to work with industry, academia and other government agencies to promote the use of more effective and efficient techniques for manipulating (largely) unstructured textual information, especially the browsing, searching, and presentation of that information\". The group carries on the work in text retrieval started in 1988.Because of the initial work in text retrieval, the majority of the current projects involve improving the transfer of better text retrieval technology into commercial systems. Two approaches are being followed. The first approach (started in 1988) is to build a large-scale prototype retrieval system (the PRISE system) capable of handling over three gigabytes of data. This system uses natural language input and state-of-the-art statistical ranking mechanisms. The prototype has become the focus for continued research by the group into more effective and efficient retrieval systems using natural language queries. Additionally it serves as a vehicle for work in the NISO Z39.50 Information Retrieval standard and for in-house usability testing.The second approach (started in 1992) is to conduct a conference attracting international participation from researchers in information retrieval. The Text REtrieval Conference (TREC) is now starting its fifth year. Participating groups work with a large test collection built at NIST, submit their results for a common evaluation, and meet for a three-day workshop to compare techniques and results. The conference is starting to serve as a major technology-transfer mechanism in the area of information retrieval. New methods of evaluating retrieval technology are also being developed based on work for this conference.Several other projects represent a broadening of group's interests to include additional areas of natural language processing and evaluation. Projects are being pursued in the area of improved handling of large text files, including how to automatically create hypertext links and how to facilitate machine-aided editing of massive text files. Also planned are projects in the area of human computer interaction techniques for information access, including advanced interfaces, i.e., mixed mode and multimedia, for information retrieval.6",
        "Describing and Predicting Online Items with Reshare Cascades via Dual Mixture Self-exciting Processes A massive amount of news is being shared online by individuals and news agencies, making it difficult to take advantage of these news and analyse them in traditional ways. In view of this, there is an urgent need to use recent technologies to analyse all news relevant information that is being shared in natural language and convert it into forms that can be more easily and precisely processed by computers. Knowledge Graphs (KGs) offer offer a good solution for such processing. Natural Language Processing (NLP) offers the possibility for mining and lifting natural language texts to knowledge graphs allowing to exploit its semantic capabilities, facilitating new possibilities for news analysis and understanding. However, the current available techniques are still away from perfect. Many approaches and frameworks have been proposed to track and analyse news in the last few years. The shortcomings of those systems are that they are static and not updateable, are not designed for largescale data volumes, did not support real-time processing, dealt with limited data resources, used traditional lifting pipelines and supported limited tasks, or have neglected the use of knowledge graphs to represent news into a computer-processable form. Therefore, there is a need to better support lifting natural language into a KG. With the continuous development of NLP techniques, the design of new dynamic NLP lifters that can cope with all the previous shortcomings is required. This paper introduces a general NLP lifting architecture for automatically lifting and processing news reports in real-time based on the recent development of the NLP methods.",
        "WWW 2008 workshop: NLPIX2008 summary. The amount of information available on the Web has increased rapidly, reaching levels that few would ever have imagined possible. We live in what could be called the \"informationexplosion era,\" and this situation poses new problems for computer scientists. Users demand useful and reliable information from the Web in the shortest time possible, but the obstacles to fulfilling this demand are many including language barriers and the so-called \"long tail.\" Even worse, users may provide only vague specifications of the information that they actually want, so that a more concrete specification must somehow be inferred by Web access tools. Natural language processing (NLP) is one of the key technologies for solving the above Web usability problems. Almost all the Web page provide with the essential information in the form of natural language texts, and the amount of these text information is huge. In order to offer solutions to these problems we must perform searching and extracting information from the Web texts using NLP technologies. The aim of this workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008) is to bring researchers and practitioners together in order to discuss our most pressing needs with respect to accessing information on the Web, and to discuss new ideas in NLP technologies that might offer viable solutions for those issues.",
        "WebShodh: A Code Mixed Factoid Question Answering System for Web. Code-Mixing (CM) is a natural phenomenon observed in many multilingual societies and is becoming the preferred medium of expression and communication in online and social media fora. In spite of this, current Question Answering (QA) systems do not support CM and are only designed to work with a single interaction language. This assumption makes it inconvenient for multi-lingual users to interact naturally with the QA system especially in scenarios where they do not know the right word in the target language. In this paper, we present WebShodh -an end-end web-based Factoid QA system for CM languages. We demonstrate our system with two CM language pairs: Hinglish (Matrix language: Hindi, Embedded language: English) and Tenglish (Matrix language: Telugu, Embedded language: English). Lack of language resources such as annotated corpora, POS taggers or parsers for CM languages poses a huge challenge for automated processing and analysis. In view of this resource scarcity, we only assume the existence of bi-lingual dictionaries from the matrix languages to English and use it for lexically translating the question into English. Later, we use this loosely translated question for our downstream analysis such as Answer Type(AType) prediction, answer retrieval and ranking. Evaluation of our system reveals that we achieve an MRR of 0.37 and 0.32 for Hinglish and Tenglish respectively. We hosted this system online and plan to leverage it for collecting more CM questions and answers data for further improvement.",
        "On the Construction of Effective Vocabularies for Information Retrieval. Natural language query formulations exhibit advantages over artificial language statements since they permit the user to approach the retrieval environment without prior training and without using intermediaries. To obtain adequate retrieval output, it is however necessary to emphasize the good terms and to deemphasize the bad ones. The usefulness of the terms in a natural language vocabulary is first characterized in terms of their frequency distribution over the documents of a collection. The construction of \"good\" natural language vocabularies is then described, and methods are given for improving the vocabulary by transforming terms that operate poorly for retrieval purposes into better ones.",
        "Automatic word spacing of erroneous sentences in mobile devices with limited hardware resources a b s t r a c tWith the rapid evolution of the mobile environment, the demand for natural language applications on mobile devices is increasing. This paper proposes an automatic word spacing system, the first step module of natural language processing (NLP) for many languages with their own word spacing rules, that is designed for mobile devices with limited hardware resources. The proposed system uses two stages. In the first stage, it preliminarily corrects word spacing errors by using a modified hidden Markov model based on character unigrams. In the second stage, the proposed system re-corrects the miscorrected word spaces by using lexical rules based on character bigrams or longer combinations. By using this hybrid method, the proposed system improves the robustness against unknown word patterns, reduces memory usage, and increases accuracy. To evaluate the proposed system in a realistic mobile environment, we constructed a mobile-style colloquial corpus using a simple simulation method. In experiments with a commercial mobile phone, the proposed system showed good performances (a response time of 0.20 s per sentence, a memory usage of 2.04 MB, and an accuracy of 92-95%) in the various evaluation measures.",
        "Transporting the Linguistic String Project System from a Medical to a Navy Domain The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems.",
        "Natural language and the information layer My talk has three parts: on the first phase of natural language processing research and its lessons; on subsequent developments up to the present and their lessons; and on where we are now and what I think are the wider implications for the future. Words, classification, and retrievalWhen I began research in computing nearly fifty years ago, people were very excited about what could be done with computers, challenged by how to do it, and pushing applications that offered new opportunities in dealing with information. One of these areas was NLP -natural language processing -or NLIP -processing information conveyed in natural language. At that time, most researchers thought primarily of the efficiency gains that could be Copyright is held by the author/owner(s). SIGIR'07, July 23-27, 2007, Amsterdam, The Netherlands. ACM 978-1-59593-597-7/07/0007. made by emulating or supporting people, e.g. in translation or document retrieval, though earlier visionaries like Warren Weaver had seen how computers could open up quite novel opportunities, and a little later Doug Englebart would illustrate some of these.My own research focussed on automatic classification (nowadays called unsupervised machine learning). It was clear that humans rely in NLP on the use of general conceptual classifications of words -like a thesaurus -to resolve ambiguity in language (in research practice, text). Individual words are ambiguous, and so are the structures represented by word strings. These ambiguities are resolved by the fact that discourse is coherent, and effective because concepts and topics are repeated, and repeated enough to get them across. Repeated individual concepts and standard conceptual patterns enable us to select the right meanings for words and structural relationships for word sequences. This very general idea fairly obviously applies to translation, but it also applies to document retrieval: different words in a query and a document may still stand for the same concept and thus be allowed to match.The question is, where to get the lexical classification and stock of text patterns from. The obvious answer is, from text. If words tend to co-occur in many texts this similar behaviour suggests they are conceptually related. Thus in principle one should be able to build a thesaurus automatically from a vast text corpus, and analogously to extract repeating conceptual patterns.I was interested in building a thesaurus for translation, but of course had no corpus. I managed to finesse this by exploiting some limited dictionary (not thesaurus) data, for a pilot demonstration. For retrieval the situation is easier. The collection of documents or texts from which you want to retrieve supplies the corpus to build the classification. This is the line we followed, with some success in retrieval performance, particularly when relative word frequency was adequately factored in and eventually captured by word weighting. Documents score not just by the number of words matching, but by the sum of their weights.All of this was essentially statistical in character. Facts about word occurrences and co-occurrences were used to capture meaning in a way that could be manipulated without needing to know what that meaning was. An unusually frequent word in a document is a good indicator of an important topic in the document, so if the query uses the word, the document is likely to be relevant to your information need.One very simple word weighting formula captures this effectively. If a word occurs in many documents most of these -other things being equal -are unlikely to be relevant to the user's information need. Weighting by inverse document frequency, idf , relatively favours less common and more discriminating terms. If you SIGIR 2007 Proceedings ACM Athena Award Lecture 3",
        "SIGIR 2017 Workshop on Open Knowledge Base and Question Answering (OKBQA2017). Over the past years, several challenges and calls for research projects have pointed out the dire need for pushing natural language interfaces. In this context, the importance of Semantic Web data as a premier knowledge source is rapidly increasing. But we are still far from having accurate natural language interfaces that allow handling complex information needs in a user-centric and highly performant manner. The development of such interfaces requires collaboration of a range of different fields, including natural language processing, information extraction, knowledge base construction and population, reasoning, and question answering. With the goal to join forces in the collaborative development of natural language QA systems, the second OKBQA workshop is organized within the 40th SIGIR conference."
    ],
    "principle of a information retrieval indexing": [
        "Learning Term Discrimination Document indexing is a key component for efficient information retrieval (IR). After preprocessing steps such as stemming and stopword removal, document indexes usually store term-frequencies (tf). Along with tf (that only reflects the importance of a term in a document), traditional IR models use term discrimination values (TDVs) such as inverse document frequency (idf) to favor discriminative terms during retrieval. In this work, we propose to learn TDVs for document indexing with shallow neural networks that approximate traditional IR ranking functions such as TF-IDF and BM25. Our proposal outperforms, both in terms of nDCG and recall, traditional approaches, even with few positively labelled querydocument pairs as learning data. Our learned TDVs, when used to filter out terms of the vocabulary that have zero discrimination value, allow to both significantly lower the memory footprint of the inverted index and speed up the retrieval process (BM25 is up to 3 times faster), without degrading retrieval quality.",
        "A time machine for text search. Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient \"time machine\" scalable to large versioned text collections.",
        "Indexing Text Documents Based on Topic Identification. This work provides algorithms and heuristics to index text documents by determining important topics in the documents. To index text documents, the work provides algorithms to generate topic candidates, determine their importance, detect similar and synonym topics, and to eliminate incoherent topics. The indexing algorithm uses topic frequency to determine the importance and the existence of the topics. Repeated phrases are topic candidates. For example, since the phrase 'index text documents' occurs three times in this abstract, the phrase is one of the topics of this abstract. It is shown that this method is more effective than either a simple word count model or approaches based on term weighting.",
        "The Feasibility of Brute Force Scans for Real-Time Tweet Search. The real-time search problem requires making ingested documents immediately searchable, which presents architectural challenges for systems built around inverted indexing. In this paper, we explore a radical proposition: What if we abandon document inversion and instead adopt an architecture based on brute force scans of document representations? In such a design, \"indexing\" simply involves appending the parsed representation of an ingested document to an existing buffer, which is simple and fast. Quite surprisingly, experiments with TREC Microblog test collections show that query evaluation with brute force scans is feasible and performance compares favorably to a traditional search architecture based on an inverted index, especially if we take advantage of vectorized SIMD instructions and multiple cores in modern processor architectures. We believe that such a novel design is worth further exploration by IR researchers and practitioners.",
        "Locality preserving indexing for document representation. Document representation and indexing is a key problem for document analysis and processing, such as clustering, classification and retrieval. Conventionally, Latent Semantic Indexing (LSI) is considered effective in deriving such an indexing. LSI essentially detects the most representative features for document representation rather than the most discriminative features. Therefore, LSI might not be optimal in discriminating documents with different semantics. In this paper, a novel algorithm called Locality Preserving Indexing (LPI) is proposed for document indexing. Each document is represented by a vector with low dimensionality. In contrast to LSI which discovers the global structure of the document space, LPI discovers the local structure and obtains a compact document representation subspace that best detects the essential semantic structure. We compare the proposed LPI approach with LSI on two standard databases. Experimental results show that LPI provides better representation in the sense of semantic structure.",
        "Permutation indexing: fast approximate retrieval from large corpora. Inverted indexing is a ubiquitous technique used in retrieval systems including web search. Despite its popularity, it has a drawback -query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique, permutation indexing, where retrieval cost is strictly bounded and has only logarithmic dependence on the corpus size. Our approach is based on two novel techniques: (a) partitioning of the term space into overlapping clusters of terms that frequently co-occur in queries, and (b) a data structure for compactly encoding results of all queries composed of terms in a cluster as continuous sequences of document ids. Then, query results are retrieved by fetching few small chunks of these sequences. There is a price though: our encoding is lossy and thus returns approximate result sets. The fraction of the true results returned, recall, is controlled by the level of redundancy. The more space is allocated for the permutation index the higher is the recall. We analyze permutation indexing both theoretically under simplified document and query models, and empirically on a realistic document and query collections. We show that although permutation indexing can not replace traditional retrieval methods, since high recall cannot be guaranteed on all queries, it covers up to 77% of tail queries and can be used to speed up retrieval for these queries.",
        "Efficiency vs. Effectiveness in Terabyte-Scale Information Retrieval We describe indexing and retrieval techniques that are suited to perform terabyte-scale information retrieval tasks on a standard desktop PC. Starting from an Okapi-BM25-based default baseline retrieval function, we explore both sides of the effectiveness spectrum. On one side, we show how term proximity can be integrated into the scoring function in order to improve the search results. On the other side, we show how index pruning can be employed to increase retrieval efficiency-at the cost of reduced retrieval effectiveness. We show that, although index pruning can harm the quality of the search results considerably, according to standard evaluation measures, the actual loss of precision, according to other measures that are more realistic for the given task, is rather small and is in most cases outweighed by the immense efficiency gains that come along with it.",
        "Neural embedding-based indices for semantic search A B S T R A C TTraditional information retrieval techniques that primarily rely on keyword-based linking of the query and document spaces face challenges such as the vocabulary mismatch problem where relevant documents to a given query might not be retrieved simply due to the use of different terminology for describing the same concepts. As such, semantic search techniques aim to address such limitations of keyword-based retrieval models by incorporating semantic information from standard knowledge bases such as Freebase and DBpedia. The literature has already shown that while the sole consideration of semantic information might not lead to improved retrieval performance over keyword-based search, their consideration enables the retrieval of a set of relevant documents that cannot be retrieved by keyword-based methods. As such, building indices that store and provide access to semantic information during the retrieval process is important. While the process for building and querying keyword-based indices is quite well understood, the incorporation of semantic information within search indices is still an open challenge. Existing work have proposed to build one unified index encompassing both textual and semantic information or to build separate yet integrated indices for each information type but they face limitations such as increased query process time. In this paper, we propose to use neural embeddings-based representations of term, semantic entity, semantic type and documents within the same embedding space to facilitate the development of a unified search index that would consist of these four information types. We perform experiments on standard and widely used document collections including Clueweb09-B and Robust04 to evaluate our proposed indexing strategy from both effectiveness and efficiency perspectives. Based on our experiments, we find that when neural embeddings are used to build inverted indices; hence relaxing the requirement to explicitly observe the posting list key in the indexed document: (a) retrieval efficiency will increase compared to a standard inverted index, hence reduces the index size and query processing time, and (b) while retrieval efficiency, which is the main objective of an efficient indexing mechanism improves using our proposed method, retrieval effectiveness also retains competitive performance compared to the baseline in terms of retrieving a reasonable number of relevant documents from the indexed corpus.",
        "Query-Structure Based Web Page Indexing Indexing is a crucial technique for dealing with the massive amount of data present on the web. In our third participation in the web track at TREC 2012, we explore the idea of building an efficient query-based indexing system over Web page collection. Our prototype explores the trends in user queries and consequently indexes texts using particular attributes available in the documents. This paper provides an in-depth description of our approach for indexing web documents efficiently; that is, topics available in the web documents are discovered with the assistance of knowledge available in Wikipedia. The welldefined articles in Wikipedia are shown to be valuable as a training set when indexing Webpages. Our complex index structure also records information from titles and urls, and pays attention to web domains. Our approach is designed to close the gaps in our approaches from the previous two years, for some queries. Our framework is able to efficiently index the 50 million pages available in the subset B of the ClueWeb09 collection. Our preliminary experiments on the TREC 2012 testing queries showed that our indexing scheme is robust and efficient for both indexing and retrieving relevant web pages, for both the ad-hoc and diversity task.",
        "Reverted indexing for feedback and expansion. Traditional interactive information retrieval systems function by creating inverted lists, or term indexes. For every term in the vocabulary, a list is created that contains the documents in which that term occurs and its relative frequency within each document. Retrieval algorithms then use these term frequencies alongside other collection statistics to identify the matching documents for a query. In this paper, we turn the process around: instead of indexing documents, we index query result sets. First, queries are run through a chosen retrieval system. For each query, the resulting document IDs are treated as terms and the score or rank of the document is used as the frequency statistic. An index of documents retrieved by basis queries is created. We call this index a reverted index. With reverted indexes, standard retrieval algorithms can retrieve the matching queries (as results) for a set of documents (used as queries). These recovered queries can then be used to identify additional documents, or to aid the user in query formulation, selection, and feedback.",
        "The Subspace Coding Method: A New Indexing Scheme for High-Dimensional Data. This paper presents a new indexing scheme, the Subspace Coding Method (SCM), that offers high performance similarity retrieval. It is based on an analysis of the two superior access methods proposed so far: the SR-tree and the VAFile. Our experimental test by real data provides that the SR-tree offers better performance. However, as dimensionality increases, the large volume of entries in non-leaf nodes degrades the search performance. Based on the analysis, we introduce the SCM, a new indexing scheme applicable to any tree index employing MBR (Minimum Bounding Rectangle) and/or MBS (Minimum Bounding Sphere). The basic idea of the SCM is the introduction of Virtual Bounding Rectangle (VBR) and Virtual Bounding quasiSphere (VBS), which contain and approximate MBR and MBS, respectively. Unlike the approximation of absolute vector positions used in the VA-File, VBRs and VBSs are relative to the parent's VBR. The experimental results demonstrate the effectiveness of the SCM.",
        "Integrating a Dynamic Lexicon with a Dynamic Full-Text Retrieval System. There has been a great deal of interest within the Information Retrieval community in evahrating the use of linguistic knowledge to improve the indexing and searching of textual databases.Such systems must often employ a lexicon to store information about the words end phrases comprising the application's domain. Unlike a static lexicon, a dynumic lexicon raises practical concerns about the coordination between the state of the lexicon and JR indexing schemes based on lexical knowledge. Additionally, it introduces a host of database management issues, many of which are similar to those found in the text databases as well.In this paper, we explore a range of system design and performance issues that arise when integrating a dynamic lexicon with a dynamic full-text information retrieval system. We observe that the principle of functional isolation argues against the use of language-dependent information in article indexes and favors the use of query-time strategies for applying lexical knowledge.We propose and evaluate a system architecture which embodies this principle.We also show how a storage and retrieval infrastructure based on Burkowski's [BURKOW-SK192] \"containment model\" abstraction can be employed to implement both the text retrieval and lexicon facilities required in an integrated system.",
        "Configurable indexing and ranking for XML information retrieval. Indexing and ranking are two key factors for efficient and effective XML information retrieval. Inappropriate indexing may result in false negatives and false positives, and improper ranking may lead to low precisions. In this paper, we propose a configurable XML information retrieval system, in which users can configure appropriate index types for XML tags and text contents. Based on users' index configurations, the system transforms XML structures into a compact tree representation, Ctree, and indexes XML text contents. To support XML ranking, we propose the concepts of \"weighted term frequency\" and \"inverted element frequency,\" where the weight of a term depends on its frequency and location within an XML element as well as its popularity among similar elements in an XML dataset. We evaluate the effectiveness of our system through extensive experiments on the INEX 03 dataset and 30 content and structure (CAS) topics. The experimental results reveal that our system has significantly high precision at low recall regions and achieves the highest average precision (0.3309) as compared with 38 official INEX 03 submissions using the strict evaluation metric.",
        "Comparative study of four impact measures and qualitative conclusions a b s t r a c tWe present a comparative study of four impact measures: the h-index, the g-index, the Rindex and the j-index. The g-index satisfies the transfer principle, the j-index satisfies the opposite transfer principle while the h-and R-indices do not satisfy any of these principles. We study general inequalities between these measures and also determine their maximal and minimal values, given a fixed total number of citations.",
        "Inverted indexes for phrases and strings. Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed tradeoffs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.",
        "Fast-Forward Index Methods for Pseudo-Relevance Feedback Retrieval The inverted index is the dominant indexing method in information retrieval systems. It enables fast return of the list of all documents containing a given query term. However, for retrieval schemes involving query expansion, as in pseudo-relevance feedback (PRF), the retrieval time based on an inverted index increases linearly with the number of expansion terms. In this regard, we have examined the use of a forward index, which consists of the mapping of each document to its constituent terms. We propose a novel forward indexbased reranking scheme to shorten the PRF retrieval time. In our method, a first retrieval of the original query is performed using an inverted index, and then a forward index is employed for the PRF part. We have studied several new forward indexes, including using a novel spstring data structure and the weighted variable bit-block compression (wvbc) signature. With modern hardware such as solid-state drives (SSDs) and sufficiently large main memory, forward index methods are particularly promising. We find that with the whole index stored in main memory, PRF retrieval using a spstring or wvbc forward index excels in time efficiency over an inverted index, being able to obtain the same levels of performance measures at shorter times. Categories and Subject"
    ],
    "Architecture of web search engine": [
        "Scalability and efficiency challenges in large-scale web search engines. The main goals of a web search engine are quality, e ciency, and scalability. In this tutorial, we focus on the last two goals, providing a fairly comprehensive overview of the scalability and e ciency challenges in large-scale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, and query processing components. The scalability and e ciency issues encountered in these components are presented at four di\u21b5erent granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at open research problems and provides recommendations to researchers who are new to the field.",
        "Tuning the capacity of search engines: Load-driven routing and incremental caching to reduce and balance the load This article introduces an architecture for a document-partitioned search engine, based on a novel approach combining collection selection and load balancing, called load-driven routing. By exploiting the query-vector document model, and the incremental caching technique, our architecture can compute very high quality results for any query, with only a fraction of the computational load used in a typical document-partitioned architecture. By trading off a small fraction of the results, our technique allows us to strongly reduce the computing pressure to a search engine back-end; we are able to retrieve more than 2/3 of the top-5 results for a given query with only 10% the computing load needed by a configuration where the query is processed by each index partition. Alternatively, we can slightly increase the load up to 25% to improve precision and get more than 80% of the top-5 results. In fact, the flexibility of our system allows a wide range of different configurations, so as to easily respond to different needs in result quality or restrictions in computing power. More important, the system configuration can be adjusted dynamically in order to fit unexpected query peaks or unpredictable failures. This article wraps up some recent works by the authors, showing the results obtained by tests conducted on 6 million documents, 2,800,000 queries and real query cost timing as measured on an actual index.",
        "Scalability and efficiency challenges in commercial web search engines. Commercial web search engines rely on very large compute infrastructures to be able to cope with the continuous growth of the Web and user bases. Achieving scalability and efficiency in such large-scale search engines requires making careful architectural design choices while devising algorithmic performance optimizations. Unfortunately, most details about the internal functioning of commercial web search engines remain undisclosed due to their financial value and the high level of competition in the search market. The main objective of this tutorial is to provide an overview of the fundamental scalability and efficiency challenges in commercial web search engines, bridging the existing gap between the industry and academia. Categories and Subject Descriptors General TermsAlgorithms, Design, Performance KeywordsWeb search engines, crawling, indexing, query processing, caching, efficiency, scalability. DESCRIPTIONCommercial web search engines need to process thousands of queries every second and provide responses to user queries within a few hundred milliseconds. As a consequence of these tight performance constraints, search engines construct and maintain very large compute infrastructures for crawling the Web, indexing discovered pages, and processing user queries . The scalability and efficiency of these infrastructures require careful performance optimizations in every major component of the search engine [2].Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). This tutorial aims to provide a fairly comprehensive overview of the scalability and efficiency challenges in largescale web search engines . In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, query processing, and caching components. The scalability and efficiency issues encountered in the above-mentioned components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at the open research problems and provides recommendations to researchers who are new to the field. OBJECTIVESThe tutorial aims to achieve the following objectives:1. To provide in-depth background on the basic architectural components in a web search engine.2. To present the fundamental scalability and efficiency issues which have been often addressed in the information retrieval literature.3. To shed some light into the techniques used in largescale commercial search engines and bridge the gap between the industry and academia. 4.To identify the open research problems in the context of web search engine scalability and efficiency, promoting further research on the topic. REFERENCES[1] B. B. Cambazoglu and R. Baeza-Yates. Scalability challenges in web search engines. In M. Melucci, R.",
        "Distributed web retrieval. In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (over 270 millions at the beginning of 2011) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability; in spite of network latency and scattered data. In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling, indexing, and query processing.",
        "Scalability and Efficiency Challenges in Large-Scale Web Search Engines. Commercial web search engines need to process thousands of queries every second and provide responses to user queries within a few hundred milliseconds. As a consequence of these tight performance constraints, search engines construct and maintain very large computing infrastructures for crawling the Web, indexing discovered pages, and processing user queries. The scalability and efficiency of these infrastructures require careful performance optimizations in every major component of the search engine.This tutorial aims to provide a fairly comprehensive overview of the scalability and efficiency challenges in largescale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, and query processing components. The scalability and efficiency issues encountered in these components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points out some open research problems and provides recommendations to researchers who are new to the field. KeywordsWeb search engines; crawling; indexing; query processing; efficiency; scalability OUTLINEThe tutorial has four sections. The first section provides the main concepts. Each of the remaining sections covers a particular architectural component of a web search engine. Main ConceptsIn this section, we provide the necessary background for the following sections. First, we explain the main challenges posed by the Web and its characteristics. Second, we present the main components of a search engine and possible software/hardware architectures. Finally, we summarize the inPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. teractions among the three main components: crawler, indexer, and query processor. Web CrawlingThe web crawler is responsible for discovering new web pages and downloading their content while refreshing the content of previously downloaded pages . The main performance objective for a web crawler is to achieve high page download rates. The efficiency of a sequential crawler is mainly determined by the selection and implementation of proper data structures . Scalability is achieved through multi-threading and parallelization . In addition to such techniques, our tutorial covers the URL reordering and web repository refreshing [43] problems. IndexingThe indexing process involves various parsing, extraction, and classification tasks, through which certain features are extracted and the textual content of downloaded pages are converted into an inverted index . The main performance metrics for an indexer are the length of deployment cycles, compactness, and the speed of index updates. So far, most algorithmic improvements are concentrated on index compression , duplicate detection , and document reordering . At the architectural level, the efficiency and scalability are improved via online index construction , index partitioning , and static index pruning . Our tutorial also covers the indexing strategies for geographically distributed web search engines . Query ProcessingQuery processing involves generating the set of bestmatching results for a given user query . The success of a query processor is mainly assessed by its throughput and average response latency. A large body of research work on relevance ranking assume relatively standard processing techniques using an inverted index while large-scale search engines rely on more sophisticated techniques. In addition to the state-of-the-art ranking techniques employed in web search engines, our tutorial covers a variety of problems including query processing on multi-core architectures , and green search engines . We also briefly present learning-to-rank optimizations in web search and discuss the impact of efficiency improvements on user engagement . 1223",
        "Scalability and Efficiency Challenges in Large-Scale Web Search Engines. Commercial web search engines need to process thousands of queries every second and provide responses to user queries within a few hundred milliseconds. As a consequence of these tight performance constraints, search engines construct and maintain very large computing infrastructures for crawling the Web, indexing discovered pages, and processing user queries. The scalability and efficiency of these infrastructures require careful performance optimizations in every major component of the search engine.This tutorial aims to provide a fairly comprehensive overview of the scalability and efficiency challenges in largescale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, and query processing components. The scalability and efficiency issues encountered in the above-mentioned components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at the open research problems and provides recommendations to researchers who are new to the field.",
        "Using graphics processors for high-performance IR query processing. Web search engines are facing formidable performance challenges as they need to process thousands of queries per second over billions of documents. To deal with this heavy workload, current engines use massively parallel architectures of thousands of machines that require large hardware investments.We investigate new ways to build such high-performance IR systems based on Graphical Processing Units (GPUs). GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism. Recently a number of researchers have studied how to use GPUs for other problem domains including databases and scientific computing [2, 3, 5], but we are not aware of previous attempts to use GPUs for large-scale web search. Our contribution here is to design a basic system architecture for GPU-based highperformance IR, and to describe how to perform highly efficient query processing within such an architecture. Preliminary experimental results based on a prototype implementation suggest that significant gains in query processing performance might be obtainable with such an approach. Categories and Subject Descriptors: H.3 [INFORMA-TION STORAGE AND RETRIEVAL]General Terms: Performance. Keywords: Web search, query processing, GPU. TECHNICAL PRELIMINARIESFor a good overview of IR query processing, see . We assume that each document (e.g., each page covered by the engine) is identified by a unique document ID (docID). Our approach is based on an inverted index structure, used in essentially all current web search engines, which allows efficient retrieval of documents containing a particular set of words (or terms). An inverted index consists of many inverted lists, where each inverted list Iw contains the docIDs of all documents in the collection that contain the word w, sorted by document ID or some other measure, plus possibly the number of occurrences in the document and their positions. Inverted indexes are usually stored in highly compressed form on disk, or sometimes in main memory if space is available.Given an inverted index, the basic structure of query processing is as follows: The inverted lists of the query terms are first fetched from disk or main memory and decompressed, and then an intersection or other Boolean filter between the lists is applied to determine those docIDs that contain all or most of the query terms. For these docIDs, the additional information associated with the docID in the index (such as number of occurrences and their positions) is used to compute a score for the document, and the k top-scoring documents are returned. These operations are usually pipelined so that the score of a document is computed immediately after the Boolean filter (usually intersection) is applied to it, which is itself done right after decompressing the relevant index entries, with no need to temporarily write the decompressed data to main memory. Thus, the main operations required are index decompression, intersection, and score computation. Query processing is responsible for a large fraction of the total hardware cost of a large state-of-the-art engine.Index Compression: Compression of inverted indexes greatly reduces disk space use as well as disk and main memory accesses, resulting in faster query evaluation. There are many index compression methods ; the basic idea in most of them is to first compute the differences (gaps) between the sorted docIDs in an inverted list, and then apply a suitable integer compression scheme to the gaps. During decompression, the gaps are decoded and then summed up again in a prefix sum operation. In our prototype, we focus on two methods that we believe are particularly suitable for GPUs: Rice coding, and a recent method in called PForDelta.To compress a sequence of gaps with Rice coding, we first choose a b such that 2 b is close to the average of the gaps to be coded. Then each gap n is encoded in two parts: a quotient q = n/(2 b ) stored in unary code, and a remainder r = n mod 2 b stored in binary using b bits. While Rice coding is considered somewhat slow in decompression speed, we consider here a new implementation proposed in that is much faster than the standard one. The second compression method we consider is the PForDelta method proposed in , which was shown to decompress up to a billion integers per second on current CPUs. This method first determines a b such that most of the values in the list (say, 90%) are less than 2 b and thus fit into a fixed bit field of b bits each. The remaining integers, called exceptions, are coded separately. Both methods were recently evaluated for CPUs in .Graphical Processing Units (GPUs): The current generations of GPUs arose due to the increasing demand for processing power by graphics-oriented applications such as computer games. Because of this, GPUs are highly optimized towards the types of operations needed in graphics, but researchers have recently studied how to exploit the computing power of these processors for other types of applications . Modern GPUs offer multiple computing cores that can perform many operations in parallel, plus a very high memory bandwidth that allows processing of large amounts of data. However, to be efficient, computations need to the carefully structured to conform to the programming model offered by the GPU, which is a data-parallel model reminiscent of the massively parallel SIMD models studied in the 1980s.Recently, GPU vendors have started to offer better support for general-purpose computation on GPUs, thus removing some of the hassle of programming them. However, the requirements of the basic data-parallel programming model remain; in particular, it is important to structure computation in a very regular (oblivious) manner, such that each concurrently executed thread performs essentially the same sequence of basic steps. This is especially challenging for operations such as decompression and intersection that tend to be more adaptive in nature. One major vendor of GPUs,",
        "Scalability and efficiency challenges in large-scale web search engines. Large-scale web search engines rely on massive compute infrastructures to be able to cope with the continuous growth of the Web and their user bases. In such search engines, achieving scalability and efficiency requires making careful architectural design choices while devising algorithmic performance optimizations. Unfortunately, most details about the internal functioning of commercial web search engines remain undisclosed due to their financial value and the high level of competition in the search market. The main objective of this tutorial is to provide an overview of the fundamental scalability and efficiency challenges in commercial web search engines, bridging the existing gap between the industry and academia. Categories and Subject Descriptors KeywordsWeb search engines; crawling; indexing; query processing; efficiency; scalability DESCRIPTIONCommercial web search engines need to process thousands of queries every second and provide responses to user queries within a few hundred milliseconds. As a consequence of these tight performance constraints, search engines construct and maintain very large compute infrastructures for crawling the Web, indexing discovered pages, and processing user queries. Achieving scalability and efficiency requires making careful performance optimizations in every major component of the search engine.This tutorial aims to provide a fairly comprehensive overview of the scalability and efficiency challenges in largescale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6-11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2602291. mainly focusing on the web crawling, indexing, and query processing components. The scalability and efficiency issues encountered in the above-mentioned components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at the open research problems and provides recommendations to researchers who are new to the field. PREVIOUS EDITIONSThe content and structure of the tutorial are based on a book chapter published by the presenters . A small subset of this tutorial is presented in the 2nd COST 804 Training School on Energy Efficiency in Large Scale Distributed Systems in 2012. More recently, the tutorial was presented in SIGIR '13 [3] and WWW'14 [1]. OBJECTIVESThe following are the main objectives of the tutorial.\u2022 To provide an in-depth background on the architectural components of a web search engine.\u2022 To present the fundamental scalability and efficiency issues which have been often addressed in the information retrieval literature.\u2022 To shed some light into the techniques used in largescale commercial search engines and bridge the gap between the industry and academia.\u2022 To identify open research problems in the context of web search engine scalability and efficiency, promoting further research on the topic. REFERENCES[1] R. Baeza-Yates and B. B. Cambazoglu. Scalability and efficiency challenges in large-scale web search engines.",
        "The Architecture of eBay Search The architecture of a large-scale search engine is dependent on the application. In the case of a web search engine, the document collection can be considered to be a constantly growing, but slowly changing, archive. The task is to find and crawl good pages and to index them. The rate of change of these pages can be estimated and the pages re-crawled and re-indexed periodically. Users issue queries, and the results to those queries are likely to be the same from day to day so can be cached. eCommerce, on the other hand, is quite different. The document collection can change very quickly, the results of queries may differ from user to user, and crawling may not be required. In this contribution, we outline the architecture of Cassini, the eBay search engine. eBay tackles a problem quite different from that of a traditional web search and consequently chose to design and build a search engine from scratch and to customize it to the nature of the problem.",
        "Frankenplace: Interactive Thematic Mapping for Ad Hoc Exploratory Search. Ad hoc keyword search engines built using modern information retrieval methods do a good job of handling fine-grained queries. However, they perform poorly at facilitating spatial and spatially-embedded thematic exploration of the results, despite the fact that many queries, e.g. civil war, refer to different documents and topics in different places. This is not for lack of data: geographic information, such as place names, events, and coordinates are common in unstructured document collections on the web. The associations between geographic and thematic contents in these documents can provide a rich groundwork to organize information for exploratory research. In this paper we describe the architecture of an interactive thematic map search engine, Frankenplace, designed to facilitate document exploration at the intersection of theme and place. The map interface enables a user to zoom the geographic context of their query in and out, and quickly explore through thousands of search results in a meaningful way. And by combining topic models with geographically contextualized search results, users can discover related topics based on geographic context. Frankenplace utilizes a novel indexing method called geoboost for boosting terms associated with cells on a discrete global grid. The resulting index factors in the geographic scale of the place or feature mentioned in related text, the relative textual scope of the place reference, and the overall importance of the containing document in the document network. The system is currently indexed with over 5 million documents from the web, including the English Wikipedia and online travel blog entries. We demonstrate that Frankenplace can support four distinct types of exploratory search tasks while being adaptive to scale and location of interest."
    ],
    "What is AHP": [
        "A decision-theoretic approach to the evaluation of information retrieval systems. Evaluation research on information retrieval (IR) systems has thus far been narrowly focused and disjointed. This research attempts to narrow the gap by providing a comprehensive and integrated multiple criteria decision-theoretic approach for the evaluation of IR systems. The approach, which is based on the Analytic Hierarchy Process (AHP), is illustrated in the context of a domain-specific IR system. The novelty of this approach lies in the focus on the user aspect and the application of decision-making theories in the IR field. \u00d3 2005 Elsevier Ltd. All rights reserved.Keywords: Information retrieval system; Evaluation; Information search process; Decision making; Multi-criteria model; Analytic hierarchy process BackgroundInformation retrieval (IR) systems play an important role in information focusing and knowledge extension. Evaluation is a major force in research, development and applications related to IR . ''The challenge for the next decade,'' as Robertson and Hancock-Beaulieu (1992, p. 465) pointed out, ''is to explore the multiple dimensions and components of the new generation of information retrieval systems by experimenting with a diversity of evaluative approaches.''The overwhelming majority of IR evaluations have used the system-centered approach, which focuses on the assessment of search algorithms using statistical measures, with the best-known being recall (the ratio of relevant items retrieved to all relevant items) and precision (the ratio of relevant items retrieved to all 0306-4573/$ -see front matter \u00d3",
        "IIoT-SIDefender: Detecting and defense against the sensitive information leakage in industry IoT. With Industry 4.0 and Internet of Things (IoT) era coming, remote passwords and control-flow vulnerabilities play a key role to detect attackers in Industry IoT (IIoT), who can easily complete remote session and control-flow hijacking on leverage of these types of Sensitive Information (SI). However, how to measure security degree of Sensitive Information is an open issue. To our best knowledge, no effective method can detect secret trace of SI thieves in Advanced Persistent Threat (APT), especially for backdoors and vulnerabilities in software or firmware. To deal with these problems, we propose a new design, called, IIoT-SIDefender (IIoT-SID), we measure security degree of Sensitive Information via Analytic Hierarchy Process (AHP) and Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS), based on selected taint tracking and real-time memory modification, attack-defense and fix-distribution approaches are proposed. Until now, it is the first defined SI guard method to detect SI-leakage scenarios and reject SI-leverage attack. To verify our proposal, experimental tests are verified in a large number of IIoT applications and devices, including IP cameras, smart meters, PLCs and smart routers. Test results have demonstrated that we can capture security level for Sensitive Information as expected, detect potential leakage points in data lifetime (including unknown backdoors and vulnerabilities), describe fine-grained semantics of accidental leakage and secret leverage points, and generate relative hot fix to prevent further attack."
    ],
    "What is information retrieval?": [
        "On the Allocation of Documents in Multiprocessor Information Retrieval Systems. Information retrieval is the selection of documents that are potentially relevant to a user's information need. Given the vast volume of data stored in modern information retrieval systems, searching the document database requires vast computational resources. To meet these computational demands, various researchers have developed parallel information retrieval systems. As",
        "Exploiting Entity Linking in Queries for Entity Retrieval. The premise of entity retrieval is to better answer search queries by returning specific entities instead of documents. Many queries mention particular entities; recognizing and linking them to the corresponding entry in a knowledge base is known as the task of entity linking in queries. In this paper we make a first attempt at bringing together these two, i.e., leveraging entity annotations of queries in the entity retrieval model. We introduce a new probabilistic component and show how it can be applied on top of any termbased entity retrieval model that can be emulated in the Markov Random Field framework, including language models, sequential dependence models, as well as their fielded variations. Using a standard entity retrieval test collection, we show that our extension brings consistent improvements over all baseline methods, including the current state-of-the-art. We further show that our extension is robust against parameter settings.",
        "Utilizing Knowledge Bases in Text-centric Information Retrieval. General-purpose knowledge bases are increasingly growing in terms of depth (content) and width (coverage). Moreover, algorithms for entity linking and entity retrieval have improved tremendously in the past years. These developments give rise to a new line of research that exploits and combines these developments for the purposes of text-centric information retrieval applications. This tutorial focuses on a) how to retrieve a set of entities for an ad-hoc query, or more broadly, assessing relevance of KB elements for the information need, b) how to annotate text with such elements, and c) how to use this information to assess the relevance of text. We discuss different kinds of information available in a knowledge graph and how to leverage each most effectively.We start the tutorial with a brief overview of different types of knowledge bases, their structure and information contained in popular general-purpose and domain-specific knowledge bases. In particular, we focus on the representation of entity-centric information in the knowledge base through names, terms, relations, and type taxonomies. Next, we will provide a recap on ad-hoc object retrieval from knowledge graphs as well as entity linking and retrieval. This is essential technology, which the remainder of the tutorial builds on. Next we will cover essential components within successful entity linking systems, including the collection of entity name information and techniques for disambiguation with contextual entity mentions. We will present the details of four previously proposed systems that successfully leverage knowledge bases to improve ad-hoc document retrieval. These systems combine the notion of entity retrieval and semantic search on one hand, with text retrieval models and entity linking on the other. Finally, we also touch on entity aspects and links in the knowledge graph as it can help to understand the entities' context. This tutorial is the first to compile, summarize, and disseminate progress in this emerging area and we provide both an overview of state-of-the-art methods and outline open research problems to encourage new contributions.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Her scientific contributions span from entity linking to the prediction of influences in citation graphs. In this tutorial, she will cover her seminal publication on entity query feature expansion and her work on finding relevant relations. ICTIR '16Prof. Dr. Alexander Kotov is an Assistant Professor in the Department of Computer Science at Wayne State University. His general research interests lie at the intersection of information retrieval, textual data mining and health informatics. Before joining Wayne State, he was a post-doctoral fellow at Emory University working with Prof. Eugene Agichtein. Dr. Kotov obtained his PhD from the University of Illinois at Urbana-Champaign, under the supervision of Professor ChengXiang Zhai. At Wayne State has been teaching graduate courses on Information Retrieval and NoSQL databases as well as undergraduate courses. This tutorial will cover his work on using semantic networks for query expansion and his recent work on entity retrieval from knowledge graphs.Dr. Edgar Meij is a senior scientist at Bloomberg. Before this, he was a research scientist at Yahoo Labs and a postdoc at the University of Amsterdam, where he also obtained his Ph.D. He regularly teaches at the (post-)graduate level, including university courses and conference tutorials, e.g., at EACL 2009, SIGIR 2013, WWW 2013, and WSDM 2014. His research focuses on all applications and aspects of knowledge graphs, entity linking, and semantic search. This tutorial will cover his contributions on entity aspect mining and finding support passages for relations. AcknowledgmentsThis work was in part funded by the Elitepostdoc program of the BW-Stiftung and the University of New Hampshire. 5",
        "Visualizing and mapping the intellectual structure of information retrieval a b s t r a c tInformation retrieval is a long established subfield of library and information science. Since its inception in the early-to mid -1950s, it has grown as a result, in part, of well-regarded retrieval system evaluation exercises/campaigns, the proliferation of Web search engines, and the expansion of digital libraries. Although researchers have examined the intellectual structure and nature of the general field of library and information science, the same cannot be said about the subfield of information retrieval. We address that in this work by sketching the information retrieval intellectual landscape through visualizations of citation behaviors. Citation data for 10 years (2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008)(2009) were retrieved from the Web of Science and analyzed using existing visualization techniques. Our results address information retrieval's co-authorship network, highly productive authors, highly cited journals and papers, author-assigned keywords, active institutions, and the import of ideas from other disciplines.",
        "On Biases in Information Retrieval Models and Evaluation. The advent of the modern information technology has benefited society as the digitization of content increased over the last half-century. While the processing capability of our species has remained unchanged, the information available to us has been notably increasing. In this overload of information, Information Retrieval (IR) has been playing a prominent role by developing systems capable of separating relevant information from the rest. This separation, however, is a difficult task rooted in the complexity of understanding of what is and what is not relevant. To manage this complexity, IR has developed a strong empirical nature, which has led to the development of grounded retrieval models, resulting in the development of retrieval systems empirically designed to be biased towards relevant information. However, other biases have been observed, which counteract retrieval performance. In this thesis, the reduction of retrieval systems to filters of information, or sampling processes, has allowed us to systematically investigate these biases.We study biases manifesting in two aspects of IR research: retrieval models and retrieval evaluation. We start by identifying retrieval biases in probabilistic IR models and then develop new document priors to improve retrieval performance . Next, we discuss the accessibility bias of retrieval models, and for Boolean retrieval models we develop a mathematical framework of retrievability . For retrieval evaluation biases, we study how test collections are built using the pooling method and how this method introduces bias . Then, to improve the reliability of the evaluation, we first develop new pooling strategies to mitigate this bias at test collection build time and then, for two IR evaluation measures, Precision and Recall at cut-off (P@n and R@n), we develop new pool bias estimators to mitigate it at evaluation time .Through a large scale experimentation involving up to 15 test collections, four IR evaluation measures and three bias measures, we demonstrate that including document priors based on verboseness improves the performance of probabilistic retrieval models; that the accessibility bias of Boolean retrieval models quickly worsens for conjunctive queries with the increase of the query length (while slightly improving for disjunctive queries); that the test collection bias can be lowered at test collection build time by pooling strategies inspired by a well-known problem in reinforcement learning, the multi-armed bandit problem; and that this bias can also be improved at evaluation time by analyzing the runs participating in the pool. For this last point in particular, we show that for P@n, bias reduction is done",
        "Linguistic and semantic passage retrieval strategies for question answering. Question Answering (QA) is the task of searching a large text collection for specific answers to questions posed in natural language. Though they often have access to rich linguistic and semantic analyses of their input questions, QA systems often rely on off-theshelf bag-of-words Information Retrieval (IR) solutions to retrieve passages matching a set of terms extracted from the question.There is a fundamental disconnect between the capabilities of the bag-of-words retrieval model and the retrieval needs of the QA system. Bag-of-words IR retrieves documents matching a query, but the QA system really needs documents that contain answers. Through question analysis, the QA system has compiled a sophisticated information need representation for what constitutes an answer to the question. This representation is composed of a set of linguistic and semantic constraints satisfied by answer-bearing passages. Unfortunately, off-the-shelf IR libraries commonly used in QA systems can not, in general, check these types of constraints at query-time. Poor quality retrieval can cause a QA system to fail if no answer-bearing text is retrieved, if it is not ranked highly enough, or if it is outranked or overwhelmed by false positives, text that matches the query well, yet supports a wrong answer.This thesis proposes two linguistic and semantic passage retrieval methods for QA, one based on structured retrieval and the other on rank-learning techniques. In addition, a methodology is proposed for mapping annotated text consisting of labeled spans and typed relations between them into an annotation graph representation. The annotation graph supports query-time linguistic and semantic constraint-checking, and serves as a unifying formalism for the QA system's information need and for retrieved passages. The proposed methods rely only on the relatively weak assumption that the QA system's information need can be represented as an annotation graph. The two approaches are shown to retrieve more answer-bearing text, more highly ranked, compared to a bag-of-words baseline for two different QA tasks. Linguistic and semantic passage retrieval methods are also shown to improve end-to-end QA system accuracy and answer MRR.. Question Answering (QA) is the task of searching a large text collection for specific answers to questions posed in natural language. Though they often have access to rich linguistic and semantic analyses of their input questions, QA systems often rely on off-theshelf bag-of-words Information Retrieval (IR) solutions to retrieve passages matching a set of terms extracted from the question.There is a fundamental disconnect between the capabilities of the bag-of-words retrieval model and the retrieval needs of the QA system. Bag-of-words IR retrieves documents matching a query, but the QA system really needs documents that contain answers. Through question analysis, the QA system has compiled a sophisticated information need representation for what constitutes an answer to the question. This representation is composed of a set of linguistic and semantic constraints satisfied by answer-bearing passages. Unfortunately, off-the-shelf IR libraries commonly used in QA systems can not, in general, check these types of constraints at query-time. Poor quality retrieval can cause a QA system to fail if no answer-bearing text is retrieved, if it is not ranked highly enough, or if it is outranked or overwhelmed by false positives, text that matches the query well, yet supports a wrong answer.This thesis proposes two linguistic and semantic passage retrieval methods for QA, one based on structured retrieval and the other on rank-learning techniques. In addition, a methodology is proposed for mapping annotated text consisting of labeled spans and typed relations between them into an annotation graph representation. The annotation graph supports query-time linguistic and semantic constraint-checking, and serves as a unifying formalism for the QA system's information need and for retrieved passages. The proposed methods rely only on the relatively weak assumption that the QA system's information need can be represented as an annotation graph. The two approaches are shown to retrieve more answer-bearing text, more highly ranked, compared to a bag-of-words baseline for two different QA tasks. Linguistic and semantic passage retrieval methods are also shown to improve end-to-end QA system accuracy and answer MRR.",
        "Active feedback in ad hoc information retrieval. Information retrieval is, in general, an iterative search process, in which the user often has several interactions with a retrieval system for an information need. The retrieval system can actively probe a user with questions to clarify the information need instead of just passively responding to user queries. A basic question is thus how a retrieval system should propose questions to the user so that it can obtain maximum benefits from the feedback on these questions. In this paper, we study how a retrieval system can perform active feedback, i.e., how to choose documents for relevance feedback so that the system can learn most from the feedback information. We present a general framework for such an active feedback problem, and derive several practical algorithms as special cases. Empirical evaluation of these algorithms shows that the performance of traditional relevance feedback (presenting the top K documents) is consistently worse than that of presenting documents with more diversity. With a diversity-based selection algorithm, we obtain fewer relevant documents, however, these fewer documents have more learning benefits.",
        "Diagnostic Evaluation of Information Retrieval Models Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.",
        "Distilling Knowledge for Fast Retrieval-based Chat-bots Response retrieval is a subset of neural ranking in which a model selects a suitable response from a set of candidates given a conversation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as customer support agents. To make pairwise comparisons between a conversation history and a candidate response, two approaches are common: cross-encoders performing full self-attention over the pair and bi-encoders encoding the pair separately. The former gives better prediction quality but is too slow for practical use. In this paper, we propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model using distillation. This effectively boosts bi-encoder performance at no cost during inference time. We perform a detailed analysis of this approach on three response retrieval datasets. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; \u2022 Computing methodologies \u2192 Natural language processing.",
        "IR Research: Challenges and Long-range Opportunities. The field of Information Retrieval (IR) is under steady development and change. Information collections have become larger and more diversified; elaborated IT platforms penetrate most life situations; computers have become more powerful; networks and devices more widespread; social media and information interaction in many forms are increasingly used in daily life, etc. Simultaneously, the information experiences of users and use of information has become increasingly di\u21b5erentiated. IR is well-integrated into all kinds of mobile, transactional and traditional information systems dealing with most kinds of media. 3. Personalization issues -should user models be socially constructed or real-time dependent? Social media in integration -useful to IR?Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. 5. Non-textual media -which models are scientifically sound?6. Small and medium-sized integrated collections -enterprise IR: where do we stand?7. Evaluation and metrics -theory-driven or ad-hoc?Some of the themes can be combined in response to the questions, for instance, evaluation methodologies for integrated information systems, and sub-themes can be evoked, e.g. tasks, authority or importance vs. relevance and relevance feedback for information interaction. Remember that IR theory is not just a mathematical-logical enterprise but includes conceptual models, frameworks and perspectives as well.",
        "Towards a Game-Theoretic Framework for Information Retrieval. The task of information retrieval (IR) has traditionally been defined as to rank a collection of documents in response to a query. While this definition has enabled most research progress in IR so far, it does not model accurately the actual retrieval task in a real IR application, where users tend to be engaged in an interactive process with multipe queries, and optimizing the overall performance of an IR system on an entire search session is far more important than its performance on an individual query.In this talk, I will present a new game-theoretic formulation of the IR problem where the key idea is to model information retrieval as a process of a search engine and a user playing a cooperative game, with a shared goal of satisfying the user's information need (or more generally helping the user complete a task) while minimizing the user's effort and the resource overhead on the retrieval system. Such a game-theoretic framework offers several benefits. First, it naturally suggests optimization of the overall utility of an interactive retrieval system over a whole search session, thus breaking the limitation of the traditional formulation that optimizes ranking of documents for a single query. Second, it models the interactions between users and a search engine, and thus can optimize the collaboration of a search engine and its users, maximizing the \"combined intelligence\" of a system and users. Finally, it can serve as a unified framework for optimizing both interactive information retrieval and active relevance judgment acquisition through crowdsourcing. I will discuss how the new framework can not only cover several emerging directions in current IR research as special cases, but also open up many interesting new research directions in IR.",
        "How Deep Learning Works for Information Retrieval Information retrieval (IR) is the science of search, the search of user query relevant pieces of information from a collection of unstructured resources. Information in this context includes text, imagery, audio, video, xml, program, and metadata. The journey of an IR process begins with a user query sent to the IR system which encodes the query, compares the query with the available resources, and returns the most relevant pieces of information. Thus, the system is equipped with the ability to store, retrieve and maintain information.",
        "ICTNET at TREC2017 Complex Answer Retrieval Track Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers) [1]. The most common and popular information retrieval application is web search engine such as Google [2] , Baidu [3] , Bing [4] and Sogou [5]. These application will return top-N best retrieval result to users. Information Retrieval systems for the Web, i.e., web search engines, are mainly devoted to finding relevant web documents in response to a user's query [6]. Current retrieval systems performance well in phrase-level retrieval tasks which provide simple fact and entitycentric needs. Complex Answer Retrieval Track is a new track in 2017, which requests a more complex and longer retrieval result to answer a query. It focuses on developing systems that are capable of answering complex information needs by collating relevant information from an entire corpus. Given an article stub Q, retrieval for each of its sections Hi, a ranking of relevant entity-passage tuples (E, P). Tow tasks are offered: passage ranking and entity ranking. This paper introduces an algorithm and a system for passage ranking. The retrieval queries are outlines which consist of titles and section titles of articles. The retrieval collection consists of paragraphs which are come from Wikipedia articles. We use the BM25 algorithm and develop a system to retrieval the top-100 most relevant paragraphs."
    ],
    "Efficient retrieval algorithms": [
        "Impact of the Query Set on the Evaluation of Expert Finding Systems Expertise is a loosely defined concept that is hard to formalize. Much research has focused on designing efficient algorithms for expert finding in large databases in various application domains. The evaluation of such recommender systems lies most of the time on humanannotated sets of experts associated with topics. The protocol of evaluation consists in using the namings or short descriptions of these topics as raw queries in order to rank the available set of candidates. Several measures taken from the field of information retrieval are then applied to rate the rankings of candidates against the ground truth set of experts. In this paper, we apply this topic-query evaluation methodology with the AMiner data and explore a new document-query methodology to evaluate experts retrieval from a set of queries sampled directly from the experts documents. Specifically, we describe two datasets extracted from AMiner, three baseline algorithms from the literature based on several document representations and provide experiment results to show that using a wide range of more realistic queries provides different evaluation results to the usual topic-queries.",
        "Workshop on large-scale and distributed systems for information retrieval (LSDS-IR 2014). The LSDS-IR'14 workshop aims to bring together information retrieval practitioners from industry and academic researchers concerned with efficient and distributed IR systems. The workshop also welcomes contributions that propose different ways of leveraging diversity and multiplicity of resources available in distributed systems. The main goal of the workshop is to attract people from industry and academia to present and discuss ideas, problems, and results related to the efficiency of large scale and distributed information retrieval systems. Categories and Subject Descriptors KeywordsDistributed search; large-scale systems; scalability; efficiency OVERVIEWThe growth of the Web and other Big Data sources lead to important performance problems for large-scale distributed information retrieval (IR) systems. Scalability and efficiency of these systems also have an impact on their effectiveness. Consequently, scalability and efficiency in the context of retrieval systems continue to be an important topic in IR research. Even within research laboratory settings, the size of the standard datasets have become exceedingly large, requiring efficient and often distributed architectures to facilitate large-scale experimentation. Traditionally, web-scale search engines employ large and highly replicated systems, operating on computer clusters in one or few data centers.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). Coping with the increasing number of user requests and indexable pages requires adding more resources. However, data centers cannot grow indefinitely. Scalability problems in information retrieval have to be addressed and new distributed applications are likely to drive the way in which people search the Web. Distributed IR is the point in which these two directions converge. This workshop provides space for researchers to discuss these problems and to define new directions for the work on distributed information retrieval. TOPICS OF INTERESTWe solicited technical and position paper submissions in the following research areas:\u2022 Novel distributed IR applications\u2022 Web search in the cloud: applications of cloud computing to IR\u2022 Big Data techniques for IR applications\u2022 Energy-aware processing techniques for web search\u2022 IR algorithms on novel computing architectures\u2022 Efficiency for feature-based models\u2022 Efficiency and scalability in distributed query processing\u2022 Multi-site web search engines\u2022 Inverted index partitioning and replication models\u2022 Lossy and lossless inverted index compression\u2022 Collection selection and result merging in federated IR\u2022 Distributed web crawling\u2022 Distributed indexing and retrieval of dynamic information sources\u2022 Caching for search engines\u2022 Retrieval models for distributed IR\u2022 Standards and benchmarks for distributed IR\u2022 Evaluation of the impact of search efficiency on user satisfaction\u2022 IR on large-scale web archives\u2022 IR algorithms for exotic high-performance computer architectures 691",
        "Efficient retrieval of recommendations in a matrix factorization framework. Low-rank Matrix Factorization (MF) methods provide one of the simplest and most effective approaches to collaborative filtering. This paper is the first to investigate the problem of efficient retrieval of recommendations in a MF framework. We reduce the retrieval in a MF model to an apparently simple task of finding the maximum dot-product for the user vector over the set of item vectors. However, to the best of our knowledge the problem of efficiently finding the maximum dot-product in the general case has never been studied. To this end, we propose two techniques for efficient search -(i) We index the item vectors in a binary spatialpartitioning metric tree and use a simple branch-and-bound algorithm with a novel bounding scheme to efficiently obtain exact solutions. (ii) We use spherical clustering to index the users on the basis of their preferences and pre-compute recommendations only for the representative user of each cluster to obtain extremely efficient approximate solutions. We obtain a theoretical error bound which determines the quality of any approximate result and use it to control the approximation. Both these simple techniques are fairly independent of each other and hence are easily combined to further improve recommendation retrieval efficiency. We evaluate our algorithms on real-world collaborative-filtering datasets, demonstrating more than \u00d77 speedup (with respect to the naive linear search) for the exact solution and over \u00d7250 speedup for approximate solutions by combining both techniques.",
        "Full and Mini-batch Clustering of News Articles with Star-EM. We present a new threshold-based clustering algorithm for news articles. The algorithm consists of two phases: in the first, a local optimum of a score function that captures the quality of a clustering is found with an Expectation-Maximization approach. In the second phase, the algorithm reduces the number of clusters and, in particular, is able to build non-spherical-shaped clusters. We also give a mini-batch version which allows an efficient dynamic processing of data points as they arrive in groups. Our experiments on the TDT5 benchmark collection show the superiority of both versions of this algorithm compared to other stateof-the-art alternatives.",
        "Computing Maximum Number of Runs in Strings. A run (also called maximal repetition) in a word is a nonextendable repetition. Finding the maximum number \u03c1(n) of runs in a string of length n is a challenging problem. Although it is known that \u03c1(n) \u2264 1.029n for any n and there exists large n such that \u03c1(n) \u2265 0.945n, the exact value of \u03c1(n) is still unknown. Several algorithms have been proposed to count runs in a string efficiently, and \u03c1(n) can be obtained for small n by these algorithms. In this paper, we focus on computing \u03c1(n) for given length parameter n, instead of exhaustively counting all runs for every string of length n. We report exact values of \u03c1(n) for binary strings for n \u2264 66, together with the strings which contain \u03c1(n) runs.",
        "An Integrated Deterministic and Nondeterministic Inference Algorithm for Sequential Labeling. In this paper, we present a new search algorithm for sequential labeling tasks based on the conditional Markov models (CMMs) frameworks. Unlike conventional beam search, our method traverses all possible incoming arcs and also considers the \"local best\" so-far of each previous node. Furthermore, we propose two heuristics to fit the efficiency requirement. To demonstrate the effect of our method, six variant and large-scale sequential labeling tasks were conducted in the experiment. In addition, we compare our method to Viterbi and Beam search approaches. The experimental results show that our method yields not only substantial improvement in runtime efficiency, but also slightly better accuracy. In short, our method achieves 94.49 F (\u03b2) rate in the well-known CoNLL-2000 chunking task.",
        "Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce. This paper explores the problem of computing pairwise similarity on document collections, focusing on the application of \"more like this\" queries in the life sciences domain. Three MapReduce algorithms are introduced: one based on brute force, a second where the problem is treated as large-scale ad hoc retrieval, and a third based on the Cartesian product of postings lists. Each algorithm supports one or more approximations that trade effectiveness for efficiency, the characteristics of which are studied experimentally. Results show that the brute force algorithm is the most efficient of the three when exact similarity is desired. However, the other two algorithms support approximations that yield large efficiency gains without significant loss of effectiveness.",
        "Using Local Optimality Criteria for Efficient Information Retrieval with Redundant Information Filters We consider information retrieval when the data-for instance, multimedia-is computationally expensive to fetch. Our approach uses \"information filters\" to considerably narrow the universe of possibilities before retrieval. We are especially interested in redundant information filters that save time over more general but more costly filters. Etlicient retrieval requires that decisions must be made about the necessity, order, and concurrent processing of proposed filters (an \"execution plan\"). We develop simple polynomial-time local criteria for optimal execution plans and show that most forms of concurrency are suboptimal with information filters. Although the general problem of finding an optimal execution plan is likely to be exponential in the number of filters, we show experimentally that our local optimality criteria, used in a polynomial-time algorithm, nearly always find the global optimum with 15 filters or less, a sufilcient number of filters for most applications. Our methods require no special hardware and avoid the high processor idleness that is characteristic of massive-parallelism solutions to this problem. We apply our ideas to an important application, information retrieval of captioned data using natural-language understanding, a problem for which the natural-language processing can be the bottleneck if not implemented well.",
        "An efficient algorithm for approximate biased quantile computation in data streams. We propose an efficient algorithm for approximate biased quantile computation in large data streams. Our algorithm computes decomposable biased quantile summaries on fixed sized blocks and dynamically maintains the biased quantile summary for the entire stream as the exponential histogram over the block-wise quantile summaries. The algorithm is computationally efficient and achieves an amortized computational cost of O(log( 1 \u01eb log(\u01ebn))) and a space requirement of O(). Our algorithm does not assume prior knowledge of the stream sizes or the range of data values in the streams. In practice, our algorithm is able to efficiently maintain summaries over large data streams with over tens of millions of observations and achieves significant performance improvement over prior algorithms.",
        "Constructing efficient information extraction pipelines. Information Extraction (IE) pipelines analyze text through several stages. The pipeline's algorithms determine both its effectiveness and its run-time efficiency. In real-world tasks, however, IE pipelines often fail acceptable run-times because they analyze too much task-irrelevant text. This raises two interesting questions: 1) How much \"efficiency potential\" depends on the scheduling of a pipeline's algorithms? 2) Is it possible to devise a reliable method to construct efficient IE pipelines? Both questions are addressed in this paper. In particular, we show how to optimize the run-time efficiency of IE pipelines under a given set of algorithms. We evaluate pipelines for three algorithm sets on an industrially relevant task: the extraction of market forecasts from news articles. Using a system-independent measure, we demonstrate that efficiency gains of up to one order of magnitude are possible without compromising a pipeline's original effectiveness.",
        "Supporting efficient top-k queries in type-ahead search. Type-ahead search can on-the-fly find answers as a user types in a keyword query. A main challenge in this search paradigm is the high-efficiency requirement that queries must be answered within milliseconds. In this paper we study how to answer top-k queries in this paradigm, i.e., as a user types in a query letter by letter, we want to efficiently find the k best answers. Instead of inventing completely new algorithms from scratch, we study challenges when adopting existing top-k algorithms in the literature that heavily rely on two basic list-access methods: random access and sorted access. We present two algorithms to support random access efficiently. We develop novel techniques to support efficient sorted access using list pruning and materialization. We extend our techniques to support fuzzy type-ahead search which allows minor errors between query keywords and answers. We report our experimental results on several real large data sets to show that the proposed techniques can answer top-k queries efficiently in type-ahead search.",
        "GRAS: An effective and efficient stemming algorithm for information retrieval A novel graph-based language-independent stemming algorithm suitable for information retrieval is proposed in this article. The main features of the algorithm are retrieval effectiveness, generality, and computational efficiency. We test our approach on seven languages (using collections from the TREC, CLEF, and FIRE evaluation platforms) of varying morphological complexity. Significant performance improvement over plain word-based retrieval, three other language-independent morphological normalizers, as well as rule-based stemmers is demonstrated.",
        "Top-k most influential locations selection. We propose and study a new type of facility location selection query, the top-k most influential location selection query. Given a set M of customers and a set F of existing facilities, this query finds k locations from a set C of candidate locations with the largest influence values, where the influence of a candidate location c (c \u2208 C) is defined as the number of customers in M who are the reverse nearest neighbors of c. We first present a naive algorithm to process the query. However, the algorithm is computationally expensive and not scalable to large datasets. This motivates us to explore more efficient solutions. We propose two branch and bound algorithms, the Estimation Expanding Pruning (EEP) algorithm and the Bounding Influence Pruning (BIP) algorithm. These algorithms exploit various geometric properties to prune the search space, and thus achieve much better performance than that of the naive algorithm. Specifically, the EEP algorithm estimates the distances to the nearest existing facilities for the customers and the numbers of influenced customers for the candidate locations, and then gradually refines the estimation until the answer set is found, during which distance metric based pruning techniques are used to improve the refinement efficiency. BIP only estimates the numbers of influenced customers for the candidate locations. But it uses the existing facilities to limit the space for searching the influenced customers and achieve a better estimation, which results in an even more efficient algorithm. Extensive experiments conducted on both real and synthetic datasets validate the efficiency of the algorithms.",
        "Scalable and Efficient Web Search Result Diversification It has been shown that top-k retrieval quality can be considerably improved by taking not only relevance but also diversity into account. However, currently proposed diversification approaches have not put much attention on practical usability in large-scale settings, such as modern web search systems. In this work, we make two contributions toward this goal. First, we propose a combination of optimizations and heuristics for an implicit diversification algorithm based on the desirable facility placement principle, and present two algorithms that achieve linear complexity without compromising the retrieval effectiveness. Instead of an exhaustive comparison of documents, these algorithms first perform a clustering phase and then exploit its outcome to compose the diverse result set. Second, we describe and analyze two variants for distributed diversification in a computing cluster, for large-scale IR where the document collection is too large to keep in one node. Our contribution in this direction is pioneering, as there exists no earlier work in the literature that investigates the effectiveness and efficiency of diversification on a distributed setup. Extensive evaluations on a standard TREC framework demonstrate a competitive retrieval quality of the proposed optimizations to the baseline algorithm while reducing the processing time by more than 80% and up to 97%, and shed light on the efficiency and effectiveness tradeoffs of diversification when applied on top of a distributed architecture.",
        "Efficient PageRank with Same Out-Link Groups. Traditional PageRank algorithm suffers from heavy computation cost due to the huge number of web pages. In this paper, we propose a more efficient algorithm to compute the pagerank value for each web page directly on the same out-link groups. This new algorithm groups the pages with the same out-link behavior (SOLB) as a unit. It is proved that the derived PageRank is the same as that from the original PageRank algorithm which calculates over single webpage; while our proposed algorithm improve the efficiency greatly. For simplicity, we restrict the group within a directory and define metrics to measure the similarity of the pages in same out-link behavior. We design the experiments to group from 0.5 liked to exact SOLB pages; the results show that such group offers similar rank scores as traditional PageRank algorithm does and achieves a remarkable 50% on efficiency.",
        "Efficient Data Structures for the Factor Periodicity Problem. We present several efficient data structures for answering queries related to periods in words. For a given word w of length n the Period Query given a factor of w (represented by an interval) returns its shortest period and a compact representation of all periods. Several algorithmic solutions are proposed that balance the data structure space (ranging from O(n) to O(n log n)), and the query time complexity (ranging from O(log 1+\u03b5 n) to O(log n)).",
        "X-CLEaVER: Learning Ranking Ensembles by Growing and Pruning Trees Learning-to-Rank (LtR) solutions are commonly used in large-scale information retrieval systems such as Web search engines, which have to return highly relevant documents in response to user query within fractions of seconds. The most effective LtR algorithms adopt a gradient boosting approach to build additive ensembles of weighted regression trees. Since the required ranking effectiveness is achieved with very large ensembles, the impact on response time and query throughput of these solutions is not negligible. In this article, we propose X-CLEaVER, an iterative meta-algorithm able to build more efficient and effective ranking ensembles. X-CLEaVER interleaves the iterations of a given gradient boosting learning algorithm with pruning and re-weighting phases. First, redundant trees are removed from the given ensemble, then the weights of the remaining trees are fine-tuned by optimizing the desired ranking quality metric. We propose and analyze several pruning strategies and we assess their benefits showing that interleaving pruning and re-weighting phases during learning is more effective than applying a single post-learning optimization step. Experiments conducted using two publicly available LtR datasets show that X-CLEaVER can be successfully exploited on top of several LtR algorithms as it is effective in optimizing the effectiveness of the learnt ensembles, thus obtaining more compact forests that hence are much more efficient at scoring time. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. The problem of ranking items in response to a given query is of paramount importance for information retrieval systems. As an exemplary case of how challenging that is, consider the processing of queries submitted to a Web Search Engine (WSE), where a small and relevant set of documents must be retrieved in a fraction of a second from a huge collection. The state of the art in ranking exploits supervised machine-learning techniques based on Learning-to-Rank (LtR) algorithms . Ranking models are in this case learnt from ground truth datasets composed of labeled training examples. The ranking function obtained assigns a relevance score to each candidate document with respect to the user query, where scores allow ranking and selecting the best documents that are eventually returned to the user.While effectiveness of LtR methods has been always considered of primary importance, only recently the efficiency of learnt models attracted the interest of the scientific community [1-3, 6, 19, 33]. Ranking models deployed in large-scale information retrieval systems must, in fact, feature very low latency as well as high throughput, due to the high rate of incoming user queries.In this article, we tackle the problem of improving both efficiency and effectiveness of LtR models based on ensembles of additive regression trees, such as the ones learnt by gradient boosting algorithms, e.g., the state-of-the-art \u03bb-MART . We move from the simple observation that the cost of applying such ensemble models is linear in the number of their trees, and that ensembles composed of thousands of trees, despite being accurate, are very expensive when exploited for ranking large sets of candidate documents . We thus propose a meta-algorithm, named X-CLEaVER (eXtended-CLEaVER), which interleaves two novel steps of tree pruning and re-weighting within the usual iterative ensemble learning process: the pruning step aims at reducing the number of trees in the ensemble to improve its efficiency at scoring time, while tree re-weighting is an optimization process that aims to maximize the ranking quality of the pruned ensemble by tuning the weights associated with each tree. Any gradient boosting algorithm that produces a weighted ensemble of predictors can be used in conjunction with the X-CLEaVER meta-algorithm to grow the tree forest.X-CLEaVER stems from and extends the CLEaVER algorithm , which applies similar optimizations after the completion of the learning phase to reduce the size of a given ensemble without affecting its quality. While CLEaVER is a single-pass algorithm applied once to the learnt ensemble, X-CLEaVER employs an iterative strategy where pruning and re-weighting optimizations are repeatedly applied during the learning process. Indeed, X-CLEaVER improves some of the pruning strategies proposed in and, more importantly, it shows that embedding such steps within the boosting LtR algorithm is a profitable strategy to achieve more compact and effective ranking models. It is worth remarking that other approaches were proposed to produce simpler and faster tree-based ensembles . However, these proposals aimed at finding a tradeoff between efficiency and effectiveness during the learning phase, while X-CLEaVER aims at improving ranking quality and decreasing scoring cost at the same time. This twofold opportunity is justified by two observations that we illustrate below.The first observation is that regression/classification ensembles are known to encompass a high level of redundancy, and that some of their trees can be removed without significantly harming the effectiveness of the resulting ensemble . We aim at designing efficient pruning strategies able to deal with large LtR ensembles and to remove a portion of their trees without affecting ranking accuracy. Pruning the ensemble obviously impacts the scores predicted by the ranking model. We thus propose to optimize the accuracy of the trees survived to the pruning step by fine-tuning their weights with an optimization heuristic driven by a ranking quality metrics.A second observation makes this weight optimization step particularly interesting. It is related to the measure we use to evaluate the quality of a ranker, namely, Normalized Discounted",
        "Quality versus efficiency in document scoring with learning-to-rank models a b s t r a c tLearning-to-Rank ( LtR ) techniques leverage machine learning algorithms and large amounts of training data to induce high-quality ranking functions. Given a set of documents and a user query, these functions are able to precisely predict a score for each of the documents, in turn exploited to effectively rank them. Although the scoring efficiency of LtR models is critical in several applications -e.g., it directly impacts on response time and throughput of Web query processing -it has received relatively little attention so far.The goal of this work is to experimentally investigate the scoring efficiency of LtR models along with their ranking quality. Specifically, we show that machine-learned ranking models exhibit a quality versus efficiency trade-off. For example, each family of LtR algorithms has tuning parameters that can influence both effectiveness and efficiency, where higher ranking quality is generally obtained with more complex and expensive models. Moreover, LtR algorithms that learn complex models, such as those based on forests of regression trees, are generally more expensive and more effective than other algorithms that induce simpler models like linear combination of features.We extensively analyze the quality versus efficiency trade-off of a wide spectrum of stateof-the-art LtR , and we propose a sound methodology to devise the most effective ranker given a time budget. To guarantee reproducibility, we used publicly available datasets and we contribute an open source C++ framework providing optimized, multi-threaded implementations of the most effective tree-based learners: Gradient Boosted Regression Trees ( GBRT ), Lambda-Mart ( \u03bb-MART ), and the first public-domain implementation of Oblivious Lambda-Mart ( \u03bb -MART ), an algorithm that induces forests of oblivious regression trees.We investigate how the different training parameters impact on the quality versus efficiency trade-off, and provide a thorough comparison of several algorithms in the qualitycost space. The experiments conducted show that there is not an overall best algorithm, but the optimal choice depends on the time budget.",
        "FindStem: Analysis and Evaluation of a Turkish Stemming Algorithm. In this paper, we evaluate the effectiveness of a new stemming algorithm, FINDSTEM, for use with Turkish documents and queries, and compare the use of this algorithm with the other two previously defined Turkish stemmers, namely \"A-F\" and \"L-M\" algorithms. Of them, the FINDSTEM and A-F algorithms employ inflectional and derivational stemmers, whereas the L-M one handles only inflectional rules. Comparison of stemming algorithms was done manually using 5,000 distinct words out of which the FINDSTEM, A-F, and L-M failed on, in respect, 49, 270, and 559 cases. A medium-size collection, which is comprised of 2,468 law records with 280K document words, 15 queries in natural language with average length of 17 search words, and a complete relevancy information for each query, was used for the effectiveness of the stemming algorithm FINDSTEM. We localized SMART retrieval system in terms of a stopping list, introduction of Turkish characters, i.e., the ISO8859-9 (Latin-5) code set, a stemming algorithm (FINDSTEM), and a Turkish translation at message level. Our results based on average precision values at 11-point recall levels shows that indexing document as well as search terms with the use of FINDSTEM for stemming is clearly and consistently more effective than the one where the terms are indexed as they are (that is, no stemming at all).",
        "Fast T-overlap query algorithms using graphics processor units and its applications in web data query. Given a collection of sets and a query set, a T-Overlap query identifies all sets having at least T common elements with the query. T-Overlap query is the foundation of set similarity query and join and plays an important role on web data query and processing, such as the behavior analysis of web users and the near duplicated detection of web documents. To address T-Overlap query efficiently, unlike traditional algorithms based on CPU, we aim at designing efficient GPU based algorithms. We firstly design inverted index in GPU, then choose ScanCount, a straightforward but efficient T-Overlap algorithm, as underlying algorithm to develop our GPU based T-Overlap algorithms. Depending on queries processed serially or in parallel, three new efficient algorithms are proposed based on our GPU based inverted index. Among all these three algorithms, GS-Parallel-Group processes a group of queries in",
        "Approximate Period Detection and Correction. Periodicity has been historically well studied and has numerous applications. In nature, however, few cyclic phenomena have an exact period.This paper surveys some recent results in approximate periodicity: concept definition, discovery or recovery, techniques and efficient algorithms. We will also show some interesting connections between error correction codes and periodicity.We will try to pinpoint the issues involved, the context in the literature, and possible future research directions.",
        "An Efficient Linear Space Algorithm for Consecutive Suffix Alignment under Edit Distance (Short Preliminary Paper)",
        "Bandit Algorithms in Interactive Information Retrieval. e multi-armed bandit problem models an agent that simultaneously aempts to acquire new knowledge (exploration) and optimize his decisions based on existing knowledge (exploitation). e agent aempts to balance these competing tasks in order to maximize his total value over the period of time considered. ere are many practical applications of the bandit model, such as clinical trials, adaptive routing or portfolio design. Over the last decade there has been an increased interest in developing bandit algorithms for specic problems in information, such as diverse document ranking, news recommendation or ranker evaluation. e aim of this tutorial is to provide an overview of the various applications of bandit algorithms in information retrieval as well as issues related to their practical deployment and performance in real-life systems/applications. CCS CONCEPTS\u2022Information systems \u2192Personalization; Information retrieval diversity; \u2022Computing methodologies \u2192Online learning seings; KEYWORDS bandit algorithms, information retrieval, recommender systems, exploration-exploitation trade-o, interactive search, personalization, system optimization MOTIVATIONWith ever increasing amount and type of data available on the web, search engines have gradually developed into complex systems that combine many ranking criteria with the aim of producing the optimal result list in response to users queries. Traditional approaches, where a ranking algorithm are trained o-line with manually annotated data can be very expensive due to the involvement of human experts. Creation of new algorithms in this way can also be to slow to respond to rapid growth of new data or changes of documents' relevance to a given query over time. Additionally, it may not always be possible for experts to annotate data as might be the case in personalised search, where the perceived relevance of a given document is usually very subjective.A response to these issues has been emergence of new branches of information retrieval, such as online learning to rank, interactive information retrieval or dynamic information retrieval. ese Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICTIR'17, October 1-4, 2017, Amsterdam, e Netherlands. approaches oen employ various reinforcement learning methods, of which bandit algorithms are the most popular. e bandit problem models an agent that simultaneously aempts to acquire new knowledge (exploration) and optimize his decisions based on existing knowledge (exploitation). e agent aempts to balance these competing tasks in order to maximize his total value over the period of time considered. is aspect of bandit algorithms allow the search engine to gradually build the user model without geing stuck in a local search space, while engaging the user in the search loop. e simplicity and ease of implementation of bandit algorithms also adds to their popularity in the information retrieval community. Over the past decade there has been an increase in the number of new bandit algorithms as well as new applications for these algorithms. However, not all of these algorithms are equality suitable to all information retrieval seings or all types of data.Bandit algorithms have been studied extensively by the statistics and machine learning communities since 1950-ies. However, only recently has there been a growing interest in the application of bandits in information retrieval and related areas. We now have a handful of bandit algorithms aimed specically at solving IR problems, such as online learning to rank, ranker evaluation or click models. e goal of this tutorial is to bring together current eorts in the area, summarize the research performed so far and give a holistic view on the challenges of applying bandit algorithms in the information retrieval domain. FORMAT AND DETAILED SCHEDULEe tutorial will consist of three parts: 1) overview of bandit algorithms; 2) application of bandits in information retrieval; 3) Optimization of information retrieval systems based on bandit algorithms. In the rst part will provide an overview of bandit algorithms and how they develop overtime starting from Giins indices and then gradually follow the algorithmic development of bandits: Upper Condence Bound (UCB) algorithms, multi-armed bandits, dependent arm bandits, contextual bandits, dueling bandits, collaborative bandits, etc. e basic aspects of bandit algorithms, such as the reward function and the exploration-exploitation trade-o will be introduced as well. e rst part of the tutorial aims to not only familiarize the audience with the mathematical and statistical foundations of of bandits but also provide them with an intuition how they can be applied to real-life problems through examples from various areas, such as clinical trials, economics or information retrieval.Topics covered in the rst part of the tutorial:\u2022 Giins indices , UCB [4]\u2022 multi-armed bandits \u2022 dependent arm bandits \u2022 contextual bandits \u2022 dueling bandits Tutorial",
        "A cascade ranking model for efficient ranked retrieval. There is a fundamental tradeoff between effectiveness and efficiency when designing retrieval models for large-scale document collections. Effectiveness tends to derive from sophisticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems.To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to minimize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cascades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.",
        "Scalable Algorithms in the Age of Big Data and Network Sciences: Characterization, Primitives, and Techniques. In the age of network sciences and machine learning, efficient algorithms are now in higher demand more than ever before. Big Data fundamentally challenges the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to polynomial-time characterization, may no longer be adequate for solving today's problems. It is not just desirable, but essential, that efficient algorithms should be scalable. In other words, their complexity should be nearly linear or sub-linear with respect to the problem size. Thus, scalability, not just polynomial-time computability, should be elevated as the central complexity notion for characterizing efficient computation. Using several basic tasks in network analysis, machine learning, and optimization as examples -in this talk -I will highlight a family of fundamental algorithmic techniques for designing provably-good scalable algorithms.",
        "Efficient and effective solutions for search engines. Effectiveness and efficiency are two of the main issues in Information Retrieval (IR). IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. This research is about (1) identifying the bottlenecks in a search engine, (2) devising efficient and effective solutions to minimise or eliminate the bottlenecks and (2) adopting the solutions for distributed IR.As shown previously , a large portion of the performance of the search engine is dominated by (1) slow disk read of dictionary terms and the corresponding postings lists, (2) CPU-intensive decompression of postings lists, (3) complex similarity ranking functions and (4) sorting a large number of possible candidate documents.In order to speed up disk access, operating systems usually provide general-purpose buffer caching, prefetching and scheduling optimisation algorithms. However for specialpurpose applications, it is better for application to bypass the general ones and deploy their own I/O optimisation algorithms. In Jia et al. , a number of application-specific I/O optimisation algorithms have been proposed and tested.",
        "Juru at TREC 2004: Experiments with Prediction of Query Difficulty Our experiments in the Robust track this year focused on predicting query difficulty and using this prediction for improving information retrieval. We developed two prediction algorithms and used the subsequent prediction in several ways in order to improve the performance of the search engine. These included modifying the search engine parameters, using selective query expansion, and switching between different topic parts. We also experimented with a new scoring model based on ideas from the field of machine learning. Our results show that query prediction is indeed efficient in improving retrieval, although further work is needed in order to improve the performance of the prediction algorithms and their uses.",
        "Learning to predict response times for online query scheduling. Dynamic pruning strategies permit efficient retrieval by not fully scoring all postings of the documents matching a query -without degrading the retrieval effectiveness of the topranked results. However, the amount of pruning achievable for a query can vary, resulting in queries taking different amounts of time to execute. Knowing in advance the execution time of queries would permit the exploitation of online algorithms to schedule queries across replicated servers in order to minimise the average query waiting and completion times. In this work, we investigate the impact of dynamic pruning strategies on query response times, and propose a framework for predicting the efficiency of a query. Within this framework, we analyse the accuracy of several query efficiency predictors across 10,000 queries submitted to in-memory inverted indices of a 50-million-document Web crawl. Our results show that combining multiple efficiency predictors with regression can accurately predict the response time of a query before it is executed. Moreover, using the efficiency predictors to facilitate online scheduling algorithms can result in a 22% reduction in the mean waiting time experienced by queries before execution, and a 7% reduction in the mean completion time experienced by users.",
        "A novel image retrieval algorithm based on transfer learning and fusion features. With proliferation of social media, image has become ubiquitous giving rise to the demand and importance of image semantic analysis and retrieval to access information quickly on social media. However, even with humongous information available, there are certain categories of images which are important for certain applications but are very scarce. Convolutional neural network is an effective method to extract high-level semantic features for image database retrieval. To overcome the problem of over-fitting when the number of training samples in dataset is limited, this paper proposes an image database retrieval algorithm based on the framework of transfer learning and feature fusion. Based on the fine-tuning of the pre-trained Convolutional Neural Network (CNN), the proposed algorithm first extracts the semantic features of the images. Principal Component Analysis (PCA) is then applied for dimension reduction and to reduce the computational complexity. Last, the semantic feature extracted from the CNN is fused with traditional low-level visual feature to improve the retrieval accuracy further. Experimental results demonstrated the effectiveness of the proposed method for image database retrieval.",
        "IMRank: influence maximization via finding self-consistent ranking. Influence maximization, fundamental for word-of-mouth marketing and viral marketing, aims to find a set of seed nodes maximizing influence spread on social network. Early methods mainly fall into two paradigms with certain benefits and drawbacks: (1) Greedy algorithms, selecting seed nodes one by one, give a guaranteed accuracy relying on the accurate approximation of influence spread with high computational cost; (2) Heuristic algorithms, estimating influence spread using efficient heuristics, have low computational cost but unstable accuracy.We first point out that greedy algorithms are essentially finding a self-consistent ranking, where nodes' ranks are consistent with their ranking-based marginal influence spread. This insight motivates us to develop an iterative ranking framework, i.e., IMRank, to efficiently solve influence maximization problem under independent cascade model. Starting from an initial ranking, e.g., one obtained from efficient heuristic algorithm, IMRank finds a selfconsistent ranking by reordering nodes iteratively in terms of their ranking-based marginal influence spread computed according to current ranking. We also prove that IMRank definitely converges to a self-consistent ranking starting from any initial ranking. Furthermore, within this framework, a last-to-first allocating strategy and a generalization of this strategy are proposed to improve the efficiency of estimating ranking-based marginal influence spread for a given ranking. In this way, IMRank achieves both remarkable efficiency and high accuracy by leveraging simultaneously the benefits of greedy algorithms and heuristic algorithms. As demonstrated by extensive experiments on large scale real-world social networks, IMRank always achieves high accuracy comparable to greedy algorithms, while the computational cost is reduced dramatically, about 10 \u2212 100 times faster than other scalable heuristics.",
        "Fast top-k retrieval for model based recommendation. A crucial task in many recommender problems like computational advertising, content optimization, and others is to retrieve a small set of items by scoring a large item inventory through some elaborate statistical/machine-learned model. This is challenging since the retrieval has to be fast (few milliseconds) to load the page quickly. Fast retrieval is well studied in the information retrieval (IR) literature, especially in the context of document retrieval for queries. When queries and documents have sparse representation and relevance is measured through cosine similarity (or some variant thereof), one could build highly efficient retrieval algorithms that scale gracefully to increasing item inventory. The key components exploited by such algorithms is sparse query-document representation and the special form of the relevance function. Many machine-learned models used in modern recommender problems do not satisfy these properties and since brute force evaluation is not an option with large item inventory, heuristics that filter out some items are often employed to reduce model computations at runtime.In this paper, we take a two-stage approach where the first stage retrieves top-K items using our approximate procedures and the second stage selects the desired top-k using brute force model evaluation on the K retrieved items. The main idea of our approach is to reduce the first stage to a standard IR problem, where each item is represented by a sparse feature vector (a.k.a. the vector-space representation) and the query-item relevance score is given by vector dot product. The sparse item representation is learnt to closely approximate the original machine-learned score by using retrospective data. Such a reduction allows leveraging extensive work in IR that resulted in highly efficient retrieval systems. Our approach is model-agnostic, relying only on data generated from the machine-learned model. We obtain significant improvements in the computational cost vs. accuracy tradeoff compared to several baselines in our empirical evaluation on both synthetic models and on a click-through (CTR) model used in online advertising.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
        "Can We Get A Better Retrieval Function From Machine? The quality of an information retrieval system heavily depends on its retrieval function, which returns a similarity measurement between the query and each document in the collection. Documents are sorted according to their similarity values with the query and those with high rank are assumed to be relevant. Okapi BM25 and their variations are very popular retrieval functions and they seem to be the default retrieval function for the IR research community; and there are many other widely used and well studied functions, for example, Pivoted TFIDF and INQUERY. Most of these retrieval functions being used today are made based on probabilistic theories and they are adjusted in real world according to different contexts and information needs. In this paper, we propose the idea that a good retrieval function can be discovered by a pure machine learning approach, without using probabilistic theories and knowledge-based techniques. Two machine learning algorithms, Support Vector Machine (SVM) and Genetic Programming (GP) are used for retrieval function discovery, and GP is found to be a more effective approach. The retrieval functions discovered by GP might be hard for human interpretation, but their performance is superior to Okapi BM25, one of the most popular functions. The new retrieval function is combined with query expansion techniques and the retrieval performance is improved significantly. Based on our observations in the empirical study, the GP function is more reliable and effective than Okapi BM25 when query expansion techniques are used.",
        "Practical k-agents search algorithm towards information retrieval in complex networks. Spanking information retrieval in large-scale Web and network has attracted increasing interest in the research community, many typical approaches have been recalled such as greedy, random-walk and high degree seeking since the search capabilities of complex networks are proved by Kleinberg in 2000. Unfortunately, the retrieval efficiency of these classic approaches is not ideal, and they are only suitable for the specific networks due to their defects. The motivation of this paper is to increase the retrieval efficiency, and we thus proposed a novel k-agents search approach for different types of networks which searches the networks with k-agents, simultaneously. Besides, to better test the efficiency of algorithms, a new evaluation method which considers search steps and query information both is put forward to measure the cost of the search algorithm. The complexity analysis also will be discussed, and the comparison with other algorithms will be displayed in detail to show its superiority. In the end, for the purpose of displaying a universe application of our algorithm, the simulations in W S (proposed by Watts and Strogatz), NW (proposed by Newman and Watts) small-world and BA (proposed by Barab\u00e1i and Albert) scale-free network model are carried out to illustrate the performance of the proposed method.",
        "Condorcet fusion for improved retrieval. We present a new algorithm for improving retrieval results by combining document ranking functions: Condorcet-fuse. Beginning with one of the two major classes of voting procedures from Social Choice Theory, the Condorcet procedure, we apply a graph-theoretic analysis that yields a sortingbased algorithm that is elegant, efficient, and effective. The algorithm performs very well on TREC data, often outperforming existing metasearch algorithms whether or not relevance scores and training data is available. Condorcet-fuse significantly outperforms Borda-fuse, the analogous representative from the other major class of voting algorithms. CATEGORIES AND SUBJECT DESCRIPTORSInformation systems: information storage and retrieval: information search and retrieval: search process. GENERAL TERMSAlgorithms and performance. Metasearch is the term usually applied to techniques that combine final results from a number of search engines in order to improve retrieval. In its simplest form, a metasearch engine takes as input the n ranked lists output by each of n search engines in response to a given query. It then computes a single ranked list as output, which is usually an improvement over any of the input lists (as measured by standard IR performance metrics).Current metasearch algorithms can be characterized by the data they require: whether they need relevance scores or only ranks, and whether they require training data or not. See . * This work generously supported by NSF grants EIA-9802068, BCS-9978116, and Career Award CCR 00-93131.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. , where a metasearch engine is used to combine three complete and independent search engines, versus internal metasearch (right), where a metasearch algorithm is used to combine three cooperating sub-engines over the same set of documents.External metasearch, one use of fusion, takes existing, \"complete\" search systems and tries to improve upon them by combining them. External metasearch engines include 538",
        "Incorporating Different Search Models into One Document Retrieval System. Many effective search strategies derived from different models are available for document retrieval systems.However, it does not appear that there is a single most effective strategy. Instead, different strategies perform optimally under different conditions. This paper outlines the design of an adaptive document retrieval system that chooses the best search strategy for a particular situation and user.In, order to be able to support a variety of search strategies, a general network representation of the documents and terms in the database is proposed.This network representation leads to efficient methods of generating and using document and term classifications.One of the most desirable features of an adaptive system would be the ability to learn from experience. A method of incorporating this learning ability into the system is described.The adaptive control strategy for choosing search strategies enables the system to base its actions on a number of factors, including a model of the current user.Finally, some ideas for a flexible interface for casual users are suggested.Part of this interface is the heuristic search, which is used when searches based on formal models have failed.The heuristic search provides a browsing capability for the user.",
        "Axiomatic Analysis and Optimization of Information Retrieval Models. The accuracy of a search engine is mostly determined by the optimality of the retrieval model used in the search engine. Develoing optimal retrieval models has always been a very important fundamental research problem in information retrieval because an improved general retrieval model would enable all search engines to be more useful, thus have immediate broad impact. Extensive research has been done on developing an optimal retrieval model since 1960s, leading to multiple effective retrieval models, including, e.g., Pivoted Normalization Vector Space model, BM25, Dirichlet Prior Query Likelihood, and PL2. However, these state of the art retrieval models were all developed at least a decade ago, suggesting that it has been difficult to further improve them. One reason why we could not easily improve these models is because we do not have a good understanding of their deficiencies and have mostly relied on empirical evaluation to assess the superiority of a retrieval model.Recently, an axiomatic way of analyzing and optimizing retrieval models has been developed and shown great promise in both understanding the deficiencies of retrieval models and developing more effective ones. The basic idea of this axiomatic framework is to specify a number of formal constraints that an optimal retrieval model is expected to satisfy, and use them to assess the optimality of a retrieval model. Such an axiomatic way of modeling relevance provides a theoretical way to study how to develop an ultimately optimal retrieval model, enables analytical comparison of different retrieval models without necessarily requiring empirical evaluation, and has led to the development of multiple more effective retrieval models.The purpose of this tutorial is to systematically explain this emerging axiomatic approach to developing optimal retrieval models, review and summarize the research progress achieved so far on this topic, and discuss promising future research directions in optimizing general retrieval models. Tutorial attendees can expect to learn, among others, (1) the basic methodology of axiomatic analysis and optimization of retrieval models, (2) how to formalize retrieval heuristics with mathematical constraints, (3) the major retrieval Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). ICTIR '13, Sep 29 -Oct 02 2013, Copenhagen, Denmark ACM 978-1-4503-2107-5/13/09 http://dx.doi.org/10.1145 constraints proposed so far, (4) the new retrieval functions derived by using the axiomatic approaches, (5) specific research directions to further develop more effective retrieval models, and general open challenges in developing an ultimately optimal retrieval model. The tutorial should appeal to those who work on information retrieval models and those who are interested in applying axiomatic analysis to optimize specific retrieval functions in real applications. The tutorial should also be interesting to researchers who work on ranking problems in general. Attendees will be assumed to know the basic concepts in information retrieval models.",
        "An Efficient Incremental Nearest Neighbor Algorithm for Processing k-Nearest Neighbor Queries with Visal and Semantic Predicates in Multimedia Information Retrieval System",
        "ERkNN: efficient reverse k-nearest neighbors retrieval with local kNN-distance estimation. The Reverse k-Nearest Neighbors (RkNN) queries are important in profile-based marketing, information retrieval, decision support and data mining systems. However, they are very expensive and existing algorithms are not scalable to queries in high dimensional spaces or of large values of k. This paper describes an efficient estimation-based RkNN search algorithm (ERkNN) which answers RkNN queries based on local kNN-distance estimation methods. The proposed approach utilizes estimation-based filtering strategy to lower the computation cost of RkNN queries. The results of extensive experiments on both synthetic and real life datasets demonstrate that ERkNN algorithm retrieves RkNN efficiently and is scalable with respect to data dimensionality, k, and data size."
    ],
    "How to avoid spam results": [
        "UNIK: unsupervised social network spam detection. Social network spam increases explosively with the rapid development and wide usage of various social networks on the Internet. To timely detect spam in large social network sites, it is desirable to discover unsupervised schemes that can save the training cost of supervised schemes. In this work, we first show several limitations of existing unsupervised detection schemes. The main reason behind the limitations is that existing schemes heavily rely on spamming patterns that are constantly changing to avoid detection. Motivated by our observations, we first propose a sybil defense based spam detection scheme SD2 that remarkably outperforms existing schemes by taking the social network relationship into consideration. In order to make it highly robust in facing an increased level of spam attacks, we further design an unsupervised spam detection scheme, called UNIK. Instead of detecting spammers directly, UNIK works by deliberately removing non-spammers from the network, leveraging both the social graph and the user-link graph. The underpinning of UNIK is that while spammers constantly change their patterns to evade detection, non-spammers do not have to do so and thus have a relatively non-volatile pattern. UNIK has comparable performance to SD2 when it is applied to a large social network site, and outperforms SD2 significantly when the level of spam attacks increases. Based on detection results of UNIK, we further analyze several identified spam campaigns in this social network site. The result shows that different spammer clusters demonstrate distinct characteristics, implying the volatility of spamming patterns and the ability of UNIK to automatically extract spam signatures.",
        "Spam double-funnel: connecting web spammers with advertisers. Spammers use questionable search engine optimization (SEO) techniques to promote their spam links into top search results. In this paper, we focus on one prevalent type of spam -redirection spam -where one can identify spam pages by the third-party domains that these pages redirect traffic to. We propose a fivelayer, double-funnel model for describing end-to-end redirection spam, present a methodology for analyzing the layers, and identify prominent domains on each layer using two sets of commercial keywords -one targeting spammers and the other targeting advertisers. The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam, for legitimate website owners to locate and remove spam doorway pages, and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages.",
        "Let web spammers expose themselves. This paper is concerned with mining link spams (e.g., link farm and link exchange) from search engine optimization (SEO) forums. To provide quality services, it is critical for search engines to address web spam. Several techniques such as TrustRank, BadRank, and SpamRank have been proposed for this purpose. Most of these methods try to downgrade the effects of the spam websites by identifying specific link patterns of them. However, spam websites have appeared to be more and more similar to normal or even good websites in their link structures, by reforming their spam techniques. As a result, it is very challenging to automatically detect link spams from the Web graph. In this paper, we propose a different approach, which detects link spams by looking at how web spammers make link spam happen. We find that web spammers usually ally with each other, and SEO forum is one of the major means for them to form the alliance. We therefore propose mining suspicious link spams directly from the posts in the SEO forums. However, the task is non-trivial because there are also other information and even noises contained in these posts, in addition to useful clues of link spam. To tackle the challenges, we first extract all the URLs contained in the posts of the SEO forums. Second, we extract features for the URLs from their relationships with forum users (potential spammers) and from their link structure in the web graph. Third, we build a semi-supervised learning framework to calculate the spam scores for the URLs, which encodes several heuristics such as spam websites usually linking to each other, and good websites seldom linking to spam websites. We tested * This work was performed when the first and the third authors were interns at Microsoft Research Asia.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WSDM '11, February 9-12, 2011, Hong Kong, China. Copyright 2011 ACM 978-1-4503-0493-1/11/02 ...$10.00. our approach on seven major SEO forums. A lot of spam websites were identified, a significant proportion of which cannot be detected by conventional anti-spam methods. It indicates that the proposed approach can be a good complement of existing anti-spam techniques.",
        "Spam detection with a content-based random-walk algorithm. In this work we tackle the problem of the spam detection on the Web. Spam web pages have become a problem for Web search engines, due to the negative effects that this phenomenon can cause in their retrieval results. Our approach is based on a random-walk algorithm that obtains a ranking of pages according to their relevance and their spam likelihood. We introduce the novelty of taking into account the content of the web pages to characterize the web graph and to obtain an a-priori estimation of the spam likekihood of the web pages. Our graph-based algorithm computes two scores for each node in the graph. Intuitively, these values represent how bad or good (spam-like or not) is a web page, according to its textual content and the relations in the graph. Our experiments show that our proposed technique outperforms other link-based techniques for spam detection.",
        "Link analysis for Web spam detection We propose link-based techniques for automatic detection of Web spam, a term referring to pages which use deceptive techniques to obtain undeservedly high scores in search engines. The use of Web spam is widespread and difficult to solve, mostly due to the large size of the Web which means that, in practice, many algorithms are infeasible.We perform a statistical analysis of a large collection of Web pages. In particular, we compute statistics of the links in the vicinity of every Web page applying rank propagation and probabilistic counting over the entire Web graph in a scalable way. These statistical features are used to build Web spam classifiers which only consider the link structure of the Web, regardless of page contents. We then present a study of the performance of each of the classifiers alone, as well as their combined performance, by testing them over a large collection of Web link spam. After tenfold cross-validation, our best classifiers have a performance comparable to that of state-of-the-art spam classifiers that use content attributes, but are orthogonal to content-based methods.",
        "Identifying Web Spam with the Wisdom of the Crowds Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spamdetection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following.(1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.",
        "Effect of Spam on Hashtag Recommendation for Tweets. Presence of spam tweets in a dataset may affect the choices of feature selection, algorithm formulation, and system evaluation for many applications. However, most existing studies have not considered the impact of spam tweets. In this paper, we study the impact of spam tweets on hashtag recommendation for hyperlinked tweets (i.e., tweets containing URLs) in HSpam14 dataset. HSpam14 is a collection of 14 million tweets with annotations of being spam and ham (i.e., non-spam). In our experiments, we observe that it is much easier to recommend \"correct\" hashtags for spam tweets than ham tweets, because of the near duplicates in spam tweets. Simple approaches like recommending most popular hashtags achieves very good accuracy on spam tweets. On the other hand, features that are highly effective on ham tweets may not be effective on spam tweets. Our findings suggest that without removing spam tweets from the data collection (as in most studies), the results obtained could be misleading for hashtag recommendation tasks.",
        "Predicting web spam with HTTP session information. Web spam is a widely-recognized threat to the quality and security of the Web. Web spam pages pollute search engine indexes, burden Web crawlers and Web mining services, and expose users to dangerous Web-borne malware. To defend against Web spam, most previous research analyzes the contents of Web pages and the link structure of the Web graph. Unfortunately, these heavyweight approaches require full downloads of both legitimate and spam pages to be effective, making real-time deployment of these techniques infeasible for Web browsers, high-performance Web crawlers, and real-time Web applications. In this paper, we present a lightweight, predictive approach to Web spam classification that relies exclusively on HTTP session information (i.e., hosting IP addresses and HTTP session headers). Concretely, we built an HTTP session classifier based on our predictive technique, and by incorporating this classifier into HTTP retrieval operations, we are able to detect Web spam pages before the actual content transfer. As a result, our approach protects Web users from Webpropagated malware, and it generates significant bandwidth and storage savings. By applying our predictive technique to a corpus of almost 350,000 Web spam instances and almost 400,000 legitimate instances, we were able to successfully detect 88.2% of the Web spam pages with a false positive rate of only 0.4%. These classification results are superior to previous evaluation results obtained with traditional linkbased and content-based techniques. Additionally, our experiments show that our approach saves an average of 15.4 KB of bandwidth and storage resources for every successfully identified Web spam page, while only adding an average of 101\u00b5s to each HTTP retrieval operation. Therefore, our predictive technique can be successfully deployed in applications that demand real-time spam detection.",
        "Collusion-aware detection of review spammers in location based social networks. To ensure the quality of online review, more and more location-based social networks (LBSNs), like Yelp, have established the filtering systems to detect groups of review spammers. This is not an easy task. Review spammers use camouflage methods to maintain their spam behavior in a very low density to try to conceal themselves in normal users. These camouflaged spammers, driven by profits, are hired by some stores to write fake reviews in groups so as to raise these stores or to belittle their competitors. To avoid the unhealthy competition, in this paper, we propose a novel detection mechanism to discern collusive review spammers, including individuals and groups. The key point of our mechanism is to identify hidden spammers through multiple anomalous relationship features, especially the collusive relation between review spammers and the business competition between locations. Based on multi-view anomalous features, two detection models are proposed for individual and group discovery, respectively. For malicious individuals, a detection model based on Markov Random Field (MRF) is constructed to formalize an inference problem, where the corresponding marginal",
        "Spam and popularity ratings for combating link spam. We present a new approach for propagating spam scores in web graphs, in order to combat link spam. The resulting spam rating is then used for propagating popularity scores like PageRank. Both propagations work even in presence of censure links that represent distrust. Initial testing using a C++ prototype on small examples show more reasonable results than other published approaches.",
        "Web Spam Detection: New Approach with Hidden Markov Models. Web Spam is the result of a number of methods to deceive search engine algorithms so as to obtain higher ranks in the search results. Advanced spammers use keyword and link stuffing methods to create farms of spam pages. Most of the recent works in the web spam detection literature utilize graph based methods to enhance the accuracy of this task. This paper is basically a probabilistic approach that uses content and link based features to detect the web spam pages. Since we observe there is a high connectivity between web spam pages, we adopt a method based on Hidden Markov Model to exploit conditional dependency of a sequence of hosts and their spam/normal class distribution of each host. Experimental results show that the proposed method can significantly improve the performance of baseline classifier.",
        "Fighting against web spam: a novel propagation method based on click-through data. Combating Web spam is one of the greatest challenges for Web search engines. State-of-the-art anti-spam techniques focus mainly on detecting varieties of spam strategies, such as content spamming and link-based spamming. Although these anti-spam approaches have had much success, they encounter problems when fighting against a continuous barrage of new types of spamming techniques. We attempt to solve the problem from a new perspective, by noticing that queries that are more likely to lead to spam pages/sites have the following characteristics: 1) they are popular or reflect heavy demands for search engine users and 2) there are usually few key resources or authoritative results for them. From these observations, we propose a novel method that is based on click-through data analysis by propagating the spamicity score iteratively between queries and URLs from a few seed pages/sites. Once we obtain the seed pages/sites, we use the link structure of the click-through bipartite graph to discover other pages/sites that are likely to be spam. Experiments show that our algorithm is both efficient and effective in detecting Web spam. Moreover, combining our method with some popular anti-spam techniques such as TrustRank achieves improvement compared with each technique taken individually.",
        "Detecting spam blogs from blog search results a b s t r a c tBlogging has been an emerging media for people to express themselves. However, the presence of spam blogs (also known as splogs) may reduce the value of blogs and blog search engines. Hence, splog detection has recently attracted much attention from research. Most existing works on splog detection identify splogs using their content/link features and target on spam filters protecting blog search engines' index from spam. In this paper, we propose a splog detection framework by monitoring the on-line search results. The novelty of our splog detection is that our detection capitalizes on the results returned by search engines. The proposed method therefore is particularly useful in detecting those splogs that have successfully slipped through the spam filters that are also actively generating spam-posts. More specifically, our method monitors the top-ranked results of a sequence of temporally-ordered queries and detects splogs based on blogs' temporal behavior. The temporal behavior of a blog is maintained in a blog profile. Given blog profiles, splog detecting functions have been proposed and evaluated using real data collected from a popular blog search engine. Our experiments have demonstrated that splogs could be detected with high accuracy. The proposed method can be implemented on top of any existing blog search engine without intrusion to the latter.",
        "Spammers' networks within online social networks: a case-study on Twitter. We analyze the strategies employed by contemporary spammers in Online Social Networks (OSNs) by identifying a set of spam-accounts in Twitter and monitoring their linkcreation strategies. Our analysis reveals that spammers adopt intelligent 'collaborative' strategies of link-formation to avoid detection and to increase the reach of their generated spam, such as forming 'spam-farms' and creating large number of links with targeted legitimate users. The observations are verified through the analysis of a giant 'spam-farm' embedded within the Twitter OSN.",
        "Spam Filtering Using Inexact String Matching in Explicit Feature Space with On-Line Linear Classifiers Contemporary spammers commonly seek to defeat statistical spam filters through the use of word obfuscation. Such methods include character level substitutions, repetitions, and insertions to reduce the effectiveness of word-based features. We present an efficient method for combating obfuscation through the use of inexact string matching kernels, which were first developed to measure similarity among mutating genes in computational biology. Our system avoids the high classification costs associated with these kernel methods by working in an explicit feature space, and employs the Perceptron Algorithm using Margins for fast on-line training. No prior domain knowledge was incorporated into this system. We report strong experimental results on the TREC 2006 spam data sets and on other publicly available spam data, including near-perfect performance on the TREC 2006 Chinese spam data set. These results invite further exploration of the use of inexact string matching for spam filtering.",
        "Using evolutionary computation for discovering spam patterns from e-mail samples A B S T R A C TOne of the most relevant problems affecting the efficient use of e-mail to communicate worldwide is the spam phenomenon. Spamming involves flooding Internet with undesired messages aimed to promote illegal or low value products and services. Beyond the existence of different well-known machine learning techniques, collaborative schemes and other complementary approaches, some popular anti-spam frameworks such as SpamAssassin or Wirebrush4SPAM enabled the possibility of using regular expressions to effectively improve filter performance. In this work, we provide a review of existing proposals to automatically generate fully functional regular expressions from any input dataset combining spam and ham messages. Due to configuration difficulties and the low performance achieved by analysed schemes, in this work we introduce DiscoverRegex, a novel automatic spam pattern-finding tool. Patterns generated DiscoverRegex outperform those created by existing approaches (able to avoid FP errors) whilst minimising the computational resources required for its proper operation. DiscoverRegex source code is publicly available at https://github.com/sing-group/DiscoverRegex.",
        "PRIS Kidult Anti-SPAM Solution at the TREC 2005 Spam Track: Improving the Performance of Naive Bayes for Spam Detection Recently, the spam already constituted a serious problem for both e-mail users and Internet Service Providers (ISP). Solutions to the abuse of spam would be both technical and legal regulatory. This paper reports our solution for the TREC 2005 spam track, in which we consider the use of Naive Bayes spam filter for its desirable properties (simplicity, low time and memory requirements, etc.). Then the approaches to modify the Naive Bayes by simply introducing weight and classifier assemble based on dynamic threshold are proposed, which can help to improve the accuracy of a Naive Bayes spam classifier dramatically. Additionally, we discuss some steps that must be adopted naturally thought before, such as stop list, word stemming, feature selection, class prior probabilities. The theory analysis implies these steps are not necessarily the best way to extend the Bayesian classifier, and these were also verified empirically. Many of these techniques appear to be counterintuitive but can be explained by the statistical properties of e-mail itself. Experiment results of TREC 2005 spam track demonstrate the effectiveness of the proposed method.",
        "Cost-effective spam detection in p2p file-sharing systems. Spam is highly pervasive in P2P file-sharing systems and is difficult to detect automatically before actually downloading a file due to the insufficient and biased description of a file returned to a client as a query result. To alleviate this problem, we propose probing technique to collect more complete feature information of query results from the network and apply feature-based ranking for automatically detecting spam in P2P query result sets. Furthermore, we examine the tradeoff between the spam detection performance and the network cost. Different ways of probing are explored to reduce the network cost. Experimental results show that the proposed techniques successfully decrease the amount of spam by 9% in the top-200 results and by 92% in the top-20 results with reasonable cost.",
        "Live and learn from mistakes: A lightweight system for document classification a b s t r a c tWe present a Life-Long Learning from Mistakes (3LM) algorithm for document classification, which could be used in various scenarios such as spam filtering, blog classification, and web resource categorization. We extend the ideas of online clustering and batch-mode centroid-based classification to online learning with negative feedback. The 3LM is a competitive learning algorithm, which avoids over-smoothing, characteristic of the centroidbased classifiers, by using a different class representative, which we call clusterhead. The clusterheads competing for vector-space dominance are drawn toward misclassified documents, eventually bringing the model to a ''balanced state'' for a fixed distribution of documents. Subsequently, the clusterheads oscillate between the misclassified documents, heuristically minimizing the rate of misclassifications, an NP-complete problem. Further, the 3LM algorithm prevents over-fitting by ''leashing'' the clusterheads to their respective centroids. A clusterhead provably converges if its class can be separated by a hyper-plane from all other classes. Lifelong learning with fixed learning rate allows 3LM to adapt to possibly changing distribution of the data and continually learn and unlearn document classes. We report on our experiments, which demonstrate high accuracy of document classification on Reuters21578, OHSUMED, and TREC07p-spam datasets. The 3LM algorithm did not show over-fitting, while consistently outperforming centroid-based, Na\u00efve Bayes, C4.5, AdaBoost, kNN, and SVM whose accuracy had been reported on the same three corpora.",
        "Know your neighbors: web spam detection using the web topology. Web spam can significantly deteriorate the quality of search engine results. Thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately. In this paper we present a spam detection system that combines link-based and content-based features, and uses the topology of the Web graph by exploiting the link dependencies among the Web pages. We find that linked hosts tend to belong to the same class: either both are spam or both are non-spam. We demonstrate three methods of incorporating the Web graph topology into the predictions obtained by our base classifier: (i) clustering the host graph, and assigning the label of all hosts in the cluster by majority vote, (ii) propagating the predicted labels to neighboring hosts, and (iii) using the predicted labels of neighboring hosts as new features and retraining the classifier. The result is an accurate system for detecting Web spam, tested on a large and public dataset, using algorithms that can be applied in practice to large-scale Web data.",
        "Spam Mobile Apps: Characteristics, Detection, and in the Wild Analysis The increased popularity of smartphones has attracted a large number of developers to offer various applications for the different smartphone platforms via the respective app markets. One consequence of this popularity is that the app markets are also becoming populated with spam apps. These spam apps reduce the users' quality of experience and increase the workload of app market operators to identify these apps and remove them. Spam apps can come in many forms such as apps not having a specific functionality, those having unrelated app descriptions or unrelated keywords, or similar apps being made available several times and across diverse categories. Market operators maintain antispam policies and apps are removed through continuous monitoring. Through a systematic crawl of a popular app market and by identifying apps that were removed over a period of time, we propose a method to detect spam apps solely using app metadata available at the time of publication. We first propose a methodology to manually label a sample of removed apps, according to a set of checkpoint heuristics that reveal the reasons behind removal. This analysis suggests that approximately 35% of the apps being removed are very likely to be spam apps. We then map the identified heuristics to several quantifiable features and show how distinguishing these features are for spam apps. We build an Adaptive Boost classifier for early identification of spam apps using only the metadata of the apps. Our classifier achieves an accuracy of over 95% with precision varying between 85% and 95% and recall varying between 38% and 98%. We further show that a limited number of features, in the range of 10-30, generated from app metadata is sufficient to achieve a satisfactory level of performance. On a set of 180,627 apps that were present at the app market during our crawl, our classifier predicts 2.7% of the apps as potential spam. Finally, we perform additional manual verification and show that human reviewers agree with 82% of our classifier predictions.",
        "ICTNET at Web Track 2010 Spam Task Web Spamming refers those web pages deceive search engines so as to get a higher rank in their search result. We work on the data set TrecWeb09, based on a content-based spamming classifier, to check the two ends of a hyperlink; if the two end pages either is content spamming, or both are not so good, then the hyperlink will be discarded. After all hyperlinks have been checked, PageRank value shall be recount on the rebuilt web network. The balance of one page's PageRank value will be regarded as its link spamming. Then the link spamming score and the result of content deceiving analyzer will be combined as the final estimation of one page's spamming.",
        "On the Temporal Dynamics of Opinion Spamming: Case Studies on Yelp. Recently, the problem of opinion spam has been widespread and has attracted a lot of research attention. While the problem has been approached on a variety of dimensions, the temporal dynamics in which opinion spamming operates is unclear. Are there specific spamming policies that spammers employ? What kind of changes happen with respect to the dynamics to the truthful ratings on entities. How do buffered spamming operate for entities that need spamming to retain threshold popularity and reduced spamming for entities making better success? We analyze these questions in the light of time-series analysis on Yelp. Our analyses discover various temporal patterns and their relationships with the rate at which fake reviews are posted. Building on our analyses, we employ vector autoregression to predict the rate of deception across different spamming policies. Next, we explore the effect of filtered reviews on (long-term and imminent) future rating and popularity prediction of entities. Our results discover novel temporal dynamics of spamming which are intuitive, arguable and also render confidence on Yelp's filtering. Lastly, we leverage our discovered temporal patterns in deception detection. Experimental results on large-scale reviews show the effectiveness of our approach that significantly improves the existing approaches.",
        "Spam characterization and detection in peer-to-peer file-sharing systems. Spam is highly pervasive in P2P file-sharing systems and is difficult to detect automatically before actually downloading a file due to the insufficient and biased description of a file returned to a client as a query result. To alleviate this problem, we first characterize spam and spammers in the P2P file-sharing environment and then describe feature-based techniques for automatically detecting spam in P2P query result sets. Experimental results show that the proposed techniques successfully decrease the amount of spam by 9% in the top-200 results and by 92% in the top-20 results.",
        "Enhanced email spam filtering through combining similarity graphs. Over the last decade Email Spam has evolved from being just an irritant to users to being truly dangerous. This has led web-mail providers and academic researchers to dedicate considerable resources towards tackling this problem . However, we argue that some aspects of the spam filtering problem are not handled appropriately in existing work. Principal among these are adversarial spammer efforts -spammers routinely tune their spam emails to bypass spam-filters, and contaminate ground truth via fake Ham/Spam votes -and the scale and sparsity of the problem, which essentially precludes learning with a very large set of parameters.In this paper we propose an approach that learns to filter spam by striking a balance between generalizing Ham/Spam votes across users and emails (to alleviate sparsity) and learning local models for each user (to limit effect of adversarial votes); votes are shared only amongst users and emails that are \"similar\" to one another. Moreover, we define useruser and email-email similarities using spam-resilient features that are extremely difficult for spammers to fake. We give a methodology that learns to combine multiple features into similarity values while directly optimizing the objective of better spam filtering. A useful side effect of this methodology is that the number of parameters that need to be estimated is very small: this helps us use off-the-shelf learning algorithms to achieve good accuracy while preventing over-training to the adversarial noise in the data. Finally, our approach gives a systematic way to incorporate existing spam-fighting technologies such as IP blacklists, keyword based classifiers, etc into one framework. Experiments on a real-world email dataset show that our approach leads to significant improvements compared to two state-of-the-art baselines.",
        "Opinion Spam Detection in Web Forum: A Real Case Study. Opinion spamming refers to the illegal marketing practice which involves delivering commercially advantageous opinions as regular users. In this paper, we conduct a real case study based on a set of internal records of opinion spams leaked from a shady marketing campaign. We explore the characteristics of opinion spams and spammers in a web forum to obtain some insights, including subtlety property of opinion spams, spam post ratio, spammer accounts, first post and replies, submission time of posts, activeness of threads, and collusion among spammers. Then we present features that could be potentially helpful in detecting spam opinions in threads. The results of spam detection on first posts show: (1) spam first posts put more focus on certain topics such as the user experiences' on the promoted items, (2) spam first posts generally use more words and pictures to showcase the promoted items in an attempt to impress people, (3) spam first posts tend to be submitted during work time, and (4) the threads that spam first posts initiate are more active to be placed at striking positions. The spam detection on replies is more challenging. Besides lower spam ratio and less content, replies even do not mention the promoted items. Their major intention is to keep the discussion in a thread alive to attract more attention on it. Submission time of replies, thread activeness, position of replies, and spamicity of first post are more useful than contentbased features in spam detection on replies."
    ],
    "Information Retrieval with algorithms": [
        "Document Clustering Using Linear Partitioning Hyperplanes and Reallocation. This paper presents a novel algorithm for document clustering based on a combinatorial framework of the Principal Direction Divisive Partitioning (PDDP) algorithm [1] and a simplified version of the EM algorithm called the spherical Gaussian EM (sGEM) algorithm. The idea of the PDDP algorithm is to recursively split data samples into two sub-clusters using the hyperplane normal to the principal direction derived from the covariance matrix. However, the PDDP algorithm can yield poor results, especially when clusters are not well-separated from one another. To improve the quality of the clustering results, we deal with this problem by re-allocating new cluster membership using the sGEM algorithm with different settings. Furthermore, based on the theoretical background of the sGEM algorithm, we can naturally extend the framework to cover the problem of estimating the number of clusters using the Bayesian Information Criterion. Experimental results on two different corpora are given to show the effectiveness of our algorithm.",
        "International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2020) Both search and recommendation algorithms provide results based on their relevance for the current user. In order to do so, such a relevance is usually computed by models trained on historical data, which is biased in most cases. Hence, the results produced by these algorithms naturally propagate, and frequently reinforce, biases hidden in the data, consequently strengthening inequalities. Being able to measure, characterize, and mitigate these biases while keeping high effectiveness is a topic of central interest for the information retrieval community. In this workshop, we aim to collect novel contributions in this emerging field and to provide a common ground for interested researchers and practitioners.",
        "A Tree Algorithm For Nearest Neighbor Searching In Document Retrieval Systems. The problem of finding nearest neighbors to a query in a document collection is a special case of associative retrieval, in which searches are performed using more than one key.A nearest neighbors associative retrieval algorithm, suitable for document retrieval using similarity matching, is described. The basic structure used is a binary tree, at each node a set of keys (concepts) is tested to select the most promising branch. Backtracking to initially rejected branches is allowed and often necessary.Under certain conditions, the search time required by this algorithm is 0(log2N) k N is the number of documents, and k is a systemdependent parameter. A series of experiments with a small collection confirm the predictions made using the analytic model; k is approximately 4 in this situation.This algorithm is compared with two other searching algorithms; sequential search and clustered search. For large collections, the average search time for this algorithm is less than that for a sequential search and greater than that for a clustered search. However, the clustered search,unlike the sequential search and this algorithm, does not guarantee that the near neighbors found are actually the nearest neighbors. 131i.",
        "Algorithmic mediation for collaborative exploratory search. We describe a new approach to information retrieval: algorithmic mediation for intentional, synchronous collaborative exploratory search. Using our system, two or more users with a common information need search together, simultaneously. The collaborative system provides tools, user interfaces and, most importantly, algorithmically-mediated retrieval to focus, enhance and augment the team's search and communication activities. Collaborative search outperformed post hoc merging of similarly instrumented single user runs. Algorithmic mediation improved both collaborative search (allowing a team of searchers to find relevant information more efficiently and effectively), and exploratory search (allowing the searchers to find relevant information that cannot be found while working individually).",
        "A Comparison of Data-Driven Automatic Syllabification Methods. Although automatic syllabification is an important component in several natural language tasks, little has been done to compare the results of data-driven methods on a wide range of languages. This article compares the results of five data-driven syllabification algorithms (Hidden Markov Support Vector Machines, IB1, Liang's algorithm, the Look Up Procedure, and Syllabification by Analogy) on nine European languages in order to determine which algorithm performs best over all. Findings show that all algorithms achieve a mean word accuracy across all lexicons of over 90%. However, Syllabification by Analogy performs better than the other algorithms tested with a mean word accuracy of 96.84% (standard deviation of 2.93) whereas Liang's algorithm, the standard for hyphenation (used in T E X), produces the second best results with a mean of 95.67% (standard deviation of 5.70).",
        "Personalized Social Search Based on Agglomerative Hierarchical Graph Clustering. This paper describes a personalized social search algorithm for retrieving multimedia contents of a consumer generated media (CGM) site having a social network service (SNS). The proposed algorithm generates cluster information on users in the social network by using an agglomerative hierarchical graph clustering, and stores it to a contents database (DB). Retrieved contents are sorted by scores calculated according to similarities of cluster information between a searcher and authors of contents. This paper also describes the evaluation experiments to confirm effectiveness of the proposed algorithm by implementing the proposed algorithm in a video retrieval system of a CGM site.",
        "A split-list approach for relevance feedback in information retrieval a b s t r a c tIn this paper we present a new algorithm for relevance feedback (RF) in information retrieval. Unlike conventional RF algorithms which use the top ranked documents for feedback, our proposed algorithm is a kind of active feedback algorithm which actively chooses documents for the user to judge. The objectives are (a) to increase the number of judged relevant documents and (b) to increase the diversity of judged documents during the RF process. The algorithm uses document-contexts by splitting the retrieval list into sub-lists according to the query term patterns that exist in the top ranked documents. Query term patterns include a single query term, a pair of query terms that occur in a phrase and query terms that occur in proximity. The algorithm is an iterative algorithm which takes one document for feedback in each of the iterations. We experiment with the algorithm using the TREC-6, -7, -8, -2005 and GOV2 data collections and we simulate user feedback using the TREC relevance judgements. From the experimental results, we show that our proposed split-list algorithm is better than the conventional RF algorithm and that our algorithm is more reliable than a similar algorithm using maximal marginal relevance.",
        "Intelligent algorithms for improving communication patterns in thematic P2P search a b s t r a c tThe Internet is a cooperative and decentralized network built out of millions of participants that store and share large amounts of information with other users. Peer-to-peer systems go hand-in-hand with this huge decentralized network, where each individual node can serve content as well as request it. In this scenario, the analysis, development and testing of distributed search algorithms is a key research avenue. In particular, thematic search algorithms should lead to and benefit from the emergence of semantic communities that are the result of the interaction among participants. As a result, intelligent algorithms for neighbor selection should give rise to a logical network topology reflecting efficient communication patterns. This paper presents a series of algorithms which are specifically aimed at reducing the propagation of queries in the network, by applying a novel approach for learning peers' interests. These algorithms were constructed in an incremental way so that each new algorithm presents some improvements over the previous ones. Several simulations were completed to analyze the connectivity and query propagation patterns of the emergent logical networks. The results indicate that the algorithms with better behavior are those that induce greater collaboration among peers.",
        "Which Algorithm Performs Best: Algorithm Selection for Community Detection. A myriad of community detection methods have been designed to discover communities based on specific network features in different disciplines, such as sociology, physics, and computer science. Consequentially, we have to face the problem of Algorithm Selection for Community Detection (ASCD): Given a specific network, which algorithm should we select to reveal its latent community structures? In this study, we propose a model called CYDES to address the ASCD problem. CYDES consists of two parts, namely feature matrix generation and algorithm classification. We combine three effective feature extraction methods with the idea of BOW model to construct a fixed-size feature matrix. After a nonlinear transformation to the feature matrix, a softmax regression model is utilized to generate a classification label representing the best community detection algorithm we select. Extensive experimental results demonstrate that CYDES has high algorithm selection quality for community detection in networks. CCS CONCEPTS MOTIVATIONThis paper addresses the problem of Algorithm Selection for Community Detection (ASCD), which is illustrated in . To the best of our knowledge, this is the first time that the problem is proposed.We use an undirected graph G = (V , E) to denote a network, where V is the node set and E is the edge set. Let A = {a 1 , a 2 , . . . , a k } denote the community detection algorithm set consisting of k different algorithms. Given a network G, the ASCD problem is A Community Detection ToolboxFigure 1: An illustration of the ASCD problem. A community detection toolbox, such as CoDeT , includes CNM, Spectral, and other algorithms. Given a network, the aim of the ASCD problem is to select a community detection algorithm from the toolbox which performs best on the network. to select an algorithm a \u2208 A which has the best performance for G. Formally, the problem aims at learning a function T mapping a network G into the best algorithm a \u2208 A for G, i.e., a = T (G). We use the popular metric NMI (Normalized Mutual Information) to measure the performance of algorithms for a network G in this paper. The algorithm with the highest NMI value is the best algorithm. PROPOSED METHODSThe CYDES model consists of two parts. Given a network G, 1) it generates a feature matrix D for G, and then 2) it performs a nonlinear transformation on D and predicts the best community detection algorithm a \u2208 A with a softmax regression model. Feature matrix generationWe propose three effective feature extraction methods, which are based on 2-hop neighbor structure, normalized neighbor structure and 2 nd order random walk [1], respectively.2-hop neighbor structure. For each node v \u2208 V , we construct a local feature vector b 2\u2212hop based on its 2-hop neighbor structure. We use BFS (Breadth First Search) strategy to extract the subgraph G \u2032 within 2 hops starting from v. b 2\u2212hop for v consists of three parts: 1) the size of G \u2032 , 2) the density of G \u2032 , 3) the maximum p degree values in G \u2032 , where p is a constant. The three parts of values are concatenated to construct b 2\u2212hop . Let B 2\u2212hop denote the set of all |V | local feature vectors based on 2-hop neighbor structure in G.Normalized neighbor structure. For each node v \u2208 V , we construct a fixed-length local feature vector b norm based on its normalized neighbor structure. We also utilize the BFS strategy to get the subgraph G \u2032 within 2 hops starting from v. We perform graph normalization [2] on G \u2032 , which is an injective graph labeling procedure. According to the labeling results, the top q nodes are chosen to form a new subgraph G \u2032\u2032 , where q is a constant. b norm for v is constructed simply by expanding G \u2032\u2032 into one dimension. The advantage of this method is that the graph normalization ensures a unified way to get the information of different neighbor structures.",
        "Proposal for the 1st Interdisciplinary Workshop on Algorithm Selection and Meta-Learning in Information Retrieval (AMIR). The algorithm selection problem describes the challenge of identifying the best algorithm for a given problem space. In many domains, particularly artificial intelligence, the algorithm selection problem is well studied, and various approaches and tools exist to tackle it in practice. Especially through meta-learning impressive performance improvements have been achieved. The information retrieval (IR) community, however, has paid little attention to the algorithm selection problem, although the problem is highly relevant in information retrieval. This workshop will bring together researchers from the fields of algorithm selection and meta-learning as well as information retrieval. We aim to raise the awareness in the IR community of the algorithm selection problem; identify the potential for automatic algorithm selection in information retrieval; and explore possible solutions for this context. In particular, we will explore to what extent existing solutions to the algorithm selection problem from other domains can be applied in information retrieval, and also how techniques from IR can be used for automated algorithm selection and metalearning. MotivationThere is a plethora of algorithms for information retrieval applications, such as search engines and recommender systems. There are about 100 approaches to recommend research papers alone . The question that researchers and practitioners alike are faced with is which one of these approaches to choose for their particular problem. This is a difficult choice even for experts, compounded by ongoing research that develops ever more approaches.",
        "Comparing Pointwise and Listwise Objective Functions for Random-Forest-Based Learning-to-Rank Current random-forest (RF)-based learning-to-rank (LtR) algorithms use a classification or regression framework to solve the ranking problem in a pointwise manner. The success of this simple yet effective approach coupled with the inherent parallelizability of the learning algorithm makes it a strong candidate for widespread adoption. In this article, we aim to better understand the effectiveness of RF-based rank-learning algorithms with a focus on the comparison between pointwise and listwise approaches.We introduce what we believe to be the first listwise version of an RF-based LtR algorithm. The algorithm directly optimizes an information retrieval metric of choice (in our case, NDCG) in a greedy manner. Direct optimization of the listwise objective functions is computationally prohibitive for most learning algorithms, but possible in RF since each tree maximizes the objective in a coordinate-wise fashion. Computational complexity of the listwise approach is higher than the pointwise counterpart; hence for larger datasets, we design a hybrid algorithm that combines a listwise objective in the early stages of tree construction and a pointwise objective in the latter stages. We also study the effect of the discount function of NDCG on the listwise algorithm.Experimental results on several publicly available LtR datasets reveal that the listwise/hybrid algorithm outperforms the pointwise approach on the majority (but not all) of the datasets. We then investigate several aspects of the two algorithms to better understand the inevitable performance tradeoffs. The aspects include examining an RF-based unsupervised LtR algorithm and comparing individual tree strength. Finally, we compare the the investigated RF-based algorithms with several other LtR algorithms.",
        "A Robust Framework for Content-based Retrieval by Spatial Similarity in Image Databases A framework for retrieving images by spatial similarity (FRISS) in image databases is presented. In this framework, a robust retrieval by spatial similarity (RSS) algorithm is defined as one that incorporates both directional and topological spatial constraints, retrieves similar images, and recognizes images even after they undergo translation, scaling, rotation (both perfect and multiple), or any arbitrary combination of transformations. The FRISS framework is discussed and used as a base for comparing various existing RSS algorithms. Analysis shows that none of them satisfies all the FRISS specifications. An algorithm, SIM DTC , is then presented. SIM DTC introduces the concept of a rotation correction angle (RCA) to align objects in one image spatially closer to matching objects in another image for more accurate similarity assessment. Similarity between two images is a function of the number of common objects between them and the closeness of directional and topological spatial relationships between object pairs in both images. The SIM DTC retrieval is invariant under translation, scaling, and perfect rotation, and the algorithm is able to rank multiple rotation variants. The algorithm was tested using synthetic images and the TESSA image database. Analysis shows the robustness of the SIM DTC algorithm over current algorithms.",
        "Induced Sorting Suffixes in External Memory with Better Design and Less Space. Recently, several attempts have been made to extend the internal memory suffix array (SA) construction algorithm SA-IS to the external memory model, e.g., eSAIS, EM-SA-DS and DSA-IS. While the developed programs for these algorithms achieve remarkable performance in terms of I/O complexity and speed, their designs are quite complex and their disk requirements remain rather heavy. Currently, the core algorithmic part of each of these programs consists of thousands of lines in C++, and the average peak disk requirement is over 20n bytes for an input string of size n < 2 40 . We re-investigate the problem of induced sorting suffixes in external memory and propose a new algorithm SAIS-PQ (SAIS with Priority Queue) and its enhanced alternative SAIS-PQ+. Using the library STXXL, the core algorithmic parts of SAIS-PQ and SAIS-PQ+ are coded in around 800 and 1600 lines in C++, respectively. The time and space performance of these two programs are evaluated in comparison with eSAIS that is also implemented using STXXL. In our experiment, eSAIS runs the fastest for the input strings not larger than 16 GiB, but it is slower than SAIS-PQ+ for the only two input strings of 32 and 48.44 GiB. For the average peak disk requirements, eSAIS and SAIS-PQ+ are around 23n and 15n bytes, respectively.",
        "Integral based source selection for uncooperative distributed information retrieval environments. In this paper, a new source selection algorithm for uncooperative distributed information retrieval environments is presented. The algorithm functions by modeling each information source as an integral, using the relevance score and the intra-collection position of its sampled documents in reference to a centralized sample index and selects the collections that cover the largest area in the rank -relevance space. Based on the above novel metric, the algorithm explicitly focuses on addressing the two goals of source selection; high recall, which is important for source recommendation applications and high precision, aiming to produce a high precision final merged list. For the latter goal in particular, the new approach steps away from the usual practice of DIR systems of explicitly declaring the number of collections that must be queried and instead receives as input only the number of retrieved documents in the final merged list, dynamically calculating the number of collections that are selected and the number of documents requested from each. The algorithm is tested in a wide range of testbeds in both recall and precision oriented settings and its effectiveness is found to be equal or better than other state-of-the-art algorithms.",
        "Automated Algorithm Selection - Predict which algorithm to use! To achieve state-of-the-art performance, it is often crucial to select a suitable algorithm for a given problem instance. For example, what is the best search algorithm for a given instance of a search problem; or what is the best machine learning algorithm for a given dataset? By trying out many different algorithms on many problem instances, developers learn an intuitive mapping from some characteristics of a given problem instance (e.g., the number of features of a dataset) to a well-performing algorithm (e.g., random forest). The goal of automated algorithm selection is to learn from data, how to automatically select a well-performing algorithm given such characteristics. In this talk, I will give an overview of the key ideas behind algorithm selection and different approaches addressing this problem by using machine learning.",
        "Sorting using BItonic netwoRk wIth CUDA Novel \"manycore\" architectures, such as graphics processors, are high-parallel and high-performance shared-memory architectures [7] born to solve specific problems such as the graphical ones. Those architectures can be exploited to solve a wider range of problems by designing the related algorithm for such architectures. We present a fast sorting algorithm implementing an efficient bitonic sorting network. This algorithm is highly suitable for information retrieval applications. Sorting is a fundamental and universal problem in computer science. Even if sort has been extensively addressed by many research works, it still remains an interesting challenge to make it faster by exploiting novel technologies. In this light, this paper shows how to use graphics processors as coprocessors to speed up sorting while allowing CPU to perform other tasks. Our new algorithm exploits a memory-efficient data access pattern maintaining the minimum number of accesses to the memory out of the chip. We introduce an efficient instruction dispatch mechanism to improve the overall sorting performance. We also present a cache-based computational model for graphics processors. Experimental results highlight remarkable improvements over prior CPU-based sorting methods, and a significant improvement over previous GPU-based sorting algorithms.",
        "High Performance Clustering Based on the Similarity Join. A broad class of algorithms for knowledge discovery in databases (KDD) relies heavily on similarity queries, i.e. range queries or nearest neighbor queries, in multidimensional feature spaces. Many KDD algorithms perform a similarity query for each point stored in the database. This approach causes serious performance degenerations if the considered data set does not fit into main memory. Usual cache strategies such as LRU fail because the locality of KDD algorithms is typically not high enough. In this paper, we propose to replace repeated similarity queries by the similarity join, a database primitive prevalent in multimedia database systems. We present a schema to transform query intensive KDD algorithms into a representation using the similarity join as a basic operation without affecting the correctness of the result of the considered algorithm. In order to perform a comprehensive experimental evaluation of our approach, we apply the proposed transformation to the clustering algorithm DBSCAN and to the hierarchical cluster structure analysis method OPTICS. Our technique allows the application of any similarity join algorithm, which may be based on index structures or not. In our experiments, we use a similarity join algorithm based on a variant of the X-tree. The experiments yield substantial performance improvements of our technique over the original algorithms. The traditional techniques are outperformed by factors of up to 33 for the X-tree and 54 for the R*-tree. KeywordsData mining, clustering, database primitives, similarity join, multidimensional index structure. MotivationKnowledge discovery in databases (KDD) is the complex process of extracting implicit, previously unknown and potentially useful information from data in databases . In recent years, KDD has gained increasing interest in the research community as well as in traditional business data processing. The field of KDD knows various standard tasks such as classification , mining association rules [3], trend detection , and visualization . One of the most important tasks among those is clustering . The goal of a clustering algorithm is to group the objects of a database into a set of meaningful subclasses (clusters), such that objects in the same cluster are more similar to each other than objects belonging to different clusters. There are numerous applications of clustering .Recently, algorithms for extracting knowledge from large multidimensional data sets have become more and more important due to the fact that multidimensional data are prevalent in numerous non-standard applications for database systems. Examples of such systems include CAD databases , medical imaging [32] and molecular biology .When considering algorithms for KDD, we can observe that many algorithms rely heavily on repeated similarity queries, i.e. range queries or nearest neighbor queries, among feature vectors. For example, the algorithm for mining spatial association rules proposed in [25] performs a similarity query for each object of a specified type, such as a town. For various other KDD algorithms, this situation comes to an extreme: a similarity query has to be answered for each object in the database which obviously leads to a considerable computational effort.In order to accelerate this massive similarity query load, multidimensional index structures are usually applied for the management of the feature vectors. Provided that the index quality is high enough, which can usually be assumed for low and medium dimensional data spaces, such index structures accelerate the similarity queries to a logarithmic complexity. Therefore, the overall runtime complexity of the KDD algorithm is in O(n log n). Unfortunately, the overhead of executing all similarity queries separately is large. The locality of the queries is often not high enough, so that usual caching strategies for index pages such as LRU fail, which results in serious performance degenerations of the underlying KDD algorithms. Several solutions to alleviate this problem have been proposed, e.g. sampling or dimensionality reduction . These techniques imply some loss of information which may not be acceptable in some application domains. Both can, however, also be applied to our approach as a preprocessing step.The basic intention of our solution is to substitute the great multitude of expensive similarity queries by a similarity join operation using a distance-based join predicate, without affecting the correctness of the result of the given KDD algorithm: Consider a KDD algorithm that performs a range query (with range \u03b5) in a large database of points P i (0<i<n) for a large set of query points Q j (0<j<m). During the processing of such an algorithm, each point P i in the database is combined with each query point Q j which has a distance of no more than \u03b5. This is essentially a join operation between the two point sets P and Q with a distance-based join predicate, a so-called distance join or similarity join. The general idea of our approach is to transform query intensive KDD algorithms such that the transformed algorithms are based on a similarity join instead of repeated similarity queries. In this paper, we",
        "A hierarchical monothetic document clustering algorithm for summarization and browsing search results. Organizing Web search results into a hierarchy of topics and subtopics facilitates browsing the collection and locating results of interest. In this paper, we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search results retrieved in response to a query. At every level of the hierarchy, the new algorithm progressively identifies topics in a way that maximizes the coverage while maintaining distinctiveness of the topics. We refer the proposed algorithm to as DisCover. Evaluating the quality of a topic hierarchy is a non-trivial task, the ultimate test being user judgment. We use several objective measures such as coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithms to demonstrate its superiority. Even though our algorithm is slightly more computationally intensive than one of the algorithms, it generates better hierarchies. Our user studies also show that the proposed algorithm is superior to the other algorithms as a summarizing and browsing tool.",
        "RARD II: The 94 Million Related-Article Recommendation Dataset The main contribution of this paper is to introduce and describe a new recommender-systems dataset (RARD II). It is based on data from Mr. DLib, a recommender-system as-a-service in the digital library and reference-management-software domain. As such, RARD II complements datasets from other domains such as books, movies, and music. The dataset encompasses 94m recommendations, delivered in the two years from September 2016 to September 2018. The dataset covers an item-space of 24m unique items. RARD II provides a range of rich recommendation data, beyond conventional ratings. For example, in addition to the usual (implicit) ratings matrices, RARD II includes the original recommendation logs, which provide a unique insight into many aspects of the algorithms that generated the recommendations. The logs enable researchers to conduct various analyses about a real-world recommender system. This includes the evaluation of meta-learning approaches for predicting algorithm performance. In this paper, we summarise the key features of this dataset release, describe how it was generated and discuss some of its unique features. Compared to its predecessor RARD, RARD II contains 64% more recommendations, 187% more features (algorithms, parameters, and statistics), 50% more clicks, 140% more documents, and one additional service partner (JabRef).",
        "A new family of online algorithms for category ranking. We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory efficient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio's algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the first to report performance results with the entire new Reuters corpus.",
        "A novel image retrieval algorithm based on transfer learning and fusion features. With proliferation of social media, image has become ubiquitous giving rise to the demand and importance of image semantic analysis and retrieval to access information quickly on social media. However, even with humongous information available, there are certain categories of images which are important for certain applications but are very scarce. Convolutional neural network is an effective method to extract high-level semantic features for image database retrieval. To overcome the problem of over-fitting when the number of training samples in dataset is limited, this paper proposes an image database retrieval algorithm based on the framework of transfer learning and feature fusion. Based on the fine-tuning of the pre-trained Convolutional Neural Network (CNN), the proposed algorithm first extracts the semantic features of the images. Principal Component Analysis (PCA) is then applied for dimension reduction and to reduce the computational complexity. Last, the semantic feature extracted from the CNN is fused with traditional low-level visual feature to improve the retrieval accuracy further. Experimental results demonstrated the effectiveness of the proposed method for image database retrieval.",
        "Bandit Algorithms in Interactive Information Retrieval. e multi-armed bandit problem models an agent that simultaneously aempts to acquire new knowledge (exploration) and optimize his decisions based on existing knowledge (exploitation). e agent aempts to balance these competing tasks in order to maximize his total value over the period of time considered. ere are many practical applications of the bandit model, such as clinical trials, adaptive routing or portfolio design. Over the last decade there has been an increased interest in developing bandit algorithms for specic problems in information, such as diverse document ranking, news recommendation or ranker evaluation. e aim of this tutorial is to provide an overview of the various applications of bandit algorithms in information retrieval as well as issues related to their practical deployment and performance in real-life systems/applications. CCS CONCEPTS\u2022Information systems \u2192Personalization; Information retrieval diversity; \u2022Computing methodologies \u2192Online learning seings; KEYWORDS bandit algorithms, information retrieval, recommender systems, exploration-exploitation trade-o, interactive search, personalization, system optimization MOTIVATIONWith ever increasing amount and type of data available on the web, search engines have gradually developed into complex systems that combine many ranking criteria with the aim of producing the optimal result list in response to users queries. Traditional approaches, where a ranking algorithm are trained o-line with manually annotated data can be very expensive due to the involvement of human experts. Creation of new algorithms in this way can also be to slow to respond to rapid growth of new data or changes of documents' relevance to a given query over time. Additionally, it may not always be possible for experts to annotate data as might be the case in personalised search, where the perceived relevance of a given document is usually very subjective.A response to these issues has been emergence of new branches of information retrieval, such as online learning to rank, interactive information retrieval or dynamic information retrieval. ese Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICTIR'17, October 1-4, 2017, Amsterdam, e Netherlands. approaches oen employ various reinforcement learning methods, of which bandit algorithms are the most popular. e bandit problem models an agent that simultaneously aempts to acquire new knowledge (exploration) and optimize his decisions based on existing knowledge (exploitation). e agent aempts to balance these competing tasks in order to maximize his total value over the period of time considered. is aspect of bandit algorithms allow the search engine to gradually build the user model without geing stuck in a local search space, while engaging the user in the search loop. e simplicity and ease of implementation of bandit algorithms also adds to their popularity in the information retrieval community. Over the past decade there has been an increase in the number of new bandit algorithms as well as new applications for these algorithms. However, not all of these algorithms are equality suitable to all information retrieval seings or all types of data.Bandit algorithms have been studied extensively by the statistics and machine learning communities since 1950-ies. However, only recently has there been a growing interest in the application of bandits in information retrieval and related areas. We now have a handful of bandit algorithms aimed specically at solving IR problems, such as online learning to rank, ranker evaluation or click models. e goal of this tutorial is to bring together current eorts in the area, summarize the research performed so far and give a holistic view on the challenges of applying bandit algorithms in the information retrieval domain. FORMAT AND DETAILED SCHEDULEe tutorial will consist of three parts: 1) overview of bandit algorithms; 2) application of bandits in information retrieval; 3) Optimization of information retrieval systems based on bandit algorithms. In the rst part will provide an overview of bandit algorithms and how they develop overtime starting from Giins indices and then gradually follow the algorithmic development of bandits: Upper Condence Bound (UCB) algorithms, multi-armed bandits, dependent arm bandits, contextual bandits, dueling bandits, collaborative bandits, etc. e basic aspects of bandit algorithms, such as the reward function and the exploration-exploitation trade-o will be introduced as well. e rst part of the tutorial aims to not only familiarize the audience with the mathematical and statistical foundations of of bandits but also provide them with an intuition how they can be applied to real-life problems through examples from various areas, such as clinical trials, economics or information retrieval.Topics covered in the rst part of the tutorial:\u2022 Giins indices , UCB [4]\u2022 multi-armed bandits \u2022 dependent arm bandits \u2022 contextual bandits \u2022 dueling bandits Tutorial",
        "A Linear-Time Burrows-Wheeler Transform Using Induced Sorting. To compute Burrows-Wheeler Transform (BWT), one usually builds a suffix array (SA) first, and then obtains BWT using SA, which requires much redundant working space. In previous studies to compute BWT directly , one constructs BWT incrementally, which requires O(n log n) time where n is the length of the input text. We present an algorithm for computing BWT directly in linear time by modifying the suffix array construction algorithm based on induced sorting . We show that the working space is O(n log \u03c3 log log \u03c3 n) for any \u03c3 where \u03c3 is the alphabet size, which is the smallest among the known linear time algorithms.",
        "Mixture model with multiple centralized retrieval algorithms for result merging in federated search. Result merging is an important research problem in federated search for merging documents retrieved from multiple ranked lists of selected information sources into a single list. The state-of-the-art result merging algorithms such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) try to map document scores retrieved from different sources to comparable scores according to a single centralized retrieval algorithm for ranking those documents. Both SSL and SAFE arbitrarily select a single centralized retrieval algorithm for generating comparable document scores, which is problematic in a heterogeneous federated search environment, since a single centralized algorithm is often suboptimal for different information sources.Based on this observation, this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithms. One simple approach is to learn a set of combination weights for multiple centralized retrieval algorithms (e.g., logistic regression) to compute comparable document scores. The paper shows that this simple approach generates suboptimal results as it is not flexible enough to deal with heterogeneous information sources. A mixture probabilistic model is thus proposed to learn more appropriate combination weights with respect to different types of information sources with some training data. An extensive set of experiments on three datasets have proven the effectiveness of the proposed new approach.",
        "Minimal document set retrieval. This paper presents a novel formulation and approach to the minimal document set retrieval problem. Minimal Document Set Retrieval (MDSR) is a promising information retrieval task in which each query topic is assumed to have different subtopics; the task is to retrieve and rank relevant document sets with maximum coverage but minimum redundancy of subtopics in each set. For this task, we propose three document set retrieval and ranking algorithms: Novelty Based method, Cluster Based method and Subtopic Extraction Based method. In order to evaluate the system performance, we design a new evaluation framework for document set ranking which evaluates both relevance between set and query topic, and redundancy within each set. Finally, we compare the performance of the three algorithms using the TREC interactive track dataset. Experimental results show the effectiveness of our algorithms.",
        "Quantitative evaluation of passage retrieval algorithms for question answering. Passage retrieval is an important component common to many question answering systems. Because most evaluations of question answering systems focus on end-to-end performance, comparison of common components becomes difficult. To address this shortcoming, we present a quantitative evaluation of various passage retrieval algorithms for question answering, implemented in a framework called Pauchok. We present three important findings: Boolean querying schemes perform well in the question answering task. The performance differences between various passage retrieval algorithms vary with the choice of document retriever, which suggests significant interactions between document retrieval and passage retrieval. The best algorithms in our evaluation employ density-based measures for scoring query terms. Our results reveal future directions for passage retrieval and question answering.",
        "Preface: The 1st Interdisciplinary Workshop on Algorithm Selection and Meta-Learning in Information Retrieval (AMIR) Algorithm selection is a key challenge for most, if not all, computational problems. Typically, there are several potential algorithms that can solve a problem, but which algorithm would perform best (e.g. in terms of runtime or accuracy) is often unclear. In many domains, particularly artificial intelligence, the algorithm selection problem is well-studied, and various approaches and tools exist to tackle it in practice. Especially through meta-learning, impressive performance improvements have been achieved. The information retrieval (IR) community, however, has paid relatively little attention to the algorithm selection problem. The 1st Interdisciplinary Workshop on Algorithm Selection and Meta-Learning in Information Retrieval (AMIR) brought together researchers from the IR community as well as from the machine learning (ML) and meta-learning community. Our goal was to raise the awareness in the IR community of the algorithm selection problem; identify the potential for automatic algorithm selection in information retrieval; and explore possible solutions for this context. AMIR was co-located with the 41st European Conference on Information Retrieval (ECIR) in Cologne, Germany, and held on the 14 th of April 2019. Out of ten submissions, five (50%) were accepted at AMIR, and an estimated 25 researchers attended the workshop.",
        "On arabic search: improving the retrieval effectiveness via a light stemming approach. The inflectional structure of a word impacts the retrieval accuracy of information retrieval systems of Latin-based languages. We present two stemming algorithms for Arabic information retrieval systems. We empirically investigate the effectiveness of surfacebased retrieval. This approach degrades retrieval precision since Arabic is a highly inflected language. Accordingly, we propose root-based retrieval. We notice a statistically significant improvement over the surface-based approach. Many variant word senses are based on an identical root; thus, the root-based algorithm creates invalid conflation classes that result in an ambiguous query which degrades the performance by adding extraneous terms. To resolve ambiguity, we propose a novel lightstemming algorithm for Arabic texts. This automatic rule-based stemming algorithm is not as aggressive as the root extraction algorithm. We show that the light stemming algorithm significantly outperforms the root-based algorithm. We also show that a significant improvement in retrieval precision can be achieved with light inflectional analysis of Arabic words.",
        "Training Algorithms for Linear Text Classifiers. Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms.Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks.",
        "Clustering-Based Searching and Navigation in an Online News Source. The growing amount of online news posted on the WWW demands new algorithms that support topic detection, search, and navigation of news documents. This work presents an algorithm for topic detection that considers the temporal evolution of news and the structure of web documents. Then, it uses the results of the topic detection algorithm for searching and navigating in an online news source. An experimental evaluation with a collection of online news in Spanish indicates the advantages of incorporating the temporal aspect and structure of documents in the topic detection of news. In addition, topic-based clusters are well suited for guiding the search and navigation of news.",
        "Algorithm Selection with Librec-auto",
        "A Novel Approach to Define and Model Contextual Features in Recommender Systems. Recommender Systems(RS) provide more accurate and more relevant recommendations using contextual feature(s). This accuracy improvement is at the cost of computational expenses. Therefore, finding and selecting the most relevant contextual features is an important problem. Moreover, modeling and incorporating the selected contextual features in RS algorithms has an impact on both the accuracy and computational cost. We are conducting a series of studies to detect, define, select, model and incorporate the most relevant contextual features for RS algorithms. The feature detection, definition and selection approach involves the evaluation of features derived from implicit and explicit information. The selected features from this approach can be modeled and incorporated in any selected RS algorithm. In our recent works, we also propose a series of algorithms that incorporates multiple contextual features in the baseline matrix factorization (MF) algorithm. We use the selected contextual features to modify user biases and item biases in the baseline MF. KeywordsContext Aware Recommender Systems; Matrix Factorization; Recommender Systems; Contextual Feature Selection RESEARCH PROPOSALTraditional RS consider only the information regarding the items and the users in order to provide recommendation. However, in some tasks user's opinion about an item might vary from one situation to another . For these cases, it is important to consider contextual information as additional information to generate more relevant recommendations. Context Aware Recommender Systems (CARS) are one of the emerging topics of study in the field of RS. The studies in this field includes the investigations on how to model and incorporate contextual information in a traditional RS. Detecting, defining and selecting contextual features to be modeled and incorporated in RS algorithms is Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). non-trivial. For this reason, it is important to carefully evaluate each contextual information to determine if it is relevant and ultimately if it can improve the accuracy of the system. SIGIR '16 July 17-21, 2016, Pisa, ItalyThe general research question to be addressed in our proposed work is How to detect, define, select, model and incorporate contextual information into RS algorithms? This question is broken down into four specific questions: RQ1. Which information can be considered as contextual information for a specific recommendation task? In any recommendation task there is an interaction between the user and the system that provides recommendation. Through this interaction there are several user related foot prints that may reveal the characteristics of the situation that user interacts with the system. RQ2. How to define contextual information as contextual features to be used in RS algorithms? Contextual information, in their original format, often can not be incorporated in RS algorithms. RQ3. How to select the most relevant contextual features to be used in RS algorithms? Not all contextual features can increase the accuracy of prediction. It is important to select the most relevant contextual features. We proposed a novel contextual feature selection in a recent work. RQ4. How to model and incorporate contextual features into a traditional RS algorithm? Each RS algorithm has unique feature modeling and incorporation requirements. Selected contextual features should then be modeled to match the requirements of the selected RS algorithm. In our prior work [2] we evaluated baseline MF algorithm and its requirements. We propose an extension to MF algorithm that incorporates multiple \"user-based\" and \"item-based\" contextual features. In a recent work, we proposed and tested a context aware MF algorithm that incorporates multiple \"user-based\" contextual features.",
        "Second International Workshop on Algorithmic Bias in Search and Recommendation (BIAS@ECIR2021) Providing efficient and effective search and recommendation algorithms has been traditionally the main objective for the industrial and academic research communities. However, recent studies have shown that optimizing models through these algorithms may reinforce the existing societal biases, especially under certain circumstances (e.g., when historical users' behavioral data is used for training). Identifying and mitigating data and algorithmic biases thus becomes a crucial aspect, ensuring that these models have a positive impact on the stakeholders involved in the search and recommendation processes. The BIAS 2021 workshop aims to collect novel contributions in this emerging field, providing a common ground for researchers and practitioners.",
        "Recognition and classification of noun phrases in queries for effective retrieval. It has been shown that using phrases properly in the document retrieval leads to higher retrieval effectiveness. In this paper, we define four types of noun phrases and present an algorithm for recognizing these phrases in queries. The strengths of several existing tools are combined for phrase recognition. Our algorithm is tested using a set of 500 web queries from a query log, and a set of 238 TREC queries. Experimental results show that our algorithm yields high phrase recognition accuracy. We also use a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries. A document retrieval experiment is conducted using the TREC queries (1) without any phrases, (2) with the phrases recognized from a baseline noun phrase recognition algorithm, and (3) with the phrases recognized from our algorithm respectively. The retrieval effectiveness of (3) is better than that of (2), which is better than that of (1). This demonstrates that utilizing phrases in queries does improve the retrieval effectiveness, and better noun phrase recognition yields higher retrieval performance."
    ],
    "misspellings in queries": [
        "Online spelling correction for query completion. In this paper, we study the problem of online spelling correction for query completions. Misspelling is a common phenomenon among search engines queries. In order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. As latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient.To tackle this problem, we propose and study a generative model for input queries, based on a noisy channel transformation of the intended queries. Utilizing spelling correction pairs, we train a Markov n-gram transformation model that captures user spelling behavior in an unsupervised fashion. To find the top spellcorrected completion suggestions in real-time, we adapt the A* search algorithm with various pruning heuristics to dynamically expand the search space efficiently. Evaluation of the proposed methods demonstrates a substantial increase in the effectiveness of online spelling correction over existing techniques.",
        "Spelling checkers, spelling correctors and the misspellings of poor spellers",
        "Managing misspelled queries in IR applications",
        "Query spelling correction using multi-task learning. This paper explores the use of online multi-task learning for search query spelling correction, by effectively transferring information from different and biased training datasets for improving spelling correction across datasets. Experiments were conducted on three query spelling correction datasets, including the well-known TREC benchmark data. Our experimental results demonstrate that the proposed method considerably outperforms existing baseline systems in terms of accuracy. Importantly, the proposed method is about oneorder of magnitude faster than baseline systems in terms of training speed. In contrast to existing methods which typically require more than (e.g.,) 50 training passes, our algorithm can very closely approach the empirical optimum in around five passes.",
        "CloudSpeller: query spelling correction by using a unified hidden markov model with web-scale resources. Query spelling correction is an important component of modern search engines that can help users to express an information need more accurately and thus improve search quality. In this work we proposed and implemented an end-to-end speller correction system, namely CloudSpeller. The CloudSpeller system uses a Hidden Markov Model to effectively model major types of spelling errors in a unified framework, in which we integrate a large-scale lexicon constructed using Wikipedia, an error model trained from high confidence correction pairs, and the Microsoft Web N-gram service. Our system achieves excellent performance on two search query spelling correction datasets, reaching 0.960 and 0.937 F1 scores on the TREC dataset and the MSN dataset respectively.",
        "Studying the effect and treatment of misspelled queries in Cross-Language Information Retrieval",
        "A generalized hidden Markov model with discriminative training for query spelling correction. Query spelling correction is a crucial component of modern search engines. Existing methods in the literature for search query spelling correction have two major drawbacks. First, they are unable to handle certain important types of spelling errors, such as concatenation and splitting. Second, they cannot efficiently evaluate all the candidate corrections due to the complex form of their scoring functions, and a heuristic filtering step must be applied to select a working set of top-K most promising candidates for final scoring, leading to non-optimal predictions. In this paper we address both limitations and propose a novel generalized Hidden Markov Model with discriminative training that can not only handle all the major types of spelling errors, including splitting and concatenation errors, in a single unified framework, but also efficiently evaluate all the candidate corrections to ensure the finding of a globally optimal correction. Experiments on two query spelling correction datasets demonstrate that the proposed generalized HMM is effective for correcting multiple types of spelling errors. The results also show that it significantly outperforms the current approach for generating top-K candidate corrections, making it a better firststage filter to enable any other complex spelling correction algorithm to have access to a better working set of candidate corrections as well as to cover splitting and concatenation errors, which few existing method in academic literature can correct.",
        "Multi-Candidate Ranking Algorithm Based Spell Correction Spell correction is an important component in Natural Language Processing (NLP). In the context of a product search engine, an effective spell correction system can improve the accuracy of the search results and reduce the occurrence of No Results Found (NRF). Conversely, a sub-optimal spell correction has negative effects, e.g., failing to correct misspelled queries, modifying correct queries into wrong ones. In this paper, three novel components / algorithms currently used in The Home Depot (THD) spell correction service is presented: (1) word embedding based dictionary construction; (2) multi-path candidates generation; (3) high dimensional cluster analysis based ranking model. The dictionary provides data about the inner relationships among the words for a given corpus. With the dictionary, the candidate generation algorithm recommends a set of correction candidates for several misspelling hypothesis, e.g., word editing error, word breaking error, word concatenation error, fat finger typing error, and so on. Then the ranking model projects the candidates into a high dimensional space and sorts them based on cluster density analysis. In the experiment, the new THD spell correction is compared with the old version (without these features), Lucene spell correction and Grammarly spell correction. The evaluation results indicated the THD spell correction has higher correction accuracy than the other widely used implementations."
    ],
    "information in different language": [
        "Cross-Language Information Retrieval (CLIR) Track Overview A cross-language retrieval track was offered for the third time at TREC-8. The main task was the same as that of the previous year: the goal was for groups to use queries written in a single language in order to retrieve documents from a multilingual pool of documents written in many different languages. Compared to the usual definition of cross-language information retrieval, where systems work with a single language pair, retrieving documents in a language L1 using queries in language L2, this is a slightly more comprehensive task, and we feel one that more closely meets the demands of real world applications. The document languages used were the same as for TREC-7: English, German, French and Italian. The queries were available in all of these languages. Monolingual non-English retrieval was offered to new participants who preferred to begin with an easier task. However, all the groups which did not tackle the full task opted for limited cross-language rather than monolingual runs. These experiments were evaluated by NIST and are published as unofficial (\"alternate\") runs. We also offered a subtask, working with documents from the field of social sciences. This collection (known as \"GIRT\") has some very interesting features, such as controlled vocabulary terms, title translations, and an associated multilingual thesaurus. The track was coordinated at Eurospider Information Technology AG in Zurich. Due to its multilingual nature, the topic creation and relevance assessment tasks were distributed over four sites in different countries: NIST (English), IZ Bonn (German), IEI-CNR (Italian) and University of Zurich (French). The University of Hildesheim invested considerable effort into rendering the topics homogeneous and consistent over languages. The participating groups experimented with a wide variety of strategies, ranging machine translation, corpus-, and dictionary-based approaches. Some results are given in Section 4. There were, however, also some striking similarities between many of the runs, such as the choice of English as topic language the majority, and the use of Systran by a lot of groups. Some implications of these findings are discussed in Section 5. The main goal of the TREC CLIR activities has been the creation of a multilingual test collection that is re-usable for a wide range of evaluation experiments. This means that the quality of the relevance assessments is very important. The Twenty-One group conducted an interesting analysis with respect to the completeness of the assessments and the impact of this on the pool. We address some of their findings in Section 5. The paper concludes with an indication of our plans for the future of the cross-language track, which will bring substantial changes to the format and coordination of the activities.",
        "Concept unification of terms in different languages via web mining for Information Retrieval a b s t r a c tFor historical and cultural reasons, English phases, especially proper nouns and new words, frequently appear in Web pages written primarily in East Asian languages such as Chinese, Korean, and Japanese. Although such English terms and their equivalences in these East Asian languages refer to the same concept, they are often erroneously treated as independent index units in traditional Information Retrieval (IR). This paper describes the degree to which the problem arises in IR and proposes a novel technique to solve it. Our method first extracts English terms from native Web documents in an East Asian language, and then unifies the extracted terms and their equivalences in the native language as one index unit. For Cross-Language Information Retrieval (CLIR), one of the major hindrances to achieving retrieval performance at the level of Mono-Lingual Information Retrieval (MLIR) is the translation of terms in search queries which can not be found in a bilingual dictionary. The Web mining approach proposed in this paper for concept unification of terms in different languages can also be applied to solve this well-known challenge in CLIR. Experimental results based on NTCIR and KT-Set test collections show that the high translation precision of our approach greatly improves performance of both Mono-Lingual and Cross-Language Information Retrieval.",
        "Cross-Language Microblog Retrieval using Latent Semantic Modeling. Microblogging has become one of the major tools of sharing real-time information for people around the world. Finding relevant information across different languages on microblogs is highly desirable especially for the large number of multilingual users. However, the characteristics of microblog content pose great challenges to the existing cross-language information retrieval approaches. In this paper, we address the task of retrieving relevant tweets given another tweet in a different language. We build parallel corpora for tweets in different languages by bridging them via shared hashtags. We propose a latent semantic approach to model the parallel corpora by mapping the parallel tweets to a low-dimensional shared semantic space. The relevance between tweets in different languages is measured in this shared latent space and the model is trained on a pairwise loss function. The preliminary experiments on a Twitter dataset demonstrate the effectiveness of the proposed approach.",
        "Regular Sound Changes for Cross-language Information Retrieval The aim of this project is the automatic conversion of query terms in one language into their equivalents in a second, historically related, language, so that documents in the second language can be retrieved. The method is to compile lists of regular sound changes which occur between related words of a language pair, and substitute these in the source language words to generate target language words. For example, if we know b in Italian often corresponds with a v in Spanish, an unaccented o in Italian with \u00f3 in Spanish, and a terminal e in Italian is replaced with a null in Spanish, we can construct the Spanish word aut\u00f3movil (car) from the Italian automobile. A bilingual word list or dictionary is needed at first to first discover the set of regular sound changes, but once this is known, there is no further need for a dictionary to look up individual query words. The method is language pair independent, as long as the two languages belong to the same language family, such as the Romance languages. Buckley et al. (2000) proposed a related method based on knowledge of regular orthographic changes between languages, when corresponding words in two languages are pronounced alike, but the method proposed here is novel in that it also incorporates regular, linguistically attested, sound changes between historically related languages.",
        "The Effects of Query Structure and Dictionary Setups in Dictionary-Based Cross-Language Information Retrieval. In this study, the effects of query structure and various setups of translation dictionaries on the performance of cross-language information retrieval (CLIR) were tested. The document collection was a subset of the TREC collection, and as test requests the study used TREC's health related topics. The test system was the INQUERY' retrieval system. The performance of translated Finnish queries against English documents was compared to the performance of original English queries against English documents. Four natural language query types and three query translation methods, using a general dictionary and a domain specific (= medical) dictionary, were studied. There was only a slight difference in performance between the original English queries and the best crosslanguage queries, i.e., structured queries with medical dictionary and general dictionary translation. The structuring of queries was done on the basis of the output of dictionaries. In recent years, the development of Internet and related technology has created world-wide multilingual document collections. At the same time information flow across languages has grown due to increased international collaboration. With these factors in the background, IR research has paid increasing attention to cross-language IR (CLIR) systems, where the user presents a query in one language, and in response the system retrieves documents in another language. The need of CLIR systems in today's world is obvious. Moving from the global perspective to an individual level, CLIR is useful, for example, for the people, who are able to understand a foreign language, but have difficulty in using it actively. Other examples of the use of CLIR are given by .In cross-language IR either documents or queries have to be translated. Research has concentrated on query translation, as it is computationally less expensive than document translation, which requires a lot of memory and processing capacity . Within the query translation framework, basic approaches to CLIR are machine translation (MT), corpus-based methods, and dictionary-based methods.In current MT systems the quality of translations is often low Yama-Permission to make digitafiard copy of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear. and notice is given that copying is by petmission of ACM, Inc. To copy otherwise, to republish, to post on sewers or to redisttibure fo lists, requires prior specific permission and/or fee. SIGIR'98, Melbourne. Australia Q 1998 ACM l-58113415-58/98 $5.00. bana et al., 1996). High quality translations can be obtained only when the applicable domain is limited, so that the system can provide sufficient domain knowledge. For IR, the basic problem is, however, that the user requests often are mere sequences of words, without proper internal syntactic structure. Disambiguation in MT systems is, however, based on syntactic analysis. Therefore, MT is not regarded as a promising method for query translation in CLIR.In corpus-based methods queries are translated and expanded on the basis of multilingual terminology derived from comparable document collections or parallel corpora, these containing similar or identical documents in different languages. A variety of methods based on this approach have been developed, and the performance of queries has varied depending on the method used . found that their corpus-based CLIR queries performed almost as well as the monolingual baseline queries.Dictionary-based translation is often easier way to implement query translation than the methods based on the comparable documents or the parallel corpora, as these are not readily available. Therefore in the University of Tampere we have adopted the dictionary-based method for our CLIR studies. The dictionaries used in CLIR are often bilingual machine readable dictionaries (MRD), designed for human readers but converted for the purpose of cross-language retrieval, or bilingual or multilingual thesauri, developed specifically for CLIR (see Gilarmnz et al., 1997). MRD translation uses a trivial method, in which a source language word is replaced by all of its target language equivalents, all of which are taken to the final query. Studies have shown that the performance of MRD-based system in its simplest form has, roughly, only half of the performance of its monolingual counterpart .The basic problems associated with MRD translation are (1) phrase translation, (2) translation polysemy (translation ambiguity), and (3) the coverage of dictionaries. If phrases arc not identified, MRD translates phrase constituents instead of full phrases, and the senses of multi-word keywords are lost. This results in decreased precision. Automatic phrase identification methods have been developed for CLIR environment . Translation polysemy is a phenomenon, in which the number of word senses increases when a source language word is translated to a target language by replacing it with all of its target language equivalents. 'The INQUERY information retrieval system is provided by the Information",
        "Two-Stage Refinement of Transitive Query Translation with English Disambiguation for Cross-Language Information Retrieval: An Experiment at CLEF 2004",
        "Cross-Language Information Retrieval and Evaluation, Workshop of Cross-Language Evaluation Forum, CLEF 2000, Lisbon, Portugal, September 21-22, 2000, Revised Papers This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are liable for prosecution under the German Copyright Law. PrefaceThe first evaluation campaign of the Cross-Language Evaluation Forum (CLEF) for European languages was held from January to September 2000. The campaign culminated in a two-day workshop in Lisbon, Portugal, 21-22 September, immediately following the fourth European Conference on Digital Libraries (ECDL 2000). The first day of the workshop was open to anyone interested in the area of Cross-Language Information Retrieval (CLIR) and addressed the topic of CLIR system evaluation. The goal was to identify the actual contribution of evaluation to system development and to determine what could be done in the future to stimulate progress. The second day was restricted to participants in the CLEF 2000 evaluation campaign and to their experiments. This volume constitutes the proceedings of the workshop and provides a record of the campaign. CLEF is currently an activity of the DELOS Network of Excellence for Digital Libraries, funded by the EC Information Society Technologies to further research in digital library technologies. The activity is organized in collaboration with the US National Institute of Standards and Technology (NIST). The support of DELOS and NIST in the running of the evaluation campaign is gratefully acknowledged.I should also like to thank the other members of the Workshop Steering Committee for their assistance in the organization of this event. April 2001Carol Peters The objective of the Cross-Language Evaluation Forum (CLEF) is to develop and maintain an infrastructure for the testing and evaluation of information retrieval systems operating on European languages, in both monolingual and cross-language contexts, and to create test-suites of reusable data that can be employed by system developers for benchmarking purposes. The first CLEF evaluation campaign started in early 2000 and ended with a workshop in Lisbon, Portugal, 22-23 September 2000. This volume constitutes the proceedings of the workshop and also provides a record of the results of the campaign. It consists of two parts and an appendix. The first part reflects the presentations and discussions on the topic of evaluation for crosslanguage information retrieval systems during the first day of the workshop, whereas the second contains papers from the individual participating groups reporting their experiments and analysing their results. The appendix presents the evaluation techniques and measures used to derive the results and provides the run statistics. The aim of this Introduction is to present the main issues discussed at the workshop and also to provide the reader with the necessary background to the experiments through a description of the tasks set for CLEF 2000. In conclusion, our plans for future CLEF campaigns are outlined. CLEF 2000 Workshop Steering Committee Evaluation for CLIR SystemsThe first two papers in Part I of the proceedings describe the organization of crosslanguage evaluation campaigns for text retrieval systems. CLEF is a continuation and expansion of the cross-language system evaluation activity for European languages begun in 1997 with the track for Cross-Language Information Retrieval (CLIR) in the Text REtrieval Conference (TREC) series. The paper by Harman et al. gives details on how the activity was organized, the various issues that had to be addressed, and the results obtained. The difficulties experienced during the first year, in which the track was coordinated centrally at NIST (US National Institute for Standards and Technology) led to the setting up of a distributed coordination in four countries (USA, Germany, Italy and Switzerland) with native speakers being responsible for the preparation of topics (structured statements of possible information needs) and relevance judgments (assessment of the relevance of the ranked lists of results submitted by participating systems). A natural consequence of this distributed coordination was the 2 Carol Peters decision, in 1999, to transfer the activity to Europe and set it up independently as CLEF. The infrastructure and methodology adopted in CLEF is based on the experience of the CLIR tracks at TREC.The second paper by Kando presents the NTCIR Workshops, a series of evaluation workshops for text retrieval systems operating on Asian languages. The 2000-2001 campaign conducted by NTCIR included cross-language system evaluation for Japanese-English and Chinese-English. Although both CLEF and NTCIR have a common basis in TREC there are interesting differences between the methodology adopted by the two campaigns. In particular, NTCIR employs multigrade relevance judgments rather than the binary system used by CLEF and inherited from TREC. Kando motivates this decision and discusses the effects.The CLEF campaign provides participants with the possibility to test their systems on both general-purpose texts (newspapers and newswires) and domain-specific collections. The third paper by Kluck and Gey examines the domain-specific task, begun in TREC and continued in CLEF, and describes the particular document collection used: the GIRT database for social sciences.The rest of the papers in the first part of this volume focus on some of the main issues that were discussed during the first day of the workshop. These included the problem of resources, the transition from the evaluation of cross-language text retrieval systems to systems running on other media, the need to consider the user perspective rather than concentrating attention solely on system performance, and the importance of being able to evaluate single system components rather than focusing on overall performance. A further point for discussion was the addition of new languages to the multilingual document collection.The problem of resources has always been seen as crucial in cross-language system development. In order to be able to match queries against documents, some kind of lexical resource is needed to provide the transfer mechanism, e.g. bilingual or multilingual dictionaries, thesauri, or corpora. In order to be able to process a number of different languages, suitable language processing tools are needed, e.g. languagespecific tokenizers, stemmers, morphologies, etc.. It is generally held that the quality of the resource used considerably affects system performance. This question was discussed at length during the workshop. The paper by Gonzalo presents a survey on the different language resources used by the CLEF 2000 participants. Many of the resources listed were developed by the participants themselves, thus showing that an evaluation exercise of this type is not only evaluating systems but also the resources used by the systems. The need for more pooling and sharing of resources between groups in order to optimize effort emerges clearly from this survey. Gonzalo concludes with some interesting proposals for the introduction of additional tasks, aimed at measuring the effect of the resources used on overall system performance, in a future campaign.The papers by Oard and by Jones both discuss CLIR from the user perspective. Oard focuses on the document selection question: how the users of a CLIR system can correctly identify the -for them -most useful documents from a ranked list of results when they cannot read the language of the target collection. He advocates the advantages of an interactive CLIR evaluation and makes a proposal as to how an evaluation of this type could be included in CLEF. Jones also supports the extension of evaluation exercises in order to assess the usefulness of techniques that can assist the user with Introduction 3 relevance judgment and information extraction. In this respect, he mentions the importance of document summarization -already included in the NTCIR evaluation programme. In addition, Jones talks about work in cross-language multimedia information retrieval and suggests directions for future research. He asserts that specifically-developed standard test collections are needed to advance research in this area.In the final paper in Part I, Gey lists several areas in which research could lead to improvement in cross-language information retrieval including resource enrichment, the use of pivot languages and phonetic transliteration. In particular, he discusses the need for post-evaluation failure analysis and shows how this could provide important feedback resulting in improved system design and performance. CLEF provides the research community with the necessary infrastructure for studies of this type. The CLEF 2000 ExperimentsThere were several reasons behind the decision to coordinate the cross-language system evaluation activity for European languages independently and to move it to Europe. One was the desire to extend the number of languages covered, another was the intention to offer a wider range of retrieval tasks to better meet the needs of the multilingual information retrieval research community.As can be seen from the descriptions of the experiments in Part II of this volume, CLEF 2000 included four separate evaluation tracks:\u2022 multilingual information retrieval \u2022 bilingual information retrieval \u2022 monolingual (non-English) information retrieval \u2022 cross-language domain-specific information retrieval The main task -inherited from TREC -required searching a multilingual document collection, consisting of national newspapers in four languages (English, French, German and Italian) of the same time period, in order to retrieve relevant documents. Forty topics were developed on the basis of the contents of the multilingual collection -ten topics for each collection -and complete topic sets were produced in all four languages. Topics are structured statements of hypothetical user needs. Each topic consisted of three fields: a brief title statement; a one-sentence description; a more complex narrative specifying the relevance assessment criteria. Queries are constructed using one of more of these fields. Additional topic sets were then created for Dutch, Finnish, Spanish and Swedish, in each case translating from the original. The main requirement was that, for each language, the topic set should be as linguistically representative as possible, i.e. using the terms that would naturally be expected to represent the set of topic concepts in the given language. The methodology followed was that described in the paper by Harman et al..A bilingual system evaluation task was also offered, consisting of querying the English newspaper collection using any topic language (except English). Many newcomers to cross-language system evaluation prefer to begin with the simpler bilingual task before moving on to tackle the additional issues involved in truly multilingual retrieval. Carol PetersOne of the aims of the CLEF activity is to encourage the development of tools to manipulate and process languages other than English. Different languages present different problems. Methods that may be efficient for certain language typologies may not be so effective for others. Issues that have to be catered for include word order, morphology, diacritic characters, language variants. For this reason, CLEF 2000 included a track for French, German and Italian monolingual information retrieval.The cross-language domain-specific task has been offered since TREC-7. The rationale of this subtask is to test retrieval on another type of document collection, serving a different kind of information need. The implications are discussed in the paper by Kluck and Gey in the first part of this volume.The papers in Part II describe the various experiments by the participating groups with these four tasks. Both traditional and innovative approaches to CLIR were experimented, and different query expansion techniques were tried. All kinds of source to target transfer mechanisms were employed, including both query and document translation. Commercial and in-house resources were used and included machine translation, dictionary and corpus-based methods. The strategies used varied from traditional IR to a considerable employment of natural language processing techniques. Different groups focused on different aspects of the overall problem, ranging from the development of language-independent tools such as stemmers to much work on language-specific features like morphology and compounding. Many groups compared different techniques in different runs in order to evaluate the effect of a given technique on performance. Overall, CLEF 2000 offered a very good picture of current issues and approaches in CLIR.The first paper in this part by Martin Braschler provides an overview and analysis of all the results, listing the most relevant achievements and comparing them with those of previous years in the CLIR track at TREC. As one of the main objectives of CLEF is to produce evaluation test-suites that can be used by the CLIR research community, Braschler also provides an analysis of the test collection resulting from the CLEF 2000 campaign, demonstrating its validity for future system testing, tuning and development activities. The appendix presents the evaluation results for each group, run by run. CLEF in the FutureThe CLEF 2001 campaign is now under way. The main tasks are similar to those of the first campaign. There are, however, some extensions and additions. In particular the multilingual corpus has been considerably enlarged and Spanish (news agency) and Dutch (national newspaper) collections for 1994 have been added. The multilingual task in CLEF 2001 involves querying collections in five languages (English, French, German, Italian and Spanish) and there will be two bilingual tracks: searching either the English or the Dutch collections. Spanish and Dutch have also been included in the monolingual track. There will be seven official topic languages, including Japanese. Additional topics will be provided in a number of other European languages, including Finnish, Swedish and Russian, and also in Chinese and Thai.Introduction 5 CLEF 2000 concentrated on the traditional metrics of recall and precisionhowever these have limitations in what they tell us about the usefulness of a retrieval system to the user. CLEF 2001 will thus also include an experimental track designed to test interactive CLIR systems and to establish baselines against which future research progress can be measured. The introduction of this track is a direct result of discussions which began in the workshop with the presentations by Oard and by Jones, and of the proposal by Oard reported in Part I of this volume.Two main issues must be considered when planning future CLEF campaigns: the addition of more languages, and the inclusion of new tasks.The extension of language coverage, discussed considerably at the workshop, depends on two factors: the demand from potential participants and the existence of sufficient resources to handle the requirements of new language collections. It was decided that Spanish and Dutch met these criteria for CLEF . CLEF 2002 and 2003 will be mainly funded by a contract from the European Commission (IST-2000-31002) but it is probable that, in the future, it will be necessary to seek support from national funding agencies as well if more languages are to be included. The aim will be to cover not only the major European languages but also some representative samples of minority languages, including members from each major group: e.g. Germanic, Romance, Slavic, and Ugro-Finnic languages. Furthermore, building on the experience of CLEF 2001, we intend to continue to provide topics in Asian languages.CLEF 2000 concentrated on cross-language text retrieval and on measuring overall system performance. However, in the future, we hope to include tracks to evaluate CLIR systems working on media other than text. We are now beginning to examine the feasibility of organizing a spoken CLIR track in which systems would have to process and match spoken queries in more than one language against a spoken document collection. Another important innovation would be to devise methods that enable the assessment of single system components, as suggested in the paper by Gonzalo.CLIR system development is still very much in the experimental stage and involves expertise from both the natural language processing and the information retrieval fields. The CLEF 2000 Workshop provided an ideal opportunity for a number of key players, with very different backgrounds, to come together and exchange ideas and compare results on the basis of a common experience: participation in the CLEF evaluation campaign. CLEF is very much a collaborative effort between organizers and participants with the same common goal: the improvement of CLIR system performance. The discussions at the workshop have had considerable impact on the organization of the 2001 campaign. The success of future campaigns will depend on the continuation and strengthening of this collaboration.More information on the organization of the current CLEF campaign and instructions on how to contact us can be found at: http://www.clef-campaign.org/. AcknowledgementsTo a large extent, CLEF depends on voluntary work. I should like to acknowledge the generous collaboration of a number of people and organizations. First of all, I wish to It is not easy to set up an infrastructure that meets the needs of a large number of languages. I should like to thank the following organisations who voluntarily engaged translators to provide topic sets in Dutch, Finnish and Swedish, working on the basis of the set of source topics:\u2022 the DRUID project for the Dutch topics;\u2022 the Department of Information Studies, University of Tampere, Finland, engaged the UTA Language Centre for the Finnish topics; \u2022 SICS Human Computer Interaction and Language Engineering Laboratory for the Swedish topics. The support of all the data providers and copyright holders is also gratefully acknowledged, and in particular:\u2022 The Los Angeles Times, for the English data collection;\u2022 Le Monde S.A. and ELDA: European Language Resources Distribution Agency, for the French data.\u2022 Frankfurter Rundschau, Druck und Verlagshaus Frankfurt am Main; Der Spiegel, Spiegel Verlag, Hamburg, for the German newspaper collections.\u2022 InformationsZentrum Sozialwissenschaften, Bonn, for the GIRT database.\u2022 Hypersystems Srl, Torino and La Stampa, for the Italian data.\u2022 Schweizerische Depeschenagentur (SDA) and Associated Press (AP) for the newswire data of the training collection. Without their help, this evaluation activity would be impossible.Last, but not least, I thank Julio Gonzalo for his help and encouragement in the preparation of this volume. CLIR Evaluation at TREC paraic@textwise.com. . Starting in 1997, the National Institute of Standards and Technology conducted 3 years of evaluation of cross-language information retrieval systems in the Text REtrieval Conference (TREC). Twentytwo participating systems used topics (test questions) in one language to retrieve documents written in English, French, German, and Italian. A large-scale multilingual test collection has been built and a new technique for building such a collection in a distributed manner was devised.",
        "A Comparative Study of Online Translation Services for Cross Language Information Retrieval. Technical advances and its increasing availability, mean that Machine Translation (MT) is now widely used for the translation of search queries in multilingual search tasks. A number of free-to-use high-quality online MT systems are now available and, although imperfect in their translation behaviour, are found to produce good performance in CrossLanguage Information Retrieval (CLIR) applications. Users of these MT systems in CLIR tasks generally assume that they all behave similarly in CLIR applications, and the choice of MT system is often made on the basis of convenience. We present a set of experiments which compare the impact of applying two of the best known online systems, Google and Bing translation, for query translation across multiple language pairs and for two very different CLIR tasks. Our experiments show that the MT systems perform differently on average for different tasks and language pairs, but more significantly for different individual queries. We examine the differing translation behaviour of these tools and seek to draw conclusions in terms of their suitability for use in different settings.",
        "Managing and mining multilingual documents: Introduction to the special topic issue of information processing management Due to the popularity of the World Wide Web and the advance of Internet search engines, information in many different languages is accessible online nowadays. For example, one can easily access news stories in real time in over 30 languages on the Web. The number of non-English documents on the Web is growing faster than it was 10 years ago due to the significant growth of Internet user population in developing countries. Although it is convenient to obtain multilingual information, these online documents are usually organized or managed in each language separately. Internet search engines provide hierarchical directories of documents for each language at independent portals even if these portals are provided by the same organization. We seldom find any hierarchical directory that provides multilingual classification. The lack of coordination among documents in different languages makes it inefficient for multilingual users to identify useful resources in multiple languages. Users are required to search by each language and concatenate the results. Such searching process is redundant and time consuming.Furthermore, in global business environments, mining knowledge from text in a single language may not provide sufficient support to knowledge workers. We often need to integrate multilingual text before applying text mining techniques or integrate the knowledge discovered from text in different languages for obtaining global knowledge. For example, opinion mining from the Web requires multilingual text mining because user opinions are available in different languages from all over the world. Hence, there is an urge need of advanced techniques in managing and mining multilingual documents.Substantial research efforts have been made toward facilitating cross-lingual information retrieval in the last decade It is an important research area to be explored such that we can fully utilize the multilingual resources for better knowledge management.In this special issue, we have selected three papers covering the related topics: multilingual Web directory generation, multilingual novelty mining, and multilingual information retrieval.Yang et al. developed an approach to generate multilingual Web directory. Self-organizing map was first constructed on multiple sets of Web pages, one for each language, independently. Monolingual hierarchies were then generated. A hierarchy alignment method was applied to discover the associations between nodes from different hierarchies and a multilingual Web directory was constructed. A promising result was shown by experiments.Zhang et al. developed sentence categorization and novelty mining in multiple languages including Malay, Chinese, and English. The Rocchio algorithm was adopted for sentence categorization. The novelty of each sentence was computed by measuring the cosine similarities with the sentences extracted in sentence categorization. The experimental results showed that sentence-level novelty mining had similar performance in Malay, English and Chinese. It also showed that categorization improved multilingual novelty mining significantly.Tsai et al. developed a learning-based ranking algorithm, FRank, to construct a merging model for multilingual information retrieval. Cross-lingual information retrieval was first processed on separate collections, one for each language. The lists of monolingual results were then merged to produce a multilingual result list. Sixty-two features were extracted from query, document, and translation levels. The FRank ranking algorithm used these features to construct a merging model. The experimental results showed significant improvement on the merging quality.",
        "Pre-Retrieval based Strategies for Cross Language News Story Search. The task of measuring text reuse and linking target and source documents becomes challenging if reused text has been translated in other language and even more challenging if translation language uses a different script and syntactical structure. This year, in CLINSS, focus was on journalistic text reuse between texts written in two different languages. This paper focuses on identifying text reuse between texts written in two different languagesEnglish and Hindi and aims at evaluating the CLINSS dataset by linking 25 target English news stories with top 100 Hindi news stories out of corpora of 50691 stories. This is our first participation at CLINSS task and we have submitted two sets of runs as MANIT-1 and MANIT-2. In MANIT-1 two pre-retrieval strategies 1) Query formed using Proper Noun and 2) Query formed using words whose frequency is equal to or higher than average frequency are used to formulate the query. Query is translated using either dictionary based or machine translation CLIR based approach before retrieving the documents. In Run 1 and 2 preretrieval strategies clubbed up with CLIR's dictionary based approach is used to link English news stories with Hindi news stories. In Run-3 instead of using dictionary based approach, freely available online Google translate [18] FIRE '13, December 04 -06 2013, New Delhi, India Copyright 2013 ACM 978-1-4503-2830-2/13/12\u2026$15.00 http://dx.doi.org/10.1145 is to evaluate the performance of two pre-retrieval strategies and to compare the existing dictionary based and machine translation based approaches of CLIR. It is observed that dictionary based approach clubbed up with proper noun based pre-retrieval strategy performed better.",
        "Building parallel corpora by automatic title alignment using length-based and text-based approaches. Cross-lingual semantic interoperability has drawn significant attention in recent digital library and World Wide Web research as the information in languages other than English has grown exponentially. Cross-lingual information retrieval (CLIR) across different European languages, such as English, Spanish, and French, has been widely explored; however, CLIR across European languages and Oriental languages is still in the initial stage. To cross language boundary, corpus-based approach is promising to overcome the limitation of the knowledge-based and controlled vocabulary approaches but collecting parallel corpora between European language and Oriental language is not an easy task. Length-based and text-based approaches are two major approaches to align parallel documents. In this paper, we investigate several techniques using these approaches and compare their performances in aligning English and Chinese titles of parallel documents available on the Web.",
        "Using Unigram and Bigram Language Models for Monolingual and Cross-Language IR Due to the lack of explicit word boundaries in Chinese, and Japanese, and to some extent in Korean, an additional problem in IR in these languages is to determine the appropriate indexing units. For CLIR with these languages, we also need to determine translation units. Both words and ngrams of characters have been used in IR in these languages; however, only words have been used as translation units in previous studies. In this paper, we compare the utilization of words and n-grams for both monolingual and cross-lingual IR in these languages. Our experiments show that Chinese character n-grams are reasonable alternative indexing and translation units to words, and they lead to retrieval effectiveness comparable to or higher than words. For Japanese and Korean IR, bigrams or a combination of bigrams and unigrams produce the highest effectiveness.",
        "Toward Cross-Language and Cross-Media Image Retrieval",
        "The CLEF 2004 Cross-Language Image Retrieval Track. The purpose of this paper is to outline efforts from the 2004 CLEF cross-language image retrieval campaign (ImageCLEF). The aim of this CLEF track is to explore the use of both text and content-based retrieval methods for cross-language image retrieval. Three tasks were offered in the ImageCLEF track: a TREC-style ad-hoc retrieval task, retrieval from a medical collection, and a user-centered (interactive) evaluation task. Eighteen research groups from a variety of backgrounds and nationalities participated in ImageCLEF. In this paper we describe the ImageCLEF tasks, submissions from participating groups and summarise the main findings.",
        "Summarization Design for Interactive Cross-Language Question Answering. This paper describes an experimental investigation of interactive techniques for cross-language information access. The task was to answer factual questions from a large collection of documents written in a language in which the user has little proficiency. An interactive cross-language retrieval system that included optional user-assisted query translation, display of translated summaries for individual document ranked in order of decreasing degree of match to the user's query, and optional full-text examination of individual documents was provided. Two alternative types of extractive summaries were tried using a systematically varied presentation order, one drawn from a single segment of the translated document and the other drawn from three (usually) shorter segments of the translated document. On average, users were able to correctly answer just 62% of the sixteen assigned questions in an average of 176 seconds per question. Little difference was found between the two summary types for this task in an experiment using eight human subjects. Time on task and the number of query iterations were found to exhibit a positive correlation with question difficulty.",
        "Interactive Cross-Language Question Answering: Searching Passages Versus Searching Documents. iCLEF 2004 is the first comparative evaluation of interactive Cross-Language Question Answering systems. The UNED group has participated in this task comparing two strategies to help users in the answer finding task: the baseline system is just a standard document retrieval engine searching machine-translated versions of the documents; the contrastive system is identical, but searches passages which contain expressions of the appropriate answer type. Although the users prefer the passage system because searching is faster and simpler, it leads to slightly worse results, because the document context (which is not available in the passage retrieval system) turns out to be useful to verify the correctness of candidate answers; this makes an interesting difference with automatic Q&A systems. In addition, our experiment sets a strong baseline of 69% strict accuracy, showing that Cross-Language Question Answering can be efficiently accomplished by users without using dedicated Q&A technology.",
        "Applying Query Formulation and Fusion Techniques For Cross Language News Story Search. Cross Language News story search (CLNSS) is concerned with finding documents describing the same events in documents in different languages. As well as supporting information retrieval (IR), CLNSS has other applications in mining parallel and comparable data across different languages. In this paper, we present an overview of the work carried out for our participation in the Cross Language !ndian News Story Search (CL!NSS) task at FIRE 2013. In the CL!NSS task we explored the problem of cross language news search for the English-Hindi language pair. English news stories are used as queries to seek similar news documents from Hindi news articles. Hindi being a resource-scarce language offers many challenges towards retrieving relevant news articles. We investigate and contrast translation of input queries from English to Hindi using the Google and Bing translation services. To support translation of out-of-vocabulary words we use the Google transliteration service. A key challenge of the CL!NSS task is formation of search queries from the English news articles, since they are much longer than the much shorter queries typically used in IR applications. To address this problem, we explore the use of summarization to extract a query from the input news documents, and use these summarized queries as the input to the cross language IR system. We explore the use of query expansion using pseudo relevance feedback (PRF) in the IR process, since this has been shown to be effective for cross language IR in many previous investigations. We also explore in detail the use of data fusion techniques over different sets of retrieved results obtained using diverse query formulation techniques. For the CL!NSS task our team submitted 3 main runs. The results of our best run was ranked first among official submissions based on NDCG@5 and NDCG@10 values and second for NDCG@1 values. For the 25 test queries the results of our best main run were NDCG@1 0.7400, NDCG@5 0.6809 and NDCG@10 0.7268. We present our methodology, official results and results of a number of post-task experiments that were conducted to further examine the cross language search problem. Our experiments reveal that query formuPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. FIRE '13, December 04 -06, 2013, New Delhi, India Copyright 2013 ACM 978-1-4503-2830-2/13/12...$15.00 http://dx.doi.org/10.1145/2701336.2701650 lation plays a vital role in improving search results for news documents across different languages. Instead of using the complete news documents the summarized queries show better performance. Data fusion techniques also help to improve the performance of the system by boosting the rank of documents, thus improving the NDCG scores.",
        "Dublin City University at CLEF 2004: Experiments in Monolingual, Bilingual and Multilingual Retrieval. The Dublin City University group participated in the monolingual, bilingual and multilingual retrieval tasks. The main focus of our investigation for CLEF 2004 was extending our information retrieval system to document languages other than English, and completing the multilingual task comprising four languages: English, French, Russian and Finnish. Our retrieval system is based on the City University Okapi BM25 system with document preprocessing using the Snowball stemming software and stopword lists. Our French monolingual experiments compare retrieval using French documents and topics, and documents and topics translated into English. Our results indicate that working directly in French is more effective for retrieval than adopting document and topic translation. A breakdown of our multilingual retrieval results by the individual languages shows that similar overall average precision can be achieved when there is significant underlying variation in performance for individual languages.",
        "Cross-Language Plagiarism Detection Using a Multilingual Semantic Network. Cross-language plagiarism refers to the type of plagiarism where the source and suspicious documents are in different languages. Plagiarism detection across languages is still in its infancy state. In this article, we propose a new graph-based approach that uses a multilingual semantic network to compare document paragraphs in different languages. In order to investigate the proposed approach, we used the German-English and Spanish-English cross-language plagiarism cases of the PAN-PC'11 corpus. We compare the obtained results with two state-of-the-art models. Experimental results indicate that our graph-based approach is a good alternative for cross-language plagiarism detection.",
        "CLEF 2004 Cross-Language Spoken Document Retrieval Track"
    ],
    "Abbreviations in queries": [
        "ShARe/CLEF eHealth 2013 Normalization of Acronyms/Abbreviations Challenge Objective: Abbreviations and acronyms are widely used in the clinical documents. This paper describes using of a machine learner to automatically extract spans of abbreviations and acronyms from clinical notes and map them to the UMLS (Unified Medical Language System) CUI (Concept Unique Identifier). Tasks: A Conditional Random Field (CRF) machine learner was used to identify abbreviations and acronyms. Firstly, the training data was converted to the CRF format. The different feature sets were applied with 10-fold cross validation to find the best feature set to create the machine learning model. Secondly, the identified spans for abbreviation/acronyms were mapped to the UMLS (Unified Medical Language System) CUIs. Thirdly, a rule based engine was applied for disambiguation of terms with multiple abbreviations or acronyms. Approach: A novel supervised learning model was developed that incorporates a machine learning algorithm and a rule-based engine. Evaluation of each step included precision, recall and F-score metrics for span detection and accuracy for CUI mapping. Resources: Several tools which were created in our laboratory were used, including a Text to SNOMED CT (TTSCT) service, Lexical Management System (LMS) and Ring-fencing approach. Also a set of gazetteers which had been created from the training data was employed. Results: A 10-fold cross validation on the training data showed 0.911 of precision, 0.887 of recall and a F-score of 0.899 for detecting the boundary of abbreviation/acronyms and an accuracy of 0.760 for CUI mapping while the official results on the test data showed strict accuracy of 0.447 and relaxed accuracy of 0.488 which is the third team out of the five participating teams. A supervised machine learning method with mixed computational strategies and rule based method for disambiguation of expansions seems to provide a nearoptimal strategy for automated extraction of abbreviation/acronyms.",
        "Automatic expansion of abbreviations by using context and character information",
        "Finding Cross-Lingual Spelling Variants Finding term translations as cross-lingual spelling variants on the fly is an important problem for cross-lingual information retrieval (CLIR). CLIR is typically approached by automatically translating a query into the target language. For an overview of crosslingual information retrieval, see [1]. When automatically translating the query, specialized terminology is often missing from the translation dictionary. The analysis of query properties in [2] has shown that proper names and technical terms often are prime keys in queries, and if not properly translated or transliterated, query performance may deteriorate significantly. As proper names often need no translation, a trivial solution is to include the untranslated keys as such into the target language query. However, technical terms in European languages often have common Greek or Latin roots, which allows for a more advanced solution using approximate string matching to find the word or words most similar to the source keys in the index of the target language text database [3].In European languages the loan words are often borrowed with minor but language specific modifications of the spelling. A comparison of methods applied to cross-lingual spelling variants in CLIR for a number of European languages is provided in [4]. They compare exact match, simple edit distance, longest common subsequence, digrams, trigrams and tetragrams as well as skipgrams, i.e. digrams with gaps. Skipgrams perform best in their comparison with a relative improvement of 7.5 % on the average on the simple edit distance baseline. They also show that among the baselines, the simple edit distance baseline is in general the hardest baseline to beat. They use no explicit n-gram transformation information. In [5], explicit n-gram transformations are based on digrams and trigrams. Trigrams are better than digrams, but no comparison is made to the edit distance baseline. In both of the previous studies on European languages most of the distance measures for finding the closest matching transformations is based on a bag of n-grams ignoring the order of the n-grams.Between languages with different writing systems foreign words are often borrowed based on phonetic rather than orthographic transliterations. In [6], a generative model is introduced which transliterates words from Japanese to English using weighted finitestate transducers. The transducer model only uses context-free transliterations which do not account for the fact that a sound may be spelled differently in different contexts. This is likely to produce heavily overgenerating systems.Assume that we have a word in a foreign language. We call this the source word S. We want to know the possible meanings of the word in a language known to us without having a translation dictionary. We take the word and compare it to all the words in a word list L of the target language in order to determine which target word T is most similar to the unknown word. In the beginning we only compare how many letters or sounds are similar. As we learn the regularities involved, we observe that the likelihood A.",
        "Mining acronym expansions and their meanings using query click log. Acronyms are abbreviations formed from the initial components of words or phrases. Acronym usage is becoming more common in web searches, email, text messages, tweets, blogs and posts. Acronyms are typically ambiguous and often disambiguated by context words. Given either just an acronym as a query or an acronym with a few context words, it is immensely useful for a search engine to know the most likely intended meanings, ranked by their likelihood. To support such online scenarios, we study the offline mining of acronyms and their meanings in this paper. For each acronym, our goal is to discover all distinct meanings and for each meaning, compute the expanded string, its popularity score and a set of context words that indicate this meaning. Existing approaches are inadequate for this purpose. Our main insight is to leverage \"co-clicks\" in search engine query click log to mine expansions of acronyms. There are several technical challenges such as ensuring 1:1 mapping between expansions and meanings, handling of \"tail meanings\" and extracting context words. We present a novel, end-to-end solution that addresses the above challenges. We further describe how web search engines can leverage the mined information for prediction of intended meaning for queries containing acronyms. Our experiments show that our approach (i) discovers the meanings of acronyms with high precision and recall, (ii) significantly complements existing meanings in Wikipedia and (iii) accurately predicts intended meaning for online queries with over 90% precision.",
        "Popular Acronym Retrieval through Text Messaging. We present a prototype system for providing quick information on common and popular abbreviations through text messaging. The system receives a text input of acronym (possibly wrongly typed) in Roman script. The application returns a very brief information from the first few lines of corresponding English Wikipedia page. The system is designed especially for low-cost mobile phones having text only messaging facility but without Internet and native language support. The target users are primarily semi-literate people who may not have sufficient knowledge of English. The output is translated to native language of user query (Hindi) as transliterated text.",
        "A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations Abbreviations and acronyms are widely used in the biomedical literature and many of them represent important biomedical concepts. Because many abbreviations are ambiguous (e.g., CAT denotes both chloramphenicol acetyl transferase and computed axial tomography, depending on the context), recognizing the full form associated with each abbreviation is in most cases equivalent to identifying the meaning of the abbreviation. This, in turn, allows us to perform more accurate natural language processing, information extraction, and retrieval. In this study, we have developed supervised approaches to identifying the full forms of ambiguous abbreviations within the context they appear. We first automatically assigned multiple possible full forms for each abbreviation; we then treated the in-context full-form prediction for each specific abbreviation occurrence as a case of word-sense disambiguation. We generated automatically a dictionary of all possible full forms for each abbreviation. We applied supervised machine-learning algorithms for disambiguation. Because some of the links between abbreviations and their corresponding full forms are explicitly given in the text and can be recovered automatically, we can use these explicit links to automatically provide training data for disambiguating the abbreviations that are not linked to a full form within a text. We evaluated our methods on over 150 thousand abstracts and obtain for coverage and precision results of 82% and 92%, respectively, when performed as tenfold cross-validation, This research was supported in part by the National Science Foundation Innovative Technology Research Grant EIA-0121687 and the National Institute of Health Grant R01 GM61372-01A2. Authors' addresses: H. Yu, Department of Health Sciences, University of Wisconsin-Milwaukee, Milwaukee, WI 53211; email: yuh9001@dbmi.columbia.edu; W. Kim, J. Wilbur, National Center for Biotechnology Information, Bethesda, MD 20894; V. Hatzivassiloglou, Department of Computer Science, University of Texas at Dallas, Richardson, TX 75083. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored.. ing with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 Automatically Disambiguating Biomedical Abbreviations\u2022 381 and 79% and 80%, respectively, when evaluated against an external set of abstracts in which the abbreviations are not defined.",
        "Disambiguating biomedical acronyms using EMIM. Expanding a query with acronyms or their corresponding 'long-forms' has not been shown to provide consistent improvements in the biomedical IR literature. The major open issue with expanding acronyms in a query is their inherent ambiguity, as an acronym can refer to multiple long-forms. At the same time, a long-form identified in a query can be expanded with its acronym(s); however, some of these may be also ambiguous and lead to poor retrieval performance. In this work, we propose the use of the EMIM (Expected Mutual Information Measure) between a long-form and its abbreviated acronym to measure ambiguity. We experiment with expanding both acronyms and long-forms identified in the queries from the adhoc task of the TREC 2004 Genomics track. Our preliminary analysis shows the potential of both acronym and long-form expansions for biomedical IR.",
        "Normalization of Abbreviations/Acronyms: THCIB at CLEF eHealth 2013 Task 2 This paper describes the THCIB systems that used in the ShARe/CLEF eHealth Lab 2013 task 2. We built a baseline system using open source software, and improve the performance by adding dictionaries. The dictionary is built from training set and web resource using the existing technologies. The experimental results show that adding dictionary of acronym/abbreviation can improve the performance significantly.",
        "Task 2: ShARe/CLEF eHealth Evaluation Lab 2013 In this pilot study, we aimed to generate a reference standard of clinical acronyms and abbreviations normalized to concepts from a standardized, medical vocabulary for the ShARe/CLEF eHealth 2013 challenge. In this paper, we review prior text normalization shared tasks, reference standard generation approaches, and recent clinical acronym and abbreviation normalization research. We report inter-annotator agreement for the reference standard and performance for participant systems.",
        "Analyzing web text association to disambiguate abbreviation in queries. We introduce a statistical model for abbreviation disambiguation in Web search, based on analysis of Web data resources, including anchor text, click log and query log. By combining evidence from multiple sources, we are able to accurately disambiguate the abbreviation in queries. Experiments on real Web search queries show promising results."
    ],
    "lemmatization algorithms": [
        "Benefits of deep NLP-based Lemmatization for Information Retrieval This paper reports on our system used in the CLEF 2006 ad hoc mono-lingual Hungarian retrieval task. Our experiments focus on the benefits that deeper NLP-based lemmatization (as opposed to simpler stemmers) can contribute to mean average precision. Our results show that these benefits counterweight the disadvantage of using an off-the-shelf retrieval toolkit.",
        "On knowledge-poor methods for person name matching and lemmatization for highly inflectional languages",
        "A lemmatization method for Mongolian and its application to indexing for information retrieval a b s t r a c tIn Mongolian, two different alphabets are used, Cyrillic and Mongolian. In this paper, we focus solely on the Mongolian language using the Cyrillic alphabet, in which a content word can be inflected when concatenated with one or more suffixes. Identifying the original form of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Mongolian. The advantage of our lemmatization method is that it does not rely on noun dictionaries, enabling us to lemmatize out-of-dictionary words. We also apply our method to indexing for information retrieval. We use newspaper articles and technical abstracts in experiments that show the effectiveness of our method. Our research is the first significant exploration of the effectiveness of lemmatization for information retrieval in Mongolian.",
        "A novel Arabic lemmatization algorithm. Tokenization is a fundamental step in processing textual data preceding the tasks of information retrieval, text mining, and natural language processing. Tokenization is a languagedependent approach, including normalization, stop words removal, lemmatization and stemming. Both stemming and lemmatization share a common goal of reducing a word to its base. However, lemmatization is more robust than stemming as it often involves usage of vocabulary and morphological analysis, as opposed to simply removing the suffix of the word. In this work, we introduce a novel lemmatization algorithm for the Arabic Language. The new lemmatizer proposed here is a part of a comprehensive Arabic tokenization system, with a stop words list exceeding 2200 Arabic words. Currently, there are two Arabic leading stemmers: the root-based stemmer and the light stemmer. We hypothesize that lemmatization would be more effective than stemming in mining Arabic text. We investigate the impact of our new lemmatizer on unsupervised data mining techniques in comparison to the leading Arabic stemmers. We conclude that lemmatization is a better word normalization method than stemming for Arabic text.",
        "Stemming and lemmatization in the clustering of finnish text documents. Stemming and lemmatization were compared in the clustering of Finnish text documents. Since Finnish is a highly inflectional and agglutinative language, we hypothesized that lemmatization, involving splitting of the compound words, would be more appropriate normalization approach than the straightforward stemming. The relevance of the documents were evaluated with a four-point relevance assessment scale, which was collapsed into binary one by considering all the relevant and only the highly relevant documents relevant, respectively. Experiments with four hierarchical clustering methods supported the hypothesis. The stringent relevance scale showed that lemmatization allowed the single and complete linkage methods to recover especially the highly relevant documents better than stemming. In comparison with stemming, lemmatization together with the average linkage and Ward's methods produced higher precision. We conclude that lemmatization is a better word normalization method than stemming, when Finnish text documents are clustered for information retrieval."
    ],
    "filter ad rich documents": [
        "Nameles: An intelligent system for Real-Time Filtering of Invalid Ad Traffic",
        "Understanding Mobile Searcher Attention with Rich Ad Formats",
        "Defeating script injection attacks with browser-enforced embedded policies. Web sites that accept and display content such as wiki articles or comments typically filter the content to prevent injected script code from running in browsers that view the site. The diversity of browser rendering algorithms and the desire to allow rich content make filtering quite difficult, however, and attacks such as the Samy and Yamanner worms have exploited filtering weaknesses. This paper proposes a simple alternative mechanism for preventing script injection called Browser-Enforced Embedded Policies (BEEP). The idea is that a web site can embed a policy in its pages that specifies which scripts are allowed to run. The browser, which knows exactly when it will run a script, can enforce this policy perfectly. We have added BEEP support to several browsers, and built tools to simplify adding policies to web applications. We found that supporting BEEP in browsers requires only small and localized modifications, modifying web applications requires minimal effort, and enforcing policies is generally lightweight.",
        "Rough sets based reasoning and pattern mining for a two-stage information filtering system. This paper presents a novel two-stage information filtering model which combines the merits of term-based and patternbased approaches to effectively filter sheer volume of information. In particular, the first filtering stage is supported by a novel rough analysis model which efficiently removes a large number of irrelevant documents, thereby addressing the overload problem. The second filtering stage is empowered by a semantically rich pattern taxonomy mining model which effectively fetches incoming documents according to the specific information needs of a user, thereby addressing the mismatch problem. The experimental results based on the RCV1 corpus show that the proposed two-stage filtering model significantly outperforms other types of \"two-stage\" information filtering models.",
        "A collaborative filtering approach to ad recommendation using the query-ad click graph. Search engine logs contain a large amount of click-through data that can be leveraged as soft indicators of relevance. In this paper we address the sponsored search retrieval problem which is to find and rank relevant ads to a search query. We propose a new technique to determine the relevance of an ad document for a search query using click-through data. The method builds on a collaborative filtering approach to discover new ads related to a query using a click graph. It is implemented on a graph with several million edges and scales to larger sizes easily. The proposed method is compared to three different baselines that are state-of-the-art for a commercial search engine. Evaluations on editorial data indicate that the model discovers many new ads not retrieved by the baseline methods. The ads from the new approach are on average of better quality than the baselines."
    ],
    "Advancements in Information Retrieval": [
        "Preface Information access technologies provide the interface between human information needs and digital information resources. The reliable evaluation of these technologies has been recognized for decades as central to the advancement of the field. As information retrieval becomes more pervasive, the forms of retrieval more diverse, and retrieval tools richer, the importance of effective, efficient, and innovative evaluation grows as well. Following the success of the",
        "Recent Advances in Conversational Information Retrieval",
        "Advances in Multidisciplinary Retrieval, First Information Retrieval Facility Conference, IRFC 2010, Vienna, Austria, May 31, 2010. Proceedings",
        "Introduction to the special issue of the World Wide Web journal on \"Social Media Preservation and Applications\" Social media have become firmly established as a new publishing and communication tool with significant value for all the aspects of contemporary society, culture and science. Social media feature a dynamic and continuously evolving nature, fostering the creation and exchange of concepts and ideas in an unprecedented scale.The leading role of social media coupled with the inherent volatility and the ephemeral nature of this medium, compared with other forms of electronic communication, has rendered social media preservation as necessary. It is critical to preserve social media as they are an essential part of our heritage and they can prove valuable for current and future generations.Social media preservation is an advanced topic of digital preservation, encompassing the capture, management, preservation and availability of social media for future use and research. To achieve this goal, we need to advance the state of the art in a wide range of diverse topics such as social media modeling and analysis, classification, information retrieval, content migration, replication, emulation and web preservation strategies. Furthermore, we need to focus not only on theoretical advances but also on innovative applications and systems. In this special issue, we are aiming to exchange the latest fundamental advances in the state of the art and practice of social media preservation and important related areas.In this issue we have included 7 papers, which have been selected out of 17 papers that were submitted in response to our call. Some information about these papers follows.The paper by L. Akritidis and P. Bozanis entitled \"Improving Opinionated Blog Retrieval Effectiveness with Quality Measures and Temporal Features\" deals with the problem of retrieval of blog entries, which contain opinions about entries by paying special attention to the importance of such entries. The proposed ranking method takes into account the influence of the blogger who authored an opinion, the reputation of the specific blog site and the impact of the blog post itself. Experiments with TREC Blogs08 dataset shows enhancement of retrieval precision.The paper by G. Gkotsis, K. Stepanyan, A. Cristea and M. Joy entitled \"Entropy-based Automated Wrapper Generation for Web Data Extraction\" focus on the process of weblog data extraction. The approach includes a model for generating a wrapper that exploits web feeds for deriving probabilistically (based on entropy) a set of extraction rules in an automatic way. The World Wide Web",
        "Preface Information access technologies provide the interface between human information needs and digital information resources. The reliable evaluation of these technologies has been recognized for decades as central to the advancement of the field. As information retrieval becomes more pervasive, the forms of retrieval more diverse, and retrieval tools richer, the importance of effective, efficient, and innovative evaluation grows as well. Following the success of the",
        "Advances in Information Retrieval - 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016. Proceedings",
        "Entity linking and retrieval for semantic search. More and more search engine users are expecting direct answers to their information needs, rather than links to documents. Semantic search and its recent applications enabled search engines to organize their wealth of information around entities. Entity linking and retrieval provide the building stones for organizing the web of entities. This tutorial aims to cover all facets of semantic search from a unified point of view and connect real-world applications with results from scientific publications. We provide a comprehensive overview of entity linking and retrieval in the context of semantic search and thoroughly explore techniques for query understanding, entity-based retrieval and ranking on unstructured text, structured knowledge repositories, and a mixture of these. We point out the connections between published approaches and applications, and provide handson examples on real-world use cases and datasets. Categories and Subject Descriptors KeywordsEntity linking, entity retrieval, semantic search OVERVIEWThe explosive increase in the amount of unstructured textual data being produced calls for advanced methodologies for making sense of this data. Recent advances have enabled a precise manner of analysis, where phrases-consisting of a single term or sequence of terms-are automatically linked to entries in a knowledge base. This process is commonly known as entity linking. Entity linking facilitates advanced forms of searching and browsing in various domains and contexts. For example, it can be used to anchor textual resources in background knowledge. In search engines, linking Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s queries to entities to improve the user experience is becoming increasingly prevalent. More and more, users want to find the actual entities that satisfy their information need, rather than merely the documents that mention them; a process known as entity retrieval.Entities are a key enabling component for semantic search, as many information needs can be answered by returning a list of entities, their properties, and/or their relations. They can be used to enrich the search result page to enable direct answers or serendipitous results, and are able to bridge the gap between unstructured and structured data.The goal of this tutorial is to cover all facets of semantic search from a unified point of view and connect realworld applications with results from scientific publications. Its main highlights are that we: (i) provide a comprehensive overview of entity linking and retrieval in the context of semantic search, (ii) thoroughly explore techniques for query understanding, entity-based retrieval and ranking on unstructured text, structured knowledge repositories, and a mixture of these, and (iii) point out the connections between published approaches and applications, and provide hands-on examples on real-world use cases and datasets.The half-day tutorial is structured into five parts. Part I sets the scene by introducing the data landscape. We make a case for entity retrieval that is built on the observation that web search engine users prefer to express information needs using short keyword queries and that many of these revolve around entities. Part II zooms in on the congruent relationship between textual and structural evidence for entities. These come in two main flavors: (i) entity linking and (ii) knowledge base population/acceleration, i.e., given a massive text corpus, populate a knowledge base with new entities, relations, and attributes. Part III starts with an overview of traditional methods for keyword-based search in the IR, DB, and semantic web communities. Then, we discuss methods that combine these aspects and where query understanding may involve some inferencing. Part IV continues with settings where the aim is to provide the user with an improved overall search experience, e.g. by providing an enriched SERP which lists recent match results when searching for a football team or displaying restaurants on a map. Alternatively, the standard document-based result list can be augmented with entity-oriented components for serendipitous discovery. Finally, Part V concludes the tutorial with an overview and an outlook of future challenges. Each block discusses a particular topic and is be followed by hands-on exercises, illustrating the theory presented before.The slides, bibliography, and resources of the current and previous editions can be found at http://bit.ly/ELR-slides. 683",
        "Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part I",
        "Recent Developments in Information Retrieval",
        "Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II",
        "Machine learning for information retrieval. In recent years, we have witnessed successful application of machine learning techniques to a wide range of information retrieval problems, including Web search engines, recommendation systems, online advertising, etc. It is thus critical for researchers in the information retrieval community to understand the core machine learning techniques. In order to accommodate audiences with different levels of understanding of machine learning, we divide this tutorial into two sessions: the first session will focus on basic machine learning concepts and tools; in the second session, we will introduce more advanced topics in machine learning, and will present recent developments in machine learning and its application to information retrieval. Each season is self-contained. Session 1: Core Learning Technologies for Information Retrieval.This session of the tutorial will cover the core machine learning methods, basic optimization techniques and key information retrieval applications. In particular, it includes: 1). Core concepts in machine learning, such as supervised learning/unsupervised learning, bias and variance trade off, and probabilistic models; 2). Useful concepts and algorithms in optimization including the first and second order gradient methods, and Expectation and Maximization; 3). The application of machine learning methods to key information retrieval problems including text classification, collaborative filtering, clustering and learning to rank; Session 2: Emerging Learning Technologies for InformationRetrieval. This session will cover more advanced machine learning techniques that have started to be utilized in information retrieval applications. In particular, it will cover: 1). Advanced Optimization Techniques including stochastic optimization and smooth minimization; 2). Emerging Learning Techniques such as Multiple-Instance Learning, Active Learning and Semi-supervised Learning.The tutorial will benefit a large body of audience in the information retrieval community, ranging from students who are new to machine learning to the seasoned researchers who would like to understand the recent advance in machine learning for information retrieval research. This tutorial will also benefit the practitioners who apply learning techniques to real-world information retrieval systems.",
        "Advances in Information Retrieval, 28th European Conference on IR Research, ECIR 2006, London, UK, April 10-12, 2006, Proceedings",
        "ICTIR Tutorial: Modern Query Performance Prediction: Theory and Practice Query performance prediction (QPP) is a core information retrieval (IR) task whose primary goal is to assess retrieval quality in the absence of relevance judgments. Applications of QPP are numerous, and include, among others, automatic query reformulation, fusion and ranker selection, distributed search and content analysis. The main objective of this tutorial is to introduce recent advances in the sub-research area of QPP in IR, covering both theory and applications. On the theoretical side, we will introduce modern QPP frameworks, which have advanced our understanding of the core QPP task. On the application side, the tutorial will set the connection between QPP theory and its usage in various modern IR applications, discussing the pros and cons, limitations, challenges and open research questions. 2 FORMAT AND THE DETAILED SCHEDULE OF THE TUTORIAL The tutorial will have three main parts. The three parts are organized in a way that allows incremental understanding of the QPP task and recent advancements in the field.",
        "Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users' real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.",
        "Current Developments in Information Retrieval Evaluation",
        "Progress in Information Retrieval",
        "QuickView: advanced search of tweets EXTENDED. With tweets being a comprehensive repository for super fresh information, tweet search becomes increasingly popular. However, existing tweet search services, e.g., Twitter 1 , offer only simple keyword based search. Owing to the noisy and informal nature of tweets, the returned list does not contain meaningful information in many cases.This demonstration introduces QuickView, an advanced tweet search service to address this issue. It adopts a series of natural language processing technologies to extract useful information from a large volume of tweets. Specifically, for each tweet, it first conducts tweet normalization, followed by named entity recognition(NER). Our NER component is a combination of a k-nearest neighbors(KNN) classifier (to collect global information across recently labeled tweets) with a Conditional Random Fields (CRF) labeler (to exploit information from a single tweet and the gazetteers). Then it extracts opinions (e.g., positive or negative comments about a product). After that it conducts semantic role labeling(SRL) to get predicate-argument structures(e.g.,verbs and their agents or patients), which are further converted into events (i.e., triples of who did what). We follow Liu et al.[1] to construct our SRL component. Next, tweets are classified into predefined categories. Finally, non-noisy tweets together with the mined information are indexed.On top of the index, QuickView enables the following two brand new scenarios, allowing users to effectively access their interested tweets or fine-grained information mined from tweets. Categorized Browsing. As illustrated in , QuickView shows recent popular tweets, entities, events, opinions and so on, which are organized by categories. It also extracts and classifies URL links in tweets to allow users to check out popular links in a categorized way. Advanced Search. As shown in , QuickView provides four advanced search functions: 1) search results are clustered so that tweets about the same/similar topic are grouped together, and for each cluster only the informative tweets are kept; 2) when the query is a person or a company name, two bars are presented followed by the words that strongly suggest opinion polarity. The bar's width is proportional to the number of associated opinions; 3) similarly, the top 6 most frequent words that most clearly express event occurrences are presented; 4) users can search tweets with opinions or events, e.g., search tweets containing any positive/negative opinion about Obama or any event involving Obama. REFERENCES[1] X.",
        "Advances in Information Retrieval - 36th European Conference on IR Research, ECIR 2014, Amsterdam, The Netherlands, April 13-16, 2014. Proceedings",
        "Advances in Information Retrieval, 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14-16, 2003, Proceedings",
        "Advances in Information Retrieval - 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part I",
        "Advances in Information Retrieval Theory, Second International Conference on the Theory of Information Retrieval, ICTIR 2009, Cambridge, UK, September 10-12, 2009, Proceedings",
        "Sponsored Search Auctions: Recent Advances and Future Directions Sponsored search has been proven to be a successful business model, and sponsored search auctions have become a hot research direction. There have been many exciting advances in this field, especially in recent years, while at the same time, there are also many open problems waiting for us to resolve. In this article, we provide a comprehensive review of sponsored search auctions in hopes of helping both industry practitioners and academic researchers to become familiar with this field, to know the state of the art, and to identify future research topics. Specifically, we organize the article into two parts. In the first part, we review research works on sponsored search auctions with basic settings, where fully rational advertisers without budget constraints, preknown click-through rates (CTRs) without interdependence, and exact match between queries and keywords are assumed. Under these assumptions, we first introduce the generalized second price (GSP) auction, which is the most popularly used auction mechanism in the industry. Then we give the definitions of several well-studied equilibria and review the latest results on GSP's efficiency and revenue in these equilibria. In the second part, we introduce some advanced topics on sponsored search auctions. In these advanced topics, one or more assumptions made in the basic settings are relaxed. For example, the CTR of an ad could be unknown and dependent on other ads; keywords could be broadly matched to queries before auctions are executed; and advertisers are not necessarily fully rational, could have budget constraints, and may prefer rich bidding languages. Given that the research on these advanced topics is still immature, in each section of the second part, we provide our opinions on how to make further advances, in addition to describing what has been done by researchers in the corresponding direction.",
        "Advances in Information Retrieval, 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings",
        "Investigating the querying and browsing behavior of advanced search engine users. One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing, and use this knowledge to benefit everyone. In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search. The results show that there are marked differences in the queries, result clicks, post-query browsing, and search success of users we classify as advanced (based on their use of query operators), relative to those classified as non-advanced. Our findings have implications for how advanced users should be supported during their searches, and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.",
        "Advances in Information Retrieval - 40th European Conference on IR Research, ECIR 2018, Grenoble, France, March 26-29, 2018, Proceedings",
        "Complex Linguistic Features for Text Classification: A Comprehensive Study. Previous researches on advanced representations for document retrieval have shown that statistical state-of-the-art models are not improved by a variety of different linguistic representations. Phrases, word senses and syntactic relations derived by Natural Language Processing (NLP) techniques were observed ineffective to increase retrieval accuracy. For Text Categorization (TC) are available fewer and less definitive studies on the use of advanced document representations as it is a relatively new research area (compared to document retrieval). In this paper, advanced document representations have been investigated. Extensive experimentation on representative classifiers, Rocchio and SVM, as well as a careful analysis of the literature have been carried out to study how some NLP techniques used for indexing impact TC. Cross validation over 4 different corpora in two languages allowed us to gather an overwhelming evidence that complex nominals, proper nouns and word senses are not adequate to improve TC accuracy.",
        "The anatomy of an ad: structured indexing and retrieval for sponsored search. The core task of sponsored search is to retrieve relevant ads for the user's query. Ads can be retrieved either by exact match, when their bid term is identical to the query, or by advanced match, which indexes ads as documents and is similar to standard information retrieval (IR). Recently, there has been a great deal of research into developing advanced match ranking algorithms. However, no previous research has addressed the ad indexing problem. Unlike most traditional search problems, the ad corpus is defined hierarchically in terms of advertiser accounts, campaigns, and ad groups, which further consist of creatives and bid terms. This hierarchical structure makes indexing highly non-trivial, as na\u00efvely indexing all possible \"displayable\" ads leads to a prohibitively large and ineffective index. We show that ad retrieval using such an index is not only slow, but its precision is suboptimal as well. We investigate various strategies for compact, hierarchy-aware indexing of sponsored search ads through adaptation of standard IR indexing techniques. We also propose a new ad retrieval method that yields more relevant ads by exploiting the structured nature of the ad corpus. Experiments carried out over a large ad test collection from a commercial search engine show that our proposed methods are highly effective and efficient compared to more standard indexing and retrieval approaches."
    ]
}