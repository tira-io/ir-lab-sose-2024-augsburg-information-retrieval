{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().endswith('pagerank'):\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pagerank/pageranks.json', 'r') as file:\n",
    "    pageranks = json.load(file)\n",
    "\n",
    "with open('data/tira_documents_retrieved.json', 'r') as file:\n",
    "    retrieved_papers_info = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit the entries in retrieved_papers_info where the value is None\n",
    "retrieved_papers_info = {tira_id: v for tira_id, v in retrieved_papers_info.items() if v is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the pagerank\n",
    "imputed_pagerank = np.mean(list(pageranks.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Swap the tiraId and paperId (Id from the Semantic Scholar API) of the retrieved_papers_info dictionary\n",
    "tiraId_to_pagerank = defaultdict(int)  \n",
    "\n",
    "for tiraId, value in retrieved_papers_info.items():\n",
    "    paperId = value['paperId']\n",
    "    \n",
    "    if paperId in pageranks:\n",
    "        tiraId_to_pagerank[tiraId] = pageranks[paperId]\n",
    "    else:\n",
    "        tiraId_to_pagerank[tiraId] = imputed_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tira_documents.json', 'r') as file:\n",
    "    tira_documents = json.load(file)\n",
    "\n",
    "tira_documents = {item['docno']: item['text'] for item in tira_documents} # Convert the list of dictionaries to a dictionary\n",
    "\n",
    "docs_with_infos = defaultdict(dict)\n",
    "\n",
    "for tira_id, text in tira_documents.items():\n",
    "    title = text.split('\\n')[0]\n",
    "\n",
    "    if len(text.split('\\n')) > 2:\n",
    "        abstract = text.split('\\n')[3]\n",
    "    else:\n",
    "        abstract = None\n",
    "        \n",
    "    if abstract == '':\n",
    "        abstract = None\n",
    "\n",
    "    if abstract is None:\n",
    "        try:\n",
    "            abstract = retrieved_papers_info[tira_id]['abstract'] # Insert abstract from the retrieved_papers_info\n",
    "\n",
    "            if abstract == None:\n",
    "                abstract = \"\"\n",
    "            \n",
    "        except:\n",
    "            abstract = \"\"\n",
    "    try:\n",
    "        pagerank = tiraId_to_pagerank[tira_id] # Insert pagerank\n",
    "        \n",
    "        if pagerank == 0:\n",
    "            pagerank = imputed_pagerank\n",
    "        \n",
    "    except:\n",
    "        pagerank = imputed_pagerank\n",
    "\n",
    "\n",
    "    docs_with_infos[tira_id]['title'] = title\n",
    "    docs_with_infos[tira_id]['abstract'] = abstract\n",
    "    docs_with_infos[tira_id]['pagerank'] = pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing abstracts: 14108\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing abstracts:\", len([doc for doc in docs_with_infos.values() if doc['abstract'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Bootstrapping Large Sense Tagged Corpora', 'abstract': 'The performance of Word Sense Disambiguation systems largely depends on the availability of sense tagged corpora. Since the semantic annotations are usually done by humans, the size of such corpora is limited to a handful of tagged texts. This paper proposes a generation algorithm that may be used to automatically create large sense tagged corpora. The approach is evaluated through comparative sense disambiguation experiments performed on data provided during the SENSEVAL-2 English all words and English lexical sample tasks.', 'pagerank': 3.489394849931522e-06}\n"
     ]
    }
   ],
   "source": [
    "print(docs_with_infos['L02-1310'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the docs_with_infos\n",
    "with open('data/docs_with_all_info.json', 'w') as file:\n",
    "    json.dump(docs_with_infos, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126958\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_with_infos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
