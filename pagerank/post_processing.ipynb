{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().endswith('pagerank'):\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pagerank/pageranks.json', 'r') as file:\n",
    "    pageranks = json.load(file)\n",
    "\n",
    "with open('pagerank/pub_dates.json', 'r') as file:\n",
    "    pub_dates = json.load(file)\n",
    "\n",
    "with open('data/tira_documents_retrieved.json', 'r') as file:\n",
    "    retrieved_papers_info = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104397\n",
      "538561\n",
      "126958\n"
     ]
    }
   ],
   "source": [
    "print(len(pub_dates))\n",
    "print(len(pageranks))\n",
    "print(len(retrieved_papers_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit the entries in retrieved_papers_info where the value is None\n",
    "retrieved_papers_info = {tira_id: v for tira_id, v in retrieved_papers_info.items() if v is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-01\n"
     ]
    }
   ],
   "source": [
    "# Convert the pub_dates from string to datetime64\n",
    "def convert_date(date_str):\n",
    "    if date_str is None:\n",
    "        return None\n",
    "    return np.datetime64(date_str)\n",
    "\n",
    "pub_dates = {paperId: convert_date(date_str) for paperId, date_str in pub_dates.items()}\n",
    "\n",
    "print(pub_dates[\"ec87bf9b1423a6598d0ea43d7fb9f6db0fd6305b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "# Find the year with the most publications\n",
    "years = defaultdict(int)\n",
    "for date in pub_dates.values():\n",
    "    if date is not None:\n",
    "        years[date.astype(datetime.datetime).year] += 1\n",
    "\n",
    "max_pubs_year = max(years, key=lambda x: years[x])\n",
    "print(max_pubs_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values for pagerank and publication date\n",
    "imputed_pagerank = np.mean(list(pageranks.values())) # Use mean pagerank\n",
    "\n",
    "imputed_pub_date = np.datetime64(f'{int(max_pubs_year)}-01-01') # Use 1. Jan of most published year\n",
    "print(imputed_pub_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Swap the tiraId and paperId (Id from the Semantic Scholar API) of the retrieved_papers_info dictionary\n",
    "tiraId_to_pagerank = defaultdict(int)  \n",
    "tiraId_to_pub_date = defaultdict(np.datetime64)\n",
    "\n",
    "for tiraId, value in retrieved_papers_info.items():\n",
    "    paperId = value['paperId']\n",
    "    \n",
    "    if paperId in pageranks:\n",
    "        tiraId_to_pagerank[tiraId] = pageranks[paperId]\n",
    "    else:\n",
    "        tiraId_to_pagerank[tiraId] = imputed_pagerank\n",
    "\n",
    "    if paperId in pub_dates:\n",
    "        tiraId_to_pub_date[tiraId] = pub_dates[paperId]\n",
    "    else:\n",
    "        tiraId_to_pub_date[tiraId] = imputed_pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_pagerank(pub_date, pagerank):\n",
    "    '''\n",
    "    Discount the pagerank by 10% for each year since publication\n",
    "    '''\n",
    "    current_date = np.datetime64(\"today\")\n",
    "\n",
    "    age = current_date - pub_date # age in days\n",
    "    age  = age.astype(int) / 365.25 # age in years\n",
    "\n",
    "    return (0.9 ** age) * pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tira_documents.json', 'r') as file:\n",
    "    tira_documents = json.load(file)\n",
    "\n",
    "tira_documents = {item['docno']: item['text'] for item in tira_documents} # Convert the list of dictionaries to a dictionary\n",
    "\n",
    "docs_with_infos = defaultdict(dict)\n",
    "\n",
    "for tira_id, text in tira_documents.items():\n",
    "    title = text.split('\\n')[0]\n",
    "\n",
    "    if len(text.split('\\n')) > 2:\n",
    "        abstract = text.split('\\n')[3]\n",
    "    else:\n",
    "        abstract = None\n",
    "        \n",
    "    if abstract == '':\n",
    "        abstract = None\n",
    "\n",
    "    if abstract is None:\n",
    "        try:\n",
    "            abstract = retrieved_papers_info[tira_id]['abstract'] # Insert abstract from the retrieved_papers_info\n",
    "\n",
    "            if abstract == None:\n",
    "                abstract = \"\"\n",
    "            \n",
    "        except:\n",
    "            abstract = \"\"\n",
    "    try:\n",
    "        pagerank = tiraId_to_pagerank[tira_id] # Insert pagerank\n",
    "        \n",
    "        if pagerank == 0:\n",
    "            pagerank = imputed_pagerank\n",
    "        \n",
    "    except:\n",
    "        pagerank = imputed_pagerank\n",
    "\n",
    "    try:\n",
    "        pub_date = tiraId_to_pub_date[tira_id] # Insert pagerank\n",
    "        \n",
    "    except:\n",
    "        pub_date = imputed_pub_date\n",
    "\n",
    "    if pub_date == None or pub_date == np.datetime64('NaT') or np.isnat(pub_date):\n",
    "        pub_date = imputed_pub_date\n",
    "    \n",
    "\n",
    "    docs_with_infos[tira_id]['title'] = title\n",
    "    docs_with_infos[tira_id]['abstract'] = abstract\n",
    "    docs_with_infos[tira_id]['pagerank'] = pagerank\n",
    "    docs_with_infos[tira_id]['discounted_pagerank'] = discount_pagerank(pub_date, pagerank)\n",
    "    docs_with_infos[tira_id]['pub_date'] = str(pub_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing abstracts: 14104\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing abstracts:\", len([doc for doc in docs_with_infos.values() if doc['abstract'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Bootstrapping Large Sense Tagged Corpora', 'abstract': 'The performance of Word Sense Disambiguation systems largely depends on the availability of sense tagged corpora. Since the semantic annotations are usually done by humans, the size of such corpora is limited to a handful of tagged texts. This paper proposes a generation algorithm that may be used to automatically create large sense tagged corpora. The approach is evaluated through comparative sense disambiguation experiments performed on data provided during the SENSEVAL-2 English all words and English lexical sample tasks.', 'pagerank': 3.489394849931522e-06, 'discounted_pagerank': 3.343848786101388e-07, 'pub_date': '2002-05-01'}\n"
     ]
    }
   ],
   "source": [
    "print(docs_with_infos['L02-1310'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the docs_with_infos\n",
    "with open('data/docs_with_all_info.json', 'w') as file:\n",
    "    json.dump(docs_with_infos, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126958\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_with_infos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
