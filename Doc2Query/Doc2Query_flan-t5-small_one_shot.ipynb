{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Doc2Query.Doc2Query import Doc2Query\n",
    "from Doc2Query import Doc2Query\n",
    "import pyterrier as pt\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyterrier.datasets.IRDSDataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTerrier dataset to pass\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "type(pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Doc2Query\n",
    "# if you are working on Licca you have to change the path to the model\n",
    "Doc2Query_object = Doc2Query(\"google/flan-t5-small\", temperature=0.7, promting_technique='One-Shot')\n",
    "#Doc2Query_object = Doc2Query(\"gbt2\", temperature=0.7, promting_technique='Few-Shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 48786.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# The text documents from the PyTerrier dataset\n",
    "documents = Doc2Query_object.getDocumentsDfFromPtDataset(pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries for the dataset and extend the documents by the queries\n",
    "expanded_documents = Doc2Query_object.expandDocumentsByQueries(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check wether the generated queries make sense\n",
    "expanded_documents[['text']].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index using PyTerrier\n",
    "indexer = pt.IterDictIndexer(\n",
    "    \"./indexes/index_Doc2Query-flan-t5-small-oneShot-BM25\",\n",
    "    overwrite=True,\n",
    "    fields=[\"text\"],\n",
    "    meta=[\"docno\"]\n",
    ")\n",
    "# Index the documents\n",
    "expanded_documents = expanded_documents.to_dict(orient='records')\n",
    "indexref = indexer.index(expanded_documents)\n",
    "\n",
    "# Retrieve documents using BM25\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "\n",
    "# Perform retrieval\n",
    "#queries_df = pt_dataset.get_topics()\n",
    "#run = bm25.transform(queries_df)\n",
    "run = bm25(pt_dataset.get_topics('text'))\n",
    "\n",
    "# Evaluate the results\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "eval = pt.Evaluate(run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter run to include only judged documents\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "judged_docnos = qrels_df['docno'].unique()\n",
    "filtered_run = run[run['docno'].isin(judged_docnos)]\n",
    "\n",
    "# Evaluate the results\n",
    "eval = pt.Evaluate(filtered_run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the run file for subsequent evaluations\n",
    "persist_and_normalize_run(run, system_name='Doc2Query-flan-t5-small-oneShot-BM25', default_output='../runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
