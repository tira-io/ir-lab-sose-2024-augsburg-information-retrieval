{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Query using a T5 model specifically trained for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "tira = Client()\n",
    "ensure_pyterrier_is_loaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 4.81MiB [00:00, 28.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-benchmarks/ir-acl-anthology-20240504-training/seanmacavaney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126958it [00:00, 264458.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an IRDSDataset object and registers it under the name provided as an argument.\n",
    "dataset = 'ir-lab-sose-2024/ir-acl-anthology-20240504-training'\n",
    "pt_dataset = pt.get_dataset(f'irds:{dataset}')\n",
    "\n",
    "def doc_t5_query(dataset):\n",
    "    docs = tira.get_run_output('ir-benchmarks/seanmacavaney/DocT5Query', dataset) + '/documents.jsonl.gz'\n",
    "    with gzip.open(docs, 'rt') as f:\n",
    "        for l in tqdm(f):\n",
    "            l = json.loads(l)\n",
    "            l['text'] = l['querygen']\n",
    "            l['docno'] = l['doc_id']\n",
    "            del l['doc_id']\n",
    "            del l['querygen']\n",
    "            yield l\n",
    "\n",
    "# Expand the documents\n",
    "# Preprocess document_expansions into a dictionary for faster lookup\n",
    "expansions_dict = {expansion['docno']: expansion['text'] for expansion in doc_t5_query(dataset)}\n",
    "#expansions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-inputs.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 39.4M/39.4M [00:00<00:00, 69.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.', 'docno': 'O02-2002'}\n",
      "what is the study of word similarity called?\n",
      "what is the measure of word similarity\n",
      "why measure word similarity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Original documents and document expansion for first document\n",
    "for document in iter(pt_dataset.get_corpus_iter()):\n",
    "  print(document)\n",
    "  # Show document expansion for first document\n",
    "  print(expansions_dict[document['docno']])\n",
    "  # we only show the first one\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansions dictionary created with 126958 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 46942.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if expansions_dict was created correctly\n",
    "print(f\"Expansions dictionary created with {len(expansions_dict)} items.\")\n",
    "\n",
    "# Initialize a list to store the combined documents\n",
    "expanded_documents = []\n",
    "\n",
    "# Iterate through the documents and combine them with their expansions\n",
    "for document in iter(pt_dataset.get_corpus_iter()):\n",
    "    # Get the document's unique identifier\n",
    "    docno = document['docno']\n",
    "    \n",
    "    # Check if there's an expansion for the current document\n",
    "    if docno in expansions_dict:\n",
    "        # Append the expansion text to the document's text\n",
    "        original_text = document['text']\n",
    "        expansion_text = expansions_dict[docno]\n",
    "        combined_text = original_text + expansion_text\n",
    "        \n",
    "        # Create a new dictionary with the combined text and add it to the list\n",
    "        expanded_documents.append({'docno': docno, 'text': combined_text})\n",
    "        \n",
    "        # Debug print to confirm concatenation\n",
    "        # print(f\"Document {docno} expanded. Original length: {len(original_text)}, Expansion length: {len(expansion_text)}, New length: {len(combined_text)}\")\n",
    "    else:\n",
    "        # If there's no expansion, just add the original document\n",
    "        expanded_documents.append({'docno': docno, 'text': document['text']})\n",
    "        print(f\"No expansion found for document {docno}.\")\n",
    "\n",
    "# Check the number of documents combined\n",
    "print(f\"Total combined documents: {len(expanded_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'text': 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.what is the study of word similarity called?\\nwhat is the measure of word similarity\\nwhy measure word similarity?'}\n"
     ]
    }
   ],
   "source": [
    "# Test if the combination worked\n",
    "for document in expanded_documents:\n",
    "  print(document)\n",
    "  # Show document expansion for first document\n",
    "  #print(expansions_dict[document['docno']])\n",
    "  # we only show the first one\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest docno is: '2007.wwwconf_conference-GeorgakopoulosBNC07.0' with a length of 45\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the longest docno and its length\n",
    "longest_docno = ''\n",
    "max_length = 0\n",
    "\n",
    "# Iterate over the list of documents\n",
    "for document in expanded_documents:\n",
    "    # Get the current docno\n",
    "    docno = document['docno']\n",
    "    \n",
    "    # Check if the current docno is longer than the longest found so far\n",
    "    if len(docno) > max_length:\n",
    "        longest_docno = docno\n",
    "        max_length = len(docno)\n",
    "\n",
    "# Output the longest docno and its length\n",
    "print(f\"The longest docno is: '{longest_docno}' with a length of {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-truth.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 29.6k/29.6k [00:00<00:00, 1.49MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n",
      "Evaluation Metrics:\n",
      "{'map': 0.22193809243315205, 'ndcg': 0.5157937714634933, 'ndcg_cut.10': 0.28701453202938193, 'recip_rank': 0.484459943836947, 'recall_100': 0.5656737564073434}\n"
     ]
    }
   ],
   "source": [
    "# Create the Index\n",
    "# Index the documents using PyTerrier\n",
    "#indexer = pt.IterDictIndexer(\"./index_Doc2QueryT5\")\n",
    "#indexref = indexer.index(expanded_documents, fields=[\"text\"], meta=[\"docno\"])\n",
    "\n",
    "# Create the index using PyTerrier\n",
    "indexer = pt.IterDictIndexer(\n",
    "    \"./indexes/index_Doc2QueryT5\",\n",
    "    overwrite=True,\n",
    "    fields=[\"text\"],\n",
    "    meta=[\"docno\"],\n",
    "    meta_lengths=[max_length],  # Adjust length based on expected docno length\n",
    ")\n",
    "# Index the documents\n",
    "indexref = indexer.index(expanded_documents)\n",
    "\n",
    "# Retrieve documents using BM25\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "\n",
    "# Perform retrieval\n",
    "#queries_df = pt_dataset.get_topics()\n",
    "#run = bm25.transform(queries_df)\n",
    "run = bm25(pt_dataset.get_topics('text'))\n",
    "\n",
    "# Evaluate the results\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "eval = pt.Evaluate(run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "{'map': 0.5258490016383709, 'ndcg': 0.7128565585418872, 'ndcg_cut.10': 0.6234282848610385, 'recip_rank': 0.7543521429550843, 'recall_100': 0.8264849391152223}\n"
     ]
    }
   ],
   "source": [
    "# Filter run to include only judged documents\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "judged_docnos = qrels_df['docno'].unique()\n",
    "filtered_run = run[run['docno'].isin(judged_docnos)]\n",
    "\n",
    "# Evaluate the results\n",
    "eval = pt.Evaluate(filtered_run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.939318</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>1</td>\n",
       "      <td>15.905389</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>2</td>\n",
       "      <td>15.580315</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>82472</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>3</td>\n",
       "      <td>15.103510</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>74730</td>\n",
       "      <td>2008.clef_workshop-2008.10</td>\n",
       "      <td>4</td>\n",
       "      <td>14.658269</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94415</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>5</td>\n",
       "      <td>14.268209</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>6</td>\n",
       "      <td>14.238205</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>82490</td>\n",
       "      <td>1998.sigirconf_conference-98.33</td>\n",
       "      <td>8</td>\n",
       "      <td>14.228081</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>125153</td>\n",
       "      <td>2008.ipm_journal-ir0volumeA44A3.9</td>\n",
       "      <td>13</td>\n",
       "      <td>13.963897</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>101686</td>\n",
       "      <td>2018.ictir_conference-2018.20</td>\n",
       "      <td>14</td>\n",
       "      <td>13.949933</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid   docid                               docno  rank      score  \\\n",
       "0    1  125137   1989.ipm_journal-ir0volumeA25A4.2     0  16.939318   \n",
       "1    1   94858        2004.cikm_conference-2004.47     1  15.905389   \n",
       "2    1  125817  2005.ipm_journal-ir0volumeA41A5.11     2  15.580315   \n",
       "3    1   82472     1998.sigirconf_conference-98.15     3  15.103510   \n",
       "4    1   74730          2008.clef_workshop-2008.10     4  14.658269   \n",
       "5    1   94415       2008.cikm_conference-2008.183     5  14.268209   \n",
       "6    1   84876       2016.ntcir_conference-2016.90     6  14.238205   \n",
       "8    1   82490     1998.sigirconf_conference-98.33     8  14.228081   \n",
       "13   1  125153   2008.ipm_journal-ir0volumeA44A3.9    13  13.963897   \n",
       "14   1  101686       2018.ictir_conference-2018.20    14  13.949933   \n",
       "\n",
       "                                       query  \n",
       "0   retrieval system improving effectiveness  \n",
       "1   retrieval system improving effectiveness  \n",
       "2   retrieval system improving effectiveness  \n",
       "3   retrieval system improving effectiveness  \n",
       "4   retrieval system improving effectiveness  \n",
       "5   retrieval system improving effectiveness  \n",
       "6   retrieval system improving effectiveness  \n",
       "8   retrieval system improving effectiveness  \n",
       "13  retrieval system improving effectiveness  \n",
       "14  retrieval system improving effectiveness  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "# Persist the run file for subsequent evaluations\n",
    "persist_and_normalize_run(run, system_name='Doc2QueryT5-BM25', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to BM25 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/2024-05-04-16-05-53.zip\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download:   3%|▎         | 500k/19.5M [00:00<00:03, 5.12MiB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 19.5M/19.5M [00:00<00:00, 43.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-lab-sose-2024/ir-acl-anthology-20240504-training/tira-ir-starter\n",
      "Evaluation Metrics:\n",
      "{'map': 0.2623109779858802, 'ndcg': 0.5494611680377397, 'ndcg_cut.10': 0.3740414675768205, 'recip_rank': 0.5798765367925459, 'recall_100': 0.6013331716358514}\n"
     ]
    }
   ],
   "source": [
    "# Baseline without Doc2Query\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "# Retrieve documents using BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "# Perform retrieval\n",
    "run = bm25(pt_dataset.get_topics('text'))\n",
    "# Evaluate the results\n",
    "eval = pt.Evaluate(run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "{'map': 0.5307283855863614, 'ndcg': 0.71563397739462, 'ndcg_cut.10': 0.6443137714663788, 'recip_rank': 0.7607843137254903, 'recall_100': 0.817271215638198}\n"
     ]
    }
   ],
   "source": [
    "# Filter run to include only judged documents\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "judged_docnos = qrels_df['docno'].unique()\n",
    "filtered_run = run[run['docno'].isin(judged_docnos)]\n",
    "\n",
    "# Evaluate the results\n",
    "eval = pt.Evaluate(filtered_run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the BM25 Baseline is almost always better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
