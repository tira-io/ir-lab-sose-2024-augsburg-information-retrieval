{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Doc2Query.Doc2Query import Doc2Query\n",
    "from Doc2Query import Doc2Query\n",
    "import pyterrier as pt\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyterrier.datasets.IRDSDataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTerrier dataset to pass\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "type(pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Doc2Query\n",
    "# if you are working on Licca you have to add the path to the model\n",
    "Doc2Query_object = Doc2Query(\"google/flan-t5-small\", temperature=0.7, promting_technique='Few-Shot')\n",
    "#Doc2Query_object = Doc2Query(\"gbt2\", temperature=0.7, promting_technique='Few-Shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 48786.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# The text documents from the PyTerrier dataset\n",
    "documents = Doc2Query_object.getDocumentsDfFromPtDataset(pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries for the dataset and extend the documents by the queries\n",
    "expanded_documents = Doc2Query_object.expandDocumentsByQueries(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {0: 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  1: 'Bootstrapping Large Sense Tagged Corpora [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website.\"]',\n",
       "  2: 'Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads\\n\\n\\n Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus. [\"A randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website\"]',\n",
       "  3: 'Aligning Words in {E}nglish-{H}indi Parallel Corpora\\n\\n\\n In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data. [\"There is a randomized field experiment with 184,375 users split into a top North American retailer\\'s website comparing both product attributes and consumer reviews.\"]',\n",
       "  4: 'Proposal of a very-large-corpus acquisition method by cell-formed registration [\"We conducted a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site with 184,375 users split into a top e-commerce site.\"]',\n",
       "  5: 'Recognizing semantic relations within {P}olish noun phrase: A rule-based approach\\n\\n\\n The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora. [\"We conduct a field experiment on a top North American retailer\\'s website with 184,375 users split into a top consumer review website with 184,375 users split into a top North American retailer.\"]',\n",
       "  6: '{LIHLA}: Shared Task System Description\\n\\n\\n In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top web retailer\\'s website with 184,375 users split into a top online retailer\\'s website with 184,375 users split into a top online retailer\\'s website with 184,375 users split into a top online retailer\\'s website with 184,375 users split into a top online retailer\\'s website with \"]',\n",
       "  7: 'Enhanced {J}apanese Electronic Dictionary Look-up [\"We run a random field experiment on a top North American retailer\\'s website with 184,375 users split into a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a randomized field experiment on an e-commerce site.\"]',\n",
       "  8: 'Unsupervised Induction of {A}rabic Root and Pattern Lexicons using Machine Learning\\n\\n\\n We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text. [\"A group of users split into a product, which has an average of 184,375 users, and a randomized field experiment on the top North American retailer\\'s website.\"]',\n",
       "  9: 'Improved Language Modeling for Statistical Machine Translation\\n\\n\\n Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. [\"We ran a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  10: 'Evaluation of a Vector Space Similarity Measure in a Multilingual Framework [\"A randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  11: 'Towards Domain Adaptation for Parsing Web Data\\n\\n\\n We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top online retailer\\'s website.\"]',\n",
       "  12: 'Shared Task: Statistical Machine Translation between {E}uropean Languages\\n\\n\\n Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top online retailer\\'s website.\"]',\n",
       "  13: \"Corpora as Object-Oriented System. From {UML}-notation to Implementation ['The model for the method of reverse indexing and indexing on an e-commerce site showed that there is no overlap between the inverted index and the inverted index.']\",\n",
       "  14: 'Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space\\n\\n\\n In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010)   and the linear map-based model of Baroni and Zamparelli (2010) , can be applied to detect semantically anomalous adjectivenoun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the field of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners\\' content word combinations. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top customer review site.\"]',\n",
       "  15: 'Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams\\n\\n\\n This work discusses translation results for the four Euparl data sets which were made available for the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top web store with a single user and a single user.\"]',\n",
       "  16: 'Transfer rule generation for a {J}apanese-{H}ungarian machine translation system\\n\\n\\n Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users divided into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split\"]',\n",
       "  17: 'The Lexicon-Grammar Balance in Robust Parsing of {I}talian [\"A randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a top retailer\\'s website with 184,375 users split into a\"]',\n",
       "  18: \"Incremental and Predictive Dependency Parsing under Real-Time Conditions\\n\\n\\n We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off. ['Invert indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we']\",\n",
       "  19: '{PORTAGE}: A Phrase-Based Machine Translation System\\n\\n\\n This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top customer service site.\"]',\n",
       "  20: 'Humans as Corpus - Language Learning Strategies in Virtually Mediated Authentic Environments [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site with 184,375 users split into a top e-commerce site.\"]',\n",
       "  21: 'Rationale, Concepts, and Current Outcome of the Unit Graphs Framework\\n\\n\\n The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a Top North American retailer\\'s website with 184,375 users split into a Top North American retailer.\"]',\n",
       "  22: \"Towards Automatic Generation of Gene Summary\\n\\n\\n In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp. ['This paper focuses on the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site.']\",\n",
       "  23: 'Improving {J}apanese semantic-role-labeling performance with transfer learning as case for limited resources of tagged corpora on aggregated language\\n\\n\\n In this paper we proposed the use of effective features and transfer leaning to improve the accuracies of neural-network-based models for accurate semantic role labeling (SRL) of Japanese, which is an aggregated language. We first reveal that the final morphemes in each argument, which have not been discussed in previous work on English SRL are effective features in determining semantic role labels in Japanese. We then discuss the possibility of using large-scale training corpora annotated with different semantic labels from the target semantic labels by transfer learning on CNN, 3-LNN, and GRU models. The experimental results of Japanese SRL on the proposed models indicate that all of the neural-network-based models performed better with transfer learning as well as using the feature vectors of final moprhemes in each argument. 1 In CoNLL 2009 (Hajič et al., 2009), annotated corpora with semantic roles in multiple languages including Japanese were used; however, most semantic tags in the Japanese corpus were case-marker-based relations, which are different from semantic roles annotated in PropBank. 2 [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\"]',\n",
       "  24: 'The Unit Graphs Framework: Foundational Concepts and Semantic Consequence\\n\\n\\n We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs. [\"The researchers study the results of a first randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website.\"]',\n",
       "  25: \"A Generalized Alignment-Free Phrase Extraction\\n\\n\\n In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-English. ['We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site.']\",\n",
       "  26: \"An Annotated {J}apanese {S}ign {L}anguage Corpus ['Invert indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space required of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently.']\",\n",
       "  27: 'Transforming Controlled Natural Language Biomedical Queries into Answer Set Programs\\n\\n\\n We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)-a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples. [\"We ran an experiment with a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top online retailer.\"]',\n",
       "  28: 'The binomial cumulative distribution function, or, is my system better than yours? [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top online retail store.\"]',\n",
       "  29: \"Confidence Estimation for Knowledge Base Population\\n\\n\\n Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefficient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy. ['We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site.']\",\n",
       "  30: 'Combining Linguistic Data Views for Phrase-based {SMT}\\n\\n\\n We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words. [\\'We investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender on an e-commerce site.\\']',\n",
       "  31: 'Hosting Volunteer Translators\\n\\n\\n We have developed a web site called Minna no Hon\\'yaku (\"Translation for Everyone by Everyone\"), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. [\"A randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  32: 'Constructing Web-Accessible Semantic Role Labels and Frames for {J}apanese as Additions to the {NPCMJ} Parsed Corpus\\n\\n\\n As part of constructing the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), a web-accessible language resource, we are adding frame information for predicates, together with two types of semantic role labels that mark the contributions of arguments. One role type consists of numbered semantic roles, like in PropBank, to capture relations between arguments in different syntactic patterns. The other role type consists of semantic roles with conventional names. Both role types are compatible with hierarchical frames that belong to related predicates. Adding semantic role and frame information to the NPCMJ will support a web environment where language learners and linguists can search examples of Japanese for syntactic and semantic features. The annotation will also provide a language resource for NLP researchers making semantic parsing models (e.g., for AMR parsing) following machine learning approaches. In this paper, we describe how the two types of semantic role labels are defined under the frame based approach, i.e., both types can be consistently applied when linked to corresponding frames. Then we show special cases of syntactic patterns and the current status of the annotation work. [\"The comparison of the randomized field experiment and the author\\'s opinion on product attribute and consumer reviews is a useful tool for the search engine.\"]',\n",
       "  33: \"Towards Fine-grained Citation Function Classification\\n\\n\\n We look into the problem of recognizing citation functions in scientific literature, trying to reveal authors' rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%. ['The inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists, but the index has a shortcoming, in that only predefined pattern queries can be supported efficiently.']\",\n",
       "  34: '{RALI}: {SMT} Shared Task System Description\\n\\n\\n Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  35: 'A Mathematical Model of the Vocabulary-Text Relation [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top consumer review system.\"]',\n",
       "  36: 'A Method of Augmenting Bilingual Terminology by Taking Advantage of the Conceptual Systematicity of Terminologies\\n\\n\\n In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a \"generate and validate\" framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a \"generate and validate\" framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation. [\"We conducted a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top consumer\\'s website.\"]',\n",
       "  37: 'Sentiment Analysis of Reviews: Should we analyze writer intentions or reader perceptions?\\n\\n\\n Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers\\' ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done). [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top U.S. retailer\\'s website.\"]',\n",
       "  38: 'Improving Phrase-Based Statistical Translation by Modifying Phrase Extraction and Including Several Features\\n\\n\\n Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\" (ACL Workshop on Parallel Texts 2005). [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website.\"]',\n",
       "  39: 'Translation Model Adaptation for an {A}rabic/{F}rench News Translation System by Lightly- Supervised Training\\n\\n\\n Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  40: 'Supervised Morphology Generation Using Parallel Corpus\\n\\n\\n Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We define a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inflected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences. [\"A randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website\"]',\n",
       "  41: 'First Steps towards Multi-Engine Machine Translation\\n\\n\\n We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed. [\"We ran a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top consumer\\'s website.\"]',\n",
       "  42: 'Terminology-driven Augmentation of Bilingual Terminologies\\n\\n\\n This paper proposes a way of augmenting bilingual terminologies by using a \"generate and validate\" method. Using existing bilingual terminologies, the method generates \"potential\" bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of five domains show that the method is highly promising. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a customer rating system on an e-commerce site.\"]',\n",
       "  43: 'Using Artificial Data to Compare the Difficulty of Using Statistical Machine Translation in Different Language-Pairs [\"We conducted a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a web site.\"]',\n",
       "  44: 'Evaluation of baseline information retrieval for {P}olish open-domain Question Answering system\\n\\n\\n We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question-answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain significant improvement on simple information retrieval baseline. The improvement is observed as finding more answer-bearing documents among the top n search results. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]',\n",
       "  45: 'Competitive Grouping in Integrated Phrase Segmentation and Alignment Model\\n\\n\\n This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top retailer\\'s website.\"]',\n",
       "  46: 'A Thesaurus of Predicate-Argument Structure for {J}apanese Verbs to Deal with Granularity of Verb Meanings\\n\\n\\n In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top consumer\\'s website with 184,375 users split into a top consumer\\'s website with a randomized field experiment.\"]',\n",
       "  47: 'Revisiting the Old Kitchen Sink: Do we Need Sentiment Domain Adaptation?\\n\\n\\n In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple \"all-in-one\" classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the \"kitchen sink\" approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that \"flip\" polarity across domains is not borne out empirically. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top online retailer\\'s website with 184,375 users split into a top online retailer.\"]',\n",
       "  48: 'Novel Reordering Approaches in Phrase-Based Statistical Machine Translation\\n\\n\\n This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top online retailer\\'s website with 184,375 users split into a top web store.\"]',\n",
       "  49: 'An ontology-driven system for detecting global health events\\n\\n\\n Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as Word-Nets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages. [\"We run a randomized field experiment on a top North American retailer\\'s website with 184,375 users split into a top e-commerce site.\"]'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check wether the generated queries make sense\n",
    "expanded_documents[['text']].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index using PyTerrier\n",
    "indexer = pt.IterDictIndexer(\n",
    "    \"./indexes/index_Doc2Query-flan-t5-small\",\n",
    "    overwrite=True,\n",
    "    fields=[\"text\"],\n",
    "    meta=[\"docno\"]\n",
    ")\n",
    "# Index the documents\n",
    "expanded_documents = expanded_documents.to_dict(orient='records')\n",
    "indexref = indexer.index(expanded_documents)\n",
    "\n",
    "# Retrieve documents using BM25\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "\n",
    "# Perform retrieval\n",
    "#queries_df = pt_dataset.get_topics()\n",
    "#run = bm25.transform(queries_df)\n",
    "run = bm25(pt_dataset.get_topics('text'))\n",
    "\n",
    "# Evaluate the results\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "eval = pt.Evaluate(run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter run to include only judged documents\n",
    "qrels_df = pt_dataset.get_qrels()\n",
    "judged_docnos = qrels_df['docno'].unique()\n",
    "filtered_run = run[run['docno'].isin(judged_docnos)]\n",
    "\n",
    "# Evaluate the results\n",
    "eval = pt.Evaluate(filtered_run, qrels_df, metrics=[\"map\", \"ndcg\", \"ndcg_cut.10\", \"recip_rank\", \"recall_100\"])\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the run file for subsequent evaluations\n",
    "persist_and_normalize_run(run, system_name='Doc2Query-flan-t5-small-fewShot-BM25', default_output='../runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
