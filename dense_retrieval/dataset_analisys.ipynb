{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis\n",
    "\n",
    "This jupyter notebook serves as playground for getting to know the dataset.\n",
    "\n",
    "The dataset contains a corpus of scientific papers (title + abstracts) from the fields of IR and NLP (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). -> Dataset is the union of the IR Anthology and the ACL Anthology\n",
    "\n",
    "It also contains queries on the corpus and the corresponding relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab:\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Here I define some helper functions that will be used later to analyse the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Helper function for printing dicts\n",
    "def pp(obj):\n",
    "    print(json.dumps(obj, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "the dataset is a pyterrier dataset (genauer: ```pyterrier.datasets.IRDSDataset```) [source code of pyterrier.datasets](https://pyterrier.readthedocs.io/en/latest/_modules/pyterrier/datasets.html#Dataset.get_topics)\n",
    "\n",
    "\n",
    "The dataset has some methods that we might want to use:\n",
    "- ```get_corpus_iter()```: The document corpus as an iterator (```get_corpus()``` is not available with this dataset)\n",
    "- ```get_topics()```: The queries in a pandas DataFrame\n",
    "- ```get_qrels()```: The relevant documents for the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator for corpus\n",
    "corpus_iter = pt_dataset.get_corpus_iter(verbose=False) # verbose=True -> tqdm progress bar\n",
    "# corpus iter is a GeneratorLen object, returns dicts\n",
    "# has 2 attributes: length (number of documents in corpus) and gen (generator fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in corpus: 126958\n",
      "\n",
      "First element in corpus_iter:\n",
      "Document ID: ->R13-1044<-\n",
      "Document Text: ->Recognizing semantic relations within {P}olish noun phrase: A rule-based approach\n",
      "\n",
      "\n",
      " The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora.<-\n"
     ]
    }
   ],
   "source": [
    "print(\"number of documents in corpus:\", corpus_iter.length)\n",
    "\n",
    "try:\n",
    "    doc = next(corpus_iter.gen)  # doc is a dict with keys \"docno\" and \"text\"\n",
    "except StopIteration:\n",
    "    print(\"FEHLER: Keine Elemente im Iterator!\")\n",
    "\n",
    "print()\n",
    "print(\"First element in corpus_iter:\")\n",
    "print(f\"Document ID: ->{doc['docno']}<-\")  # unique id string\n",
    "print(f\"Document Text: ->{doc['text']}<-\") # text consisting of the title and (if available) the abstract of a paper\n",
    "# NOTE: I use the arrows '->','<-' to better identify empty strings / newlines / blanks in prints\n",
    "# NOTE: The generator returns a random document -> need to set seed in production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"R13-1044\",\n",
      "  \"abstract\": \"Recognizing semantic relations within {P}olish noun phrase: A rule-based approach The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora.\",\n",
      "  \"title\": \"Recognizing semantic relations within {P}olish noun phrase: A rule-based approach\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doc_json = {\"id\": doc[\"docno\"], \"abstract\":\"\"}\n",
    "for i, line in enumerate(doc[\"text\"].split(\"\\n\")):\n",
    "    if i == 0 and line:\n",
    "        doc_json[\"title\"] = line\n",
    "    if not line: # empty line\n",
    "        continue\n",
    "    else: # Non empty lines belong to the abstract\n",
    "        # In case the abstract contains newlines -> concatenate lines TODO: are there newlines some of the abstracts?\n",
    "        doc_json[\"abstract\"] += line #or line.strip() TODO: -> could be bad if one line ends on a blank and the next line begins with a word \n",
    "pp(doc_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will (try to) iterate over the dataset, and check if the cell above works for all elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doctext(doctext):\n",
    "    splitted = doctext.split(\"\\n\")\n",
    "    title = splitted.pop(0) # first line is always title\n",
    "    abstract = \"\".join(splitted) # following non-empty lines are abstract\n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents without an abstract: 39701/126958\n"
     ]
    }
   ],
   "source": [
    "corpus_iter = pt_dataset.get_corpus_iter(verbose=False) # verbose=True -> tqdm progress bar\n",
    "\n",
    "no_abstracts = {}\n",
    "# iterate over corpus\n",
    "for i, d in enumerate(corpus_iter):\n",
    "    title, abstract = split_doctext(d[\"text\"])\n",
    "    if not title:\n",
    "        raise Exception(f\"Document does not have a title!!! docno: {d['docno']}\")\n",
    "    if not abstract:\n",
    "        no_abstracts[d[\"docno\"]] = title\n",
    "\n",
    "print(f\"Number of documents without an abstract: {len(no_abstracts)}/{corpus_iter.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries\n",
    "Pyterrier Dataset has a function ```get_topics()``` which returns a dataframe of the topics aka queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "number of topics/queries: 68\n",
      "Index(['qid', 'text', 'title', 'query', 'description', 'narrative'], dtype='object')\n",
      "Example of one topic/query:\n",
      " query id   : 1\n",
      " title      : retrieval system improving effectiveness\n",
      " description: Relevant papers include research on what makes a retrieval system effective and what impro...\n",
      " query      : What papers focus on improving the effectiveness of a retrieval system?\n"
     ]
    }
   ],
   "source": [
    "queries = pt_dataset.get_topics()\n",
    "print(\"number of topics/queries:\", len(queries))\n",
    "print(queries.columns)\n",
    "\n",
    "topic = queries.iloc[0]\n",
    "# confusing names\n",
    "query = topic['description']  # is actually the user query\n",
    "description = topic['narrative']  #is actually the description (though i dont know what narrative means) \n",
    "print(\"Example of one topic/query:\")\n",
    "print(f\" query id   : {topic['qid']}\")\n",
    "print(f\" title      : {topic['title']}\")\n",
    "print(f\" description: {description[:90]}...\")\n",
    "print(f\" query      : {query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 2623\n",
      "columns: ['qid', 'docno', 'label', 'iteration']\n",
      "on average 39 labels per query id\n",
      "\n",
      "Label Statistics: (per qid)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['number_of_labels', 'number_of_relevant', 'relevance_rate'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m results_df \u001b[38;5;241m=\u001b[39m qrels\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     13\u001b[0m     n_labels\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m     n_relevant\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     relev_ratio\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#print(results_df)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber_of_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber_of_relevant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevance_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     19\u001b[0m             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Statistics: (per docno)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['number_of_labels', 'number_of_relevant', 'relevance_rate'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Groundtruths - relevant documents for queries\n",
    "qrels = pt_dataset.get_qrels()\n",
    "qrels[\"label\"] = qrels[\"label\"].astype(int) # label is string\n",
    "\n",
    "print(\"length:\", len(qrels))\n",
    "print(\"columns:\", list(qrels.columns))\n",
    "print(f\"on average {round(len(qrels) / len(qrels['qid'].unique()))} labels per query id\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Label Statistics: (per qid)\")\n",
    "results_df = qrels.groupby(\"qid\").agg(\n",
    "    n_labels=(\"label\", \"size\"),\n",
    "    n_relevant=(\"label\", \"sum\"),\n",
    "    relev_ratio=(\"label\", lambda x: x.sum() / x.size if x.size > 0 else 0)\n",
    ").reset_index()\n",
    "#print(results_df)\n",
    "print(results_df[[\"n_labels\", \"n_relevant\", \"relev_ratio\"]].agg(\n",
    "            [\"min\", \"max\", \"mean\", \"std\"]))\n",
    "print()\n",
    "\n",
    "print(\"Document Statistics: (per docno)\")\n",
    "results_df = qrels.groupby(\"docno\").agg(\n",
    "    n_qids=(\"qid\", \"size\"),  # number of queries a document occurs in labels\n",
    "    n_rel=(\"label\", \"sum\")\n",
    ").reset_index()\n",
    "#print(results_df)\n",
    "print(results_df[[\"n_qids\", \"n_rel\"]].agg(\n",
    "            [\"min\", \"max\", \"mean\", \"std\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den Statistiken geht hervor, dass es sowohl (1 oder mehr) queries gibt, zu dem kein einziges dokument relevant ist, und auch wo jedes document relevant ist.\n",
    "\n",
    "Frage ist, sollte man das aussortieren? bei nur 68 queries werden es dann noch weniger!\n",
    "\n",
    "Was bedeutet die spalte iteration? alle werte sind 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERIES\n",
    "\n",
    "# pyterrier dataset (class IRDSDataset) builds queries dataframe with self.irds_ref()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "number of queries: 68\n",
      "Index(['qid', 'text', 'title', 'query', 'description', 'narrative'], dtype='object')\n",
      "RangeIndex(start=0, stop=68, step=1)\n",
      "---------------------------------\n",
      "query ID: 1\n",
      "topic: retrieval system improving effectiveness\n",
      "query: What papers focus on improving the effectiveness of a retrieval system?\n",
      "description: Relevant papers include research on what makes a retrieval system effective and what improves the effectiveness of a retrieval system. Papers that focus on improving something else or improving the effectiveness of a system that is not a retrieval system are not relevant.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = pt_dataset.get_topics()\n",
    "print(\"number of queries:\", len(queries))\n",
    "print(queries.columns)\n",
    "print(queries.index)\n",
    "\n",
    "for i, row in queries.iterrows():\n",
    "    if i > 10: break\n",
    "    print(\"---------------------------------\")\n",
    "    topic = row['text']\n",
    "    query = row['description']\n",
    "    description = row['narrative']\n",
    "    print(f\"query ID: {row['qid']}\")\n",
    "    print(f\"topic: {topic}\")\n",
    "    print(f\"query: {query}\")\n",
    "    print(f\"description: {description}\")\n",
    "    break\n",
    "\n",
    "\n",
    "#print(queries[\"test\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, qid                                          1\n",
      "docno        2005.ipm_journal-ir0volumeA41A1.7\n",
      "label                                        1\n",
      "iteration                                    0\n",
      "Name: 0, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "# Q RELS\n",
    "\n",
    "qrels = pt_dataset.get_qrels()\n",
    "\n",
    "for row in qrels.iterrows():\n",
    "    print(row)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "pt_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>15.681777</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.047380</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>2</td>\n",
       "      <td>14.144223</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5868</td>\n",
       "      <td>W05-0704</td>\n",
       "      <td>3</td>\n",
       "      <td>14.025748</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>4</td>\n",
       "      <td>13.947994</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82472</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>5</td>\n",
       "      <td>13.901647</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>94415</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>6</td>\n",
       "      <td>13.808208</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>17496</td>\n",
       "      <td>O01-2005</td>\n",
       "      <td>7</td>\n",
       "      <td>13.749449</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>82490</td>\n",
       "      <td>1998.sigirconf_conference-98.33</td>\n",
       "      <td>8</td>\n",
       "      <td>13.735541</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>124801</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>9</td>\n",
       "      <td>13.569263</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94858        2004.cikm_conference-2004.47     0  15.681777   \n",
       "1   1  125137   1989.ipm_journal-ir0volumeA25A4.2     1  15.047380   \n",
       "2   1  125817  2005.ipm_journal-ir0volumeA41A5.11     2  14.144223   \n",
       "3   1    5868                            W05-0704     3  14.025748   \n",
       "4   1   84876       2016.ntcir_conference-2016.90     4  13.947994   \n",
       "5   1   82472     1998.sigirconf_conference-98.15     5  13.901647   \n",
       "6   1   94415       2008.cikm_conference-2008.183     6  13.808208   \n",
       "7   1   17496                            O01-2005     7  13.749449   \n",
       "8   1   82490     1998.sigirconf_conference-98.33     8  13.735541   \n",
       "9   1  124801   2006.ipm_journal-ir0volumeA42A3.2     9  13.569263   \n",
       "\n",
       "                                      query  \n",
       "0  retrieval system improving effectiveness  \n",
       "1  retrieval system improving effectiveness  \n",
       "2  retrieval system improving effectiveness  \n",
       "3  retrieval system improving effectiveness  \n",
       "4  retrieval system improving effectiveness  \n",
       "5  retrieval system improving effectiveness  \n",
       "6  retrieval system improving effectiveness  \n",
       "7  retrieval system improving effectiveness  \n",
       "8  retrieval system improving effectiveness  \n",
       "9  retrieval system improving effectiveness  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = bm25(pt_dataset.get_topics('text'))\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
