{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colbert with Pyterrier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- install libraries (if necessary)\n",
    "- all imports here\n",
    "- connecting to tira & printoptions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu==1.6.3 (from versions: 1.7.1.post3, 1.7.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu==1.6.3\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install transformers faiss-gpu faiss-cpu torch\n",
    "#!pip install tira ir-datasets python-terrier\n",
    "#!pip install sentence-transformers\n",
    "\n",
    "#!pip install faiss-gpu==1.6.3  # version in colbert tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+https://github.com/terrierteam/pyterrier_colbert.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-31 09:23:38--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 405924985 (387M) [application/octet-stream]\n",
      "Saving to: ‘downloads/colbertv2.0.tar.gz’\n",
      "\n",
      "colbertv2.0.tar.gz  100%[===================>] 387.12M  5.14MB/s    in 73s     \n",
      "\n",
      "2024-07-31 09:24:52 (5.30 MB/s) - ‘downloads/colbertv2.0.tar.gz’ saved [405924985/405924985]\n",
      "\n",
      "colbertv2.0/\n",
      "colbertv2.0/artifact.metadata\n",
      "colbertv2.0/vocab.txt\n",
      "colbertv2.0/tokenizer.json\n",
      "colbertv2.0/special_tokens_map.json\n",
      "colbertv2.0/tokenizer_config.json\n",
      "colbertv2.0/config.json\n",
      "colbertv2.0/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/colbertv2.0.tar.gz -C downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import importlib\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "import pyterrier as pt\n",
    "\n",
    "# Encoder and Tokenizer models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaModel, RobertaTokenizerFast, RobertaTokenizer\n",
    "from transformers import DebertaModel, DebertaTokenizerFast, DebertaTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tira and Pyterrier Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.third_party_integrations import ir_datasets\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "\n",
    "# colbert pyterrier\n",
    "import pyterrier_colbert.indexing\n",
    "\n",
    "\n",
    "\n",
    "# Print options for pandas\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# TODO: set seed!!\n",
    "\n",
    "\n",
    "COLAB='google.colab' in sys.modules\n",
    "if COLAB:\n",
    "    # mount to drive\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instanciate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 31, 09:31:59] [0] \t\t #> Local args.bsize = 128\n",
      "[Jul 31, 09:31:59] [0] \t\t #> args.index_root = .\n",
      "[Jul 31, 09:31:59] [0] \t\t #> self.possible_subset_sizes = [69905]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eea2f702fa498a819b08923f0df435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2931f0970b4649368c887b6734820858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#!rm -rf /colbertindex\u001b[39;00m\n\u001b[1;32m      6\u001b[0m indexer \u001b[38;5;241m=\u001b[39m pyterrier_colbert\u001b[38;5;241m.\u001b[39mindexing\u001b[38;5;241m.\u001b[39mColBERTIndexer(checkpoint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolbertindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_corpus_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier_colbert/indexing.py:330\u001b[0m, in \u001b[0;36mColBERTIndexer.index\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m l              \n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m convert_gen(iterator)\n\u001b[0;32m--> 330\u001b[0m ceg \u001b[38;5;241m=\u001b[39m \u001b[43mCollectionEncoderIds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids \u001b[38;5;28;01melse\u001b[39;00m CollectionEncoder_Generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    332\u001b[0m create_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mindex_root)\n\u001b[1;32m    333\u001b[0m create_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mindex_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier_colbert/indexing.py:234\u001b[0m, in \u001b[0;36mCollectionEncoder_Generator.__init__\u001b[0;34m(self, prepend_title, *args)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, prepend_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepend_title \u001b[38;5;241m=\u001b[39m prepend_title\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier_colbert/indexing.py:68\u001b[0m, in \u001b[0;36mCollectionEncoder.__init__\u001b[0;34m(self, args, process_idx, num_processes)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_main(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#> args.index_root =\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m.\u001b[39mindex_root)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_main(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#> self.possible_subset_sizes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_subset_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexmgr \u001b[38;5;241m=\u001b[39m IndexManager(args\u001b[38;5;241m.\u001b[39mdim)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier_colbert/indexing.py:84\u001b[0m, in \u001b[0;36mCollectionEncoder._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcheckpoint, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolbert, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_colbert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m colbert\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mDEVICE \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolbert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolbert\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/evaluation/loaders.py:178\u001b[0m, in \u001b[0;36mload_colbert\u001b[0;34m(args, do_print)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_colbert\u001b[39m(args, do_print\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 178\u001b[0m     colbert, checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_print\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# I.e., not their purely (i.e., training) default values.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_maxlen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_maxlen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/evaluation/load_model.py:20\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(args, do_print)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(args, do_print\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m     colbert \u001b[38;5;241m=\u001b[39m ColBERT\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                                       query_maxlen\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mquery_maxlen,\n\u001b[1;32m     16\u001b[0m                                       doc_maxlen\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdoc_maxlen,\n\u001b[1;32m     17\u001b[0m                                       dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m     18\u001b[0m                                       similarity_metric\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msimilarity,\n\u001b[1;32m     19\u001b[0m                                       mask_punctuation\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmask_punctuation)\n\u001b[0;32m---> 20\u001b[0m     colbert \u001b[38;5;241m=\u001b[39m \u001b[43mcolbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     print_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#> Loading model checkpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m, condition\u001b[38;5;241m=\u001b[39mdo_print)\n\u001b[1;32m     24\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m load_checkpoint(args\u001b[38;5;241m.\u001b[39mcheckpoint, colbert, do_print\u001b[38;5;241m=\u001b[39mdo_print)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2796\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\"\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "#!rm -rf /colbertindex\n",
    "\n",
    "indexer = pyterrier_colbert.indexing.ColBERTIndexer(checkpoint, \".\", \"colbertindex\", chunksize=3)\n",
    "indexer.index(dataset.get_corpus_iter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Test corpus - to use with the test-run cells\n",
    "corpus = {\n",
    "    \"doc1\": [\"doc1\", \"Elephants are the largest living land animals. Three living species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus). They are the only surviving members of the family Elephantidae and the order Proboscidea; extinct relatives include mammoths and mastodons. Distinctive features of elephants include a long proboscis called a trunk, tusks, large ear flaps, pillar-like legs, and tough but sensitive grey skin. The trunk is prehensile, bringing food and water to the mouth and grasping objects. Tusks, which are derived from the incisor teeth, serve both as weapons and as tools for moving objects and digging. The large ear flaps assist in maintaining a constant body temperature as well as in communication. African elephants have larger ears and concave backs, whereas Asian elephants have smaller ears and convex or level backs.\"],\n",
    "    \"doc2\": [\"doc2\", \"Ants are eusocial insects of the family Formicidae and, along with the related wasps and bees, belong to the order Hymenoptera. Ants evolved from vespoid wasp ancestors in the Cretaceous period. More than 13,800 of an estimated total of 22,000 species have been classified. They are easily identified by their geniculate (elbowed) antennae and the distinctive node-like structure that forms their slender waists.\\nAnts form colonies that range in size from a few dozen individuals often living in small natural cavities to highly organised colonies that may occupy large territories with sizeable nest that consist of millions of individuals or into the hundreds of millions in super colonies. Typical colonies consist of various castes of sterile, wingless females, most of which are workers (ergates), as well as soldiers (dinergates) and other specialised groups. Nearly all ant colonies also have some fertile males called \\\"drones\\\" and one or more fertile females called \\\"queens\\\" (gynes). The colonies are described as superorganisms because the ants appear to operate as a unified entity, collectively working together to support the colony.\"],\n",
    "    \"doc3\": [\"doc3\", \"Volkswagen (VW) is a German automobile manufacturer headquartered in Wolfsburg, Lower Saxony, Germany. Founded in 1937 by the German Labour Front under the Nazi Party and revived into the global brand it is known as today post-World War II by the British Army officer Ivan Hirst, it is known for the iconic Beetle and serves as the flagship brand of the Volkswagen Group, the largest automotive manufacturer by worldwide sales in 2016 and 2017.[1] The group's biggest market is China (including Hong Kong and Macau), which delivers 40 percent of its sales and profits.[2][3] Its name is derived from the German-language terms Volk and Wagen, translating to \\\"people's car\\\" when combined.\"],\n",
    "    \"doc4\": [\"doc4\", \"Bayerische Motoren Werke AG, commonly abbreviated to BMW (German pronunciation: [ˌbeːʔɛmˈveː]), is a German multinational manufacturer of luxury vehicles and motorcycles headquartered in Munich, Bavaria, Germany. The company was founded in 1916 as a manufacturer of aircraft engines, which it produced from 1917 to 1918 and again from 1933 to 1945 creating engines for aircraft that were used in the Second World War.\"],\n",
    "    \"doc5\": [\"doc5\",\"Dragon Ball (Japanese: ドラゴンボール, Hepburn: Doragon Bōru) is a Japanese media franchise created by Akira Toriyama in 1984. The initial manga, written and illustrated by Toriyama, was serialized in Weekly Shōnen Jump from 1984 to 1995, with the 519 individual chapters collected in 42 tankōbon volumes by its publisher Shueisha. Dragon Ball was originally inspired by the classical 16th-century Chinese novel Journey to the West, combined with elements of Hong Kong martial arts films. Dragon Ball characters also use a variety of East Asian martial arts styles, including karate[1][2][3] and Wing Chun (kung fu).[2][3][4] The series follows the adventures of protagonist Son Goku from his childhood through adulthood as he trains in martial arts. He spends his childhood far from civilization until he meets a teen girl named Bulma, who encourages him to join her quest in exploring the world in search of the seven orbs known as the Dragon Balls, which summon a wish-granting dragon when gathered. Along his journey, Goku makes several other friends, becomes a family man, discovers his alien heritage, and battles a wide variety of villains, many of whom also seek the Dragon Balls.\"],\n",
    "    \"doc6\": [\"doc6\", \"The Matrix is a 1999 science fiction action film[5][6] written and directed by the Wachowskis.[a] It is the first installment in the Matrix film series, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving and Joe Pantoliano, and depicts a dystopian future in which humanity is unknowingly trapped inside the Matrix, a simulated reality that intelligent machines have created to distract humans while using their bodies as an energy source.[7] When computer programmer Thomas Anderson, under the hacker alias \\\"Neo\\\", uncovers the truth, he joins a rebellion against the machines along with other people who have been freed from the Matrix.\"],\n",
    "    \"doc7\": [\"doc7\", \"In computer science, Backus–Naur form (/ˌbækəs ˈnaʊər/) (BNF or Backus normal form) is a notation used to describe the syntax of programming languages or other formal languages. It was developed by John Backus and Peter Naur. BNF can be described as a metasyntax notation for context-free grammars. Backus–Naur form is applied wherever exact descriptions of languages are needed, such as in official language specifications, in manuals, and in textbooks on programming language theory. BNF can be used to describe document formats, instruction sets, and communication protocols. Over time, many extensions and variants of the original Backus–Naur notation have been created; some are exactly defined, including extended Backus–Naur form (EBNF) and augmented Backus–Naur form (ABNF).\"],\n",
    "    \"doc8\": [\"doc8\", \"Spaceflight (or space flight) is an application of astronautics to fly objects, usually spacecraft, into or through outer space, either with or without humans on board. Most spaceflight is uncrewed and conducted mainly with spacecraft such as satellites in orbit around Earth, but also includes space probes for flights beyond Earth orbit. Such spaceflight operate either by telerobotic or autonomous control. The more complex human spaceflight has been pursued soon after the first orbital satellites and has reached the Moon and permanent human presence in space around Earth, particularly with the use of space stations. Human spaceflight programs include the Soyuz, Shenzhou, the past Apollo Moon landing and the Space Shuttle programs. Other current spaceflight are conducted to the International Space Station and to China's Tiangong Space Station.\"],\n",
    "    \"doc9\": [\"doc9\", \"Pippi Longstocking (Swedish: Pippi Långstrump) is the fictional main character in an eponymous series of children's books by Swedish author Astrid Lindgren. Pippi was named by Lindgren's daughter Karin, who asked her mother for a get-well story when she was off school.  Pippi is red-haired, freckled, unconventional and superhumanly strong – able to lift her horse one-handed. She is playful and unpredictable. She often makes fun of unreasonable adults, especially if they are pompous and condescending. Her anger comes out in extreme cases, such as when a man mistreats his horse. Pippi, like Peter Pan, does not want to grow up. She is the daughter of a buccaneer captain and has adventure stories to tell about that, too. Her four best friends are her horse and monkey, and the neighbours' children, Tommy and Annika.\"],\n",
    "    \"doc10\": [\"doc10\", \"Food processing is the transformation of agricultural products into food, or of one form of food into other forms. Food processing takes many forms, from grinding grain into raw flour, home cooking, and complex industrial methods used in the making of convenience foods. Some food processing methods play important roles in reducing food waste and improving food preservation, thus reducing the total environmental impact of agriculture and improving food security.\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126958 documents.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \".\"\n",
    "CORPUS_PATH = os.path.join(DATA_PATH, \"dataset_corpus.json\")\n",
    "\n",
    "# Load the dataset\n",
    "if os.path.exists(CORPUS_PATH):\n",
    "    with open(CORPUS_PATH, \"r\") as f:\n",
    "        corpus = json.load(f)\n",
    "else:\n",
    "    dataset = ir_datasets.load(\"ir-lab-sose-2024/ir-acl-anthology-20240504-training\")\n",
    "    corpus = dataset.docs_store().docs\n",
    "    with open(CORPUS_PATH, \"w\") as f:\n",
    "        json.dump(obj=corpus, fp=f, indent=2, ensure_ascii=False)\n",
    "    del dataset # Free space? or is this unnecessary??\n",
    "\n",
    "print(f\"{len(corpus)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create subset of corpus - only documents that appear in qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2405 relevant documents. (relevant to dev-set)\n"
     ]
    }
   ],
   "source": [
    "# Test corpus of only relevant document (+ a few nonrelevant)\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "relevant_docnos = dataset.get_qrels()[\"docno\"].unique()\n",
    "\n",
    "# some random choice of non-relevant docs added to corpus subset\n",
    "nonrelevant_docnos = list(corpus.keys() - set(relevant_docnos))\n",
    "\n",
    "nonrelevant_docnos = np.random.choice(nonrelevant_docnos, size=100)\n",
    "relevant_docnos = list(relevant_docnos) + list(nonrelevant_docnos)\n",
    "\n",
    "corpus = {k: corpus[k] for k in relevant_docnos}\n",
    "print(f\"{len(corpus)} relevant documents. (relevant to dev-set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### different corpus structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is originally a dict: {\"docno\": [\"docno\", \"text\"], }\n",
    "# now    like this:  records   [{\"docno\": \"docno\", \"text\": \"text\"}, ...] # easy to make a dataframe from this\n",
    "#     or like this:  list      [[\"docno\", \"text\"], ...] # same as original but as list\n",
    "#     or like this:  dict      {\"docno\": \"text\", ...} # i think this is easiest to handle.\n",
    "\n",
    "list_corpus = [v for v in corpus.values()]\n",
    "record_corpus = [{\"docno\": v[0], \"text\": v[1]} for v in corpus.values()]\n",
    "dict_corpus = {v[0]: v[1] for v in corpus.values()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen zum batchen, damit ich es nicht jedesmal umschreiben muss, wenn ich eine andere corpus struktur versuche\n",
    "def batch_dict_corpus(corpus, batch_size):\n",
    "    \"\"\" Hilfsfunktion um einen dict corpus {docno: [\"docno\", \"text\"], ...} zu batchen. \"\"\"\n",
    "    batches = []\n",
    "    corpus_keys = list(corpus.keys())\n",
    "    for anker in range(0, len(corpus), batch_size):\n",
    "        batch_keys = corpus_keys[anker:anker+batch_size]\n",
    "        batch = {k:corpus[k] for k in batch_keys}\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def batch_list_corpus(corpus, batch_size):\n",
    "    \"\"\" Hilfsfunktion um einen list corpus [[\"docno\", \"text\"], ...] oder [{\"docno\":\"docno\", \"text\":\"text\"}] zu batchen. \"\"\"\n",
    "    batches = [corpus[i:i+batch_size] for i in range(0, len(corpus), batch_size)]\n",
    "    return batches\n",
    "\n",
    "def batch_corpus(corpus, batch_size):\n",
    "    if type(corpus) == dict:\n",
    "        return batch_dict_corpus(corpus, batch_size)\n",
    "    elif type(corpus) == list:\n",
    "        return batch_list_corpus(corpus, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m----> 2\u001b[0m batched_corpus \u001b[38;5;241m=\u001b[39m batch_corpus(\u001b[43mdict_corpus\u001b[49m, \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batched_corpus)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "batched_corpus = batch_corpus(dict_corpus, 500)\n",
    "print(f\"{len(batched_corpus)} batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing\n",
    "\n",
    "TODO: \n",
    "- outlier entfernen?\n",
    "- split nach sätzen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common abbrevations in corpus and other small things to substitute\n",
    "abbrevations = {\n",
    "    \"e.g.\": \"for example\",\n",
    "    \"E.g.\": \"for example\",\n",
    "    \"U.S.\": \"united states\",\n",
    "    \"w.r.t.\": \"with respect to\",\n",
    "    \"i.e.\": \"that is\",\n",
    "    \"i.i.d.\": \"independent and identically distributed\",\n",
    "    \"i.i.\": \"independent and identically\",\n",
    "    \"v.s.\": \"versus\", \"vs.\": \"versus\",\n",
    "    \"etc.\": \"and so on\", #TODO: besser et cetera? oder ist das zu exotisch\n",
    "    \"1st\": \"first\", \"2nd\": \"second\", \"3rd\": \"third\", \"4th\": \"fourth\", \"5th\": \"fifth\",\n",
    "    \"e2e\": \"end-to-end\",\n",
    "    \"E2E\": \"end-to-end\",\n",
    "    \"iii)\": \"\", \"ii)\": \"\", \"i)\": \"\", \"iv)\": \"\", \"v)\": \"\",\n",
    "    \"?\": \".\", \"!\": \".\",\n",
    "    \"a)\": \"\", \"b)\": \"\", \"c)\": \"\", \"d)\": \"\", \"e)\": \"\"\n",
    "}\n",
    "\n",
    "# Very common letter-number-combinations that will not be substituted\n",
    "letter_number_exceptions = [\"L2\",\"F1\",\"L1\",\"F2\",\"seq2seq\",\"Seq2Seq\",\"word2vec\",\"Word2Vec\",\"2D\"]\n",
    "\n",
    "def preprocess_text(text, lower=False, years=False, percentages=False, numbers=False, \n",
    "                        letter_numbers=False, abbrev=False, special_characters=False):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Substitue Years\n",
    "    if years:\n",
    "        text = re.sub(r'\\b(19|20)\\d{2}\\b', 'YEAR', text)\n",
    "\n",
    "    # Substitue Percentages\n",
    "    if percentages:\n",
    "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%', \"PERCENTAGE\", text)\n",
    "\n",
    "    # Substitute all remaining numbers\n",
    "    if numbers:\n",
    "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', 'NUMBER', text)\n",
    "\n",
    "    # Remove words that are combinations of letters and numbers \n",
    "    # (except L2, F1, word2vec, ... common in corpus and probably important for context)\n",
    "    if letter_numbers:\n",
    "        #pattern = r'\\b(?!(L2|F1|L1|F2|seq2seq|word2vec|Seq2Seq|Word2Vec|2D)\\b)\\w*\\d+\\w*\\b'\n",
    "        pattern = rf'\\b(?!({\"|\".join(letter_number_exceptions)})\\b)\\w*\\d+\\w*\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Substitute most common abbrevations\n",
    "    if abbrev:\n",
    "        for abbrevation, substitution in abbrevations.items():\n",
    "            text = text.replace(abbrevation, substitution)\n",
    "\n",
    "    # Remove all characters that are not normal text\n",
    "    if special_characters:\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\-\\.\\,]', '', text)\n",
    "\n",
    "    # Punkt hinter Titel des papers setzen, falls bert genutzt wird, [SEP] token hinter titel setzten.?????\n",
    "    #text = re.sub(r'\\n\\n', \". \", text)\n",
    "    if len(text.split(\"\\n\\n\")) < 2:\n",
    "        text = re.sub(r'\\n\\n', \". \", text)  # if bert is used, sep is added at the end\n",
    "    else:\n",
    "        text = re.sub(r'\\n\\n', \". [SEP] \" if bert else \". \", text)\n",
    "\n",
    "    # Aufeinanderfolgende whitespaces durch einzelnes blank ersetzen.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting corpus into shorter passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpus(corpus, split_size, overlap, tokens=True):\n",
    "    \"\"\" corpus must be record_corpus \"\"\" # FIXME\n",
    "    split_corpus = []\n",
    "\n",
    "    for item in corpus:\n",
    "        docno = item[\"docno\"]\n",
    "        text = item[\"text\"]\n",
    "\n",
    "        if tokens: # split auf token-ebene\n",
    "            # preprocessing sollte vorher geschehen!\n",
    "            #text = preprocess_text(text, *([False]*7)) # shortcut for writing False seven times. I need to change the parameters...\n",
    "            text = text.split(\" \") # text becomes list of tokens\n",
    "        # else: split auf character-ebene -> string wird indiziert statt eine liste\n",
    "\n",
    "        split_text = [text[max(0,i-overlap):i+split_size] \n",
    "                        for i in range(0, len(text), split_size)]\n",
    "\n",
    "        for text_passage in split_text:\n",
    "            split_corpus.append({\"docno\": docno, \"text\": text_passage})\n",
    "\n",
    "    return split_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Schnitt 7 Korpus-Einträge pro dokument\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#record_corpus = [{\n",
    "#    \"docno\": item[\"docno\"],\n",
    "#    \"text\": preprocess_text(item[\"text\"], *([False]*7))\n",
    "#} for item in record_corpus ]\n",
    "\n",
    "token_split_size = 200\n",
    "token_overlap = 50\n",
    "splitted_corpus = split_corpus(record_corpus, token_split_size, token_overlap, tokens=False)\n",
    "\n",
    "print(f\"Im Schnitt {round(len(splitted_corpus)/len(corpus))} Korpus-Einträge pro dokument\")\n",
    "batched_corpus = batch_corpus(splitted_corpus, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### little fast analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little Dataset-text-length analysis: \n",
    "# TODO: make a \"train\"-dataset, that has outliers removed (too long /short docs)\n",
    "\n",
    "no_titles = []\n",
    "lengths = []\n",
    "for i, (docno, text) in enumerate(dict_corpus.items()):\n",
    "    if len(text.split(\"\\n\\n\")) < 2:\n",
    "        no_titles.append(docno)\n",
    "    else:\n",
    "        length = len(preprocess_text(text, *([True]*7)).split(\" \"))\n",
    "        lengths.append(length)\n",
    "\n",
    "print(\"n docs without abstract\", len(no_titles)) # too many\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "print(sum((lengths < 350)*(lengths > 50)))\n",
    "\n",
    "plt.hist([l for l in lengths if l < 350 and l > 50], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model / The Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: das hier nur schnelle lösung! umbedingt besser machen!!\n",
    "model_name_to_type_map = {\n",
    "    \"prajjwal1/bert-mini\": [BertModel, BertTokenizer],\n",
    "    \"microsoft/deberta-base\": [BertModel, BertTokenizer], # DebertaModel? oder sogar v2? was ist der unterschied??\n",
    "    \"intfloat/e5-small-v2\": [AutoModel, AutoTokenizer],\n",
    "    \"intfloat/e5-base-v2\": [AutoModel, AutoTokenizer],\n",
    "    \"thenlper/gte-small\": [AutoModel, AutoTokenizer],\n",
    "    \"thenlper/gte-base\": [AutoModel, AutoTokenizer],\n",
    "    \"olm/olm-roberta-base-dec-2022\": [RobertaModel, RobertaTokenizer],\n",
    "    \"distilbert-base-uncased\": [DistilBertModel, DistilBertTokenizer],\n",
    "    \"paraphrase-MiniLM-L6-v2\": [SentenceTransformer, None],\n",
    "    \"allenai/specter\": [AutoModel, AutoTokenizer],\n",
    "}\n",
    "\n",
    "def load_model(name):\n",
    "    try:\n",
    "        model_class, tokenizer_class = model_name_to_type_map[name]\n",
    "    except Exception as e:\n",
    "        print(f\"model {name} not defined yet. ({e})\")\n",
    "        return None\n",
    "\n",
    "    model = model_class.from_pretrained(name)\n",
    "    if tokenizer_class is not None:\n",
    "        tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    else:\n",
    "        tokenizer = None\n",
    "\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (TinyBERT or another) \n",
    "# TODO: model config (model: \"DeBerta\", and pretrained: \"microsoft/deberta-base\" )\n",
    "\n",
    "#model_name = \"prajjwal1/bert-mini\"\n",
    "#model_name = \"microsoft/deberta-base\" # ACHTUNG FEHLER BEI TOKENIZER! FIXME\n",
    "#model_name = 'intfloat/e5-base-v2' # add \"query: \" before queries and \"passage: \" before passages!\n",
    "#model_name = 'intfloat/e5-small-v2' # add \"query: \" before queries and \"passage: \" before passages!\n",
    "#model_name = \"thenlper/gte-small\"\n",
    "model_name = \"thenlper/gte-base\"\n",
    "#model_name = \"olm/olm-roberta-base-dec-2022\"  ## nicht so gut\n",
    "#model_name = 'allenai/specter' # Mit average=False benutzen! und FlatIP statt FlatL2! # Spezialisiert auf Scientific Papers\n",
    "\n",
    "model, tokenizer = load_model(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "# colbert model checkpoint\n",
    "checkpoint = 'downloads/colbertv2.0'\n",
    "doc_maxlen=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the embedding (of document corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    \"\"\" Calculates average pooling of hidden states (with attention mask) \"\"\"\n",
    "    # mask paddings with 0 -> ignore in average calculation\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0) \n",
    "    #last_hidden = last_hidden_states\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def encode(model, tokenizer, texts, max_length=512, avg_pool=False): # avg. doc length = 144 (after preprocessing only those with abstract.)\n",
    "    \"\"\" Encode texts with model \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        if avg_pool: \n",
    "            return average_pool(last_hidden_states, inputs[\"attention_mask\"])\n",
    "        else: # [CLS] embeddings\n",
    "            return last_hidden_states[:,0,:]  # cls embeddings\n",
    "    return None\n",
    "\n",
    "\n",
    "def encode_with_sentence_transformer(model, texts, prompt):\n",
    "    \"\"\" Encode texts with model \"\"\"\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(texts)#, prompt=\"Retrieve semantically relevant texts\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(corpus, model, tokenizer, batched=True, avg_pool=True, \n",
    "                     normalize=True, preprocess=True, **preprocess_params):\n",
    "    \"\"\" corpus is list-corpus \"\"\"\n",
    "    if not batched:\n",
    "        corpus = [corpus]\n",
    "    if type(corpus[0]) == list and type(corpus[0][0]) == list: # FIXME\n",
    "        print(\"WRONG CORPUS STRUCTURE; ONLY LIST-CORPUS OR RECORD-CORPUS ALLOWEDD!\")\n",
    "\n",
    "    embeddings = None # will be np.array of shape [num_docs, embedding_size]\n",
    "    docnos = []  # for embedding-vector index to docno translation\n",
    "    for j, batch in enumerate(corpus):\n",
    "        print(f\"\\rBatch {j+1:3d}/{len(corpus)} \", end=\"\")\n",
    "\n",
    "        if type(batch) == dict: # FIXME: unsauber!! irgendwie anders lösen, oder auf ein Format festlegen\n",
    "            docnos += list(batch.keys())\n",
    "            texts = list(batch.values())\n",
    "        elif type(batch) == list:\n",
    "            docnos += [i[\"docno\"] for i in batch]\n",
    "            texts = [i[\"text\"] for i in batch]\n",
    "\n",
    "        #if preprocess:\n",
    "        #    texts = [preprocess_text(t, **preprocess_params) for t in texts]\n",
    "\n",
    "        #if \"e5\" in model_name.lower():\n",
    "        #    texts = [\"passage: \"+t for t in texts]\n",
    "        \n",
    "        batch_embeddings = encode(model=model, tokenizer=tokenizer, texts=texts, avg_pool=avg_pool)\n",
    "\n",
    "        if embeddings is None:\n",
    "            embeddings = batch_embeddings\n",
    "        else:\n",
    "            embeddings = torch.concatenate([embeddings, batch_embeddings], dim=0)\n",
    "\n",
    "    if normalize:\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    return docnos, embeddings # TODO: yield docnos, embeddings!? -> speicherschonender?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the document corpus\n",
    "batched = True\n",
    "avg_pool=True\n",
    "normalize=False\n",
    "preprocess = False\n",
    "preprocess_params = {\n",
    "    \"lower\": True,\n",
    "    \"numbers\": True,\n",
    "    \"letter_numbers\": True,\n",
    "    \"abbrev\": True,\n",
    "    \"special_characters\": True,\n",
    "    \"bert\": False,\n",
    "}\n",
    "\n",
    "#docnos, embeddings = encode_documents(batched_corpus, model, tokenizer, batched=batched, normalize=normalize,\n",
    "#                                      avg_pool=avg_pool, preprocess=preprocess, **preprocess_params)\n",
    "#print(\"embeddings shape:\", embeddings.shape)\n",
    "\n",
    "with open(\"encoded_corpus/gte_base-vanilla_avgpool-docnos.txt\", \"r\") as f:\n",
    "    docnos = json.load(f)\n",
    "\n",
    "with open(\"encoded_corpus/gte_base-vanilla_avgpool-embeddings.npy\", \"rb\") as f:\n",
    "    embeddings = np.load(f)\n",
    "\n",
    "#embeddings = embeddings.cpu().numpy().astype(np.float32) # TODO: variante: gpu-index \n",
    "embedding_size = embeddings.shape[1]\n",
    "if np.isnan(embeddings).any():\n",
    "    print(\"WARNUNG: NaN-Werte in den Embeddings gefunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the index (faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index with embeddings (and docnos for indices)\n",
    "embedding_size = embeddings.shape[1]\n",
    "\n",
    "index_type = \"FlatIP\"\n",
    "#index_type = \"LSH\"\n",
    "#index_params = {\"d\":embedding_size, \"n_bits\": 4*embedding_size}\n",
    "\n",
    "if index_type == \"FlatIP\":\n",
    "    index = faiss.IndexFlatIP(embedding_size)\n",
    "elif index_type == \"FlatL2\":\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "else:\n",
    "    index = faiss.index_factory(embedding_size, index_type)#(**index_params) # TODO: wie kann man mit index factory parameter übergeben?????\n",
    "\n",
    "#index = faiss.IndexHNSWFlat(embedding_size,8) \n",
    "\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dir = \"./indexe\"\n",
    "index_name = \"tiny_bert-with_tokens.index\"\n",
    "faiss.write_index(index, os.path.join(index_dir, index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dir = \"./indexe\"\n",
    "index_name = \"ivf_10000_IP-gte_base.index\"\n",
    "index = faiss.read_index(os.path.join(index_dir, index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(embedding, index, k):\n",
    "    d, c = index.search(embedding, k) # distances, candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kleine hilfsfunktionen\n",
    "def distance2score(distances):\n",
    "    return np.exp(-distances)\n",
    "\n",
    "def softmax(scores):\n",
    "    return np.exp(scores) / sum(np.exp(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test-run (with mini-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"I like animals\",\n",
    "    \"Who builds Ferraris\",\n",
    "    \"What do kids like?\",\n",
    "    \"where is the moon\",\n",
    "#    \"how far away is the sun from the earth\",\n",
    "    \"what is nebukadnezar\",\n",
    "    \"i am interested in maths\",\n",
    "    \"is a hot-dog a sandwich\",\n",
    "    #\"What is the difference between Indian and African elephants\",\n",
    "]\n",
    "\n",
    "# for e5\n",
    "if preprocess:\n",
    "    queries = [preprocess_text(q, **preprocess_params) for q in queries]\n",
    "#queries = [\"querie: \"+q for q in queries]\n",
    "\n",
    "\n",
    "results_df = None\n",
    "for query in queries:\n",
    "    query_embedding = encode(model=model, tokenizer=tokenizer, texts=[query], avg_pool=avg_pool).cpu().numpy()\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    d, c = index.search(query_embedding, k=5) # d=distances, c=candidates\n",
    "    if index_type == \"FlatL2\":\n",
    "        d = distance2score(d)\n",
    "\n",
    "    results = sorted(list(zip(d[0], c[0])), key=lambda x: x[0], reverse=True)\n",
    "    results = [{\"docno\": docnos[candidate], \"score\": score, \"rank\": i+1, \"query\": query} \n",
    "                for i, (score,candidate) in enumerate(results)]\n",
    "\n",
    "    for item in results:\n",
    "        item_doc = item[\"docno\"]\n",
    "        item[\"doc-text\"] = dict_corpus[item_doc][:20]+\"...\"\n",
    "\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(results)\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(results)])\n",
    "\n",
    "#print(results_df)\n",
    "#display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>query</th>\n",
       "      <th>doc-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.69713</td>\n",
       "      <td>1</td>\n",
       "      <td>I like animals</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.68957</td>\n",
       "      <td>2</td>\n",
       "      <td>I like animals</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.67891</td>\n",
       "      <td>3</td>\n",
       "      <td>I like animals</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc2</td>\n",
       "      <td>0.66673</td>\n",
       "      <td>4</td>\n",
       "      <td>I like animals</td>\n",
       "      <td>Ants are eusocial in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc2</td>\n",
       "      <td>0.64393</td>\n",
       "      <td>5</td>\n",
       "      <td>I like animals</td>\n",
       "      <td>Ants are eusocial in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.65441</td>\n",
       "      <td>1</td>\n",
       "      <td>Who builds Ferraris</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.65358</td>\n",
       "      <td>2</td>\n",
       "      <td>Who builds Ferraris</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc5</td>\n",
       "      <td>0.63777</td>\n",
       "      <td>3</td>\n",
       "      <td>Who builds Ferraris</td>\n",
       "      <td>Dragon Ball (Japanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.63700</td>\n",
       "      <td>4</td>\n",
       "      <td>Who builds Ferraris</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc5</td>\n",
       "      <td>0.63136</td>\n",
       "      <td>5</td>\n",
       "      <td>Who builds Ferraris</td>\n",
       "      <td>Dragon Ball (Japanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.71507</td>\n",
       "      <td>1</td>\n",
       "      <td>What do kids like.</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.64492</td>\n",
       "      <td>2</td>\n",
       "      <td>What do kids like.</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc3</td>\n",
       "      <td>0.63979</td>\n",
       "      <td>3</td>\n",
       "      <td>What do kids like.</td>\n",
       "      <td>Volkswagen (VW) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.63977</td>\n",
       "      <td>4</td>\n",
       "      <td>What do kids like.</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.61041</td>\n",
       "      <td>5</td>\n",
       "      <td>What do kids like.</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc5</td>\n",
       "      <td>0.63431</td>\n",
       "      <td>1</td>\n",
       "      <td>where is the moon</td>\n",
       "      <td>Dragon Ball (Japanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1</td>\n",
       "      <td>0.62433</td>\n",
       "      <td>2</td>\n",
       "      <td>where is the moon</td>\n",
       "      <td>Elephants are the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.61448</td>\n",
       "      <td>3</td>\n",
       "      <td>where is the moon</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc8</td>\n",
       "      <td>0.61331</td>\n",
       "      <td>4</td>\n",
       "      <td>where is the moon</td>\n",
       "      <td>Spaceflight (or spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc3</td>\n",
       "      <td>0.60345</td>\n",
       "      <td>5</td>\n",
       "      <td>where is the moon</td>\n",
       "      <td>Volkswagen (VW) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc3</td>\n",
       "      <td>0.70398</td>\n",
       "      <td>1</td>\n",
       "      <td>what is nebukadnezar</td>\n",
       "      <td>Volkswagen (VW) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.69437</td>\n",
       "      <td>2</td>\n",
       "      <td>what is nebukadnezar</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc5</td>\n",
       "      <td>0.65882</td>\n",
       "      <td>3</td>\n",
       "      <td>what is nebukadnezar</td>\n",
       "      <td>Dragon Ball (Japanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc2</td>\n",
       "      <td>0.63883</td>\n",
       "      <td>4</td>\n",
       "      <td>what is nebukadnezar</td>\n",
       "      <td>Ants are eusocial in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.63209</td>\n",
       "      <td>5</td>\n",
       "      <td>what is nebukadnezar</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.69999</td>\n",
       "      <td>1</td>\n",
       "      <td>i am interested in maths</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc3</td>\n",
       "      <td>0.66116</td>\n",
       "      <td>2</td>\n",
       "      <td>i am interested in maths</td>\n",
       "      <td>Volkswagen (VW) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc6</td>\n",
       "      <td>0.65711</td>\n",
       "      <td>3</td>\n",
       "      <td>i am interested in maths</td>\n",
       "      <td>The Matrix is a 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.64762</td>\n",
       "      <td>4</td>\n",
       "      <td>i am interested in maths</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc5</td>\n",
       "      <td>0.63497</td>\n",
       "      <td>5</td>\n",
       "      <td>i am interested in maths</td>\n",
       "      <td>Dragon Ball (Japanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1</td>\n",
       "      <td>0.68282</td>\n",
       "      <td>1</td>\n",
       "      <td>is a hot-dog a sandwich</td>\n",
       "      <td>Elephants are the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.67326</td>\n",
       "      <td>2</td>\n",
       "      <td>is a hot-dog a sandwich</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc3</td>\n",
       "      <td>0.65863</td>\n",
       "      <td>3</td>\n",
       "      <td>is a hot-dog a sandwich</td>\n",
       "      <td>Volkswagen (VW) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc2</td>\n",
       "      <td>0.63066</td>\n",
       "      <td>4</td>\n",
       "      <td>is a hot-dog a sandwich</td>\n",
       "      <td>Ants are eusocial in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc9</td>\n",
       "      <td>0.62871</td>\n",
       "      <td>5</td>\n",
       "      <td>is a hot-dog a sandwich</td>\n",
       "      <td>Pippi Longstocking (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docno   score  rank                     query                 doc-text\n",
       "0  doc9 0.69713     1            I like animals  Pippi Longstocking (...\n",
       "1  doc9 0.68957     2            I like animals  Pippi Longstocking (...\n",
       "2  doc9 0.67891     3            I like animals  Pippi Longstocking (...\n",
       "3  doc2 0.66673     4            I like animals  Ants are eusocial in...\n",
       "4  doc2 0.64393     5            I like animals  Ants are eusocial in...\n",
       "0  doc6 0.65441     1       Who builds Ferraris  The Matrix is a 1999...\n",
       "1  doc6 0.65358     2       Who builds Ferraris  The Matrix is a 1999...\n",
       "2  doc5 0.63777     3       Who builds Ferraris  Dragon Ball (Japanes...\n",
       "3  doc6 0.63700     4       Who builds Ferraris  The Matrix is a 1999...\n",
       "4  doc5 0.63136     5       Who builds Ferraris  Dragon Ball (Japanes...\n",
       "0  doc9 0.71507     1        What do kids like.  Pippi Longstocking (...\n",
       "1  doc9 0.64492     2        What do kids like.  Pippi Longstocking (...\n",
       "2  doc3 0.63979     3        What do kids like.  Volkswagen (VW) is a...\n",
       "3  doc6 0.63977     4        What do kids like.  The Matrix is a 1999...\n",
       "4  doc6 0.61041     5        What do kids like.  The Matrix is a 1999...\n",
       "0  doc5 0.63431     1         where is the moon  Dragon Ball (Japanes...\n",
       "1  doc1 0.62433     2         where is the moon  Elephants are the la...\n",
       "2  doc9 0.61448     3         where is the moon  Pippi Longstocking (...\n",
       "3  doc8 0.61331     4         where is the moon  Spaceflight (or spac...\n",
       "4  doc3 0.60345     5         where is the moon  Volkswagen (VW) is a...\n",
       "0  doc3 0.70398     1      what is nebukadnezar  Volkswagen (VW) is a...\n",
       "1  doc6 0.69437     2      what is nebukadnezar  The Matrix is a 1999...\n",
       "2  doc5 0.65882     3      what is nebukadnezar  Dragon Ball (Japanes...\n",
       "3  doc2 0.63883     4      what is nebukadnezar  Ants are eusocial in...\n",
       "4  doc9 0.63209     5      what is nebukadnezar  Pippi Longstocking (...\n",
       "0  doc6 0.69999     1  i am interested in maths  The Matrix is a 1999...\n",
       "1  doc3 0.66116     2  i am interested in maths  Volkswagen (VW) is a...\n",
       "2  doc6 0.65711     3  i am interested in maths  The Matrix is a 1999...\n",
       "3  doc9 0.64762     4  i am interested in maths  Pippi Longstocking (...\n",
       "4  doc5 0.63497     5  i am interested in maths  Dragon Ball (Japanes...\n",
       "0  doc1 0.68282     1   is a hot-dog a sandwich  Elephants are the la...\n",
       "1  doc9 0.67326     2   is a hot-dog a sandwich  Pippi Longstocking (...\n",
       "2  doc3 0.65863     3   is a hot-dog a sandwich  Volkswagen (VW) is a...\n",
       "3  doc2 0.63066     4   is a hot-dog a sandwich  Ants are eusocial in...\n",
       "4  doc9 0.62871     5   is a hot-dog a sandwich  Pippi Longstocking (..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH SENTENCE TRANSFORMER\n",
    "queries = [ \"I like animals\", \"Who builds Ferraris\", \"What do kids like?\", \"where is the moon\", \"what is nebukadnezar\", \"maths is interesting\", \"is a hot-dog a sandwich\", ]\n",
    "queries = [preprocess_text(q, True, *([False]*5), True) for q in queries]\n",
    "\n",
    "results_df = None\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = encode_with_sentence_transformer(model, [query])\n",
    "    similarities = index.search(query_embedding, k=20) # d=distances, c=candidates # TODO: mit model den score berechnen \n",
    "\n",
    "    print(Counter(d[0]).most_common(2))\n",
    "    if index_type == \"FlatL2\":\n",
    "        d = distance2score(d)\n",
    "\n",
    "    results = sorted(list(zip(d[0], c[0])), key=lambda x: x[0], reverse=True)\n",
    "    results = [{\"docno\": docnos[candidate], \"score\": score, \"rank\": i+1, \"query\": query} \n",
    "                for i, (score,candidate) in enumerate(results)]\n",
    "\n",
    "    for item in results:\n",
    "        item_doc = item[\"docno\"]\n",
    "        item[\"doc-text\"] = dict_corpus[item_doc][:20]+\"...\"\n",
    "\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(results)\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(results)])\n",
    "\n",
    "#print(results_df)\n",
    "#display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = results_df.groupby(['query', 'docno']).size().reset_index(name='count')\n",
    "\n",
    "# Schritt 2: Ermitteln der häufigsten docno für jede query\n",
    "max_frequencies = frequencies.loc[frequencies.groupby('query')['count'].idxmax()]\n",
    "\n",
    "# Schritt 3: Zusammenführen des ursprünglichen DataFrame mit den häufigsten docno pro query\n",
    "result_df = pd.merge(results_df, max_frequencies[['query', 'docno']], on=['query', 'docno'])\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN with real corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the dataset queries\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "#queries = dataset.get_topics(variant=\"description\") # only the question / the user-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through queries and search the relevant documents\n",
    "\n",
    "#name = \"vanilla_mini-bert\"  # name of this run\n",
    "#name = \"mini-bert_with-tokens\"\n",
    "name = \"gte_base-vanilla-ivf_10000_IP\"\n",
    "\n",
    "run = []\n",
    "for i, row in enumerate(dataset.get_topics(variant=\"description\").to_dict(orient=\"records\")):\n",
    "    query = row[\"query\"]\n",
    "    #if preprocess:\n",
    "    #    query = preprocess_text(query, **preprocess_params)\n",
    "    # Encode the query\n",
    "    query_embedding = encode(model, tokenizer, [query]).cpu().numpy() # TODO: gpu variant\n",
    "    #query_embedding = query_embedding.astype(np.float32)  # brauch ich das wirklich für faiss???\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    # Search in the Index\n",
    "    scores, candidates = index.search(query_embedding, k=10)\n",
    "\n",
    "    # Case FlatL2 -> returns distances, lowest distance is best -> translate it to scores where highest is best.\n",
    "    #if index_type == \"FlatL2\":\n",
    "    #    scores = distance2score(scores)\n",
    "\n",
    "    # Ergebnisse sollten bereits sortiert sein, nur nochmal zur Sicherheit:\n",
    "    results = sorted(list(zip(scores[0], candidates[0])), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    for j, (score, candidate) in enumerate(results):\n",
    "        run.append({ \"qid\": row[\"qid\"], \"docno\": docnos[candidate],\n",
    "                     \"rank\": j+1, \"score\": score, \"name\": name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rundir = \"./runs\"\n",
    "\n",
    "runfile = os.path.join(rundir, name+\"_run.txt\")\n",
    "with open(runfile, \"w\") as f:\n",
    "    for item in run:\n",
    "        # schreibt die selben sachen, die persist_and_normalize_run() schreibt\n",
    "        f.write(f\"{item['qid']} 0 {item['docno']} {item['rank']} {item['score']} {name}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some baselines that were executed in TIRA\n",
    "bm25_baseline = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 (tira-ir-starter-pyterrier)', dataset)\n",
    "sparse_cross_encoder = tira.pt.from_submission('ir-benchmarks/fschlatt/sparse-cross-encoder-4-512', dataset)\n",
    "rank_zephyr = tira.pt.from_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>recall_100</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM 25 (Baseline)</td>\n",
       "      <td>0.37404</td>\n",
       "      <td>0.57988</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.26231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparse Cross Encoder</td>\n",
       "      <td>0.36646</td>\n",
       "      <td>0.61298</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.24126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RankZephyr</td>\n",
       "      <td>0.34707</td>\n",
       "      <td>0.56841</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.26749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gte_base-vanilla-ivf_10000_IP_run</td>\n",
       "      <td>0.10077</td>\n",
       "      <td>0.20662</td>\n",
       "      <td>0.06170</td>\n",
       "      <td>0.04235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mini-bert_with-tokens_run</td>\n",
       "      <td>0.06224</td>\n",
       "      <td>0.14412</td>\n",
       "      <td>0.02821</td>\n",
       "      <td>0.01344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mini_bert-full_preprocessing_run</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>0.15809</td>\n",
       "      <td>0.02533</td>\n",
       "      <td>0.01325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vanilla_mini-bert_run</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0.19789</td>\n",
       "      <td>0.02745</td>\n",
       "      <td>0.01708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  ndcg_cut.10  recip_rank  recall_100  \\\n",
       "0                   BM 25 (Baseline)      0.37404     0.57988     0.60133   \n",
       "1               Sparse Cross Encoder      0.36646     0.61298     0.60133   \n",
       "2                         RankZephyr      0.34707     0.56841     0.60133   \n",
       "3  gte_base-vanilla-ivf_10000_IP_run      0.10077     0.20662     0.06170   \n",
       "4          mini-bert_with-tokens_run      0.06224     0.14412     0.02821   \n",
       "5   mini_bert-full_preprocessing_run      0.05907     0.15809     0.02533   \n",
       "6              vanilla_mini-bert_run      0.07913     0.19789     0.02745   \n",
       "\n",
       "      map  \n",
       "0 0.26231  \n",
       "1 0.24126  \n",
       "2 0.26749  \n",
       "3 0.04235  \n",
       "4 0.01344  \n",
       "5 0.01325  \n",
       "6 0.01708  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# This assumes we have execited the ../baseline-retrieval-system/baseline-retrieval-system.ipynb notebook before.\n",
    "run_files = sorted(list(glob.glob(os.path.join(rundir, \"*.txt\"))))\n",
    "methods = [pt.io.read_results(run_file_path) for run_file_path in run_files]\n",
    "run_names = [name.split(\"/\")[-1].split(\".\")[0] for name in run_files]\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25_baseline, sparse_cross_encoder, rank_zephyr] + methods,\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\", \"map\"],\n",
    "    names=[\"BM 25 (Baseline)\", \"Sparse Cross Encoder\", \"RankZephyr\"]+run_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus\n",
    "del dataset\n",
    "del index\n",
    "del model\n",
    "del tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
