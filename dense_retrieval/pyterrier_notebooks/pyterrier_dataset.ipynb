{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier: Datasets, Indexes, Experiments\n",
    "\n",
    "This jupyter notebook serves as playground for getting to know the dataset (classes and contents), \n",
    "how indexes work, and how to run and evaluate experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab:\n",
    "#!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.precision\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: what is this exactly (in comarison to pyterrier datasets)\n",
    "#      when to use this, when pyterrier?\n",
    "from tira.third_party_integrations import ir_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Here I define some helper functions that will be used later to analyse the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Helper function for printing dicts\n",
    "def pp(obj):\n",
    "    print(json.dumps(obj, indent=2, ensure_ascii=False))\n",
    "\n",
    "# For printing attributes of object\n",
    "# exclude attributes that contain substrings specified in 'exclude'\n",
    "# filter attributes that contain substrings specified in 'include'\n",
    "def get_attrs(obj, exclude=None, include=None):\n",
    "    exclude = [] if exclude is None else exclude\n",
    "    if type(exclude) == str:\n",
    "        exclude = [exclude]\n",
    "    exclude += [\"__\"] # always exclude attrs like __str__ etc.\n",
    "\n",
    "    if type(include) == str:\n",
    "        include = [include]\n",
    "\n",
    "    attrs = dir(obj)\n",
    "    attrs = [a for a in attrs if \n",
    "                (all(e not in a for e in exclude)) and\n",
    "                (any(i in a.lower() for i in include) if include else True)]\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset  -  Corpus and Queries\n",
    "\n",
    "The dataset we use in the shared task contains a corpus of scientific papers (title + abstracts) from the fields of IR and NLP (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). -> Dataset is the union of the IR Anthology and the ACL Anthology\n",
    "\n",
    "It also contains queries on the corpus and the corresponding relevant documents.\n",
    "\n",
    "The dataset is a pyterrier dataset (genauer: ```pyterrier.datasets.IRDSDataset```) [source code of pyterrier.datasets](https://pyterrier.readthedocs.io/en/latest/_modules/pyterrier/datasets.html#Dataset.get_topics)\n",
    "\n",
    "\n",
    "The dataset has some methods that we might want to use:\n",
    "- ```get_corpus_iter()```: The document corpus as an iterator (```get_corpus()``` is not available with this dataset)\n",
    "- ```get_topics()```: The queries in a pandas DataFrame\n",
    "- ```get_qrels()```: The relevant documents for the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator for corpus\n",
    "corpus_iter = pt_dataset.get_corpus_iter(verbose=False) # verbose=True -> tqdm progress bar\n",
    "# corpus iter is a GeneratorLen object, returns dicts\n",
    "# has 2 attributes: length (number of documents in corpus) and gen (generator fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in corpus: 126958\n",
      "\n",
      "First element in corpus_iter:\n",
      "Document ID: ->O02-2002<-\n",
      "Document Text: ->A Study on Word Similarity using Context Vector Models\n",
      "\n",
      "\n",
      " There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.<-\n"
     ]
    }
   ],
   "source": [
    "print(\"number of documents in corpus:\", corpus_iter.length)\n",
    "\n",
    "try:\n",
    "    doc = next(corpus_iter.gen)  # doc is a dict with keys \"docno\" and \"text\"\n",
    "except StopIteration:\n",
    "    print(\"FEHLER: Keine Elemente im Iterator!\")\n",
    "\n",
    "print()\n",
    "print(\"First element in corpus_iter:\")\n",
    "print(f\"Document ID: ->{doc['docno']}<-\")  # unique id string\n",
    "print(f\"Document Text: ->{doc['text']}<-\") # text consisting of the title and (if available) the abstract of a paper\n",
    "# NOTE: I use the arrows '->','<-' to better identify empty strings / newlines / blanks in prints\n",
    "# NOTE: The generator returns a random document -> need to set seed in production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"O02-2002\",\n",
      "  \"abstract\": \" There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\",\n",
      "  \"title\": \"A Study on Word Similarity using Context Vector Models\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doc_json = {\"id\": doc[\"docno\"], \"abstract\":\"\"}\n",
    "for i, line in enumerate(doc[\"text\"].split(\"\\n\")):\n",
    "    if i == 0 and line:\n",
    "        doc_json[\"title\"] = line\n",
    "        continue\n",
    "    if not line: # empty line\n",
    "        continue\n",
    "    else: # Non empty lines belong to the abstract\n",
    "        # In case the abstract contains newlines -> concatenate lines TODO: are there newlines some of the abstracts?\n",
    "        doc_json[\"abstract\"] += line #or line.strip() TODO: -> could be bad if one line ends on a blank and the next line begins with a word?\n",
    "pp(doc_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will (try to) iterate over the dataset and apply for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doctext(doctext):\n",
    "    splitted = doctext.split(\"\\n\")\n",
    "    title = splitted.pop(0) # first line is always title\n",
    "    abstract = \"\".join(splitted) # following non-empty lines are abstract\n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents without an abstract: 39701/126958\n"
     ]
    }
   ],
   "source": [
    "corpus_iter = pt_dataset.get_corpus_iter(verbose=False) # verbose=True -> tqdm progress bar\n",
    "\n",
    "docnos = set()\n",
    "no_abstracts = {}\n",
    "# iterate over corpus\n",
    "for i, d in enumerate(corpus_iter):\n",
    "    docnos.add(d[\"docno\"])\n",
    "    title, abstract = split_doctext(d[\"text\"])\n",
    "    if not title:\n",
    "        raise Exception(f\"Document does not have a title!!! docno: {d['docno']}\")\n",
    "    if not abstract:\n",
    "        no_abstracts[d[\"docno\"]] = title\n",
    "\n",
    "print(f\"Number of documents without an abstract: {len(no_abstracts)}/{corpus_iter.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the queries\n",
    "Pyterrier Dataset has a function ```get_topics()``` which returns a dataframe of the topics aka queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "columns: ['qid', 'text', 'title', 'query', 'description', 'narrative']\n",
      "number of topics/queries: 68\n",
      "\n",
      "  qid                                                query\n",
      "0   1             retrieval system improving effectiveness\n",
      "1   2             machine learning language identification\n",
      "2   3                        social media detect self harm\n",
      "3   4                        stemming for arabic languages\n",
      "4   5                       audio based animal recognition\n",
      "5   6                comparison different retrieval models\n",
      "6   7                                   cache architecture\n",
      "7   8                             document scoping formula\n",
      "8   9                            pseudo relevance feedback\n",
      "9  10  how to represent natural conversations in word nets\n",
      "  qid                                                                                            query\n",
      "0   1                           what papers focus on improving the effectiveness of a retrieval system\n",
      "1   2                               what papers are about machine learning for language identification\n",
      "2   3         which papers focus on how to recognize signs of self harm in people s social media posts\n",
      "3   4                                     which papers focus on improving stemming in arabic languages\n",
      "4   5                                        which papers focus on classifying animal species by sound\n",
      "5   6           which different retrieval models exist and what are their similarities and differences\n",
      "6   7                                 how are caches composed and how does their architecture function\n",
      "7   8                                               how was the formula for document scoping developed\n",
      "8   9                                               how does pseudo relevance feedback improve a query\n",
      "9  10  how is it possible to filter and classify information that can be found in chats and interviews\n",
      "\n",
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "Example of the first topic/query:\n",
      " query id   : 1\n",
      " title      : retrieval system improving effectiveness\n",
      " description: Relevant papers include research on what makes a retrieval system effective and what impro...\n",
      " query      : What papers focus on improving the effectiveness of a retrieval system?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "queries = pt_dataset.get_topics()\n",
    "print(\"columns:\", list(queries.columns))\n",
    "print(\"number of topics/queries:\", len(queries))\n",
    "print()\n",
    "\n",
    "# get_topics() can be called passing the column-name/key-name you want to have:\n",
    "queries = pt_dataset.get_topics(variant=\"title\")\n",
    "print(queries.head(10).to_string())\n",
    "queries = pt_dataset.get_topics(variant=\"description\")\n",
    "print(queries.head(10).to_string())\n",
    "print()\n",
    "\n",
    "# Example of first topic\n",
    "topic = pt_dataset.get_topics().iloc[0]\n",
    "print(\"Example of the first topic/query:\")\n",
    "print(f\" query id   : {topic['qid']}\")\n",
    "print(f\" title      : {topic['title']}\") # topic name (topic[\"text\"] is the same as topic[\"title\"])\n",
    "print(f\" description: {topic['narrative'][:90]}...\") # the description of the query\n",
    "print(f\" query      : {topic['description']}\") # is the user query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The groundtruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 2623\n",
      "columns: ['qid', 'docno', 'label', 'iteration']\n",
      "on average 39 labels per query id\n",
      "\n",
      "Label Statistics: (per qid)\n",
      "       n_labels    n_relev  relev_ratio\n",
      "min   13.000000   0.000000     0.000000\n",
      "max   50.000000  43.000000     1.000000\n",
      "mean  38.573529  19.911765     0.509098\n",
      "std    7.016646  11.137854     0.258363\n",
      "\n",
      "Document Statistics: (per docno)\n",
      "        n_qids   n_relev\n",
      "min   1.000000  0.000000\n",
      "max   6.000000  3.000000\n",
      "mean  1.137961  0.587419\n",
      "std   0.435077  0.570786\n",
      "number of documents in qrels with only label 0: 1037\n"
     ]
    }
   ],
   "source": [
    "# Groundtruths - relevant documents for queries\n",
    "qrels = pt_dataset.get_qrels()\n",
    "qrels[\"label\"] = qrels[\"label\"].astype(int) # label in the dataframe is string ('0' or '1')\n",
    "\n",
    "print(\"length:\", len(qrels))\n",
    "print(\"columns:\", list(qrels.columns))\n",
    "print(f\"on average {round(len(qrels) / len(qrels['qid'].unique()))} labels per query id\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Label Statistics: (per qid)\")\n",
    "results_df = qrels.groupby(\"qid\").agg(\n",
    "    n_labels=(\"label\", \"size\"),\n",
    "    n_relev=(\"label\", \"sum\"),\n",
    "    relev_ratio=(\"label\", lambda x: x.sum() / x.size if x.size > 0 else 0)\n",
    ").reset_index()\n",
    "#print(results_df)\n",
    "print(results_df[[\"n_labels\", \"n_relev\", \"relev_ratio\"]].agg( [\"min\", \"max\", \"mean\", \"std\"]))\n",
    "print()\n",
    "\n",
    "print(\"Document Statistics: (per docno)\")\n",
    "results_df = qrels.groupby(\"docno\").agg(\n",
    "    n_qids=(\"qid\", \"size\"),  # number of queries a document occurs in labels\n",
    "    n_relev=(\"label\", \"sum\") # number of times it was relevant\n",
    ").reset_index()\n",
    "#print(results_df)\n",
    "print(results_df[[\"n_qids\", \"n_relev\"]].agg( [\"min\", \"max\", \"mean\", \"std\"]))\n",
    "print(\"number of documents in qrels with only label 0:\", (results_df[\"n_relev\"] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was bedeutet die spalte iteration? alle werte sind 0\n",
    "\n",
    "**Label statistics**\n",
    "- 1 or more labels have no relevant document.\n",
    "- 1 or more labels have only relevant documents.\n",
    "\n",
    "-> how to interpret those? \n",
    "\n",
    "filter out queries without relevant documents? but there are only 68 queries\n",
    "\n",
    "<!--Aus den Statistiken geht hervor, dass es sowohl (1 oder mehr) queries gibt, zu dem kein einziges dokument relevant ist, und auch (1 oder mehr) queries gibt wo jedes document relevant ist.\n",
    "\n",
    "Frage ist, sollte man diese aussortieren? bei nur 68 queries werden es dann noch weniger, ist das sinnvoll? -->\n",
    "\n",
    "**Document statistics**\n",
    "- every document occurs in the qrels\n",
    "- there are documents that are not relevant for any query\n",
    "- half of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "First Dataset:\n",
      " columns: ['qid', 'text', 'title', 'query', 'description', 'narrative']\n",
      " length:  68\n",
      "Second Dataset:\n",
      " columns: ['qid', 'docno', 'label', 'iteration']\n",
      " length:  2623\n"
     ]
    }
   ],
   "source": [
    "topicsqrels = pt_dataset.get_topicsqrels() # returns tuple: (DataFrame, DataFrame)\n",
    "# first Dataframe is the topics (just like get_topics())\n",
    "# second Dataframe is the qrels (just like get_qrels())\n",
    "\n",
    "df1, df2 = topicsqrels\n",
    "\n",
    "print(\"First Dataset:\")\n",
    "print(\" columns:\", list(df1.columns))\n",
    "print(\" length: \", len(df1))\n",
    "\n",
    "print(\"Second Dataset:\")\n",
    "print(\" columns:\", list(df2.columns))\n",
    "print(\" length: \", len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to ```ir_datasets```\n",
    "\n",
    "\n",
    "There is another variant to get the dataset: from tira.ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(\"ir-lab-sose-2024/ir-acl-anthology-20240504-training\")\n",
    "#print(\"IR Lab dataset attrs:\\n\", [i for i in dir(dataset) if \"__\" not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Corpus\n",
    "docs_store = dataset.docs_store()\n",
    "\n",
    "# WHAT IS THIS???\n",
    "#lookup = docs.lookup(\"docid\")\n",
    "#lookup_iter = docs.lookup_iter(\"DOCID\")\n",
    "\n",
    "#print([i for i in dir(lookup_iter) if \"__\" not in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id O02-2002\n",
      "text: A Study on Word Similarity using Context Vector Models\n",
      "\n",
      "\n",
      " There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\n"
     ]
    }
   ],
   "source": [
    "#print(dataset.docs_count()) # really slow?\n",
    "#print(dataset.docs_cls()) #  <class 'ir_datasets.formats.base.GenericDoc'> \n",
    "\n",
    "docs_iter = dataset.docs_iter()  # this is almost the same as get_corpus iter of the pt_dataset\n",
    "for doc in docs_iter: \n",
    "    print(\"id\", doc.doc_id)\n",
    "    print(\"text:\", doc.text)\n",
    "    break\n",
    "\n",
    "# document corpus\n",
    "docs_store = dataset.docs_store() # of type <class 'tira.ir_datasets_util.DictDocsstore'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of docs_store: ['docs', 'get', 'get_many_iter']\n",
      "doc_id: O02-2002\n",
      "text: A Study on Word Similarity using Context Vector Models\n",
      "\n",
      "\n",
      " There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\n",
      "----------------------------------------\n",
      "L02-1310\n",
      "Bootstrapping Large Sense Tagged Corpora\n",
      "--------------------\n",
      "R13-1042\n",
      "Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads\n",
      "\n",
      "\n",
      " Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.\n",
      "--------------------\n",
      "W05-0819\n",
      "Aligning Words in {E}nglish-{H}indi Parallel Corpora\n",
      "\n",
      "\n",
      " In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Attributes of docs_store:\", [i for i in dir(docs_store) if \"__\" not in i])\n",
    "# -> ['docs', 'get', 'get_many_iter']\n",
    "\n",
    "# DOCS\n",
    "# docs are the same docs like in pt_dataset as dict\n",
    "docs = docs_store.docs # dict keys are doc_ids, values are GenercDoc objects\n",
    "#for i, (k,v) in enumerate(docs.items()):\n",
    "#    print(\"doc_id of first document:\", k) \n",
    "#    print(\"document obj:\", v)\n",
    "#    break\n",
    "\n",
    "# GET()\n",
    "document = docs_store.get(\"O02-2002\") # class GenericDoc (like above)\n",
    "print(\"doc_id:\", document.doc_id)\n",
    "print(\"text:\", document.text)\n",
    "\n",
    "# GET_MANY_ITER()\n",
    "many_ids = [\"L02-1310\", \"R13-1042\", \"W05-0819\"]\n",
    "docs_iter = docs_store.get_many_iter(many_ids)\n",
    "print(\"----------------------------------------\")\n",
    "for doc_id, doc_text in docs_iter:\n",
    "    print(doc_id)\n",
    "    print(doc_text)\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents without an abstract: 39701/126958\n"
     ]
    }
   ],
   "source": [
    "# Is there the same amount of missing abstracts??\n",
    "\n",
    "docs_iter = dataset.docs_iter()\n",
    "\n",
    "no_abstracts = {}\n",
    "# almost same code like for pt_dataset corpus\n",
    "for i, d in enumerate(docs_iter):\n",
    "    docnos.add(d.doc_id)\n",
    "    title, abstract = split_doctext(d.text)\n",
    "    if not title:\n",
    "        raise Exception(f\"Document does not have a title!!! docno: {d.doc_id}\")\n",
    "    if not abstract:\n",
    "        no_abstracts[d.doc_id] = title\n",
    "\n",
    "print(f\"Number of documents without an abstract: {len(no_abstracts)}/{len(corpus_iter)}\")\n",
    "\n",
    "# Answer: yes, the documents seem to be exaclty the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description: What papers focus on improving the effectiveness of a retrieval system?, type: str\n",
      "narrative: Relevant papers include research on what makes a retrieval system effective and what improves the effectiveness of a retrieval system. Papers that focus on improving something else or improving the effectiveness of a system that is not a retrieval system are not relevant., type: str\n",
      "query: retrieval system improving effectiveness, type: str\n",
      "query_id: 1, type: str\n",
      "text: retrieval system improving effectiveness, type: str\n",
      "title: retrieval system improving effectiveness, type: str\n"
     ]
    }
   ],
   "source": [
    "#print(dataset.queries_cls()) # <class 'tira.ir_datasets_util.TirexQuery'>\n",
    "# Attributes of TirexQuery: ['count', 'default_text', 'description', 'index', 'narrative', 'query', 'query_id', 'text', 'title']\n",
    "queries_iter = dataset.queries_iter()\n",
    "\n",
    "for i, q in enumerate(queries_iter): # q is TirexQuery\n",
    "    for attr in ['description', 'narrative', 'query', 'query_id', 'text', 'title']:\n",
    "        print(f\"{attr}: {getattr(q, attr)}, type: {type(getattr(q, attr)).__name__}\")\n",
    "    break\n",
    "# THis is exactly the same as above:\n",
    "# - q.description is the user query\n",
    "# - q.text, q.title, q.query are exactly the same -> the topic\n",
    "# - q.narrative is the description/explanation of the query\n",
    "\n",
    "# Extra methods:\n",
    "# - default_text()   returns the title (aka q.text)\n",
    "# - count( int )     I dont know what that does\n",
    "# - index( arg )     I dont know what that does\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Groundtruths (qrels - Relevant Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrecQrel(query_id='1', doc_id='2005.ipm_journal-ir0volumeA41A1.7', relevance=1, iteration='0')\n"
     ]
    }
   ],
   "source": [
    "#og_qrels = dataset.original_qrels # = None\n",
    "#qrels = dataset.qrels   # Dataset object like dataset.docs, dataset.queries\n",
    "#qrels_cls = dataset.qrels_cls() # <class 'ir_datasets.formats.trec.TrecQrel'>\n",
    "#qrels_defs = dataset.qrels_defs() # empty dict/set\n",
    "\n",
    "qrels_iter = dataset.qrels_iter()\n",
    "for item in qrels_iter: # item of type TrecQrel with attributes [query_id, doc_id, relevance, iteration]\n",
    "    print(item)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus als JSON Speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tira_dataset = ir_datasets.load(\"ir-lab-sose-2024/ir-acl-anthology-20240504-training\")\n",
    "docs_store = tira_dataset.docs_store()\n",
    "\n",
    "corpus = docs_store.docs\n",
    "with open(\"dataset_corpus.json\", \"w\") as f:\n",
    "    json.dump(corpus, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and Retrieval\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Index\n",
    "\n",
    "The reference for the index: [Class Index](http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html)\n",
    "\n",
    "A good tutorial/introduction to pyterrier index: [ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial/blob/main/notebooks/notebook1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jnius.reflect.org.terrier.structures.Index'>\n"
     ]
    }
   ],
   "source": [
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "print(type(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* getClass:\n",
      "* getCollectionStatistics:\n",
      "* getDirectIndex:\n",
      "* getDocumentIndex:\n",
      "* getEnd:\n",
      "* getIndexRef:\n",
      "* getInvertedIndex:\n",
      "* getLexicon:\n",
      "* getMetaIndex:\n",
      "* getStart:\n",
      "Class: org.terrier.structures.IndexOnDisk\n"
     ]
    }
   ],
   "source": [
    "# just to have the list visually, (getStructure ... doesnt work)\n",
    "for attr_name in get_attrs(index, include=\"get\"):\n",
    "    print(f\"* {attr_name}:\")\n",
    "    #print(f\"{getattr(index, attr_name)()}\")  # attrs are all getters\n",
    "print(index.getClass()) # org.terrier.structures.IndexOnDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Statistics:\n",
      " number of documents: 126958\n",
      " average document length: 64.02700893208778\n",
      " number of tokens: 8128741\n",
      " number of unique terms: 97223\n",
      " number of fields: 126958\n",
      " field names: ['text']\n",
      " number of pointers: 5471851\n",
      " number of postings: 5471851\n",
      " has positions: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection_statistics = index.getCollectionStatistics()\n",
    "#print(get_attrs(collection_statistics))\n",
    "#print(collection_statistics.toString())\n",
    "\n",
    "print(\"Collection Statistics:\")\n",
    "print(\" number of documents:\", collection_statistics.getNumberOfDocuments())\n",
    "print(\" average document length:\", collection_statistics.getAverageDocumentLength())\n",
    "\n",
    "print(\" number of tokens:\", collection_statistics.getNumberOfTokens())\n",
    "print(\" number of unique terms:\", collection_statistics.getNumberOfUniqueTerms())\n",
    "\n",
    "# There is only one field \"text\" -> same numbers as above with documents and tokens.\n",
    "print(\" number of fields:\", collection_statistics.getNumberOfDocuments())\n",
    "#print(\" average field lengths:\", collection_statistics.getAverageFieldLengths())\n",
    "print(\" field names:\", collection_statistics.getFieldNames())\n",
    "#print(\" field tokens:\", collection_statistics.getFieldTokens())\n",
    "\n",
    "# What is this??? \n",
    "print(\" number of pointers:\", collection_statistics.getNumberOfPointers())\n",
    "print(\" number of postings:\", collection_statistics.getNumberOfPostings())\n",
    "print(\" has positions:\", collection_statistics.hasPositions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#direct_index = index.getDirectIndex() # Before I dont understand postings this is not useful/interesting\n",
    "# direct_index.getPostings(  pointer  )  # -> needs pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n documents: 126958\n",
      "doc entry 0: <org.terrier.structures.DocumentIndexEntry at 0x7fff49af04f0 jclass=org/terrier/structures/DocumentIndexEntry jself=<LocalRef obj=0x5555583b64e8 at 0x7fff4cc343b0>>\n",
      "doc len: 111\n",
      "doc length: 111\n",
      "file number: 0\n",
      "n entries: 66\n"
     ]
    }
   ],
   "source": [
    "document_index = index.getDocumentIndex()\n",
    "#print(get_attrs(document_index))\n",
    "#['getDocumentEntry', 'getDocumentLength', 'getNumberOfDocuments']\n",
    "\n",
    "number_of_docs = document_index.getNumberOfDocuments()\n",
    "print(\"n documents:\", number_of_docs)\n",
    "doc_entry = document_index.getDocumentEntry(0)\n",
    "print(\"doc entry 0:\", doc_entry)\n",
    "doc_len = document_index.getDocumentLength(0)\n",
    "print(\"doc len:\", doc_len)\n",
    "\n",
    "#print(get_attrs(doc_entry))\n",
    "#['getDocumentLength', 'getFileNumber', 'getNumberOfEntries']\n",
    "print(\"doc length:\", doc_entry.getDocumentLength())\n",
    "print(\"file number:\", doc_entry.getFileNumber()) \n",
    "print(\"n entries:\", doc_entry.getNumberOfEntries())\n",
    "\n",
    "# I dont really understand how to work with this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jnius.reflect.org.terrier.querying.IndexRef'>\n",
      "<org.terrier.querying.IndexRef at 0x7fffd388f740 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x5555583b6410 at 0x7fff4cc35fb0>>\n",
      "['_class', 'clone', 'equals', 'finalize', 'getClass', 'hashCode', 'location', 'notify', 'notifyAll', 'of', 'registerNatives', 'serialVersionUID', 'size', 'toString', 'wait']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_ref = index.getIndexRef()\n",
    "print(type(index_ref))\n",
    "print(index_ref)\n",
    "\n",
    "print(get_attrs(index_ref))\n",
    "\n",
    "# I dont understand this either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon\n",
    "\n",
    "A lexicon (a.k.a. dictionary, vocabulary) typically represents the list of terms in the index, \n",
    "together with their statistics (EntryStatistics) and the pointer (Pointer) \n",
    "to the offset of that term's postings in the PostingIndex returned by Index.getInvertedIndex(). \n",
    "The EntryStatistics and Pointer are combined in a single LexiconEntry object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in lexicon: 97223\n",
      "angora (str) -> term72311 Nt=1 TF=1 maxTF=2147483647 @{0 635219 0} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "angorn (str) -> term37097 Nt=1 TF=1 maxTF=2147483647 @{0 635223 4} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "angri (str) -> term13025 Nt=26 TF=33 maxTF=2147483647 @{0 635227 4} TFf=33 (org.terrier.structures.LexiconEntry)\n",
      "angrier (str) -> term13916 Nt=1 TF=1 maxTF=2147483647 @{0 635304 0} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "angstrom (str) -> term13140 Nt=1 TF=1 maxTF=2147483647 @{0 635307 4} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "angu (str) -> term67919 Nt=1 TF=1 maxTF=2147483647 @{0 635311 0} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "anguag (str) -> term526 Nt=140 TF=147 maxTF=2147483647 @{0 635315 2} TFf=147 (org.terrier.structures.LexiconEntry)\n",
      "anguagc (str) -> term67574 Nt=1 TF=1 maxTF=2147483647 @{0 635640 2} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "anguao (str) -> term62024 Nt=1 TF=1 maxTF=2147483647 @{0 635644 4} TFf=1 (org.terrier.structures.LexiconEntry)\n",
      "angular (str) -> term13148 Nt=15 TF=17 maxTF=2147483647 @{0 635648 6} TFf=17 (org.terrier.structures.LexiconEntry)\n",
      "anh (str) -> term73424 Nt=1 TF=3 maxTF=2147483647 @{0 635700 0} TFf=3 (org.terrier.structures.LexiconEntry)\n",
      "term13148 Nt=15 TF=17 maxTF=2147483647 @{0 635648 6} TFf=17\n",
      "term32530 Nt=10 TF=12 maxTF=2147483647 @{0 8089973 4} TFf=12\n"
     ]
    }
   ],
   "source": [
    "lexicon = index.getLexicon()\n",
    "\n",
    "print(\"number of entries in lexicon:\", lexicon.numberOfEntries())\n",
    "\n",
    "# Iterate over lexicon\n",
    "for i, kv in enumerate(index.getLexicon()):\n",
    "    if i < 9000: continue # first entries are just numbers\n",
    "    if i > 9010: break # print 10 entries\n",
    "    print(f\"{kv.getKey()} ({type(kv.getKey()).__name__}) -> {kv.getValue()} ({type(kv.getValue()).__name__})\")\n",
    "\n",
    "# Lexicon entries have statistics (Nt -> number of documents with term, TF total ocurrences, @{} are pointers)\n",
    "\n",
    "# lookup terms with bracket notation\n",
    "entry = lexicon[\"angular\"].toString()\n",
    "print(entry)\n",
    "\n",
    "# lookup terms with getLexiconEntry\n",
    "entry = lexicon.getLexiconEntry(\"tomato\")\n",
    "print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21341,2,F[2]) doclen=231 ->  ID: 21341 frequency: 2)\n",
      "(26729,1,F[1]) doclen=63 ->  ID: 26729 frequency: 1)\n",
      "(35901,1,F[1]) doclen=144 ->  ID: 35901 frequency: 1)\n",
      "(36834,1,F[1]) doclen=78 ->  ID: 36834 frequency: 1)\n",
      "(39721,1,F[1]) doclen=129 ->  ID: 39721 frequency: 1)\n",
      "(52494,1,F[1]) doclen=72 ->  ID: 52494 frequency: 1)\n",
      "(53515,1,F[1]) doclen=167 ->  ID: 53515 frequency: 1)\n",
      "(55481,2,F[2]) doclen=133 ->  ID: 55481 frequency: 2)\n",
      "(88962,1,F[1]) doclen=70 ->  ID: 88962 frequency: 1)\n",
      "(95102,1,F[1]) doclen=7 ->  ID: 95102 frequency: 1)\n"
     ]
    }
   ],
   "source": [
    "inverted_index = index.getInvertedIndex()\n",
    "pointer = lexicon[\"tomato\"]\n",
    "\n",
    "for posting in inverted_index.getPostings(pointer):\n",
    "    print(f\"{posting.toString()} doclen={posting.getDocumentLength()} \" \\\n",
    "          f\"->  ID: {posting.getId()} frequency: {posting.getFrequency()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E14-1071', '2020.acl-tutorials.6', 'W17-7515', 'D18-1474', 'D19-1596', 'D14-1059', '2021.eacl-main.229', '2021.emnlp-main.734', '2020.wsdm_conference-2020.112', '2020.cikm_conference-2020.181']\n",
      "-----\n",
      "['E14-1071', '2020.acl-tutorials.6', 'W17-7515', 'D18-1474', 'D19-1596', 'D14-1059', '2021.eacl-main.229', '2021.emnlp-main.734', '2020.wsdm_conference-2020.112', '2020.cikm_conference-2020.181']\n"
     ]
    }
   ],
   "source": [
    "# Docnos for the index doc-ids available with MetaIndex\n",
    "meta_index = index.getMetaIndex()\n",
    "#metadata_keys = meta_index.getKeys()\n",
    "#print(metadata_keys)\n",
    "\n",
    "# LexiconEntry is the pointer for where to find postings for that term in the inverted index\n",
    "entry = lexicon[\"tomato\"]\n",
    "\n",
    "postings = inverted_index.getPostings(entry)\n",
    "docnos = []\n",
    "for posting in postings:\n",
    "    doc_id = posting.getId()\n",
    "    docnos.append(meta_index.getItem(\"docno\", doc_id))   #getItem\n",
    "print(docnos)\n",
    "print(\"-----\")\n",
    "\n",
    "# NOTE: postings object changes after iterating over it!! -> need to assign it again\n",
    "postings = inverted_index.getPostings(entry)\n",
    "doc_ids = [posting.getId() for posting in postings]\n",
    "docnos = meta_index.getItems(\"docno\", doc_ids)  # getItems - possible is also getItems([\"docno\", \"other\", ...], doc_ids)\n",
    "print(docnos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21341</td>\n",
       "      <td>E14-1071</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55481</td>\n",
       "      <td>2021.emnlp-main.734</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26729</td>\n",
       "      <td>2020.acl-tutorials.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35901</td>\n",
       "      <td>W17-7515</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>36834</td>\n",
       "      <td>D18-1474</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>39721</td>\n",
       "      <td>D19-1596</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>52494</td>\n",
       "      <td>D14-1059</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>53515</td>\n",
       "      <td>2021.eacl-main.229</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>88962</td>\n",
       "      <td>2020.wsdm_conference-2020.112</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>95102</td>\n",
       "      <td>2020.cikm_conference-2020.181</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                          docno  rank  score   query\n",
       "0   1  21341                       E14-1071     0    2.0  tomato\n",
       "1   1  55481            2021.emnlp-main.734     1    2.0  tomato\n",
       "2   1  26729           2020.acl-tutorials.6     2    1.0  tomato\n",
       "3   1  35901                       W17-7515     3    1.0  tomato\n",
       "4   1  36834                       D18-1474     4    1.0  tomato\n",
       "5   1  39721                       D19-1596     5    1.0  tomato\n",
       "6   1  52494                       D14-1059     6    1.0  tomato\n",
       "7   1  53515             2021.eacl-main.229     7    1.0  tomato\n",
       "8   1  88962  2020.wsdm_conference-2020.112     8    1.0  tomato\n",
       "9   1  95102  2020.cikm_conference-2020.181     9    1.0  tomato"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singe-word search:\n",
    "br = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "br.search(\"tomato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid                                      docno  rank  score  \\\n",
      "0   q1   21341                                   E14-1071     0    2.0   \n",
      "1   q1   55481                        2021.emnlp-main.734     1    2.0   \n",
      "2   q1   26729                       2020.acl-tutorials.6     2    1.0   \n",
      "3   q1   35901                                   W17-7515     3    1.0   \n",
      "4   q1   36834                                   D18-1474     4    1.0   \n",
      "5   q1   39721                                   D19-1596     5    1.0   \n",
      "6   q1   52494                                   D14-1059     6    1.0   \n",
      "7   q1   53515                         2021.eacl-main.229     7    1.0   \n",
      "8   q1   88962              2020.wsdm_conference-2020.112     8    1.0   \n",
      "9   q1   95102              2020.cikm_conference-2020.181     9    1.0   \n",
      "10  q2   67458                           2021.paclic-1.47     0    2.0   \n",
      "11  q2   96877              2013.cikm_conference-2013.170     1    2.0   \n",
      "12  q2    5040                          2022.acl-long.336     2    1.0   \n",
      "13  q2   14897                         2020.repl4nlp-1.12     3    1.0   \n",
      "14  q2   32312                            2020.lrec-1.222     4    1.0   \n",
      "15  q2   42706                                   W18-3012     5    1.0   \n",
      "16  q2   42736                          2021.acl-long.391     6    1.0   \n",
      "17  q2   79736         2011.sigirconf_conference-2011.234     7    1.0   \n",
      "18  q2   90060                2019.airs_conference-2019.4     8    1.0   \n",
      "19  q2   92206              2019.ecir_conference-20192.32     9    1.0   \n",
      "20  q2   94636              2009.cikm_conference-2009.163    10    1.0   \n",
      "21  q2   96242              2010.cikm_conference-2010.180    11    1.0   \n",
      "22  q2  101028              2004.spire_conference-2004.16    12    1.0   \n",
      "23  q2  106804          2018.wwwconf_conference-2018c.168    13    1.0   \n",
      "24  q2  113864  2017.wwwjournals_journal-ir0volumeA20A5.1    14    1.0   \n",
      "25  q3    7093                                   W15-2813     0    1.0   \n",
      "26  q3   15984                                   W15-0107     1    1.0   \n",
      "27  q3   25465                                   C12-3015     2    1.0   \n",
      "28  q3   69408                                   H89-1020     3    1.0   \n",
      "29  q3   84280         2020.sigirconf_conference-2020.115     4    1.0   \n",
      "30  q3  104775          2014.wwwconf_conference-2014c.318     5    1.0   \n",
      "31  q3  106522            2018.wwwconf_conference-2018.78     6    1.0   \n",
      "32  q3  123780          2019.ipm_journal-ir0volumeA56A3.9     7    1.0   \n",
      "\n",
      "      query  \n",
      "0    tomato  \n",
      "1    tomato  \n",
      "2    tomato  \n",
      "3    tomato  \n",
      "4    tomato  \n",
      "5    tomato  \n",
      "6    tomato  \n",
      "7    tomato  \n",
      "8    tomato  \n",
      "9    tomato  \n",
      "10  angular  \n",
      "11  angular  \n",
      "12  angular  \n",
      "13  angular  \n",
      "14  angular  \n",
      "15  angular  \n",
      "16  angular  \n",
      "17  angular  \n",
      "18  angular  \n",
      "19  angular  \n",
      "20  angular  \n",
      "21  angular  \n",
      "22  angular  \n",
      "23  angular  \n",
      "24  angular  \n",
      "25    sheep  \n",
      "26    sheep  \n",
      "27    sheep  \n",
      "28    sheep  \n",
      "29    sheep  \n",
      "30    sheep  \n",
      "31    sheep  \n",
      "32    sheep  \n"
     ]
    }
   ],
   "source": [
    "# search with dataframe with transform()\n",
    "\n",
    "test_queries = pd.DataFrame([[\"q1\", \"tomato\"], [\"q2\", \"angular\"], [\"q3\", \"sheep\"]],\n",
    "                            columns=[\"qid\", \"query\"])\n",
    "\n",
    "#br.transform(queries)\n",
    "search_result = br(test_queries) # short form of br.transform(queries)\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval - Searching the Index\n",
    "\n",
    "Way to search in PyTerrier: ```BatchRetrieve```\n",
    "-> configured by specifing an _index_ and a _weighting model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighting Models\n",
    "\n",
    "Weighting Models define the ranking function for document retrieval in BatchRetrieve.\n",
    "\n",
    "List of [Supported Weighting Models](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "# configure the BatchRetrieve\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "# Do the retrieval\n",
    "run = bm25(pt_dataset.get_topics(\"text\"))\n",
    "\n",
    "# Print the result\n",
    "run.head(10)\n",
    "\n",
    "# Save the run into runfile for later evaluation\n",
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>15.6818</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0474</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>2</td>\n",
       "      <td>14.1442</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5868</td>\n",
       "      <td>W05-0704</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0257</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>4</td>\n",
       "      <td>13.9480</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>74055</td>\n",
       "      <td>2004.ntcir_workshop-2004.7</td>\n",
       "      <td>995</td>\n",
       "      <td>9.0151</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>86097</td>\n",
       "      <td>2014.clef_conference-2014w.26</td>\n",
       "      <td>996</td>\n",
       "      <td>9.0149</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>81966</td>\n",
       "      <td>2007.sigirconf_conference-2007.76</td>\n",
       "      <td>997</td>\n",
       "      <td>9.0147</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>75540</td>\n",
       "      <td>2009.clef_workshop-2009w.152</td>\n",
       "      <td>998</td>\n",
       "      <td>9.0124</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>94502</td>\n",
       "      <td>2009.cikm_conference-2009.29</td>\n",
       "      <td>999</td>\n",
       "      <td>9.0124</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid   docid                               docno  rank    score  \\\n",
       "0     1   94858        2004.cikm_conference-2004.47     0  15.6818   \n",
       "1     1  125137   1989.ipm_journal-ir0volumeA25A4.2     1  15.0474   \n",
       "2     1  125817  2005.ipm_journal-ir0volumeA41A5.11     2  14.1442   \n",
       "3     1    5868                            W05-0704     3  14.0257   \n",
       "4     1   84876       2016.ntcir_conference-2016.90     4  13.9480   \n",
       "..   ..     ...                                 ...   ...      ...   \n",
       "995   1   74055          2004.ntcir_workshop-2004.7   995   9.0151   \n",
       "996   1   86097       2014.clef_conference-2014w.26   996   9.0149   \n",
       "997   1   81966   2007.sigirconf_conference-2007.76   997   9.0147   \n",
       "998   1   75540        2009.clef_workshop-2009w.152   998   9.0124   \n",
       "999   1   94502        2009.cikm_conference-2009.29   999   9.0124   \n",
       "\n",
       "                                        query  label iteration  \n",
       "0    retrieval system improving effectiveness    1.0         0  \n",
       "1    retrieval system improving effectiveness    1.0         0  \n",
       "2    retrieval system improving effectiveness    1.0         0  \n",
       "3    retrieval system improving effectiveness    0.0         0  \n",
       "4    retrieval system improving effectiveness    1.0         0  \n",
       "..                                        ...    ...       ...  \n",
       "995  retrieval system improving effectiveness    0.0         0  \n",
       "996  retrieval system improving effectiveness    0.0         0  \n",
       "997  retrieval system improving effectiveness    0.0         0  \n",
       "998  retrieval system improving effectiveness    0.0         0  \n",
       "999  retrieval system improving effectiveness    0.0         0  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = pt_dataset.get_qrels()\n",
    "def get_res_with_labels(ranker, df):\n",
    "    # get results for the query (or queries)\n",
    "    results = ranker(df)\n",
    "    with_labels = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n",
    "    return with_labels\n",
    "\n",
    "# bm25 results for the first query (for all queries -> without head(1))\n",
    "get_res_with_labels(bm25, pt_dataset.get_topics(variant=\"title\").head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR(TF_IDF)</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.5541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name     map    ndcg\n",
       "0  BR(TF_IDF)  0.2642  0.5541"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [tfidf],\n",
    "    pt_dataset.get_topics(variant=\"title\"),\n",
    "    pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"ndcg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Pipelines\n",
    "These are not transformer models, in pyterrier transformer means SOMETHING ELSE.\n",
    "And those can be connected to form a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Baselines\n",
    "\n",
    "Baselines executed in TIRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 126kiB [00:00, 1.10MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-benchmarks/ir-acl-anthology-20240504-training/fschlatt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 683kiB [00:00, 4.28MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/workshop-on-open-web-search/ir-acl-anthology-20240504-training/fschlatt\n"
     ]
    }
   ],
   "source": [
    "bm25_baseline = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "sparse_cross_encoder = tira.pt.from_submission('ir-benchmarks/fschlatt/sparse-cross-encoder-4-512', pt_dataset)\n",
    "rank_zephyr = tira.pt.from_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Conducting Experiments with PyTerrier\n",
    "-> run transformer-pipeline* over set of queries and evaluate outcome using standart IR evaluation metrics, based on known relevant documants (qrels)\n",
    "\n",
    "evaluation metrics calculated by pytrec_eval library\n",
    "\n",
    "*(pyterrier transformers not the transformer models)\n",
    "\n",
    "\n",
    "Reference for [PyTerrier Experiments](https://pyterrier.readthedocs.io/en/latest/experiments.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      qid                               docno  rank    score           name\n",
      "0       1        2004.cikm_conference-2004.47     1  15.6818  bm25-baseline\n",
      "1       1   1989.ipm_journal-ir0volumeA25A4.2     2  15.0474  bm25-baseline\n",
      "2       1  2005.ipm_journal-ir0volumeA41A5.11     3  14.1442  bm25-baseline\n",
      "3       1                            W05-0704     4  14.0257  bm25-baseline\n",
      "4       1       2016.ntcir_conference-2016.90     5  13.9480  bm25-baseline\n",
      "...    ..                                 ...   ...      ...            ...\n",
      "66278  68                            W18-6474   996   8.7468  bm25-baseline\n",
      "66279  68        2007.cikm_conference-2007.37   997   8.7466  bm25-baseline\n",
      "66280  68     1998.sigirconf_conference-98.61   998   8.7460  bm25-baseline\n",
      "66281  68   2015.ipm_journal-ir0volumeA51A2.1   999   8.7444  bm25-baseline\n",
      "66282  68       2015.ictir_conference-2015.24  1000   8.7436  bm25-baseline\n",
      "\n",
      "[66283 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# the retireval systems specified in experiment (here [bm25, bm25_baseline, ... ]) can be either\n",
    "# systems themselves or results dataframe (like below when read from file)\n",
    "\n",
    "bm25 = pt.io.read_results('../runs/run.txt') # Read the run written above in Retrival Pipeline section\n",
    "print(bm25)\n",
    "\n",
    "#pt.Experiment(\n",
    "#    [bm25, bm25_baseline, sparse_cross_encoder, rank_zephyr],\n",
    "#    pt_dataset.get_topics(),\n",
    "#    pt_dataset.get_qrels(),\n",
    "#    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\"], # eval metrics\n",
    "#    names=[\"BM25 (Own)\", \"BM 25 (Baseline)\", \"Sparse Cross Encoder\", \"RankZephyr\"]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
