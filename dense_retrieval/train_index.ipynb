{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU-v03JWhJnM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZwyJXlNghJnO"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers faiss-gpu faiss-cpu torch\n",
        "#!pip install tira ir-datasets python-terrier\n",
        "#!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sbvL4RNmhJnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import importlib\n",
        "import random\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pyterrier as pt\n",
        "import faiss\n",
        "\n",
        "# Encoder and Tokenizer models\n",
        "from transformers import AutoTokenizer, AutoModel, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Tira and Pyterrier Imports\n",
        "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
        "from tira.third_party_integrations import ir_datasets\n",
        "from tira.rest_api_client import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKoyAT1zhJnP",
        "outputId": "03ccb14b-147a-478d-9359-6d3cd3b8103f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTerrier 0.10.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
        "ensure_pyterrier_is_loaded()\n",
        "tira = Client()\n",
        "\n",
        "# Print options for pandas\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)\n",
        "\n",
        "\n",
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "# TODO: set seed!!\n",
        "\n",
        "os.makedirs(\"./indexe\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxDuSWEdhJnQ"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr1X-3gghJnQ"
      },
      "source": [
        "### initialize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zuGd7MPhJnQ",
        "outputId": "d2541e2a-ba35-407d-9d0c-3699b45640a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "126958 documents.\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \".\"\n",
        "CORPUS_PATH = os.path.join(DATA_PATH, \"dataset_corpus.json\")\n",
        "\n",
        "# Load the dataset\n",
        "if os.path.exists(CORPUS_PATH):\n",
        "    with open(CORPUS_PATH, \"r\") as f:\n",
        "        corpus = json.load(f)\n",
        "else:\n",
        "    dataset = ir_datasets.load(\"ir-lab-sose-2024/ir-acl-anthology-20240504-training\")\n",
        "    corpus = dataset.docs_store().docs\n",
        "    with open(CORPUS_PATH, \"w\") as f:\n",
        "        json.dump(obj=corpus, fp=f, indent=2, ensure_ascii=False)\n",
        "    del dataset # Free space? or is this unnecessary??\n",
        "\n",
        "print(f\"{len(corpus)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# corpus is originally a dict: {\"docno\": [\"docno\", \"text\"], }\n",
        "#               now like this: {\"docno\": \"text\", ...}  #  easier to handle.\n",
        "\n",
        "dict_corpus = {v[0]: v[1] for v in corpus.values()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### batch the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_corpus(corpus, batch_size):\n",
        "    corpus_keys = list(corpus.keys())\n",
        "    for anker in range(0, len(corpus), batch_size):\n",
        "        batch_keys = corpus_keys[anker:anker+batch_size]\n",
        "        yield {k: corpus[k] for k in batch_keys}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ13VRHlhJnR"
      },
      "source": [
        "### preprocessing\n",
        "TOdo: outlier entfernen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Most common abbrevations in corpus and other small things to substitute\n",
        "abbrevations = {\n",
        "    \"e.g.\": \"for example\",\n",
        "    \"E.g.\": \"for example\",\n",
        "    \"U.S.\": \"united states\",\n",
        "    \"w.r.t.\": \"with respect to\",\n",
        "    \"i.e.\": \"that is\",\n",
        "    \"i.i.d.\": \"independent and identically distributed\",\n",
        "    \"i.i.\": \"independent and identically\",\n",
        "    \"v.s.\": \"versus\", \"vs.\": \"versus\",\n",
        "    \"etc.\": \"and so on\", #TODO: besser et cetera? oder ist das zu exotisch\n",
        "    \"1st\": \"first\", \"2nd\": \"second\", \"3rd\": \"third\", \"4th\": \"fourth\", \"5th\": \"fifth\",\n",
        "    \"e2e\": \"end-to-end\",\n",
        "    \"E2E\": \"end-to-end\",\n",
        "    \"iii)\": \"\", \"ii)\": \"\", \"i)\": \"\", \"iv)\": \"\", \"v)\": \"\",\n",
        "    \"?\": \".\", \"!\": \".\",\n",
        "    \"a)\": \"\", \"b)\": \"\", \"c)\": \"\", \"d)\": \"\", \"e)\": \"\"\n",
        "}\n",
        "\n",
        "# Very common letter-number-combinations that will not be substituted\n",
        "letter_number_exceptions = [\"L2\",\"F1\",\"L1\",\"F2\",\"seq2seq\",\"Seq2Seq\",\"word2vec\",\"Word2Vec\",\"2D\"]\n",
        "\n",
        "def preprocess_text(text, lower=False, years=False, percentages=False, numbers=False, \n",
        "                        letter_numbers=False, abbrev=False, special_characters=False):\n",
        "    # reihenfolge ist wichtig!\n",
        "    if lower:\n",
        "        text = text.lower()\n",
        "    if years:\n",
        "        text = re.sub(r'\\b(19|20)\\d{2}\\b', 'YEAR', text)\n",
        "    if percentages:\n",
        "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%', \"PERCENTAGE\", text)\n",
        "\n",
        "    # all remaining numbers\n",
        "    if numbers:\n",
        "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', 'NUMBER', text)\n",
        "\n",
        "    # Remove words that are combinations of letters and numbers \n",
        "    # (except L2, F1, word2vec, ... common in corpus and probably important for context)\n",
        "    if letter_numbers:\n",
        "        #pattern = r'\\b(?!(L2|F1|L1|F2|seq2seq|word2vec|Seq2Seq|Word2Vec|2D)\\b)\\w*\\d+\\w*\\b'\n",
        "        pattern = rf'\\b(?!({\"|\".join(letter_number_exceptions)})\\b)\\w*\\d+\\w*\\b'\n",
        "        text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Substitute most common abbrevations\n",
        "    if abbrev:\n",
        "        for abbrevation, substitution in abbrevations.items():\n",
        "            text = text.replace(abbrevation, substitution)\n",
        "\n",
        "    # Remove all characters that are not normal text\n",
        "    if special_characters:\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\-\\.\\,]', '', text)\n",
        "\n",
        "    # Punkt hinter Titel des papers setzen, falls bert genutzt wird, [SEP] token hinter titel setzten.?????\n",
        "    #text = re.sub(r'\\n\\n', \". \", text)\n",
        "    if len(text.split(\"\\n\\n\")) < 2:\n",
        "        text += \".\"\n",
        "    else:\n",
        "        text = re.sub(r'\\n\\n', \". \", text)\n",
        "\n",
        "    # Aufeinanderfolgende whitespaces durch einzelnes blank ersetzen.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7E-kNsnhJnS"
      },
      "source": [
        "## Encoding the Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZwy5FiVhJnS"
      },
      "source": [
        "### the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME: das hier nur schnelle lÃ¶sung! umbedingt besser machen!!\n",
        "model_name_to_type_map = { # map for models that do not use AutoModel\n",
        "    \"paraphrase-MiniLM-L6-v2\": [SentenceTransformer, None],\n",
        "}\n",
        "\n",
        "def load_model(name):\n",
        "    if name in model_name_to_type_map.keys():\n",
        "        model_class, tokenizer_class = model_name_to_type_map[name]\n",
        "    else:\n",
        "        model_class, tokenizer_class = AutoModel, AutoTokenizer\n",
        "\n",
        "    model = model_class.from_pretrained(name)\n",
        "    tokenizer = tokenizer_class.from_pretrained(name) if tokenizer_class is not None else None\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model (TinyBERT or another) \n",
        "# TODO: model config (model: \"DeBerta\", and pretrained: \"microsoft/deberta-base\" )\n",
        "\n",
        "#model_name = \"prajjwal1/bert-mini\"\n",
        "#model_name = \"microsoft/deberta-base\" # ACHTUNG FEHLER BEI TOKENIZER! FIXME\n",
        "#model_name = 'intfloat/e5-base-v2' # add \"query: \" before queries and \"passage: \" before passages!\n",
        "#model_name = 'intfloat/e5-small-v2' # add \"query: \" before queries and \"passage: \" before passages!\n",
        "#model_name = \"thenlper/gte-small\"\n",
        "model_name = \"thenlper/gte-base\"\n",
        "#model_name = \"olm/olm-roberta-base-dec-2022\"  ## nicht so gut\n",
        "#model_name = 'allenai/specter' # Mit average=False benutzen! und FlatIP statt FlatL2! # Spezialisiert auf Scientific Papers\n",
        "\n",
        "model, tokenizer = load_model(model_name)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"gte-small-ft-mlm\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvDRxV6phJnS"
      },
      "source": [
        "### the embedding (of document corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ccvQltMohJnS"
      },
      "outputs": [],
      "source": [
        "def average_pool(last_hidden_states, attention_mask):\n",
        "    \"\"\" Calculates average pooling of hidden states (with attention mask) \"\"\"\n",
        "    # mask paddings with 0 -> ignore in average calculation\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0) \n",
        "    #last_hidden = last_hidden_states # without using mask (is this even worth considering?)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "\n",
        "def encode(model, tokenizer, texts, max_length=512, avg_pool=False): # avg. doc length = 144 (after preprocessing only those with abstract.)\n",
        "    \"\"\" Encode texts with model \"\"\"\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    inputs.to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        last_hidden_states = outputs.hidden_states[-1]\n",
        "        if avg_pool: \n",
        "            result = average_pool(last_hidden_states, inputs[\"attention_mask\"])\n",
        "        else: # [CLS] embeddings\n",
        "            result = last_hidden_states[:,0,:]\n",
        "\n",
        "    # GPU RAM leeren (weil es immer voll gelaufen ist)\n",
        "    result.cpu()\n",
        "    del outputs\n",
        "    del last_hidden_states\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4bqUcNCNhJnS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_documents(corpus, model, tokenizer, batch_size=100, avg_pool=False,\n",
        "                     normalize=False, preprocess=False, **preprocess_params):\n",
        "\n",
        "    embeddings = None # will be np.array of shape [num_docs, embedding_size]\n",
        "    docnos = []  # for embedding-vector index to docno translation\n",
        "    for j, batch in enumerate(batch_corpus(corpus, batch_size)):\n",
        "        print(f\"\\rBatch {j+1:3d}/{len(corpus)} \", end=\"\")\n",
        "\n",
        "        docnos += list(batch.keys())\n",
        "        texts = list(batch.values())\n",
        "\n",
        "        if preprocess:\n",
        "            texts = [preprocess_text(t, **preprocess_params) for t in texts]\n",
        "\n",
        "        #if \"e5\" in model_name.lower(): # FIXME\n",
        "        #    texts = [\"passage: \"+t for t in texts]\n",
        "\n",
        "        batch_embeddings = encode(model=model, tokenizer=tokenizer, texts=texts, avg_pool=avg_pool)\n",
        "\n",
        "        if embeddings is None:\n",
        "            embeddings = batch_embeddings\n",
        "        else:\n",
        "            embeddings = torch.concatenate([embeddings, batch_embeddings], dim=0)\n",
        "\n",
        "    if normalize:\n",
        "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "    return docnos, embeddings # TODO: yield docnos, embeddings!? -> speicherschonender? macht generator Ã¼berhaupt sinn???\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KUylKsshJnT",
        "outputId": "0cf043fe-5f5e-48df-f203-716aa0d7e95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1270/1270 embeddings shape: torch.Size([126958, 768])\n"
          ]
        }
      ],
      "source": [
        "# Encode the document corpus\n",
        "batch_size = 500\n",
        "avg_pool   = True\n",
        "normalize  = False\n",
        "preprocess = False\n",
        "preprocess_params = {\n",
        "    \"lower\": True,\n",
        "    \"numbers\": True,\n",
        "    \"letter_numbers\": True,\n",
        "    \"abbrev\": True,\n",
        "    \"special_characters\": True,\n",
        "}\n",
        "\n",
        "docnos, embeddings = encode_documents(corpus, model, tokenizer, batch_size, normalize=normalize,\n",
        "                                      avg_pool=avg_pool, preprocess=preprocess, **preprocess_params)\n",
        "print(\"embeddings shape:\", embeddings.shape)\n",
        "\n",
        "embedding_size = embeddings.shape[1]\n",
        "if np.isnan(embeddings).any():\n",
        "    print(\"WARNUNG: NaN-Werte in den Embeddings gefunden!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tk1IR9FJkxz_"
      },
      "outputs": [],
      "source": [
        "# SAVE\n",
        "with open(\"gte_base-preprocessed_avgpool-embeddings.npy\", \"wb\") as f:\n",
        "    np.save(f, embeddings)\n",
        "\n",
        "with open(\"gte_base-preprocessed_avgpool-docnos.txt\", \"w\") as f:\n",
        "    json.dump(docnos, f)\n",
        "    #f.write(\"\\n\".join(docnos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agGNjOdm4DFs"
      },
      "outputs": [],
      "source": [
        "# LOAD\n",
        "with open(\"gte_base-preprocessed_avgpool-docnos.txt\", \"r\") as f:\n",
        "    docnos = json.load(f)\n",
        "\n",
        "with open(\"gte_base-preprocessed_avgpool-embeddings.npy\", \"rb\") as f:\n",
        "    embeddings = np.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJluk29hJnT"
      },
      "source": [
        "## the Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfb8dvyShJnT"
      },
      "source": [
        "### configuration & initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_faiss_index(embedding_size, index_type, **index_params):\n",
        "    \"\"\" Create a FAISS index using the index factory. \n",
        "    index_type (str): The type of index to create (e.g., \"Flat\", \"HNSW\", \"IVF\", \"PQ\", etc)\n",
        "    **index_params: Additional parameters for the index\n",
        "    Returns the created FAISS index with embeddings added\n",
        "    \"\"\"\n",
        "    \n",
        "    metric = index_params.get(\"metric\", \"IP\")\n",
        "    \n",
        "    # Construct the index string\n",
        "    if index_type == \"Flat\":\n",
        "        index_string = f\"Flat{metric}\"\n",
        "    elif index_type == \"HNSW\":\n",
        "        M = index_params.get('M', 16)\n",
        "        index_string = f\"HNSW{M},{metric}\"\n",
        "    elif index_type == \"IVF\":\n",
        "        nlist = index_params.get('nlist', 100)\n",
        "        index_string = f\"IVF{nlist},Flat{metric}\"\n",
        "    elif index_type == \"PQ\":\n",
        "        m = index_params.get('m', 8)\n",
        "        bits = index_params.get('bits', 8)\n",
        "        index_string = f\"PQ{m}x{bits},{metric}\"\n",
        "    elif index_type == \"IVFPQ\":\n",
        "        nlist = index_params.get('nlist', 100)\n",
        "        m = index_params.get('m', 8)\n",
        "        bits = index_params.get('bits', 8)\n",
        "        index_string = f\"IVF{nlist},PQ{m}x{bits},{metric}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported index type: {index_type}\")\n",
        "    \n",
        "    # Create the index\n",
        "    index = faiss.index_factory(embedding_size, index_string)\n",
        "    \n",
        "    # Set additional parameters\n",
        "    if index_type == \"HNSW\":\n",
        "        if hasattr(index, \"hnsw\"):\n",
        "            index.hnsw.efConstruction = index_params.get(\"efConstruction\", 150)\n",
        "            index.hnsw.efSearch = index_params.get(\"efSearch\", 50)\n",
        "    elif index_type in [\"IVF\", \"IVFPQ\"]:\n",
        "        index.nprobe = index_params.get(\"nprobe\", 10)\n",
        "\n",
        "\n",
        "def add_embeddings(index, embeddings, metric, train):\n",
        "    \"\"\" \n",
        "    metric: \"IP\" or \"L2\"\n",
        "    train: bool\n",
        "    \"\"\"\n",
        "    # Normalize for IP similarity\n",
        "    if metric == \"IP\":\n",
        "        faiss.normalize_L2(embeddings)\n",
        "    \n",
        "    if train:\n",
        "        index.train(embeddings)\n",
        "    \n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a FAISS index with embeddings (and docnos for indices)\n",
        "embedding_size = embeddings.shape[1]\n",
        "\n",
        "#index_type = \"Flat\"\n",
        "#index_type = \"LSH\"\n",
        "index_type = \"IVF\"\n",
        "#index_type = \"IVFPQ\"\n",
        "#index_type = \"HNSW\"\n",
        "\n",
        "params = {\n",
        "    \"metric\": \"IP\",\n",
        "    \"nlist\" : 5000,  # Cluster in IVFFlat\n",
        "    #\"nbits\" : 128, # LSH\n",
        "    #\"M\": 32,\n",
        "    #\"efContruction\": 1000,\n",
        "    #\"efSearch\": 500\n",
        "}\n",
        "\n",
        "#params = { # IVFPQ\n",
        "#    \"nlist\": 1000,\n",
        "#    \"m\": 8,\n",
        "#    \"bits\": 8,\n",
        "#    \"nprobe\": 10\n",
        "#}\n",
        "\n",
        "index = create_faiss_index(embedding_size, index_type, **params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6HAUhgkhJnT"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sLr6IAQmhJnT"
      },
      "outputs": [],
      "source": [
        "index = add_embeddings(index, embeddings, metric=params[\"metric\"], train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1EMt4cDrhJnT"
      },
      "outputs": [],
      "source": [
        "index_dir = \"./indexe\"\n",
        "os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "index_name = f\"ivf_{nlist}_IP-gte_base-prepr_avgpool.index\"\n",
        "faiss.write_index(index, os.path.join(index_dir, index_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3wDDBhT_3jh",
        "outputId": "9a20160a-350f-426b-941b-d28c543682c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xk0lpX5tAQRp",
        "outputId": "87519ae6-774e-47b3-bc94-ad0b53c582f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/gte_base-preprocessed_avgpool-docnos.txt'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "quelle = f\"/content/indexe/ivf_{nlist}_IP-gte_small_ft-mlm.index\"\n",
        "#quelle = \"/content/gte_small_ft-mlm-embeddings.npy\"\n",
        "#quelle = \"/content/gte_small_ft-mlm-docnos.txt\"\n",
        "ziel = \"/content/drive/My Drive/\" + quelle.split(\"/\")[-1]\n",
        "\n",
        "copyfile(quelle, ziel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1QPK9b57XX-"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nRtVezRLhJnR"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
