{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground with basic experimental setup (not using modules)\n",
    "\n",
    "In this Notebook, I took the workflow of `basic_setup.ipynb` \n",
    "and created a little mock-corpus to try and figure out why the performance \n",
    "of the \"plain vanilla\" method is so (incredibly!) bad.\n",
    "\n",
    "Thought is: with a little corpus that has documents about very different things, and having a query about some topic in the corpus, then the search should clearly score the relevant document highest. (Which is not the case!)\n",
    "\n",
    "Things I tried:\n",
    "- Using different Models (Bert-like models: mini-bert, bert-base, roberta, deberta, distilbert; e5-small and -base; gte-small and base; and some more)\n",
    "- Using different Indices (Flat, HSNW, LSH, IVP, ...)\n",
    "- Encoding: embedding vector either [CLS]-token or average-pool of last hidden layer\n",
    "- Text Preprocessing: Removing / Substituting some things like numbers/abbreviations ...\n",
    "- Text-splitting: documents splitted into overlapping \"sentences\" and those embedded individually\n",
    "\n",
    "\n",
    "I tried these things on the mini-test-corpus, and on the subset of the tira dataset that contains only relevant documents.\n",
    "(Experiments on the full corpus were only conducted now and then because embedding the full corpus takes a while)\n",
    "\n",
    "\n",
    "What i noticed:\n",
    "- The output score of the search is not well distributed, but i guess with similarity search the distribution of scores dont matter as much as the rank-order.\n",
    "- The rank order is also not consitently good, some documents seem to be prefered -> embeddings closer to center??\n",
    "\n",
    "\n",
    "\n",
    "Still, the performance is very bad and stays bad, further experiments will include finetuning the model and an additional reranker.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Dokumente: Von den 2300 die in den Qrels vorkommen, sind ca 350 ohne abstract.\n",
    "\n",
    "ca 1700 dokumente sind im bereich zwischen 50 und 350 tokens (nach vollem preprocessing)\n",
    "\n",
    "Minimum sind 5 Tokens und Maximum sind über 5000 Tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- install libraries (if necessary)\n",
    "- all imports here\n",
    "- connecting to tira & printoptions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers faiss-gpu faiss-cpu torch\n",
    "#!pip install tira ir-datasets python-terrier\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import importlib\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyterrier as pt\n",
    "import faiss\n",
    "\n",
    "# Encoder and Tokenizer models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tira and Pyterrier Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.third_party_integrations import ir_datasets\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "# Print options for pandas\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# TODO: set seed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instanciate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126958 documents.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \".\"\n",
    "CORPUS_PATH = os.path.join(DATA_PATH, \"dataset_corpus.json\")\n",
    "\n",
    "# Load the dataset\n",
    "if os.path.exists(CORPUS_PATH):\n",
    "    with open(CORPUS_PATH, \"r\") as f:\n",
    "        corpus = json.load(f)\n",
    "else:\n",
    "    dataset = ir_datasets.load(\"ir-lab-sose-2024/ir-acl-anthology-20240504-training\")\n",
    "    corpus = dataset.docs_store().docs\n",
    "    with open(CORPUS_PATH, \"w\") as f:\n",
    "        json.dump(obj=corpus, fp=f, indent=2, ensure_ascii=False)\n",
    "    del dataset # Free space? or is this unnecessary??\n",
    "\n",
    "print(f\"{len(corpus)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is originally a dict: {\"docno\": [\"docno\", \"text\"], }\n",
    "#               now like this: {\"docno\": \"text\", ...}  #  easier to handle.\n",
    "\n",
    "dict_corpus = {v[0]: v[1] for v in corpus.values()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create subset of corpus - only documents that appear in qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test corpus of only relevant document (+ a few nonrelevant)\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "relevant_docnos = dataset.get_qrels()[\"docno\"].unique()\n",
    "\n",
    "# some random choice of non-relevant docs added to corpus subset\n",
    "nonrelevant_docnos = list(corpus.keys() - set(relevant_docnos))\n",
    "\n",
    "nonrelevant_docnos = np.random.choice(nonrelevant_docnos, size=0)\n",
    "relevant_docnos = list(relevant_docnos) + list(nonrelevant_docnos)\n",
    "\n",
    "corpus = {k: corpus[k] for k in relevant_docnos}\n",
    "print(f\"{len(corpus)} relevant documents. (relevant to dev-set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_corpus(corpus, batch_size):\n",
    "    corpus_keys = list(corpus.keys())\n",
    "    for anker in range(0, len(corpus), batch_size):\n",
    "        batch_keys = corpus_keys[anker:anker+batch_size]\n",
    "        yield {k: corpus[k] for k in batch_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common abbrevations in corpus and other small things to substitute\n",
    "abbrevations = {\n",
    "    \"e.g.\": \"for example\",\n",
    "    \"E.g.\": \"for example\",\n",
    "    \"U.S.\": \"united states\",\n",
    "    \"w.r.t.\": \"with respect to\",\n",
    "    \"i.e.\": \"that is\",\n",
    "    \"i.i.d.\": \"independent and identically distributed\",\n",
    "    \"i.i.\": \"independent and identically\",\n",
    "    \"v.s.\": \"versus\", \"vs.\": \"versus\",\n",
    "    \"etc.\": \"and so on\", #TODO: besser et cetera? oder ist das zu exotisch\n",
    "    \"1st\": \"first\", \"2nd\": \"second\", \"3rd\": \"third\", \"4th\": \"fourth\", \"5th\": \"fifth\",\n",
    "    \"e2e\": \"end-to-end\",\n",
    "    \"E2E\": \"end-to-end\",\n",
    "    \"iii)\": \"\", \"ii)\": \"\", \"i)\": \"\", \"iv)\": \"\", \"v)\": \"\",\n",
    "    \"?\": \".\", \"!\": \".\",\n",
    "    \"a)\": \"\", \"b)\": \"\", \"c)\": \"\", \"d)\": \"\", \"e)\": \"\"\n",
    "}\n",
    "\n",
    "# Very common letter-number-combinations that will not be substituted\n",
    "letter_number_exceptions = [\"L2\",\"F1\",\"L1\",\"F2\",\"seq2seq\",\"Seq2Seq\",\"word2vec\",\"Word2Vec\",\"2D\"]\n",
    "\n",
    "def preprocess_text(text, lower=False, years=False, percentages=False, numbers=False, \n",
    "                        letter_numbers=False, abbrev=False, special_characters=False):\n",
    "    # reihenfolge ist wichtig!\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    if years:\n",
    "        text = re.sub(r'\\b(19|20)\\d{2}\\b', 'YEAR', text)\n",
    "    if percentages:\n",
    "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%', \"PERCENTAGE\", text)\n",
    "\n",
    "    # all remaining numbers\n",
    "    if numbers:\n",
    "        text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', 'NUMBER', text)\n",
    "\n",
    "    # Remove words that are combinations of letters and numbers \n",
    "    # (except L2, F1, word2vec, ... common in corpus and probably important for context)\n",
    "    if letter_numbers:\n",
    "        #pattern = r'\\b(?!(L2|F1|L1|F2|seq2seq|word2vec|Seq2Seq|Word2Vec|2D)\\b)\\w*\\d+\\w*\\b'\n",
    "        pattern = rf'\\b(?!({\"|\".join(letter_number_exceptions)})\\b)\\w*\\d+\\w*\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Substitute most common abbrevations\n",
    "    if abbrev:\n",
    "        for abbrevation, substitution in abbrevations.items():\n",
    "            text = text.replace(abbrevation, substitution)\n",
    "\n",
    "    # Remove all characters that are not normal text\n",
    "    if special_characters:\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\-\\.\\,]', '', text)\n",
    "\n",
    "    # Punkt hinter Titel des papers setzen, falls bert genutzt wird, [SEP] token hinter titel setzten.?????\n",
    "    #text = re.sub(r'\\n\\n', \". \", text)\n",
    "    if len(text.split(\"\\n\\n\")) < 2:\n",
    "        text += \".\"\n",
    "    else:\n",
    "        text = re.sub(r'\\n\\n', \". \", text)\n",
    "\n",
    "    # Aufeinanderfolgende whitespaces durch einzelnes blank ersetzen.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model / The Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_type_map = { # map for models that do not use AutoModel\n",
    "    \"paraphrase-MiniLM-L6-v2\": [SentenceTransformer, None],\n",
    "}\n",
    "\n",
    "def load_model(name, tokenizer_name=\"\"):\n",
    "    if name in model_name_to_type_map.keys():\n",
    "        model_class, tokenizer_class = model_name_to_type_map[name]\n",
    "    else:\n",
    "        model_class, tokenizer_class = AutoModel, AutoTokenizer\n",
    "\n",
    "    model = model_class.from_pretrained(name)\n",
    "    if tokenizer_class is None:\n",
    "        tokenizer = None\n",
    "    elif len(tokenizer_name) > 0:\n",
    "        tokenizer = tokenizer_class.from_pretrained(tokenizer_name)\n",
    "    else:\n",
    "        tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (TinyBERT or another) \n",
    "\n",
    "#model_name = \"models/bert-tiny-pt-mlm\"\n",
    "model_name = \"prajjwal1/bert-mini\"\n",
    "#model_name = \"microsoft/deberta-base\" # ACHTUNG FEHLER BEI TOKENIZER! FIXME\n",
    "#model_name = 'intfloat/e5-base-v2' # add \"query: \" before queries and \"passage: \" before passages!\n",
    "#model_name = \"thenlper/gte-small\"\n",
    "#model_name = \"thenlper/gte-base\"  # beste\n",
    "#model_name = \"olm/olm-roberta-base-dec-2022\"  ## nicht so gut\n",
    "#model_name = 'allenai/specter' # Mit average=False benutzen! und FlatIP statt FlatL2! # Spezialisiert auf Scientific Papers\n",
    "#model_name = \"sap-ai-research/BERT-Large-Contrastive-Self-Supervised-ACL2020\"\n",
    "\n",
    "model, tokenizer = load_model(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the embedding (of document corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    \"\"\" Calculates average pooling of hidden states (with attention mask) \"\"\"\n",
    "    # mask paddings with 0 -> ignore in average calculation\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0) \n",
    "    #last_hidden = last_hidden_states # without using mask (is this even worth considering?)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def encode(model, tokenizer, texts, max_length=512, avg_pool=False): # avg. doc length = 144 (after preprocessing only those with abstract.)\n",
    "    \"\"\" Encode texts with model \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        if avg_pool: \n",
    "            return average_pool(last_hidden_states, inputs[\"attention_mask\"])\n",
    "        else: # [CLS] embeddings\n",
    "            return last_hidden_states[:,0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(corpus, model, tokenizer, batch_size, avg_pool=False, \n",
    "                     normalize=False, preprocess=False, **preprocess_params):\n",
    "\n",
    "    embeddings = None  # will be np.array of shape [num_docs, embedding_size]\n",
    "    docnos = []        # for embedding-vector index to docno translation\n",
    "\n",
    "    for j, batch in enumerate(batch_corpus(corpus, batch_size=batch_size)):\n",
    "        print(f\"\\rBatch {j+1:3d}/{len(corpus)} \", end=\"\")\n",
    "\n",
    "        # corpus muss dict_corpus sein! {\"docno1\": \"text1\", ...}\n",
    "        docnos += list(batch.keys())\n",
    "        texts = list(batch.values())\n",
    "\n",
    "        if preprocess:\n",
    "            texts = [preprocess_text(t, **preprocess_params) for t in texts]\n",
    "\n",
    "        #if \"e5\" in model_name.lower(): # in preprocess params?\n",
    "        #    texts = [\"passage: \"+t for t in texts]\n",
    "        \n",
    "        batch_embeddings = encode(model=model, tokenizer=tokenizer, texts=texts, avg_pool=avg_pool)\n",
    "\n",
    "        if embeddings is None:\n",
    "            embeddings = batch_embeddings\n",
    "        else:\n",
    "            embeddings = torch.concatenate([embeddings, batch_embeddings], dim=0)\n",
    "\n",
    "    if normalize:\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return docnos, embeddings # TODO: yield docnos, embeddings!? -> speicherschonender?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch   1/254 "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# Encode the document corpus\n",
    "batch_size = 500\n",
    "avg_pool   = False\n",
    "normalize  = False\n",
    "preprocess = False\n",
    "preprocess_params = {\n",
    "    \"lower\": True,\n",
    "    \"numbers\": True,\n",
    "    \"letter_numbers\": True,\n",
    "    \"abbrev\": True,\n",
    "    \"special_characters\": True,\n",
    "}\n",
    "\n",
    "docnos, embeddings = encode_documents(corpus, model, tokenizer, batch_size, normalize=normalize,\n",
    "                                      avg_pool=avg_pool, preprocess=preprocess, **preprocess_params)\n",
    "print(\"embeddings shape:\", embeddings.shape)\n",
    "\n",
    "embedding_size = embeddings.shape[1]\n",
    "if np.isnan(embeddings).any():\n",
    "    print(\"WARNUNG: NaN-Werte in den Embeddings gefunden!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_corpus/bert-tiny-ft-mlm-ep3-docnos.txt\", \"w\") as f:\n",
    "    json.dump(docnos, f, ensure_ascii=False)\n",
    "with open(\"encoded_corpus/bert-tiny-ft-mlm-ep3-embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_corpus/bert-tiny-ft-mlm-ep3-docnos.txt\", \"r\") as f:\n",
    "    docnos = json.load(f)\n",
    "with open(\"encoded_corpus/bert-tiny-ft-mlm-ep3-embeddings.npy\", \"rb\") as f:\n",
    "    embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the index (faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index\n",
    "embedding_size = embeddings.shape[1]\n",
    "\n",
    "index_name =  \"IVF\"\n",
    "metric = \"IP\"\n",
    "index_params = {\n",
    "    \"nlist\": 500, # ivf n cluster\n",
    "    #\"n_bits\": 2*embedding_size, # hsnw\n",
    "}\n",
    "\n",
    "# index factory string\n",
    "index_string = f\"{index_name}\"\n",
    "if \"M\" in index_params:\n",
    "    index_string += f\"M{index_params.get('M', 16)}\"\n",
    "if index_name == \"IFV\":\n",
    "    index_string += f\"IVF{index_params.get('nlist', 100)}\"\n",
    "# Append the base index type\n",
    "if index_name != \"Flat\":\n",
    "    index_string += f\",Flat\"\n",
    "\n",
    "index = faiss.index_factory(embedding_size, index_string, faiss.METRIC_INNER_PRODUCT if metric == \"IP\" else faiss.METRIC_L2)\n",
    "\n",
    "# additional parameters\n",
    "if \"efConstruction\" in index_params and hasattr(index, 'hnsw'):\n",
    "    index.hnsw.efConstruction = index_params['efConstruction']\n",
    "if \"efSearch\" in index_params and hasattr(index, 'hnsw'):\n",
    "    index.hnsw.efSearch = index_params['efSearch']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Embeddings to the index\n",
    "\n",
    "# Normalize embeddings if using inner product similarity\n",
    "if metric == \"IP\":\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Train the index if necessary\n",
    "#index.train(embeddings)\n",
    "\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DIR = \"./indexe\"\n",
    "index_file = \"ivf_10000_IP-bert-tiny-ft-mlm-ep3.index\"\n",
    "faiss.write_index(index, os.path.join(INDEX_DIR, index_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DIR = \"./indexe\"\n",
    "index_file = \"ivf_10000_IP-bert-tiny-ft-mlm-ep3.index\"\n",
    "index = faiss.read_index(os.path.join(INDEX_DIR, index_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the dataset queries\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through queries and search the relevant documents\n",
    "\n",
    "#name = \"vanilla_mini-bert\"  # name of this run\n",
    "#name = \"mini-bert_with-tokens\"\n",
    "run_name = \"tinybert_finetuned-ivf10000_flatip\"\n",
    "\n",
    "run = []\n",
    "for i, row in enumerate(dataset.get_topics(variant=\"description\").to_dict(orient=\"records\")):\n",
    "    query = row[\"query\"]\n",
    "    print(query)\n",
    "\n",
    "    if preprocess:\n",
    "        query = preprocess_text(query, **preprocess_params)\n",
    "\n",
    "    # Encode the query\n",
    "    query_embedding = encode(model, tokenizer, [query]).cpu().numpy() # TODO: gpu variant\n",
    "    query_embedding = query_embedding.astype(np.float32)  # brauch ich das wirklich für faiss???\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    # Search in the Index\n",
    "    scores, candidates = index.search(query_embedding, k=10)\n",
    "\n",
    "    if metric == \"L2\":\n",
    "        scores = distance2score(scores)\n",
    "\n",
    "    # Ergebnisse sollten bereits sortiert sein, nur nochmal zur Sicherheit:\n",
    "    results = sorted(list(zip(scores[0], candidates[0])), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    for j, (score, candidate) in enumerate(results):\n",
    "        run.append({ \"qid\": row[\"qid\"], \"docno\": docnos[candidate],\n",
    "                     \"rank\": j+1, \"score\": score, \"name\": run_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = \"./runs\"\n",
    "runfile = os.path.join(RUN_DIR, run_name+\"_run.txt\")\n",
    "\n",
    "with open(runfile, \"w\") as f:\n",
    "    for item in run:\n",
    "        # schreibt die selben sachen, die persist_and_normalize_run() schreibt\n",
    "        f.write(f\"{item['qid']} 0 {item['docno']} {item['rank']} {item['score']} {run_name}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some baselines that were executed in TIRA\n",
    "bm25_baseline = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 (tira-ir-starter-pyterrier)', dataset)\n",
    "sparse_cross_encoder = tira.pt.from_submission('ir-benchmarks/fschlatt/sparse-cross-encoder-4-512', dataset)\n",
    "rank_zephyr = tira.pt.from_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>recall_100</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM 25 (Baseline)</td>\n",
       "      <td>0.37404</td>\n",
       "      <td>0.57988</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.26231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparse Cross Encoder</td>\n",
       "      <td>0.36646</td>\n",
       "      <td>0.61298</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.24126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RankZephyr</td>\n",
       "      <td>0.34707</td>\n",
       "      <td>0.56841</td>\n",
       "      <td>0.60133</td>\n",
       "      <td>0.26749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gte_base-prepr_avgpool-ivf_5000_IP_run</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>0.15825</td>\n",
       "      <td>0.04237</td>\n",
       "      <td>0.02603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gte_base-prepr_avgpool-ivf_500_IP_run</td>\n",
       "      <td>0.07396</td>\n",
       "      <td>0.15058</td>\n",
       "      <td>0.04583</td>\n",
       "      <td>0.02600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gte_base-vanilla-ivf_10000_IP_run</td>\n",
       "      <td>0.10077</td>\n",
       "      <td>0.20662</td>\n",
       "      <td>0.06170</td>\n",
       "      <td>0.04235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mini-bert_with-tokens_run</td>\n",
       "      <td>0.06224</td>\n",
       "      <td>0.14412</td>\n",
       "      <td>0.02821</td>\n",
       "      <td>0.01344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mini_bert-full_preprocessing_run</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>0.15809</td>\n",
       "      <td>0.02533</td>\n",
       "      <td>0.01325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tinybert_finetuned-FlatIP_run</td>\n",
       "      <td>0.01203</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tinybert_finetuned-ivf10000_flatip_run</td>\n",
       "      <td>0.01203</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tinybert_finetuned-ivf1000_flatip_run</td>\n",
       "      <td>0.01203</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vanilla_mini-bert_run</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0.19789</td>\n",
       "      <td>0.02745</td>\n",
       "      <td>0.01708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name  ndcg_cut.10  recip_rank  \\\n",
       "0                         BM 25 (Baseline)      0.37404     0.57988   \n",
       "1                     Sparse Cross Encoder      0.36646     0.61298   \n",
       "2                               RankZephyr      0.34707     0.56841   \n",
       "3   gte_base-prepr_avgpool-ivf_5000_IP_run      0.07413     0.15825   \n",
       "4    gte_base-prepr_avgpool-ivf_500_IP_run      0.07396     0.15058   \n",
       "5        gte_base-vanilla-ivf_10000_IP_run      0.10077     0.20662   \n",
       "6                mini-bert_with-tokens_run      0.06224     0.14412   \n",
       "7         mini_bert-full_preprocessing_run      0.05907     0.15809   \n",
       "8            tinybert_finetuned-FlatIP_run      0.01203     0.04216   \n",
       "9   tinybert_finetuned-ivf10000_flatip_run      0.01203     0.04216   \n",
       "10   tinybert_finetuned-ivf1000_flatip_run      0.01203     0.04216   \n",
       "11                   vanilla_mini-bert_run      0.07913     0.19789   \n",
       "\n",
       "    recall_100     map  \n",
       "0      0.60133 0.26231  \n",
       "1      0.60133 0.24126  \n",
       "2      0.60133 0.26749  \n",
       "3      0.04237 0.02603  \n",
       "4      0.04583 0.02600  \n",
       "5      0.06170 0.04235  \n",
       "6      0.02821 0.01344  \n",
       "7      0.02533 0.01325  \n",
       "8      0.00608 0.00365  \n",
       "9      0.00608 0.00365  \n",
       "10     0.00608 0.00365  \n",
       "11     0.02745 0.01708  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_files = sorted(list(glob.glob(os.path.join(RUN_DIR, \"*.txt\"))))\n",
    "methods = [pt.io.read_results(run_file_path) for run_file_path in run_files]\n",
    "run_names = [name.split(\"/\")[-1].split(\".\")[0] for name in run_files]\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25_baseline, sparse_cross_encoder, rank_zephyr] + methods,\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\", \"map\"],\n",
    "    names=[\"BM 25 (Baseline)\", \"Sparse Cross Encoder\", \"RankZephyr\"] + run_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
